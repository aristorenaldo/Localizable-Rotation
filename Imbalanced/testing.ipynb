{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar_train_lorot-E.py:175: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.\n",
      "  warnings.warn(\n",
      "Use GPU: 0 for training\n",
      "=> creating model 'resnet32'\n",
      "num_trans : 16\n",
      "num_flipped : 4\n",
      "num shuffled channel : 24\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[5000, 2997, 1796, 1077, 645, 387, 232, 139, 83, 50]\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "/media/aristo/Data A/documents/kuliah/Project/Localizable-Rotation/Imbalanced/losses.py:49: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/TensorCompare.cpp:402.)\n",
      "  output = torch.where(index, x_m, x)\n",
      "Epoch: [0][0/97], lr: 0.00200\tTime 2.339 (2.339)\tData 0.203 (0.203)\tLoss 14.3823 (14.3823)\tPrec@1 1.562 (1.562)\tPrec@5 59.375 (59.375)\n",
      "Epoch: [0][10/97], lr: 0.00200\tTime 0.354 (0.499)\tData 0.000 (0.019)\tLoss 8.4378 (9.1508)\tPrec@1 43.750 (36.861)\tPrec@5 78.125 (84.375)\n",
      "Epoch: [0][20/97], lr: 0.00200\tTime 0.336 (0.429)\tData 0.000 (0.018)\tLoss 7.7572 (8.4927)\tPrec@1 46.875 (44.010)\tPrec@5 88.281 (86.644)\n",
      "Epoch: [0][30/97], lr: 0.00200\tTime 0.342 (0.402)\tData 0.000 (0.017)\tLoss 8.4948 (7.9409)\tPrec@1 48.438 (46.799)\tPrec@5 85.156 (88.281)\n",
      "Epoch: [0][40/97], lr: 0.00200\tTime 0.348 (0.387)\tData 0.000 (0.017)\tLoss 6.8988 (7.7292)\tPrec@1 54.688 (48.209)\tPrec@5 91.406 (88.815)\n",
      "Epoch: [0][50/97], lr: 0.00200\tTime 0.339 (0.382)\tData 0.000 (0.017)\tLoss 5.7183 (7.5047)\tPrec@1 59.375 (49.816)\tPrec@5 91.406 (89.170)\n",
      "Epoch: [0][60/97], lr: 0.00200\tTime 0.342 (0.376)\tData 0.000 (0.017)\tLoss 6.7734 (7.2748)\tPrec@1 53.906 (50.884)\tPrec@5 95.312 (89.652)\n",
      "Epoch: [0][70/97], lr: 0.00200\tTime 0.332 (0.370)\tData 0.000 (0.017)\tLoss 5.5778 (7.0900)\tPrec@1 57.031 (51.585)\tPrec@5 92.969 (89.844)\n",
      "Epoch: [0][80/97], lr: 0.00200\tTime 0.334 (0.367)\tData 0.000 (0.017)\tLoss 4.7031 (6.8888)\tPrec@1 64.844 (52.595)\tPrec@5 92.969 (90.075)\n",
      "Epoch: [0][90/97], lr: 0.00200\tTime 0.337 (0.365)\tData 0.000 (0.017)\tLoss 5.2813 (6.7199)\tPrec@1 61.719 (53.348)\tPrec@5 93.750 (90.342)\n",
      "Epoch: [0][96/97], lr: 0.00200\tTime 0.931 (0.370)\tData 0.000 (0.018)\tLoss 4.6374 (6.6383)\tPrec@1 63.559 (53.627)\tPrec@5 95.763 (90.432)\n",
      "Test: [0/100]\tTime 0.340 (0.340)\tLoss 16.1247 (16.1247)\tPrec@1 14.000 (14.000)\tPrec@5 67.000 (67.000)\n",
      "Test: [10/100]\tTime 0.073 (0.097)\tLoss 14.0133 (15.2466)\tPrec@1 19.000 (16.545)\tPrec@5 71.000 (66.545)\n",
      "Test: [20/100]\tTime 0.073 (0.086)\tLoss 14.5764 (15.3537)\tPrec@1 18.000 (15.857)\tPrec@5 73.000 (66.333)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 14.4998 (15.2012)\tPrec@1 15.000 (16.903)\tPrec@5 67.000 (66.419)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 15.9572 (15.2760)\tPrec@1 14.000 (16.805)\tPrec@5 63.000 (65.512)\n",
      "Test: [50/100]\tTime 0.079 (0.078)\tLoss 15.8333 (15.2311)\tPrec@1 15.000 (16.725)\tPrec@5 61.000 (65.627)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 12.6411 (15.1867)\tPrec@1 23.000 (16.525)\tPrec@5 70.000 (65.508)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 14.9535 (15.1540)\tPrec@1 17.000 (16.845)\tPrec@5 68.000 (65.634)\n",
      "Test: [80/100]\tTime 0.073 (0.077)\tLoss 14.6873 (15.1289)\tPrec@1 18.000 (17.037)\tPrec@5 67.000 (65.605)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 14.2127 (15.1635)\tPrec@1 28.000 (16.835)\tPrec@5 64.000 (65.593)\n",
      "val Results: Prec@1 16.880 Prec@5 65.240 Loss 15.19055\n",
      "val Class Accuracy: [0.898,0.077,0.709,0.003,0.000,0.001,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 16.880\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [1][0/97], lr: 0.00400\tTime 1.313 (1.313)\tData 0.706 (0.706)\tLoss 5.6969 (5.6969)\tPrec@1 57.031 (57.031)\tPrec@5 93.750 (93.750)\n",
      "Epoch: [1][10/97], lr: 0.00400\tTime 0.620 (0.672)\tData 0.001 (0.073)\tLoss 5.9430 (5.5996)\tPrec@1 53.125 (57.955)\tPrec@5 87.500 (92.330)\n",
      "Epoch: [1][20/97], lr: 0.00400\tTime 0.503 (0.616)\tData 0.001 (0.046)\tLoss 4.9012 (5.4848)\tPrec@1 65.625 (58.891)\tPrec@5 92.188 (92.448)\n",
      "Epoch: [1][30/97], lr: 0.00400\tTime 0.550 (0.598)\tData 0.001 (0.035)\tLoss 5.5605 (5.4949)\tPrec@1 57.812 (58.947)\tPrec@5 90.625 (92.566)\n",
      "Epoch: [1][40/97], lr: 0.00400\tTime 0.521 (0.582)\tData 0.000 (0.030)\tLoss 5.6154 (5.4341)\tPrec@1 58.594 (59.680)\tPrec@5 92.188 (92.873)\n",
      "Epoch: [1][50/97], lr: 0.00400\tTime 0.450 (0.556)\tData 0.000 (0.026)\tLoss 5.0082 (5.4183)\tPrec@1 60.938 (59.651)\tPrec@5 96.094 (93.183)\n",
      "Epoch: [1][60/97], lr: 0.00400\tTime 0.337 (0.528)\tData 0.000 (0.024)\tLoss 5.4351 (5.4393)\tPrec@1 58.594 (59.759)\tPrec@5 89.844 (93.033)\n",
      "Epoch: [1][70/97], lr: 0.00400\tTime 0.398 (0.507)\tData 0.000 (0.023)\tLoss 5.7841 (5.3926)\tPrec@1 57.812 (60.123)\tPrec@5 96.094 (93.167)\n",
      "Epoch: [1][80/97], lr: 0.00400\tTime 0.370 (0.494)\tData 0.000 (0.022)\tLoss 5.6219 (5.3852)\tPrec@1 61.719 (60.301)\tPrec@5 89.844 (93.152)\n",
      "Epoch: [1][90/97], lr: 0.00400\tTime 0.366 (0.481)\tData 0.000 (0.022)\tLoss 5.2567 (5.3691)\tPrec@1 64.062 (60.474)\tPrec@5 96.094 (93.149)\n",
      "Epoch: [1][96/97], lr: 0.00400\tTime 0.379 (0.475)\tData 0.000 (0.022)\tLoss 5.6092 (5.3529)\tPrec@1 63.559 (60.567)\tPrec@5 94.915 (93.229)\n",
      "Test: [0/100]\tTime 0.350 (0.350)\tLoss 14.6498 (14.6498)\tPrec@1 15.000 (15.000)\tPrec@5 66.000 (66.000)\n",
      "Test: [10/100]\tTime 0.073 (0.099)\tLoss 12.2107 (13.3895)\tPrec@1 28.000 (22.636)\tPrec@5 70.000 (68.545)\n",
      "Test: [20/100]\tTime 0.074 (0.087)\tLoss 11.8887 (13.3905)\tPrec@1 24.000 (22.381)\tPrec@5 80.000 (69.619)\n",
      "Test: [30/100]\tTime 0.073 (0.083)\tLoss 12.3020 (13.3180)\tPrec@1 26.000 (22.903)\tPrec@5 73.000 (69.677)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 13.8288 (13.3479)\tPrec@1 22.000 (22.854)\tPrec@5 64.000 (69.122)\n",
      "Test: [50/100]\tTime 0.073 (0.079)\tLoss 13.3687 (13.2676)\tPrec@1 23.000 (22.863)\tPrec@5 69.000 (69.765)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 10.9112 (13.2138)\tPrec@1 30.000 (22.639)\tPrec@5 80.000 (69.852)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 12.8403 (13.1682)\tPrec@1 23.000 (22.845)\tPrec@5 72.000 (69.986)\n",
      "Test: [80/100]\tTime 0.073 (0.077)\tLoss 12.1040 (13.1289)\tPrec@1 28.000 (23.148)\tPrec@5 73.000 (70.099)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 12.6318 (13.1838)\tPrec@1 33.000 (22.934)\tPrec@5 70.000 (69.868)\n",
      "val Results: Prec@1 22.900 Prec@5 69.700 Loss 13.21031\n",
      "val Class Accuracy: [0.922,0.525,0.605,0.238,0.000,0.000,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 22.900\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [2][0/97], lr: 0.00600\tTime 0.388 (0.388)\tData 0.205 (0.205)\tLoss 4.8826 (4.8826)\tPrec@1 63.281 (63.281)\tPrec@5 96.094 (96.094)\n",
      "Epoch: [2][10/97], lr: 0.00600\tTime 0.441 (0.468)\tData 0.001 (0.034)\tLoss 5.4570 (5.0958)\tPrec@1 59.375 (61.648)\tPrec@5 92.188 (93.963)\n",
      "Epoch: [2][20/97], lr: 0.00600\tTime 0.419 (0.444)\tData 0.000 (0.025)\tLoss 4.8994 (5.2154)\tPrec@1 68.750 (61.607)\tPrec@5 95.312 (93.601)\n",
      "Epoch: [2][30/97], lr: 0.00600\tTime 0.393 (0.437)\tData 0.000 (0.022)\tLoss 4.3938 (5.1992)\tPrec@1 67.969 (61.719)\tPrec@5 95.312 (94.103)\n",
      "Epoch: [2][40/97], lr: 0.00600\tTime 0.394 (0.429)\tData 0.000 (0.020)\tLoss 4.6486 (5.1321)\tPrec@1 64.844 (62.309)\tPrec@5 94.531 (94.017)\n",
      "Epoch: [2][50/97], lr: 0.00600\tTime 0.342 (0.423)\tData 0.000 (0.019)\tLoss 4.3659 (5.0878)\tPrec@1 70.312 (62.745)\tPrec@5 95.312 (94.210)\n",
      "Epoch: [2][60/97], lr: 0.00600\tTime 0.343 (0.410)\tData 0.000 (0.019)\tLoss 5.5882 (5.0885)\tPrec@1 60.938 (62.756)\tPrec@5 91.406 (94.121)\n",
      "Epoch: [2][70/97], lr: 0.00600\tTime 0.325 (0.400)\tData 0.000 (0.019)\tLoss 4.8108 (5.0837)\tPrec@1 64.062 (62.555)\tPrec@5 93.750 (94.014)\n",
      "Epoch: [2][80/97], lr: 0.00600\tTime 0.338 (0.391)\tData 0.000 (0.018)\tLoss 5.6837 (5.0866)\tPrec@1 59.375 (62.616)\tPrec@5 91.406 (93.914)\n",
      "Epoch: [2][90/97], lr: 0.00600\tTime 0.338 (0.385)\tData 0.000 (0.018)\tLoss 5.1906 (5.0960)\tPrec@1 58.594 (62.629)\tPrec@5 90.625 (93.656)\n",
      "Epoch: [2][96/97], lr: 0.00600\tTime 0.359 (0.387)\tData 0.000 (0.019)\tLoss 4.6656 (5.0897)\tPrec@1 67.797 (62.671)\tPrec@5 94.915 (93.632)\n",
      "Test: [0/100]\tTime 0.290 (0.290)\tLoss 14.0454 (14.0454)\tPrec@1 22.000 (22.000)\tPrec@5 63.000 (63.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 11.3949 (13.0511)\tPrec@1 30.000 (23.545)\tPrec@5 69.000 (70.545)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 11.5473 (13.0423)\tPrec@1 27.000 (23.476)\tPrec@5 79.000 (71.619)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 12.0196 (12.9984)\tPrec@1 21.000 (23.194)\tPrec@5 79.000 (71.774)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 13.3398 (13.0970)\tPrec@1 20.000 (23.171)\tPrec@5 75.000 (71.780)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 13.5602 (13.0209)\tPrec@1 21.000 (23.471)\tPrec@5 67.000 (71.569)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 11.1433 (12.9628)\tPrec@1 27.000 (23.230)\tPrec@5 78.000 (71.541)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 12.7419 (12.8998)\tPrec@1 28.000 (23.437)\tPrec@5 67.000 (71.507)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 11.7984 (12.8508)\tPrec@1 32.000 (23.765)\tPrec@5 71.000 (71.802)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 12.7482 (12.8939)\tPrec@1 27.000 (23.549)\tPrec@5 73.000 (71.659)\n",
      "val Results: Prec@1 23.630 Prec@5 71.680 Loss 12.90918\n",
      "val Class Accuracy: [0.902,0.916,0.449,0.090,0.000,0.000,0.000,0.006,0.000,0.000]\n",
      "Best Prec@1: 23.630\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [3][0/97], lr: 0.00800\tTime 0.771 (0.771)\tData 0.482 (0.482)\tLoss 4.5040 (4.5040)\tPrec@1 64.844 (64.844)\tPrec@5 95.312 (95.312)\n",
      "Epoch: [3][10/97], lr: 0.00800\tTime 0.329 (0.380)\tData 0.000 (0.058)\tLoss 5.3928 (4.7833)\tPrec@1 58.594 (64.631)\tPrec@5 90.625 (94.034)\n",
      "Epoch: [3][20/97], lr: 0.00800\tTime 0.340 (0.357)\tData 0.000 (0.039)\tLoss 5.3840 (4.9022)\tPrec@1 61.719 (64.546)\tPrec@5 89.062 (93.787)\n",
      "Epoch: [3][30/97], lr: 0.00800\tTime 0.340 (0.354)\tData 0.000 (0.032)\tLoss 4.8990 (4.9162)\tPrec@1 63.281 (64.491)\tPrec@5 98.438 (94.103)\n",
      "Epoch: [3][40/97], lr: 0.00800\tTime 0.325 (0.350)\tData 0.000 (0.028)\tLoss 4.6470 (4.8711)\tPrec@1 64.844 (65.168)\tPrec@5 96.094 (94.093)\n",
      "Epoch: [3][50/97], lr: 0.00800\tTime 0.341 (0.346)\tData 0.000 (0.026)\tLoss 5.1581 (4.8716)\tPrec@1 62.500 (64.874)\tPrec@5 91.406 (93.919)\n",
      "Epoch: [3][60/97], lr: 0.00800\tTime 0.325 (0.344)\tData 0.000 (0.024)\tLoss 4.2336 (4.8862)\tPrec@1 71.875 (64.703)\tPrec@5 93.750 (93.904)\n",
      "Epoch: [3][70/97], lr: 0.00800\tTime 0.341 (0.343)\tData 0.000 (0.023)\tLoss 5.4361 (4.9019)\tPrec@1 62.500 (64.745)\tPrec@5 92.969 (94.014)\n",
      "Epoch: [3][80/97], lr: 0.00800\tTime 0.330 (0.343)\tData 0.000 (0.023)\tLoss 5.0519 (4.8809)\tPrec@1 63.281 (64.660)\tPrec@5 96.094 (94.078)\n",
      "Epoch: [3][90/97], lr: 0.00800\tTime 0.357 (0.343)\tData 0.000 (0.022)\tLoss 4.4670 (4.8578)\tPrec@1 71.875 (64.878)\tPrec@5 93.750 (94.119)\n",
      "Epoch: [3][96/97], lr: 0.00800\tTime 0.357 (0.345)\tData 0.000 (0.022)\tLoss 4.4596 (4.8328)\tPrec@1 66.102 (65.049)\tPrec@5 94.068 (94.100)\n",
      "Test: [0/100]\tTime 0.386 (0.386)\tLoss 13.5837 (13.5837)\tPrec@1 19.000 (19.000)\tPrec@5 78.000 (78.000)\n",
      "Test: [10/100]\tTime 0.073 (0.102)\tLoss 10.7759 (12.5967)\tPrec@1 30.000 (24.636)\tPrec@5 82.000 (76.182)\n",
      "Test: [20/100]\tTime 0.073 (0.088)\tLoss 11.5345 (12.6200)\tPrec@1 30.000 (24.524)\tPrec@5 81.000 (76.714)\n",
      "Test: [30/100]\tTime 0.073 (0.084)\tLoss 11.6629 (12.5184)\tPrec@1 27.000 (25.194)\tPrec@5 76.000 (76.581)\n",
      "Test: [40/100]\tTime 0.074 (0.081)\tLoss 13.0622 (12.5385)\tPrec@1 20.000 (25.195)\tPrec@5 80.000 (76.366)\n",
      "Test: [50/100]\tTime 0.073 (0.080)\tLoss 12.3291 (12.4705)\tPrec@1 27.000 (25.431)\tPrec@5 74.000 (76.392)\n",
      "Test: [60/100]\tTime 0.073 (0.079)\tLoss 10.5909 (12.4290)\tPrec@1 34.000 (25.164)\tPrec@5 76.000 (76.508)\n",
      "Test: [70/100]\tTime 0.073 (0.078)\tLoss 11.7697 (12.3769)\tPrec@1 29.000 (25.549)\tPrec@5 83.000 (76.465)\n",
      "Test: [80/100]\tTime 0.073 (0.077)\tLoss 11.4186 (12.3324)\tPrec@1 33.000 (25.790)\tPrec@5 80.000 (76.580)\n",
      "Test: [90/100]\tTime 0.073 (0.077)\tLoss 11.7917 (12.3950)\tPrec@1 35.000 (25.473)\tPrec@5 78.000 (76.396)\n",
      "val Results: Prec@1 25.420 Prec@5 76.440 Loss 12.41881\n",
      "val Class Accuracy: [0.879,0.847,0.806,0.003,0.007,0.000,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 25.420\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [4][0/97], lr: 0.01000\tTime 0.548 (0.548)\tData 0.284 (0.284)\tLoss 4.5737 (4.5737)\tPrec@1 66.406 (66.406)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [4][10/97], lr: 0.01000\tTime 0.328 (0.361)\tData 0.000 (0.039)\tLoss 4.8001 (4.5088)\tPrec@1 62.500 (66.477)\tPrec@5 94.531 (95.312)\n",
      "Epoch: [4][20/97], lr: 0.01000\tTime 0.327 (0.345)\tData 0.000 (0.029)\tLoss 4.4725 (4.6769)\tPrec@1 65.625 (65.737)\tPrec@5 92.188 (94.680)\n",
      "Epoch: [4][30/97], lr: 0.01000\tTime 0.325 (0.341)\tData 0.000 (0.025)\tLoss 5.2801 (4.6965)\tPrec@1 63.281 (66.003)\tPrec@5 86.719 (94.355)\n",
      "Epoch: [4][40/97], lr: 0.01000\tTime 0.325 (0.338)\tData 0.000 (0.023)\tLoss 4.1449 (4.6978)\tPrec@1 70.312 (66.139)\tPrec@5 92.969 (94.455)\n",
      "Epoch: [4][50/97], lr: 0.01000\tTime 0.324 (0.337)\tData 0.000 (0.022)\tLoss 5.1324 (4.7263)\tPrec@1 60.938 (66.054)\tPrec@5 94.531 (94.347)\n",
      "Epoch: [4][60/97], lr: 0.01000\tTime 0.325 (0.336)\tData 0.000 (0.021)\tLoss 4.5223 (4.6674)\tPrec@1 65.625 (66.342)\tPrec@5 96.094 (94.544)\n",
      "Epoch: [4][70/97], lr: 0.01000\tTime 0.327 (0.335)\tData 0.000 (0.021)\tLoss 5.5125 (4.6985)\tPrec@1 58.594 (66.142)\tPrec@5 94.531 (94.388)\n",
      "Epoch: [4][80/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.020)\tLoss 4.5832 (4.6765)\tPrec@1 66.406 (66.319)\tPrec@5 94.531 (94.502)\n",
      "Epoch: [4][90/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.020)\tLoss 3.9923 (4.6442)\tPrec@1 71.094 (66.621)\tPrec@5 96.094 (94.574)\n",
      "Epoch: [4][96/97], lr: 0.01000\tTime 0.316 (0.333)\tData 0.000 (0.020)\tLoss 4.6687 (4.6413)\tPrec@1 61.864 (66.605)\tPrec@5 94.915 (94.632)\n",
      "Test: [0/100]\tTime 0.237 (0.237)\tLoss 14.8049 (14.8049)\tPrec@1 18.000 (18.000)\tPrec@5 70.000 (70.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 11.5915 (13.4732)\tPrec@1 32.000 (23.455)\tPrec@5 70.000 (73.091)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 11.9838 (13.5315)\tPrec@1 28.000 (23.190)\tPrec@5 80.000 (74.667)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 12.4289 (13.4721)\tPrec@1 26.000 (23.548)\tPrec@5 77.000 (74.742)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 14.0764 (13.5598)\tPrec@1 22.000 (23.049)\tPrec@5 68.000 (74.390)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 13.4893 (13.4669)\tPrec@1 22.000 (23.529)\tPrec@5 70.000 (74.569)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 11.3245 (13.4089)\tPrec@1 30.000 (23.377)\tPrec@5 82.000 (74.459)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 13.4640 (13.3731)\tPrec@1 24.000 (23.549)\tPrec@5 75.000 (74.606)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 12.4509 (13.3175)\tPrec@1 28.000 (23.728)\tPrec@5 79.000 (75.025)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 13.1341 (13.3790)\tPrec@1 28.000 (23.286)\tPrec@5 82.000 (74.901)\n",
      "val Results: Prec@1 23.280 Prec@5 74.910 Loss 13.40599\n",
      "val Class Accuracy: [0.973,0.821,0.352,0.025,0.137,0.020,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 25.420\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [5][0/97], lr: 0.01000\tTime 0.421 (0.421)\tData 0.221 (0.221)\tLoss 4.0603 (4.0603)\tPrec@1 72.656 (72.656)\tPrec@5 93.750 (93.750)\n",
      "Epoch: [5][10/97], lr: 0.01000\tTime 0.333 (0.342)\tData 0.000 (0.035)\tLoss 4.7850 (4.4683)\tPrec@1 67.969 (67.898)\tPrec@5 96.875 (95.099)\n",
      "Epoch: [5][20/97], lr: 0.01000\tTime 0.328 (0.336)\tData 0.000 (0.026)\tLoss 4.3263 (4.5035)\tPrec@1 67.969 (67.262)\tPrec@5 94.531 (94.978)\n",
      "Epoch: [5][30/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.023)\tLoss 4.2961 (4.5012)\tPrec@1 70.312 (67.364)\tPrec@5 96.094 (94.834)\n",
      "Epoch: [5][40/97], lr: 0.01000\tTime 0.332 (0.333)\tData 0.000 (0.022)\tLoss 4.7485 (4.5401)\tPrec@1 65.625 (66.959)\tPrec@5 97.656 (94.836)\n",
      "Epoch: [5][50/97], lr: 0.01000\tTime 0.338 (0.332)\tData 0.000 (0.021)\tLoss 4.2795 (4.5121)\tPrec@1 67.969 (67.065)\tPrec@5 93.750 (95.098)\n",
      "Epoch: [5][60/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 3.6376 (4.4403)\tPrec@1 75.000 (67.623)\tPrec@5 99.219 (95.389)\n",
      "Epoch: [5][70/97], lr: 0.01000\tTime 0.329 (0.332)\tData 0.000 (0.020)\tLoss 3.9702 (4.4489)\tPrec@1 70.312 (67.694)\tPrec@5 98.438 (95.312)\n",
      "Epoch: [5][80/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 4.1466 (4.4332)\tPrec@1 73.438 (67.901)\tPrec@5 96.094 (95.390)\n",
      "Epoch: [5][90/97], lr: 0.01000\tTime 0.330 (0.332)\tData 0.000 (0.019)\tLoss 4.3004 (4.4030)\tPrec@1 65.625 (68.278)\tPrec@5 97.656 (95.441)\n",
      "Epoch: [5][96/97], lr: 0.01000\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 3.9808 (4.3996)\tPrec@1 71.186 (68.322)\tPrec@5 98.305 (95.478)\n",
      "Test: [0/100]\tTime 0.246 (0.246)\tLoss 12.5725 (12.5725)\tPrec@1 26.000 (26.000)\tPrec@5 86.000 (86.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 9.7284 (11.3329)\tPrec@1 39.000 (31.182)\tPrec@5 87.000 (88.364)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 10.1710 (11.3303)\tPrec@1 37.000 (31.476)\tPrec@5 92.000 (88.762)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 10.3890 (11.2821)\tPrec@1 36.000 (31.516)\tPrec@5 91.000 (88.806)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 11.8864 (11.3188)\tPrec@1 30.000 (31.585)\tPrec@5 87.000 (88.512)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 10.8609 (11.2086)\tPrec@1 32.000 (32.294)\tPrec@5 87.000 (88.549)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 9.6310 (11.1628)\tPrec@1 43.000 (32.311)\tPrec@5 91.000 (88.492)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 10.7506 (11.1346)\tPrec@1 32.000 (32.352)\tPrec@5 92.000 (88.479)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 10.1749 (11.0980)\tPrec@1 43.000 (32.605)\tPrec@5 84.000 (88.444)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.0173 (11.1602)\tPrec@1 34.000 (32.275)\tPrec@5 90.000 (88.154)\n",
      "val Results: Prec@1 32.140 Prec@5 88.130 Loss 11.18591\n",
      "val Class Accuracy: [0.935,0.912,0.342,0.714,0.307,0.000,0.002,0.002,0.000,0.000]\n",
      "Best Prec@1: 32.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [6][0/97], lr: 0.01000\tTime 0.444 (0.444)\tData 0.222 (0.222)\tLoss 4.4196 (4.4196)\tPrec@1 64.844 (64.844)\tPrec@5 93.750 (93.750)\n",
      "Epoch: [6][10/97], lr: 0.01000\tTime 0.327 (0.345)\tData 0.000 (0.035)\tLoss 3.8312 (4.2785)\tPrec@1 71.094 (69.034)\tPrec@5 98.438 (95.739)\n",
      "Epoch: [6][20/97], lr: 0.01000\tTime 0.330 (0.337)\tData 0.000 (0.027)\tLoss 4.1566 (4.3003)\tPrec@1 75.000 (69.978)\tPrec@5 96.094 (95.982)\n",
      "Epoch: [6][30/97], lr: 0.01000\tTime 0.331 (0.336)\tData 0.000 (0.024)\tLoss 4.8619 (4.3576)\tPrec@1 60.938 (69.380)\tPrec@5 94.531 (95.968)\n",
      "Epoch: [6][40/97], lr: 0.01000\tTime 0.327 (0.335)\tData 0.000 (0.022)\tLoss 4.7639 (4.3054)\tPrec@1 63.281 (69.627)\tPrec@5 97.656 (95.884)\n",
      "Epoch: [6][50/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.021)\tLoss 3.8676 (4.2998)\tPrec@1 71.094 (69.409)\tPrec@5 94.531 (95.772)\n",
      "Epoch: [6][60/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.020)\tLoss 5.0996 (4.3337)\tPrec@1 64.062 (69.019)\tPrec@5 95.312 (95.799)\n",
      "Epoch: [6][70/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.020)\tLoss 3.7860 (4.3455)\tPrec@1 73.438 (68.937)\tPrec@5 97.656 (95.742)\n",
      "Epoch: [6][80/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 4.3141 (4.3182)\tPrec@1 66.406 (68.962)\tPrec@5 96.094 (95.853)\n",
      "Epoch: [6][90/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.019)\tLoss 4.5649 (4.3162)\tPrec@1 66.406 (69.042)\tPrec@5 93.750 (95.673)\n",
      "Epoch: [6][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 5.1470 (4.3280)\tPrec@1 64.407 (69.039)\tPrec@5 90.678 (95.615)\n",
      "Test: [0/100]\tTime 0.256 (0.256)\tLoss 14.2342 (14.2342)\tPrec@1 19.000 (19.000)\tPrec@5 84.000 (84.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 11.4318 (13.0936)\tPrec@1 31.000 (25.000)\tPrec@5 87.000 (84.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 11.7707 (13.0163)\tPrec@1 32.000 (25.190)\tPrec@5 87.000 (86.000)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 12.3405 (12.8849)\tPrec@1 25.000 (25.742)\tPrec@5 89.000 (85.645)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 13.6980 (12.9216)\tPrec@1 20.000 (25.610)\tPrec@5 86.000 (85.366)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 13.1829 (12.8753)\tPrec@1 24.000 (25.804)\tPrec@5 83.000 (85.627)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 11.1386 (12.8811)\tPrec@1 32.000 (25.443)\tPrec@5 89.000 (85.541)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 11.9688 (12.8426)\tPrec@1 28.000 (25.718)\tPrec@5 87.000 (85.493)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 11.9465 (12.7887)\tPrec@1 34.000 (26.099)\tPrec@5 83.000 (85.457)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.9617 (12.8469)\tPrec@1 32.000 (25.890)\tPrec@5 87.000 (85.495)\n",
      "val Results: Prec@1 25.890 Prec@5 85.460 Loss 12.86180\n",
      "val Class Accuracy: [0.800,0.877,0.893,0.019,0.000,0.000,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 32.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [7][0/97], lr: 0.01000\tTime 0.406 (0.406)\tData 0.210 (0.210)\tLoss 3.1063 (3.1063)\tPrec@1 76.562 (76.562)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [7][10/97], lr: 0.01000\tTime 0.326 (0.340)\tData 0.000 (0.034)\tLoss 3.6303 (4.0364)\tPrec@1 75.781 (70.881)\tPrec@5 93.750 (96.236)\n",
      "Epoch: [7][20/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.026)\tLoss 4.1814 (4.1677)\tPrec@1 71.094 (70.164)\tPrec@5 97.656 (96.243)\n",
      "Epoch: [7][30/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.023)\tLoss 3.5193 (4.1565)\tPrec@1 72.656 (70.010)\tPrec@5 96.094 (95.968)\n",
      "Epoch: [7][40/97], lr: 0.01000\tTime 0.331 (0.332)\tData 0.000 (0.022)\tLoss 3.9160 (4.1178)\tPrec@1 75.781 (70.598)\tPrec@5 98.438 (95.979)\n",
      "Epoch: [7][50/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.021)\tLoss 4.2303 (4.1335)\tPrec@1 69.531 (70.588)\tPrec@5 96.875 (95.864)\n",
      "Epoch: [7][60/97], lr: 0.01000\tTime 0.328 (0.332)\tData 0.000 (0.020)\tLoss 4.9287 (4.1138)\tPrec@1 62.500 (70.761)\tPrec@5 95.312 (96.055)\n",
      "Epoch: [7][70/97], lr: 0.01000\tTime 0.336 (0.331)\tData 0.000 (0.020)\tLoss 3.7756 (4.1306)\tPrec@1 75.781 (70.709)\tPrec@5 96.875 (96.061)\n",
      "Epoch: [7][80/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.019)\tLoss 5.0021 (4.1469)\tPrec@1 67.188 (70.660)\tPrec@5 95.312 (96.132)\n",
      "Epoch: [7][90/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.019)\tLoss 4.1817 (4.1712)\tPrec@1 66.406 (70.433)\tPrec@5 97.656 (96.162)\n",
      "Epoch: [7][96/97], lr: 0.01000\tTime 0.316 (0.331)\tData 0.000 (0.020)\tLoss 3.6323 (4.1584)\tPrec@1 73.729 (70.530)\tPrec@5 96.610 (96.236)\n",
      "Test: [0/100]\tTime 0.241 (0.241)\tLoss 12.9826 (12.9826)\tPrec@1 33.000 (33.000)\tPrec@5 85.000 (85.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 9.9593 (11.9389)\tPrec@1 44.000 (33.909)\tPrec@5 89.000 (83.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 11.0333 (12.0493)\tPrec@1 35.000 (32.190)\tPrec@5 83.000 (83.667)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 10.8417 (11.9627)\tPrec@1 35.000 (32.710)\tPrec@5 81.000 (83.548)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 12.6804 (12.0089)\tPrec@1 32.000 (32.585)\tPrec@5 81.000 (83.122)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 11.5706 (11.9011)\tPrec@1 38.000 (33.196)\tPrec@5 82.000 (83.431)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 9.9203 (11.8407)\tPrec@1 37.000 (33.082)\tPrec@5 89.000 (83.361)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 11.6506 (11.8311)\tPrec@1 39.000 (33.225)\tPrec@5 81.000 (83.296)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 10.7579 (11.7900)\tPrec@1 46.000 (33.444)\tPrec@5 82.000 (83.481)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.6975 (11.8377)\tPrec@1 36.000 (33.363)\tPrec@5 84.000 (83.440)\n",
      "val Results: Prec@1 33.300 Prec@5 83.180 Loss 11.86884\n",
      "val Class Accuracy: [0.929,0.899,0.564,0.514,0.112,0.000,0.312,0.000,0.000,0.000]\n",
      "Best Prec@1: 33.300\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [8][0/97], lr: 0.01000\tTime 0.428 (0.428)\tData 0.218 (0.218)\tLoss 4.3546 (4.3546)\tPrec@1 69.531 (69.531)\tPrec@5 95.312 (95.312)\n",
      "Epoch: [8][10/97], lr: 0.01000\tTime 0.327 (0.343)\tData 0.000 (0.035)\tLoss 4.7759 (4.1451)\tPrec@1 63.281 (70.170)\tPrec@5 94.531 (95.739)\n",
      "Epoch: [8][20/97], lr: 0.01000\tTime 0.329 (0.336)\tData 0.000 (0.027)\tLoss 3.7396 (4.0629)\tPrec@1 77.344 (70.871)\tPrec@5 97.656 (96.243)\n",
      "Epoch: [8][30/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.024)\tLoss 3.8967 (4.0622)\tPrec@1 73.438 (71.043)\tPrec@5 96.094 (96.371)\n",
      "Epoch: [8][40/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.022)\tLoss 3.8887 (4.0006)\tPrec@1 71.875 (71.818)\tPrec@5 95.312 (96.399)\n",
      "Epoch: [8][50/97], lr: 0.01000\tTime 0.329 (0.332)\tData 0.000 (0.021)\tLoss 4.7673 (4.0396)\tPrec@1 67.188 (71.523)\tPrec@5 97.656 (96.492)\n",
      "Epoch: [8][60/97], lr: 0.01000\tTime 0.329 (0.333)\tData 0.000 (0.020)\tLoss 4.5082 (4.0505)\tPrec@1 70.312 (71.606)\tPrec@5 96.875 (96.376)\n",
      "Epoch: [8][70/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.020)\tLoss 4.0160 (4.1030)\tPrec@1 68.750 (71.072)\tPrec@5 94.531 (96.050)\n",
      "Epoch: [8][80/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 4.8760 (4.1060)\tPrec@1 67.969 (71.113)\tPrec@5 93.750 (96.026)\n",
      "Epoch: [8][90/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.019)\tLoss 4.7237 (4.1108)\tPrec@1 62.500 (70.939)\tPrec@5 96.875 (96.094)\n",
      "Epoch: [8][96/97], lr: 0.01000\tTime 0.317 (0.332)\tData 0.000 (0.020)\tLoss 3.1999 (4.0782)\tPrec@1 83.051 (71.248)\tPrec@5 95.763 (96.123)\n",
      "Test: [0/100]\tTime 0.268 (0.268)\tLoss 13.9765 (13.9765)\tPrec@1 23.000 (23.000)\tPrec@5 85.000 (85.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 10.8716 (12.9444)\tPrec@1 38.000 (29.636)\tPrec@5 87.000 (83.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 11.8642 (13.0260)\tPrec@1 31.000 (28.476)\tPrec@5 82.000 (83.048)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 11.9179 (12.9154)\tPrec@1 34.000 (29.065)\tPrec@5 82.000 (83.419)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 13.8163 (12.9952)\tPrec@1 23.000 (28.829)\tPrec@5 79.000 (83.073)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 12.7171 (12.9113)\tPrec@1 30.000 (29.020)\tPrec@5 80.000 (83.588)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 10.9131 (12.8701)\tPrec@1 34.000 (28.918)\tPrec@5 84.000 (83.574)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 12.7064 (12.8319)\tPrec@1 32.000 (29.169)\tPrec@5 82.000 (83.746)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 11.9131 (12.7816)\tPrec@1 39.000 (29.457)\tPrec@5 80.000 (83.914)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 12.6203 (12.8335)\tPrec@1 33.000 (29.253)\tPrec@5 90.000 (83.989)\n",
      "val Results: Prec@1 29.260 Prec@5 84.010 Loss 12.86129\n",
      "val Class Accuracy: [0.964,0.965,0.570,0.139,0.091,0.000,0.197,0.000,0.000,0.000]\n",
      "Best Prec@1: 33.300\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [9][0/97], lr: 0.01000\tTime 0.447 (0.447)\tData 0.224 (0.224)\tLoss 3.8222 (3.8222)\tPrec@1 71.094 (71.094)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [9][10/97], lr: 0.01000\tTime 0.329 (0.345)\tData 0.000 (0.035)\tLoss 4.5513 (3.7268)\tPrec@1 64.844 (72.727)\tPrec@5 98.438 (97.514)\n",
      "Epoch: [9][20/97], lr: 0.01000\tTime 0.326 (0.337)\tData 0.000 (0.027)\tLoss 3.8151 (3.9398)\tPrec@1 71.094 (72.061)\tPrec@5 96.094 (96.987)\n",
      "Epoch: [9][30/97], lr: 0.01000\tTime 0.327 (0.336)\tData 0.000 (0.024)\tLoss 3.7070 (3.9248)\tPrec@1 76.562 (72.757)\tPrec@5 96.094 (96.976)\n",
      "Epoch: [9][40/97], lr: 0.01000\tTime 0.331 (0.334)\tData 0.000 (0.022)\tLoss 3.5535 (4.0359)\tPrec@1 72.656 (71.818)\tPrec@5 96.875 (96.684)\n",
      "Epoch: [9][50/97], lr: 0.01000\tTime 0.328 (0.334)\tData 0.000 (0.021)\tLoss 3.5028 (4.0229)\tPrec@1 75.000 (71.860)\tPrec@5 97.656 (96.661)\n",
      "Epoch: [9][60/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.021)\tLoss 3.4922 (4.0034)\tPrec@1 76.562 (71.913)\tPrec@5 97.656 (96.606)\n",
      "Epoch: [9][70/97], lr: 0.01000\tTime 0.337 (0.333)\tData 0.000 (0.020)\tLoss 3.7363 (3.9904)\tPrec@1 69.531 (71.886)\tPrec@5 97.656 (96.677)\n",
      "Epoch: [9][80/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.020)\tLoss 4.6666 (3.9981)\tPrec@1 71.094 (72.029)\tPrec@5 96.875 (96.721)\n",
      "Epoch: [9][90/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.019)\tLoss 4.8186 (3.9992)\tPrec@1 65.625 (72.004)\tPrec@5 94.531 (96.695)\n",
      "Epoch: [9][96/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 4.0838 (3.9966)\tPrec@1 71.186 (72.022)\tPrec@5 99.153 (96.735)\n",
      "Test: [0/100]\tTime 0.258 (0.258)\tLoss 11.9352 (11.9352)\tPrec@1 31.000 (31.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 9.3041 (10.9128)\tPrec@1 41.000 (35.545)\tPrec@5 92.000 (92.000)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 10.0155 (10.9157)\tPrec@1 38.000 (35.238)\tPrec@5 95.000 (91.667)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 10.1973 (10.8472)\tPrec@1 40.000 (35.387)\tPrec@5 89.000 (91.097)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 11.5639 (10.9074)\tPrec@1 31.000 (34.805)\tPrec@5 92.000 (90.878)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 10.5430 (10.8124)\tPrec@1 35.000 (35.608)\tPrec@5 90.000 (90.961)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 9.3040 (10.7813)\tPrec@1 43.000 (35.230)\tPrec@5 88.000 (90.951)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 10.3098 (10.7563)\tPrec@1 41.000 (35.296)\tPrec@5 92.000 (91.000)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.7262 (10.7093)\tPrec@1 45.000 (35.593)\tPrec@5 90.000 (91.111)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.9998 (10.7596)\tPrec@1 35.000 (35.308)\tPrec@5 94.000 (91.022)\n",
      "val Results: Prec@1 35.240 Prec@5 90.890 Loss 10.78121\n",
      "val Class Accuracy: [0.895,0.972,0.406,0.780,0.412,0.000,0.058,0.000,0.000,0.001]\n",
      "Best Prec@1: 35.240\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [10][0/97], lr: 0.01000\tTime 0.430 (0.430)\tData 0.198 (0.198)\tLoss 4.4927 (4.4927)\tPrec@1 70.312 (70.312)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [10][10/97], lr: 0.01000\tTime 0.342 (0.344)\tData 0.000 (0.033)\tLoss 3.8530 (3.8627)\tPrec@1 74.219 (73.722)\tPrec@5 99.219 (97.230)\n",
      "Epoch: [10][20/97], lr: 0.01000\tTime 0.327 (0.337)\tData 0.000 (0.025)\tLoss 3.6009 (3.8872)\tPrec@1 75.000 (73.289)\tPrec@5 97.656 (97.210)\n",
      "Epoch: [10][30/97], lr: 0.01000\tTime 0.330 (0.336)\tData 0.000 (0.022)\tLoss 4.2660 (3.9552)\tPrec@1 71.875 (73.160)\tPrec@5 92.969 (96.799)\n",
      "Epoch: [10][40/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.021)\tLoss 4.7222 (3.9970)\tPrec@1 66.406 (72.580)\tPrec@5 96.094 (96.532)\n",
      "Epoch: [10][50/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.020)\tLoss 4.3524 (3.9954)\tPrec@1 73.438 (72.518)\tPrec@5 95.312 (96.645)\n",
      "Epoch: [10][60/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.020)\tLoss 3.9595 (3.9860)\tPrec@1 76.562 (72.707)\tPrec@5 96.094 (96.657)\n",
      "Epoch: [10][70/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.019)\tLoss 3.7965 (4.0000)\tPrec@1 75.781 (72.678)\tPrec@5 95.312 (96.578)\n",
      "Epoch: [10][80/97], lr: 0.01000\tTime 0.328 (0.332)\tData 0.000 (0.019)\tLoss 3.0877 (3.9726)\tPrec@1 78.906 (72.946)\tPrec@5 99.219 (96.470)\n",
      "Epoch: [10][90/97], lr: 0.01000\tTime 0.328 (0.331)\tData 0.000 (0.019)\tLoss 3.9741 (3.9700)\tPrec@1 72.656 (72.957)\tPrec@5 96.094 (96.506)\n",
      "Epoch: [10][96/97], lr: 0.01000\tTime 0.341 (0.331)\tData 0.000 (0.020)\tLoss 4.1195 (3.9609)\tPrec@1 68.644 (72.949)\tPrec@5 98.305 (96.574)\n",
      "Test: [0/100]\tTime 0.239 (0.239)\tLoss 12.6396 (12.6396)\tPrec@1 31.000 (31.000)\tPrec@5 83.000 (83.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 9.4551 (11.5954)\tPrec@1 48.000 (34.727)\tPrec@5 90.000 (83.909)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 10.6930 (11.6473)\tPrec@1 38.000 (34.048)\tPrec@5 87.000 (85.190)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 11.1172 (11.5441)\tPrec@1 37.000 (34.903)\tPrec@5 84.000 (84.839)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 12.7886 (11.6325)\tPrec@1 28.000 (34.366)\tPrec@5 78.000 (84.195)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 11.5554 (11.5703)\tPrec@1 34.000 (34.784)\tPrec@5 83.000 (84.431)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 9.7637 (11.5601)\tPrec@1 39.000 (34.393)\tPrec@5 89.000 (84.230)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 10.7459 (11.5489)\tPrec@1 37.000 (34.423)\tPrec@5 84.000 (84.113)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 10.6156 (11.5025)\tPrec@1 45.000 (34.728)\tPrec@5 83.000 (84.370)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.4677 (11.5604)\tPrec@1 34.000 (34.571)\tPrec@5 86.000 (84.374)\n",
      "val Results: Prec@1 34.600 Prec@5 84.150 Loss 11.58621\n",
      "val Class Accuracy: [0.826,0.937,0.846,0.460,0.078,0.000,0.313,0.000,0.000,0.000]\n",
      "Best Prec@1: 35.240\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [11][0/97], lr: 0.01000\tTime 0.431 (0.431)\tData 0.236 (0.236)\tLoss 4.4504 (4.4504)\tPrec@1 71.875 (71.875)\tPrec@5 96.094 (96.094)\n",
      "Epoch: [11][10/97], lr: 0.01000\tTime 0.328 (0.344)\tData 0.000 (0.036)\tLoss 4.2177 (3.8144)\tPrec@1 69.531 (73.580)\tPrec@5 95.312 (97.017)\n",
      "Epoch: [11][20/97], lr: 0.01000\tTime 0.332 (0.338)\tData 0.000 (0.027)\tLoss 4.2507 (3.9297)\tPrec@1 70.312 (72.693)\tPrec@5 94.531 (96.540)\n",
      "Epoch: [11][30/97], lr: 0.01000\tTime 0.328 (0.337)\tData 0.000 (0.024)\tLoss 3.8215 (3.8754)\tPrec@1 69.531 (73.337)\tPrec@5 97.656 (96.799)\n",
      "Epoch: [11][40/97], lr: 0.01000\tTime 0.326 (0.336)\tData 0.000 (0.022)\tLoss 3.9134 (3.9039)\tPrec@1 72.656 (73.018)\tPrec@5 99.219 (97.104)\n",
      "Epoch: [11][50/97], lr: 0.01000\tTime 0.322 (0.334)\tData 0.000 (0.021)\tLoss 4.6910 (3.9388)\tPrec@1 67.188 (72.886)\tPrec@5 94.531 (97.044)\n",
      "Epoch: [11][60/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.021)\tLoss 3.3409 (3.9019)\tPrec@1 75.000 (73.207)\tPrec@5 96.094 (97.054)\n",
      "Epoch: [11][70/97], lr: 0.01000\tTime 0.332 (0.333)\tData 0.000 (0.020)\tLoss 3.6313 (3.8958)\tPrec@1 77.344 (73.283)\tPrec@5 96.875 (97.128)\n",
      "Epoch: [11][80/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.020)\tLoss 4.0750 (3.9072)\tPrec@1 71.875 (73.139)\tPrec@5 94.531 (97.039)\n",
      "Epoch: [11][90/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.020)\tLoss 3.8006 (3.8964)\tPrec@1 71.875 (73.274)\tPrec@5 95.312 (97.021)\n",
      "Epoch: [11][96/97], lr: 0.01000\tTime 0.320 (0.332)\tData 0.000 (0.020)\tLoss 3.6327 (3.8796)\tPrec@1 76.271 (73.376)\tPrec@5 95.763 (97.074)\n",
      "Test: [0/100]\tTime 0.235 (0.235)\tLoss 12.4534 (12.4534)\tPrec@1 34.000 (34.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 9.2988 (11.3454)\tPrec@1 49.000 (37.455)\tPrec@5 92.000 (88.182)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 9.9900 (11.3928)\tPrec@1 43.000 (36.857)\tPrec@5 88.000 (87.762)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 10.5841 (11.3292)\tPrec@1 41.000 (37.484)\tPrec@5 85.000 (87.032)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 11.6370 (11.3887)\tPrec@1 33.000 (36.927)\tPrec@5 82.000 (86.683)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 11.3070 (11.3032)\tPrec@1 34.000 (37.333)\tPrec@5 85.000 (87.275)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 9.7235 (11.2893)\tPrec@1 42.000 (36.836)\tPrec@5 92.000 (87.246)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 10.7533 (11.2561)\tPrec@1 42.000 (36.901)\tPrec@5 87.000 (87.282)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 10.4547 (11.2139)\tPrec@1 44.000 (37.148)\tPrec@5 85.000 (87.395)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.8351 (11.2519)\tPrec@1 28.000 (36.857)\tPrec@5 89.000 (87.516)\n",
      "val Results: Prec@1 36.800 Prec@5 87.540 Loss 11.28623\n",
      "val Class Accuracy: [0.816,0.994,0.563,0.631,0.319,0.002,0.343,0.000,0.012,0.000]\n",
      "Best Prec@1: 36.800\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [12][0/97], lr: 0.01000\tTime 0.435 (0.435)\tData 0.212 (0.212)\tLoss 3.7980 (3.7980)\tPrec@1 75.781 (75.781)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [12][10/97], lr: 0.01000\tTime 0.326 (0.343)\tData 0.000 (0.034)\tLoss 3.6912 (3.6747)\tPrec@1 73.438 (74.219)\tPrec@5 97.656 (97.940)\n",
      "Epoch: [12][20/97], lr: 0.01000\tTime 0.324 (0.336)\tData 0.000 (0.026)\tLoss 4.4880 (3.7206)\tPrec@1 71.875 (74.330)\tPrec@5 99.219 (97.768)\n",
      "Epoch: [12][30/97], lr: 0.01000\tTime 0.325 (0.335)\tData 0.000 (0.023)\tLoss 3.4376 (3.7387)\tPrec@1 75.781 (74.118)\tPrec@5 96.094 (97.228)\n",
      "Epoch: [12][40/97], lr: 0.01000\tTime 0.329 (0.334)\tData 0.000 (0.022)\tLoss 3.0833 (3.7780)\tPrec@1 81.250 (74.047)\tPrec@5 96.875 (97.199)\n",
      "Epoch: [12][50/97], lr: 0.01000\tTime 0.323 (0.333)\tData 0.000 (0.021)\tLoss 3.8223 (3.7370)\tPrec@1 74.219 (74.112)\tPrec@5 97.656 (97.197)\n",
      "Epoch: [12][60/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.020)\tLoss 3.5774 (3.7626)\tPrec@1 75.781 (74.014)\tPrec@5 97.656 (97.144)\n",
      "Epoch: [12][70/97], lr: 0.01000\tTime 0.334 (0.333)\tData 0.000 (0.020)\tLoss 3.3145 (3.7687)\tPrec@1 76.562 (74.241)\tPrec@5 97.656 (97.040)\n",
      "Epoch: [12][80/97], lr: 0.01000\tTime 0.329 (0.332)\tData 0.000 (0.020)\tLoss 4.3328 (3.7445)\tPrec@1 71.875 (74.421)\tPrec@5 97.656 (97.155)\n",
      "Epoch: [12][90/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.019)\tLoss 2.8615 (3.7661)\tPrec@1 79.688 (74.348)\tPrec@5 97.656 (97.124)\n",
      "Epoch: [12][96/97], lr: 0.01000\tTime 0.311 (0.332)\tData 0.000 (0.020)\tLoss 3.3620 (3.7581)\tPrec@1 78.814 (74.472)\tPrec@5 99.153 (97.179)\n",
      "Test: [0/100]\tTime 0.249 (0.249)\tLoss 12.9680 (12.9680)\tPrec@1 27.000 (27.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 10.2702 (11.9494)\tPrec@1 45.000 (32.909)\tPrec@5 92.000 (89.091)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 11.0387 (11.8483)\tPrec@1 33.000 (33.143)\tPrec@5 88.000 (89.429)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 11.1251 (11.7787)\tPrec@1 38.000 (33.387)\tPrec@5 93.000 (89.290)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 12.5192 (11.8695)\tPrec@1 31.000 (33.049)\tPrec@5 90.000 (88.927)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 11.9018 (11.7380)\tPrec@1 36.000 (33.667)\tPrec@5 89.000 (89.255)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 10.6505 (11.7604)\tPrec@1 36.000 (33.295)\tPrec@5 93.000 (89.164)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 10.9988 (11.7369)\tPrec@1 41.000 (33.465)\tPrec@5 90.000 (89.239)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 10.9935 (11.6732)\tPrec@1 40.000 (33.790)\tPrec@5 85.000 (89.272)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 12.1764 (11.7077)\tPrec@1 32.000 (33.703)\tPrec@5 90.000 (89.231)\n",
      "val Results: Prec@1 33.740 Prec@5 89.230 Loss 11.73336\n",
      "val Class Accuracy: [0.774,0.964,0.470,0.923,0.073,0.000,0.111,0.000,0.059,0.000]\n",
      "Best Prec@1: 36.800\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [13][0/97], lr: 0.01000\tTime 0.431 (0.431)\tData 0.225 (0.225)\tLoss 3.6527 (3.6527)\tPrec@1 72.656 (72.656)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [13][10/97], lr: 0.01000\tTime 0.327 (0.343)\tData 0.000 (0.035)\tLoss 4.4174 (3.7288)\tPrec@1 71.875 (75.284)\tPrec@5 95.312 (97.017)\n",
      "Epoch: [13][20/97], lr: 0.01000\tTime 0.333 (0.338)\tData 0.000 (0.027)\tLoss 4.3019 (3.7244)\tPrec@1 70.312 (75.186)\tPrec@5 96.094 (96.875)\n",
      "Epoch: [13][30/97], lr: 0.01000\tTime 0.326 (0.335)\tData 0.000 (0.024)\tLoss 3.9961 (3.7892)\tPrec@1 74.219 (74.824)\tPrec@5 98.438 (96.925)\n",
      "Epoch: [13][40/97], lr: 0.01000\tTime 0.329 (0.334)\tData 0.000 (0.022)\tLoss 4.5865 (3.8161)\tPrec@1 69.531 (74.390)\tPrec@5 95.312 (96.894)\n",
      "Epoch: [13][50/97], lr: 0.01000\tTime 0.330 (0.333)\tData 0.000 (0.021)\tLoss 2.8631 (3.7664)\tPrec@1 81.250 (74.602)\tPrec@5 100.000 (97.059)\n",
      "Epoch: [13][60/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 3.5539 (3.7964)\tPrec@1 76.562 (74.449)\tPrec@5 94.531 (97.003)\n",
      "Epoch: [13][70/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 3.5352 (3.7739)\tPrec@1 74.219 (74.560)\tPrec@5 96.875 (97.106)\n",
      "Epoch: [13][80/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 3.3382 (3.7530)\tPrec@1 75.781 (74.653)\tPrec@5 96.094 (97.145)\n",
      "Epoch: [13][90/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.019)\tLoss 3.2771 (3.7542)\tPrec@1 77.344 (74.700)\tPrec@5 97.656 (97.124)\n",
      "Epoch: [13][96/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 3.2407 (3.7469)\tPrec@1 78.814 (74.738)\tPrec@5 99.153 (97.130)\n",
      "Test: [0/100]\tTime 0.266 (0.266)\tLoss 12.7726 (12.7726)\tPrec@1 31.000 (31.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 10.3663 (11.8699)\tPrec@1 42.000 (34.636)\tPrec@5 89.000 (92.000)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 10.7316 (11.9638)\tPrec@1 36.000 (33.619)\tPrec@5 95.000 (91.905)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 10.8274 (11.8169)\tPrec@1 41.000 (34.548)\tPrec@5 91.000 (91.742)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 12.4578 (11.8960)\tPrec@1 30.000 (34.098)\tPrec@5 88.000 (91.415)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 11.7311 (11.8004)\tPrec@1 35.000 (34.588)\tPrec@5 92.000 (91.706)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 10.1355 (11.7760)\tPrec@1 39.000 (34.295)\tPrec@5 95.000 (91.607)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 11.6020 (11.7467)\tPrec@1 37.000 (34.408)\tPrec@5 93.000 (91.535)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 10.6067 (11.7021)\tPrec@1 44.000 (34.728)\tPrec@5 93.000 (91.790)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.8295 (11.7607)\tPrec@1 34.000 (34.385)\tPrec@5 95.000 (91.747)\n",
      "val Results: Prec@1 34.350 Prec@5 91.780 Loss 11.78645\n",
      "val Class Accuracy: [0.898,0.991,0.686,0.516,0.254,0.018,0.065,0.007,0.000,0.000]\n",
      "Best Prec@1: 36.800\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [14][0/97], lr: 0.01000\tTime 0.393 (0.393)\tData 0.209 (0.209)\tLoss 4.3161 (4.3161)\tPrec@1 71.875 (71.875)\tPrec@5 96.875 (96.875)\n",
      "Epoch: [14][10/97], lr: 0.01000\tTime 0.334 (0.339)\tData 0.000 (0.034)\tLoss 3.8218 (3.6908)\tPrec@1 72.656 (74.432)\tPrec@5 96.875 (97.230)\n",
      "Epoch: [14][20/97], lr: 0.01000\tTime 0.321 (0.334)\tData 0.000 (0.026)\tLoss 2.6222 (3.5793)\tPrec@1 85.156 (75.409)\tPrec@5 97.656 (97.359)\n",
      "Epoch: [14][30/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.023)\tLoss 3.3135 (3.6618)\tPrec@1 75.000 (74.899)\tPrec@5 100.000 (97.404)\n",
      "Epoch: [14][40/97], lr: 0.01000\tTime 0.334 (0.332)\tData 0.000 (0.022)\tLoss 3.7411 (3.6090)\tPrec@1 75.781 (75.343)\tPrec@5 97.656 (97.542)\n",
      "Epoch: [14][50/97], lr: 0.01000\tTime 0.328 (0.332)\tData 0.000 (0.021)\tLoss 3.9672 (3.6088)\tPrec@1 71.875 (75.276)\tPrec@5 96.094 (97.503)\n",
      "Epoch: [14][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 4.3326 (3.6251)\tPrec@1 67.969 (75.243)\tPrec@5 96.094 (97.451)\n",
      "Epoch: [14][70/97], lr: 0.01000\tTime 0.335 (0.332)\tData 0.000 (0.020)\tLoss 2.9524 (3.6269)\tPrec@1 80.469 (75.264)\tPrec@5 96.875 (97.359)\n",
      "Epoch: [14][80/97], lr: 0.01000\tTime 0.329 (0.331)\tData 0.000 (0.019)\tLoss 4.1395 (3.6304)\tPrec@1 73.438 (75.395)\tPrec@5 94.531 (97.251)\n",
      "Epoch: [14][90/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.019)\tLoss 4.6706 (3.6373)\tPrec@1 72.656 (75.489)\tPrec@5 94.531 (97.244)\n",
      "Epoch: [14][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 3.9959 (3.6575)\tPrec@1 77.966 (75.375)\tPrec@5 94.068 (97.179)\n",
      "Test: [0/100]\tTime 0.235 (0.235)\tLoss 11.8125 (11.8125)\tPrec@1 40.000 (40.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 8.7020 (10.7433)\tPrec@1 53.000 (41.182)\tPrec@5 93.000 (91.000)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 9.7778 (10.7940)\tPrec@1 42.000 (40.571)\tPrec@5 95.000 (91.381)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 10.0060 (10.7388)\tPrec@1 43.000 (40.387)\tPrec@5 88.000 (91.258)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.7884 (10.7838)\tPrec@1 42.000 (40.000)\tPrec@5 90.000 (91.341)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 10.0690 (10.6781)\tPrec@1 42.000 (40.627)\tPrec@5 96.000 (91.608)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 9.0995 (10.6319)\tPrec@1 49.000 (40.639)\tPrec@5 92.000 (91.525)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 10.0534 (10.6034)\tPrec@1 44.000 (40.761)\tPrec@5 93.000 (91.620)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.3456 (10.5617)\tPrec@1 48.000 (40.864)\tPrec@5 92.000 (91.716)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.0386 (10.6302)\tPrec@1 42.000 (40.593)\tPrec@5 90.000 (91.538)\n",
      "val Results: Prec@1 40.420 Prec@5 91.660 Loss 10.66510\n",
      "val Class Accuracy: [0.925,0.989,0.458,0.263,0.663,0.107,0.482,0.149,0.000,0.006]\n",
      "Best Prec@1: 40.420\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [15][0/97], lr: 0.01000\tTime 0.435 (0.435)\tData 0.220 (0.220)\tLoss 3.7212 (3.7212)\tPrec@1 75.000 (75.000)\tPrec@5 93.750 (93.750)\n",
      "Epoch: [15][10/97], lr: 0.01000\tTime 0.325 (0.345)\tData 0.000 (0.035)\tLoss 3.1379 (3.3469)\tPrec@1 78.906 (77.912)\tPrec@5 96.875 (97.514)\n",
      "Epoch: [15][20/97], lr: 0.01000\tTime 0.339 (0.338)\tData 0.000 (0.027)\tLoss 3.4625 (3.3798)\tPrec@1 77.344 (77.790)\tPrec@5 96.875 (97.545)\n",
      "Epoch: [15][30/97], lr: 0.01000\tTime 0.326 (0.335)\tData 0.000 (0.023)\tLoss 3.6571 (3.4146)\tPrec@1 75.781 (76.865)\tPrec@5 98.438 (97.455)\n",
      "Epoch: [15][40/97], lr: 0.01000\tTime 0.327 (0.335)\tData 0.000 (0.022)\tLoss 3.1335 (3.4784)\tPrec@1 78.906 (76.467)\tPrec@5 93.750 (97.485)\n",
      "Epoch: [15][50/97], lr: 0.01000\tTime 0.329 (0.334)\tData 0.000 (0.021)\tLoss 4.0504 (3.5028)\tPrec@1 74.219 (76.394)\tPrec@5 96.875 (97.580)\n",
      "Epoch: [15][60/97], lr: 0.01000\tTime 0.330 (0.334)\tData 0.000 (0.020)\tLoss 3.5234 (3.5206)\tPrec@1 75.781 (76.242)\tPrec@5 97.656 (97.464)\n",
      "Epoch: [15][70/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.020)\tLoss 4.8588 (3.5327)\tPrec@1 67.969 (76.298)\tPrec@5 96.875 (97.502)\n",
      "Epoch: [15][80/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.020)\tLoss 3.5380 (3.5476)\tPrec@1 78.125 (76.321)\tPrec@5 96.875 (97.367)\n",
      "Epoch: [15][90/97], lr: 0.01000\tTime 0.322 (0.333)\tData 0.000 (0.019)\tLoss 3.5385 (3.5546)\tPrec@1 72.656 (76.142)\tPrec@5 98.438 (97.433)\n",
      "Epoch: [15][96/97], lr: 0.01000\tTime 0.317 (0.332)\tData 0.000 (0.020)\tLoss 3.1182 (3.5575)\tPrec@1 75.424 (75.995)\tPrec@5 99.153 (97.517)\n",
      "Test: [0/100]\tTime 0.290 (0.290)\tLoss 12.1629 (12.1629)\tPrec@1 39.000 (39.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 8.7701 (11.2463)\tPrec@1 52.000 (38.273)\tPrec@5 95.000 (90.545)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 10.6325 (11.3926)\tPrec@1 32.000 (36.762)\tPrec@5 95.000 (90.238)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 10.5644 (11.3347)\tPrec@1 39.000 (37.000)\tPrec@5 87.000 (89.742)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 12.0773 (11.4235)\tPrec@1 32.000 (36.000)\tPrec@5 85.000 (89.268)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 10.9860 (11.2882)\tPrec@1 38.000 (36.980)\tPrec@5 87.000 (89.451)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 9.8815 (11.2454)\tPrec@1 45.000 (37.000)\tPrec@5 89.000 (89.311)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 10.9711 (11.2222)\tPrec@1 40.000 (37.070)\tPrec@5 91.000 (89.239)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 9.8967 (11.1616)\tPrec@1 46.000 (37.457)\tPrec@5 89.000 (89.383)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.8134 (11.2085)\tPrec@1 37.000 (37.297)\tPrec@5 89.000 (89.275)\n",
      "val Results: Prec@1 37.310 Prec@5 89.360 Loss 11.23303\n",
      "val Class Accuracy: [0.934,0.987,0.379,0.617,0.274,0.064,0.466,0.010,0.000,0.000]\n",
      "Best Prec@1: 40.420\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [16][0/97], lr: 0.01000\tTime 0.428 (0.428)\tData 0.217 (0.217)\tLoss 4.0261 (4.0261)\tPrec@1 76.562 (76.562)\tPrec@5 96.875 (96.875)\n",
      "Epoch: [16][10/97], lr: 0.01000\tTime 0.338 (0.344)\tData 0.000 (0.034)\tLoss 3.4192 (3.6310)\tPrec@1 75.000 (75.710)\tPrec@5 96.094 (96.094)\n",
      "Epoch: [16][20/97], lr: 0.01000\tTime 0.356 (0.342)\tData 0.000 (0.026)\tLoss 3.3488 (3.6347)\tPrec@1 82.031 (76.153)\tPrec@5 97.656 (96.429)\n",
      "Epoch: [16][30/97], lr: 0.01000\tTime 0.329 (0.340)\tData 0.000 (0.023)\tLoss 3.0570 (3.5553)\tPrec@1 77.344 (76.537)\tPrec@5 96.875 (96.951)\n",
      "Epoch: [16][40/97], lr: 0.01000\tTime 0.329 (0.338)\tData 0.000 (0.021)\tLoss 3.4818 (3.5441)\tPrec@1 75.781 (76.486)\tPrec@5 99.219 (97.218)\n",
      "Epoch: [16][50/97], lr: 0.01000\tTime 0.325 (0.336)\tData 0.000 (0.021)\tLoss 3.1871 (3.5478)\tPrec@1 77.344 (76.394)\tPrec@5 99.219 (97.243)\n",
      "Epoch: [16][60/97], lr: 0.01000\tTime 0.326 (0.335)\tData 0.000 (0.020)\tLoss 3.8774 (3.5279)\tPrec@1 72.656 (76.511)\tPrec@5 99.219 (97.426)\n",
      "Epoch: [16][70/97], lr: 0.01000\tTime 0.332 (0.335)\tData 0.000 (0.020)\tLoss 3.4578 (3.5400)\tPrec@1 80.469 (76.651)\tPrec@5 96.875 (97.524)\n",
      "Epoch: [16][80/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.019)\tLoss 3.6725 (3.5418)\tPrec@1 75.781 (76.505)\tPrec@5 97.656 (97.579)\n",
      "Epoch: [16][90/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.019)\tLoss 2.2523 (3.5110)\tPrec@1 85.156 (76.751)\tPrec@5 99.219 (97.588)\n",
      "Epoch: [16][96/97], lr: 0.01000\tTime 0.317 (0.333)\tData 0.000 (0.020)\tLoss 3.1780 (3.5071)\tPrec@1 79.661 (76.810)\tPrec@5 98.305 (97.606)\n",
      "Test: [0/100]\tTime 0.224 (0.224)\tLoss 11.7667 (11.7667)\tPrec@1 40.000 (40.000)\tPrec@5 89.000 (89.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 8.6190 (11.0571)\tPrec@1 55.000 (40.000)\tPrec@5 90.000 (86.818)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 10.3309 (11.0748)\tPrec@1 39.000 (39.857)\tPrec@5 88.000 (87.048)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 10.5558 (11.0385)\tPrec@1 44.000 (40.226)\tPrec@5 86.000 (86.387)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 11.4578 (11.1065)\tPrec@1 39.000 (40.073)\tPrec@5 83.000 (85.951)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 10.8686 (11.0219)\tPrec@1 38.000 (40.392)\tPrec@5 86.000 (86.098)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 9.4548 (11.0230)\tPrec@1 44.000 (40.049)\tPrec@5 91.000 (85.852)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 10.3955 (10.9944)\tPrec@1 45.000 (40.324)\tPrec@5 85.000 (85.859)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.9671 (10.9468)\tPrec@1 49.000 (40.642)\tPrec@5 84.000 (86.000)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.4809 (10.9875)\tPrec@1 39.000 (40.484)\tPrec@5 89.000 (86.132)\n",
      "val Results: Prec@1 40.370 Prec@5 86.090 Loss 11.01591\n",
      "val Class Accuracy: [0.804,0.981,0.678,0.644,0.200,0.003,0.722,0.000,0.005,0.000]\n",
      "Best Prec@1: 40.420\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [17][0/97], lr: 0.01000\tTime 0.435 (0.435)\tData 0.199 (0.199)\tLoss 2.7510 (2.7510)\tPrec@1 80.469 (80.469)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [17][10/97], lr: 0.01000\tTime 0.326 (0.344)\tData 0.000 (0.033)\tLoss 3.2663 (3.3539)\tPrec@1 80.469 (77.628)\tPrec@5 94.531 (97.443)\n",
      "Epoch: [17][20/97], lr: 0.01000\tTime 0.334 (0.337)\tData 0.000 (0.025)\tLoss 3.0182 (3.3189)\tPrec@1 83.594 (78.051)\tPrec@5 97.656 (97.433)\n",
      "Epoch: [17][30/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.023)\tLoss 2.7292 (3.3450)\tPrec@1 81.250 (77.823)\tPrec@5 98.438 (97.404)\n",
      "Epoch: [17][40/97], lr: 0.01000\tTime 0.328 (0.334)\tData 0.000 (0.021)\tLoss 3.3197 (3.3015)\tPrec@1 75.781 (78.087)\tPrec@5 98.438 (97.542)\n",
      "Epoch: [17][50/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 3.6216 (3.3577)\tPrec@1 75.781 (77.711)\tPrec@5 98.438 (97.641)\n",
      "Epoch: [17][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 3.7749 (3.4022)\tPrec@1 72.656 (77.305)\tPrec@5 98.438 (97.579)\n",
      "Epoch: [17][70/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 3.6435 (3.4717)\tPrec@1 77.344 (76.816)\tPrec@5 98.438 (97.634)\n",
      "Epoch: [17][80/97], lr: 0.01000\tTime 0.328 (0.332)\tData 0.000 (0.019)\tLoss 3.7031 (3.4991)\tPrec@1 77.344 (76.775)\tPrec@5 96.875 (97.579)\n",
      "Epoch: [17][90/97], lr: 0.01000\tTime 0.328 (0.332)\tData 0.000 (0.019)\tLoss 3.5729 (3.4892)\tPrec@1 75.781 (76.940)\tPrec@5 96.094 (97.665)\n",
      "Epoch: [17][96/97], lr: 0.01000\tTime 0.314 (0.331)\tData 0.000 (0.020)\tLoss 2.6526 (3.4717)\tPrec@1 83.898 (76.971)\tPrec@5 97.458 (97.727)\n",
      "Test: [0/100]\tTime 0.287 (0.287)\tLoss 11.9913 (11.9913)\tPrec@1 34.000 (34.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 10.3878 (11.6144)\tPrec@1 42.000 (36.545)\tPrec@5 90.000 (92.545)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 9.9930 (11.6719)\tPrec@1 41.000 (35.905)\tPrec@5 95.000 (92.286)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 10.7074 (11.6195)\tPrec@1 39.000 (36.387)\tPrec@5 91.000 (91.645)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 11.9180 (11.6430)\tPrec@1 35.000 (36.195)\tPrec@5 89.000 (91.585)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 11.2743 (11.5430)\tPrec@1 41.000 (36.843)\tPrec@5 89.000 (91.725)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 9.6547 (11.5057)\tPrec@1 45.000 (36.721)\tPrec@5 94.000 (91.787)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 11.2368 (11.4611)\tPrec@1 37.000 (37.000)\tPrec@5 96.000 (91.873)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 10.2161 (11.4161)\tPrec@1 46.000 (37.284)\tPrec@5 92.000 (92.012)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.7518 (11.4764)\tPrec@1 31.000 (36.868)\tPrec@5 95.000 (92.044)\n",
      "val Results: Prec@1 36.740 Prec@5 92.050 Loss 11.50318\n",
      "val Class Accuracy: [0.964,0.982,0.504,0.644,0.416,0.043,0.096,0.014,0.011,0.000]\n",
      "Best Prec@1: 40.420\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [18][0/97], lr: 0.01000\tTime 0.412 (0.412)\tData 0.187 (0.187)\tLoss 3.1219 (3.1219)\tPrec@1 84.375 (84.375)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [18][10/97], lr: 0.01000\tTime 0.327 (0.341)\tData 0.000 (0.032)\tLoss 3.5308 (3.2737)\tPrec@1 76.562 (79.190)\tPrec@5 100.000 (98.082)\n",
      "Epoch: [18][20/97], lr: 0.01000\tTime 0.325 (0.335)\tData 0.000 (0.025)\tLoss 3.6759 (3.2476)\tPrec@1 77.344 (79.241)\tPrec@5 96.094 (97.545)\n",
      "Epoch: [18][30/97], lr: 0.01000\tTime 0.327 (0.335)\tData 0.000 (0.022)\tLoss 3.0731 (3.3575)\tPrec@1 78.906 (78.075)\tPrec@5 99.219 (97.530)\n",
      "Epoch: [18][40/97], lr: 0.01000\tTime 0.333 (0.333)\tData 0.000 (0.021)\tLoss 4.0107 (3.4000)\tPrec@1 73.438 (77.687)\tPrec@5 97.656 (97.523)\n",
      "Epoch: [18][50/97], lr: 0.01000\tTime 0.328 (0.333)\tData 0.000 (0.020)\tLoss 3.7588 (3.4122)\tPrec@1 71.094 (77.405)\tPrec@5 94.531 (97.442)\n",
      "Epoch: [18][60/97], lr: 0.01000\tTime 0.329 (0.333)\tData 0.000 (0.020)\tLoss 3.0785 (3.4128)\tPrec@1 80.469 (77.446)\tPrec@5 99.219 (97.515)\n",
      "Epoch: [18][70/97], lr: 0.01000\tTime 0.337 (0.333)\tData 0.000 (0.019)\tLoss 3.6598 (3.4250)\tPrec@1 76.562 (77.311)\tPrec@5 95.312 (97.491)\n",
      "Epoch: [18][80/97], lr: 0.01000\tTime 0.330 (0.332)\tData 0.000 (0.019)\tLoss 3.5824 (3.4432)\tPrec@1 75.781 (77.064)\tPrec@5 97.656 (97.521)\n",
      "Epoch: [18][90/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.019)\tLoss 3.8574 (3.4585)\tPrec@1 69.531 (76.854)\tPrec@5 97.656 (97.579)\n",
      "Epoch: [18][96/97], lr: 0.01000\tTime 0.322 (0.332)\tData 0.000 (0.020)\tLoss 2.5920 (3.4679)\tPrec@1 84.746 (76.818)\tPrec@5 98.305 (97.606)\n",
      "Test: [0/100]\tTime 0.250 (0.250)\tLoss 10.9007 (10.9007)\tPrec@1 43.000 (43.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 8.2253 (9.9533)\tPrec@1 54.000 (45.455)\tPrec@5 97.000 (94.545)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 9.3655 (9.8904)\tPrec@1 47.000 (45.429)\tPrec@5 97.000 (94.905)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 9.4786 (9.8581)\tPrec@1 46.000 (45.290)\tPrec@5 94.000 (94.484)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.5366 (9.8982)\tPrec@1 41.000 (44.878)\tPrec@5 94.000 (94.439)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.7970 (9.7758)\tPrec@1 44.000 (45.529)\tPrec@5 92.000 (94.412)\n",
      "Test: [60/100]\tTime 0.074 (0.076)\tLoss 8.0828 (9.7429)\tPrec@1 49.000 (45.656)\tPrec@5 95.000 (94.525)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 9.2233 (9.7396)\tPrec@1 50.000 (45.592)\tPrec@5 97.000 (94.451)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 9.0619 (9.7158)\tPrec@1 52.000 (45.556)\tPrec@5 94.000 (94.580)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.6734 (9.7791)\tPrec@1 46.000 (45.165)\tPrec@5 94.000 (94.418)\n",
      "val Results: Prec@1 44.880 Prec@5 94.430 Loss 9.82562\n",
      "val Class Accuracy: [0.877,0.925,0.600,0.526,0.804,0.150,0.471,0.003,0.008,0.124]\n",
      "Best Prec@1: 44.880\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [19][0/97], lr: 0.01000\tTime 0.445 (0.445)\tData 0.192 (0.192)\tLoss 3.6628 (3.6628)\tPrec@1 78.906 (78.906)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [19][10/97], lr: 0.01000\tTime 0.326 (0.348)\tData 0.000 (0.032)\tLoss 3.3060 (3.4466)\tPrec@1 77.344 (77.770)\tPrec@5 99.219 (98.509)\n",
      "Epoch: [19][20/97], lr: 0.01000\tTime 0.356 (0.342)\tData 0.000 (0.025)\tLoss 2.8297 (3.3862)\tPrec@1 78.906 (77.530)\tPrec@5 99.219 (98.363)\n",
      "Epoch: [19][30/97], lr: 0.01000\tTime 0.322 (0.338)\tData 0.000 (0.023)\tLoss 2.5772 (3.3483)\tPrec@1 83.594 (77.923)\tPrec@5 99.219 (98.387)\n",
      "Epoch: [19][40/97], lr: 0.01000\tTime 0.328 (0.336)\tData 0.000 (0.021)\tLoss 2.9998 (3.3391)\tPrec@1 78.125 (77.896)\tPrec@5 99.219 (98.457)\n",
      "Epoch: [19][50/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.020)\tLoss 2.8430 (3.3565)\tPrec@1 80.469 (78.002)\tPrec@5 96.875 (98.238)\n",
      "Epoch: [19][60/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.020)\tLoss 3.6104 (3.3736)\tPrec@1 78.906 (78.023)\tPrec@5 96.875 (98.220)\n",
      "Epoch: [19][70/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.020)\tLoss 2.9287 (3.3522)\tPrec@1 82.031 (78.114)\tPrec@5 99.219 (98.217)\n",
      "Epoch: [19][80/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.019)\tLoss 4.0103 (3.3483)\tPrec@1 73.438 (78.067)\tPrec@5 97.656 (98.167)\n",
      "Epoch: [19][90/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.019)\tLoss 3.6333 (3.3744)\tPrec@1 78.125 (78.022)\tPrec@5 97.656 (98.077)\n",
      "Epoch: [19][96/97], lr: 0.01000\tTime 0.321 (0.332)\tData 0.000 (0.020)\tLoss 2.6805 (3.3539)\tPrec@1 83.051 (78.188)\tPrec@5 99.153 (98.130)\n",
      "Test: [0/100]\tTime 0.265 (0.265)\tLoss 11.4954 (11.4954)\tPrec@1 39.000 (39.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 8.6372 (10.5931)\tPrec@1 54.000 (43.091)\tPrec@5 94.000 (94.364)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 9.5853 (10.6271)\tPrec@1 46.000 (42.810)\tPrec@5 97.000 (94.238)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 9.5864 (10.5513)\tPrec@1 47.000 (43.258)\tPrec@5 96.000 (94.065)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 11.0921 (10.5986)\tPrec@1 44.000 (43.244)\tPrec@5 94.000 (94.000)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 10.2180 (10.4919)\tPrec@1 42.000 (44.098)\tPrec@5 96.000 (94.275)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.8225 (10.4717)\tPrec@1 50.000 (43.721)\tPrec@5 96.000 (94.311)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 10.2537 (10.4671)\tPrec@1 43.000 (43.676)\tPrec@5 92.000 (94.254)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.8738 (10.4427)\tPrec@1 49.000 (43.667)\tPrec@5 91.000 (94.309)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.7457 (10.4964)\tPrec@1 42.000 (43.484)\tPrec@5 95.000 (94.231)\n",
      "val Results: Prec@1 43.330 Prec@5 94.320 Loss 10.53990\n",
      "val Class Accuracy: [0.937,0.987,0.713,0.492,0.535,0.137,0.491,0.041,0.000,0.000]\n",
      "Best Prec@1: 44.880\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [20][0/97], lr: 0.01000\tTime 0.440 (0.440)\tData 0.208 (0.208)\tLoss 3.0340 (3.0340)\tPrec@1 81.250 (81.250)\tPrec@5 96.875 (96.875)\n",
      "Epoch: [20][10/97], lr: 0.01000\tTime 0.328 (0.344)\tData 0.000 (0.034)\tLoss 2.9383 (3.0917)\tPrec@1 76.562 (79.190)\tPrec@5 99.219 (98.153)\n",
      "Epoch: [20][20/97], lr: 0.01000\tTime 0.326 (0.337)\tData 0.000 (0.026)\tLoss 3.7216 (3.2724)\tPrec@1 75.781 (78.609)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [20][30/97], lr: 0.01000\tTime 0.325 (0.335)\tData 0.000 (0.023)\tLoss 2.5824 (3.2028)\tPrec@1 82.031 (79.108)\tPrec@5 96.875 (97.908)\n",
      "Epoch: [20][40/97], lr: 0.01000\tTime 0.332 (0.334)\tData 0.000 (0.022)\tLoss 3.1954 (3.2595)\tPrec@1 78.906 (78.258)\tPrec@5 98.438 (98.037)\n",
      "Epoch: [20][50/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 3.8545 (3.2891)\tPrec@1 75.000 (78.140)\tPrec@5 95.312 (98.024)\n",
      "Epoch: [20][60/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 4.0240 (3.3420)\tPrec@1 75.000 (77.984)\tPrec@5 98.438 (97.964)\n",
      "Epoch: [20][70/97], lr: 0.01000\tTime 0.331 (0.333)\tData 0.000 (0.020)\tLoss 3.0846 (3.3591)\tPrec@1 80.469 (77.883)\tPrec@5 100.000 (97.920)\n",
      "Epoch: [20][80/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.019)\tLoss 3.9470 (3.3854)\tPrec@1 72.656 (77.720)\tPrec@5 100.000 (97.936)\n",
      "Epoch: [20][90/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.019)\tLoss 3.3224 (3.3946)\tPrec@1 76.562 (77.713)\tPrec@5 98.438 (97.897)\n",
      "Epoch: [20][96/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 2.5677 (3.3688)\tPrec@1 84.746 (77.882)\tPrec@5 100.000 (97.961)\n",
      "Test: [0/100]\tTime 0.261 (0.261)\tLoss 13.1590 (13.1590)\tPrec@1 33.000 (33.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 10.0997 (11.9017)\tPrec@1 48.000 (38.545)\tPrec@5 94.000 (92.909)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 10.3563 (11.9662)\tPrec@1 48.000 (37.571)\tPrec@5 94.000 (92.952)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 11.0331 (11.8967)\tPrec@1 39.000 (37.839)\tPrec@5 92.000 (92.613)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 12.3613 (11.9005)\tPrec@1 32.000 (37.707)\tPrec@5 92.000 (92.829)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 11.9409 (11.8123)\tPrec@1 37.000 (38.000)\tPrec@5 93.000 (93.118)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 9.8155 (11.7769)\tPrec@1 50.000 (37.787)\tPrec@5 94.000 (93.082)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 11.5840 (11.7520)\tPrec@1 36.000 (37.845)\tPrec@5 96.000 (92.746)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 10.5802 (11.7192)\tPrec@1 45.000 (37.827)\tPrec@5 89.000 (92.852)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.2754 (11.7907)\tPrec@1 42.000 (37.484)\tPrec@5 93.000 (92.747)\n",
      "val Results: Prec@1 37.360 Prec@5 92.670 Loss 11.82053\n",
      "val Class Accuracy: [0.967,0.939,0.785,0.426,0.316,0.159,0.078,0.066,0.000,0.000]\n",
      "Best Prec@1: 44.880\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [21][0/97], lr: 0.01000\tTime 0.440 (0.440)\tData 0.218 (0.218)\tLoss 3.2901 (3.2901)\tPrec@1 77.344 (77.344)\tPrec@5 96.875 (96.875)\n",
      "Epoch: [21][10/97], lr: 0.01000\tTime 0.327 (0.347)\tData 0.000 (0.035)\tLoss 3.1424 (3.3741)\tPrec@1 75.000 (77.486)\tPrec@5 98.438 (98.011)\n",
      "Epoch: [21][20/97], lr: 0.01000\tTime 0.333 (0.339)\tData 0.000 (0.026)\tLoss 3.6554 (3.2596)\tPrec@1 77.344 (78.497)\tPrec@5 99.219 (98.363)\n",
      "Epoch: [21][30/97], lr: 0.01000\tTime 0.330 (0.336)\tData 0.000 (0.023)\tLoss 3.7373 (3.2398)\tPrec@1 75.000 (78.679)\tPrec@5 97.656 (98.286)\n",
      "Epoch: [21][40/97], lr: 0.01000\tTime 0.327 (0.335)\tData 0.000 (0.022)\tLoss 2.7344 (3.3298)\tPrec@1 78.906 (78.049)\tPrec@5 98.438 (98.018)\n",
      "Epoch: [21][50/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.021)\tLoss 3.1257 (3.2961)\tPrec@1 76.562 (78.186)\tPrec@5 98.438 (97.947)\n",
      "Epoch: [21][60/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 2.3821 (3.2951)\tPrec@1 86.719 (78.240)\tPrec@5 99.219 (97.861)\n",
      "Epoch: [21][70/97], lr: 0.01000\tTime 0.329 (0.333)\tData 0.000 (0.020)\tLoss 3.1559 (3.2905)\tPrec@1 81.250 (78.257)\tPrec@5 98.438 (97.964)\n",
      "Epoch: [21][80/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 4.1520 (3.2676)\tPrec@1 72.656 (78.424)\tPrec@5 98.438 (98.003)\n",
      "Epoch: [21][90/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.019)\tLoss 3.4132 (3.2855)\tPrec@1 76.562 (78.322)\tPrec@5 98.438 (97.931)\n",
      "Epoch: [21][96/97], lr: 0.01000\tTime 0.330 (0.332)\tData 0.000 (0.020)\tLoss 3.6902 (3.2982)\tPrec@1 77.119 (78.196)\tPrec@5 98.305 (97.928)\n",
      "Test: [0/100]\tTime 0.280 (0.280)\tLoss 11.0765 (11.0765)\tPrec@1 41.000 (41.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 8.8336 (10.4184)\tPrec@1 51.000 (44.182)\tPrec@5 93.000 (94.818)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 9.2810 (10.4790)\tPrec@1 47.000 (43.667)\tPrec@5 95.000 (94.381)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 9.8440 (10.4189)\tPrec@1 43.000 (43.387)\tPrec@5 98.000 (94.258)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 10.7089 (10.5013)\tPrec@1 40.000 (42.829)\tPrec@5 96.000 (94.098)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 10.3696 (10.3683)\tPrec@1 44.000 (43.706)\tPrec@5 94.000 (94.216)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.7380 (10.3275)\tPrec@1 52.000 (43.721)\tPrec@5 95.000 (94.361)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 9.3806 (10.2939)\tPrec@1 48.000 (44.155)\tPrec@5 99.000 (94.535)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.5719 (10.2690)\tPrec@1 51.000 (44.346)\tPrec@5 94.000 (94.765)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.9490 (10.3221)\tPrec@1 37.000 (44.121)\tPrec@5 95.000 (94.604)\n",
      "val Results: Prec@1 44.190 Prec@5 94.690 Loss 10.33640\n",
      "val Class Accuracy: [0.910,0.984,0.646,0.806,0.374,0.077,0.466,0.138,0.017,0.001]\n",
      "Best Prec@1: 44.880\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [22][0/97], lr: 0.01000\tTime 0.423 (0.423)\tData 0.207 (0.207)\tLoss 3.2844 (3.2844)\tPrec@1 78.906 (78.906)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [22][10/97], lr: 0.01000\tTime 0.329 (0.343)\tData 0.000 (0.034)\tLoss 3.4630 (3.3476)\tPrec@1 75.000 (78.835)\tPrec@5 99.219 (98.580)\n",
      "Epoch: [22][20/97], lr: 0.01000\tTime 0.325 (0.336)\tData 0.000 (0.026)\tLoss 3.5352 (3.3669)\tPrec@1 77.344 (78.423)\tPrec@5 97.656 (98.214)\n",
      "Epoch: [22][30/97], lr: 0.01000\tTime 0.330 (0.335)\tData 0.000 (0.023)\tLoss 3.4977 (3.3369)\tPrec@1 78.906 (78.856)\tPrec@5 96.094 (98.160)\n",
      "Epoch: [22][40/97], lr: 0.01000\tTime 0.334 (0.336)\tData 0.000 (0.022)\tLoss 3.0357 (3.2623)\tPrec@1 78.125 (79.154)\tPrec@5 99.219 (98.304)\n",
      "Epoch: [22][50/97], lr: 0.01000\tTime 0.338 (0.336)\tData 0.000 (0.021)\tLoss 2.6790 (3.2165)\tPrec@1 80.469 (79.274)\tPrec@5 98.438 (98.238)\n",
      "Epoch: [22][60/97], lr: 0.01000\tTime 0.325 (0.335)\tData 0.000 (0.020)\tLoss 3.9575 (3.2605)\tPrec@1 75.781 (78.970)\tPrec@5 98.438 (98.194)\n",
      "Epoch: [22][70/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.020)\tLoss 3.0111 (3.2626)\tPrec@1 81.250 (78.983)\tPrec@5 97.656 (98.074)\n",
      "Epoch: [22][80/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.019)\tLoss 3.0335 (3.2665)\tPrec@1 79.688 (78.771)\tPrec@5 98.438 (98.042)\n",
      "Epoch: [22][90/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.019)\tLoss 2.9493 (3.2427)\tPrec@1 82.031 (78.872)\tPrec@5 99.219 (98.094)\n",
      "Epoch: [22][96/97], lr: 0.01000\tTime 0.314 (0.333)\tData 0.000 (0.020)\tLoss 5.2559 (3.2623)\tPrec@1 65.254 (78.712)\tPrec@5 93.220 (98.065)\n",
      "Test: [0/100]\tTime 0.229 (0.229)\tLoss 12.6245 (12.6245)\tPrec@1 38.000 (38.000)\tPrec@5 90.000 (90.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 9.8254 (11.6087)\tPrec@1 49.000 (39.091)\tPrec@5 93.000 (92.182)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 10.3613 (11.7322)\tPrec@1 44.000 (38.238)\tPrec@5 91.000 (91.476)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 10.1230 (11.6644)\tPrec@1 49.000 (38.548)\tPrec@5 90.000 (90.871)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 12.3582 (11.7246)\tPrec@1 36.000 (38.390)\tPrec@5 86.000 (90.683)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 11.2415 (11.5544)\tPrec@1 41.000 (39.333)\tPrec@5 88.000 (90.725)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 10.1969 (11.5025)\tPrec@1 45.000 (39.475)\tPrec@5 91.000 (90.639)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 11.5475 (11.4857)\tPrec@1 41.000 (39.577)\tPrec@5 95.000 (90.845)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 10.0863 (11.4456)\tPrec@1 48.000 (39.827)\tPrec@5 96.000 (91.160)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 12.3572 (11.5070)\tPrec@1 36.000 (39.451)\tPrec@5 90.000 (91.143)\n",
      "val Results: Prec@1 39.470 Prec@5 91.130 Loss 11.54089\n",
      "val Class Accuracy: [0.938,0.995,0.254,0.627,0.629,0.225,0.252,0.014,0.000,0.013]\n",
      "Best Prec@1: 44.880\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [23][0/97], lr: 0.01000\tTime 0.398 (0.398)\tData 0.209 (0.209)\tLoss 2.9092 (2.9092)\tPrec@1 82.031 (82.031)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [23][10/97], lr: 0.01000\tTime 0.326 (0.339)\tData 0.000 (0.034)\tLoss 2.9728 (3.3170)\tPrec@1 82.812 (78.054)\tPrec@5 97.656 (98.153)\n",
      "Epoch: [23][20/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.026)\tLoss 3.1502 (3.4455)\tPrec@1 78.906 (77.121)\tPrec@5 95.312 (97.545)\n",
      "Epoch: [23][30/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.023)\tLoss 2.7006 (3.3883)\tPrec@1 82.031 (77.571)\tPrec@5 99.219 (97.757)\n",
      "Epoch: [23][40/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.022)\tLoss 3.0117 (3.3126)\tPrec@1 79.688 (78.201)\tPrec@5 97.656 (97.980)\n",
      "Epoch: [23][50/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.021)\tLoss 3.3281 (3.2962)\tPrec@1 78.906 (78.416)\tPrec@5 97.656 (97.978)\n",
      "Epoch: [23][60/97], lr: 0.01000\tTime 0.329 (0.331)\tData 0.000 (0.020)\tLoss 3.3139 (3.2672)\tPrec@1 77.344 (78.560)\tPrec@5 99.219 (98.015)\n",
      "Epoch: [23][70/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 3.6209 (3.2523)\tPrec@1 75.781 (78.686)\tPrec@5 97.656 (98.096)\n",
      "Epoch: [23][80/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.019)\tLoss 3.0993 (3.2875)\tPrec@1 82.812 (78.492)\tPrec@5 99.219 (98.100)\n",
      "Epoch: [23][90/97], lr: 0.01000\tTime 0.330 (0.331)\tData 0.000 (0.019)\tLoss 2.8719 (3.2775)\tPrec@1 81.250 (78.529)\tPrec@5 99.219 (98.154)\n",
      "Epoch: [23][96/97], lr: 0.01000\tTime 0.318 (0.330)\tData 0.000 (0.020)\tLoss 3.6448 (3.2951)\tPrec@1 74.576 (78.414)\tPrec@5 97.458 (98.106)\n",
      "Test: [0/100]\tTime 0.255 (0.255)\tLoss 11.4276 (11.4276)\tPrec@1 44.000 (44.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 8.3683 (10.5258)\tPrec@1 57.000 (45.909)\tPrec@5 93.000 (93.364)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 9.8889 (10.5983)\tPrec@1 45.000 (44.667)\tPrec@5 92.000 (93.143)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 9.9896 (10.5330)\tPrec@1 50.000 (45.452)\tPrec@5 92.000 (92.839)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 11.0341 (10.5847)\tPrec@1 44.000 (45.073)\tPrec@5 94.000 (92.341)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 9.9639 (10.4732)\tPrec@1 46.000 (45.510)\tPrec@5 92.000 (92.490)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.5235 (10.4014)\tPrec@1 56.000 (45.623)\tPrec@5 93.000 (92.557)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 9.8229 (10.3980)\tPrec@1 50.000 (45.592)\tPrec@5 96.000 (92.563)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.3267 (10.3673)\tPrec@1 50.000 (45.432)\tPrec@5 90.000 (92.691)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.7405 (10.4395)\tPrec@1 46.000 (45.363)\tPrec@5 90.000 (92.582)\n",
      "val Results: Prec@1 45.170 Prec@5 92.570 Loss 10.48728\n",
      "val Class Accuracy: [0.967,0.953,0.667,0.520,0.647,0.154,0.560,0.041,0.000,0.008]\n",
      "Best Prec@1: 45.170\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [24][0/97], lr: 0.01000\tTime 0.423 (0.423)\tData 0.210 (0.210)\tLoss 2.8057 (2.8057)\tPrec@1 83.594 (83.594)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [24][10/97], lr: 0.01000\tTime 0.327 (0.344)\tData 0.000 (0.034)\tLoss 3.4108 (3.1352)\tPrec@1 76.562 (79.688)\tPrec@5 96.875 (98.153)\n",
      "Epoch: [24][20/97], lr: 0.01000\tTime 0.332 (0.336)\tData 0.000 (0.026)\tLoss 3.1465 (3.1110)\tPrec@1 79.688 (79.501)\tPrec@5 100.000 (98.289)\n",
      "Epoch: [24][30/97], lr: 0.01000\tTime 0.326 (0.335)\tData 0.000 (0.023)\tLoss 3.4031 (3.1773)\tPrec@1 79.688 (79.183)\tPrec@5 98.438 (98.160)\n",
      "Epoch: [24][40/97], lr: 0.01000\tTime 0.327 (0.334)\tData 0.000 (0.022)\tLoss 3.0032 (3.2148)\tPrec@1 80.469 (79.002)\tPrec@5 99.219 (98.190)\n",
      "Epoch: [24][50/97], lr: 0.01000\tTime 0.331 (0.333)\tData 0.000 (0.021)\tLoss 3.6109 (3.2090)\tPrec@1 77.344 (79.167)\tPrec@5 96.094 (98.085)\n",
      "Epoch: [24][60/97], lr: 0.01000\tTime 0.328 (0.333)\tData 0.000 (0.020)\tLoss 3.0918 (3.2057)\tPrec@1 81.250 (79.073)\tPrec@5 96.875 (98.156)\n",
      "Epoch: [24][70/97], lr: 0.01000\tTime 0.344 (0.333)\tData 0.000 (0.020)\tLoss 3.7468 (3.1880)\tPrec@1 74.219 (79.137)\tPrec@5 98.438 (98.184)\n",
      "Epoch: [24][80/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.019)\tLoss 3.2601 (3.1730)\tPrec@1 78.906 (79.225)\tPrec@5 99.219 (98.177)\n",
      "Epoch: [24][90/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.019)\tLoss 3.8582 (3.1684)\tPrec@1 73.438 (79.215)\tPrec@5 96.094 (98.163)\n",
      "Epoch: [24][96/97], lr: 0.01000\tTime 0.333 (0.333)\tData 0.000 (0.020)\tLoss 3.3812 (3.1728)\tPrec@1 77.966 (79.236)\tPrec@5 97.458 (98.170)\n",
      "Test: [0/100]\tTime 0.275 (0.275)\tLoss 12.1045 (12.1045)\tPrec@1 36.000 (36.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 9.1176 (10.9533)\tPrec@1 54.000 (41.455)\tPrec@5 94.000 (93.727)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 10.3516 (11.0882)\tPrec@1 45.000 (41.143)\tPrec@5 89.000 (92.762)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 10.0370 (11.0729)\tPrec@1 46.000 (41.516)\tPrec@5 93.000 (92.774)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 12.0482 (11.0868)\tPrec@1 36.000 (41.341)\tPrec@5 93.000 (92.634)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 10.6754 (10.9924)\tPrec@1 43.000 (41.804)\tPrec@5 91.000 (92.706)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 10.0849 (10.9542)\tPrec@1 41.000 (41.672)\tPrec@5 93.000 (92.574)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 10.7818 (10.9332)\tPrec@1 43.000 (41.930)\tPrec@5 93.000 (92.662)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.7536 (10.9065)\tPrec@1 48.000 (42.086)\tPrec@5 92.000 (92.630)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.7201 (10.9811)\tPrec@1 40.000 (41.769)\tPrec@5 92.000 (92.626)\n",
      "val Results: Prec@1 41.600 Prec@5 92.650 Loss 11.02251\n",
      "val Class Accuracy: [0.954,0.990,0.341,0.290,0.692,0.234,0.610,0.027,0.000,0.022]\n",
      "Best Prec@1: 45.170\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [25][0/97], lr: 0.01000\tTime 0.415 (0.415)\tData 0.230 (0.230)\tLoss 2.6790 (2.6790)\tPrec@1 82.812 (82.812)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [25][10/97], lr: 0.01000\tTime 0.328 (0.344)\tData 0.000 (0.036)\tLoss 2.9332 (3.1262)\tPrec@1 81.250 (80.114)\tPrec@5 97.656 (98.082)\n",
      "Epoch: [25][20/97], lr: 0.01000\tTime 0.336 (0.338)\tData 0.000 (0.027)\tLoss 2.5994 (3.1529)\tPrec@1 81.250 (79.576)\tPrec@5 100.000 (98.214)\n",
      "Epoch: [25][30/97], lr: 0.01000\tTime 0.326 (0.335)\tData 0.000 (0.024)\tLoss 3.5780 (3.1301)\tPrec@1 76.562 (79.587)\tPrec@5 96.875 (98.185)\n",
      "Epoch: [25][40/97], lr: 0.01000\tTime 0.329 (0.335)\tData 0.000 (0.022)\tLoss 3.3101 (3.0972)\tPrec@1 77.344 (79.916)\tPrec@5 96.875 (98.095)\n",
      "Epoch: [25][50/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 2.6558 (3.1079)\tPrec@1 85.938 (80.009)\tPrec@5 98.438 (98.100)\n",
      "Epoch: [25][60/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 2.6679 (3.1054)\tPrec@1 84.375 (80.123)\tPrec@5 98.438 (98.079)\n",
      "Epoch: [25][70/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.020)\tLoss 2.3422 (3.0931)\tPrec@1 85.156 (80.139)\tPrec@5 98.438 (98.151)\n",
      "Epoch: [25][80/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 3.1589 (3.0912)\tPrec@1 78.906 (80.131)\tPrec@5 98.438 (98.196)\n",
      "Epoch: [25][90/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.020)\tLoss 3.2500 (3.1135)\tPrec@1 77.344 (80.057)\tPrec@5 97.656 (98.146)\n",
      "Epoch: [25][96/97], lr: 0.01000\tTime 0.318 (0.332)\tData 0.000 (0.020)\tLoss 3.6368 (3.1227)\tPrec@1 79.661 (79.961)\tPrec@5 98.305 (98.162)\n",
      "Test: [0/100]\tTime 0.279 (0.279)\tLoss 13.3390 (13.3390)\tPrec@1 30.000 (30.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 10.0796 (12.4541)\tPrec@1 48.000 (36.364)\tPrec@5 96.000 (93.091)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 11.3183 (12.6590)\tPrec@1 39.000 (34.905)\tPrec@5 94.000 (93.286)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 11.6453 (12.5484)\tPrec@1 37.000 (35.677)\tPrec@5 93.000 (92.484)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 13.0848 (12.5794)\tPrec@1 31.000 (35.366)\tPrec@5 92.000 (92.512)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 12.5647 (12.5439)\tPrec@1 35.000 (35.275)\tPrec@5 96.000 (92.961)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 10.5064 (12.5285)\tPrec@1 42.000 (35.066)\tPrec@5 95.000 (93.098)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 12.2277 (12.5072)\tPrec@1 37.000 (35.155)\tPrec@5 97.000 (93.042)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 11.5763 (12.4786)\tPrec@1 39.000 (35.148)\tPrec@5 90.000 (93.074)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 12.4131 (12.5353)\tPrec@1 36.000 (34.901)\tPrec@5 95.000 (93.066)\n",
      "val Results: Prec@1 34.860 Prec@5 93.070 Loss 12.55822\n",
      "val Class Accuracy: [0.980,0.975,0.785,0.195,0.153,0.092,0.260,0.046,0.000,0.000]\n",
      "Best Prec@1: 45.170\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [26][0/97], lr: 0.01000\tTime 0.380 (0.380)\tData 0.203 (0.203)\tLoss 3.4159 (3.4159)\tPrec@1 78.906 (78.906)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [26][10/97], lr: 0.01000\tTime 0.328 (0.339)\tData 0.000 (0.034)\tLoss 3.4078 (3.2600)\tPrec@1 79.688 (80.114)\tPrec@5 98.438 (98.153)\n",
      "Epoch: [26][20/97], lr: 0.01000\tTime 0.333 (0.334)\tData 0.000 (0.026)\tLoss 2.5607 (3.0703)\tPrec@1 83.594 (80.952)\tPrec@5 99.219 (98.586)\n",
      "Epoch: [26][30/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.023)\tLoss 3.5259 (3.0782)\tPrec@1 74.219 (80.721)\tPrec@5 96.875 (98.412)\n",
      "Epoch: [26][40/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.022)\tLoss 3.5344 (3.0914)\tPrec@1 78.125 (80.507)\tPrec@5 96.875 (98.399)\n",
      "Epoch: [26][50/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 3.0194 (3.1238)\tPrec@1 79.688 (80.270)\tPrec@5 98.438 (98.254)\n",
      "Epoch: [26][60/97], lr: 0.01000\tTime 0.331 (0.331)\tData 0.000 (0.020)\tLoss 2.8177 (3.1040)\tPrec@1 80.469 (80.277)\tPrec@5 97.656 (98.233)\n",
      "Epoch: [26][70/97], lr: 0.01000\tTime 0.327 (0.331)\tData 0.000 (0.020)\tLoss 3.3543 (3.1398)\tPrec@1 75.781 (79.853)\tPrec@5 97.656 (98.206)\n",
      "Epoch: [26][80/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.019)\tLoss 2.7315 (3.1225)\tPrec@1 81.250 (79.909)\tPrec@5 97.656 (98.254)\n",
      "Epoch: [26][90/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.019)\tLoss 3.8409 (3.1292)\tPrec@1 71.875 (79.911)\tPrec@5 98.438 (98.292)\n",
      "Epoch: [26][96/97], lr: 0.01000\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 3.3116 (3.1401)\tPrec@1 77.119 (79.776)\tPrec@5 98.305 (98.307)\n",
      "Test: [0/100]\tTime 0.269 (0.269)\tLoss 11.7229 (11.7229)\tPrec@1 36.000 (36.000)\tPrec@5 89.000 (89.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 8.6416 (10.5276)\tPrec@1 54.000 (43.545)\tPrec@5 92.000 (90.545)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 9.8804 (10.5771)\tPrec@1 42.000 (42.905)\tPrec@5 92.000 (90.952)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 10.0681 (10.5750)\tPrec@1 43.000 (42.516)\tPrec@5 92.000 (90.677)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 11.4597 (10.6304)\tPrec@1 38.000 (41.902)\tPrec@5 90.000 (90.927)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 10.3992 (10.5203)\tPrec@1 42.000 (42.392)\tPrec@5 91.000 (91.000)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.6576 (10.4820)\tPrec@1 54.000 (42.492)\tPrec@5 92.000 (91.000)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 9.9789 (10.4664)\tPrec@1 45.000 (42.606)\tPrec@5 91.000 (90.915)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.6975 (10.4371)\tPrec@1 47.000 (42.605)\tPrec@5 88.000 (90.963)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.8918 (10.4800)\tPrec@1 37.000 (42.418)\tPrec@5 92.000 (90.835)\n",
      "val Results: Prec@1 42.390 Prec@5 90.720 Loss 10.49493\n",
      "val Class Accuracy: [0.972,0.890,0.529,0.822,0.467,0.088,0.427,0.044,0.000,0.000]\n",
      "Best Prec@1: 45.170\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [27][0/97], lr: 0.01000\tTime 0.439 (0.439)\tData 0.222 (0.222)\tLoss 3.8377 (3.8377)\tPrec@1 75.781 (75.781)\tPrec@5 96.094 (96.094)\n",
      "Epoch: [27][10/97], lr: 0.01000\tTime 0.327 (0.345)\tData 0.000 (0.035)\tLoss 3.1551 (2.9245)\tPrec@1 81.250 (80.611)\tPrec@5 100.000 (98.793)\n",
      "Epoch: [27][20/97], lr: 0.01000\tTime 0.325 (0.337)\tData 0.000 (0.027)\tLoss 3.0217 (2.9922)\tPrec@1 79.688 (80.357)\tPrec@5 99.219 (98.735)\n",
      "Epoch: [27][30/97], lr: 0.01000\tTime 0.326 (0.335)\tData 0.000 (0.024)\tLoss 2.7351 (3.0546)\tPrec@1 83.594 (80.318)\tPrec@5 98.438 (98.614)\n",
      "Epoch: [27][40/97], lr: 0.01000\tTime 0.331 (0.334)\tData 0.000 (0.022)\tLoss 2.9836 (3.0007)\tPrec@1 82.812 (80.736)\tPrec@5 96.875 (98.476)\n",
      "Epoch: [27][50/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.021)\tLoss 2.8460 (3.0385)\tPrec@1 82.031 (80.407)\tPrec@5 96.875 (98.346)\n",
      "Epoch: [27][60/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 2.8429 (3.0337)\tPrec@1 84.375 (80.546)\tPrec@5 96.875 (98.348)\n",
      "Epoch: [27][70/97], lr: 0.01000\tTime 0.329 (0.332)\tData 0.000 (0.020)\tLoss 3.3557 (3.0438)\tPrec@1 79.688 (80.425)\tPrec@5 98.438 (98.360)\n",
      "Epoch: [27][80/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 3.2432 (3.0649)\tPrec@1 79.688 (80.334)\tPrec@5 95.312 (98.293)\n",
      "Epoch: [27][90/97], lr: 0.01000\tTime 0.329 (0.332)\tData 0.000 (0.019)\tLoss 3.6113 (3.1145)\tPrec@1 75.000 (79.911)\tPrec@5 96.875 (98.197)\n",
      "Epoch: [27][96/97], lr: 0.01000\tTime 0.314 (0.331)\tData 0.000 (0.020)\tLoss 2.9069 (3.1174)\tPrec@1 83.051 (79.865)\tPrec@5 97.458 (98.194)\n",
      "Test: [0/100]\tTime 0.266 (0.266)\tLoss 13.2478 (13.2478)\tPrec@1 32.000 (32.000)\tPrec@5 88.000 (88.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 10.2762 (11.9544)\tPrec@1 48.000 (38.455)\tPrec@5 88.000 (86.727)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 10.9492 (12.1273)\tPrec@1 42.000 (36.952)\tPrec@5 86.000 (86.048)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 11.1566 (12.0705)\tPrec@1 40.000 (37.290)\tPrec@5 85.000 (85.935)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 13.0277 (12.1391)\tPrec@1 33.000 (37.244)\tPrec@5 78.000 (85.805)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 11.7257 (11.9994)\tPrec@1 40.000 (37.902)\tPrec@5 81.000 (86.176)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 10.2159 (11.9655)\tPrec@1 44.000 (37.803)\tPrec@5 87.000 (86.213)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 11.6515 (11.9381)\tPrec@1 41.000 (37.944)\tPrec@5 86.000 (86.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 10.8339 (11.8914)\tPrec@1 45.000 (38.148)\tPrec@5 86.000 (86.457)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 12.4387 (11.9187)\tPrec@1 32.000 (37.890)\tPrec@5 87.000 (86.648)\n",
      "val Results: Prec@1 37.790 Prec@5 86.600 Loss 11.95797\n",
      "val Class Accuracy: [0.873,0.998,0.484,0.584,0.475,0.043,0.309,0.000,0.013,0.000]\n",
      "Best Prec@1: 45.170\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [28][0/97], lr: 0.01000\tTime 0.434 (0.434)\tData 0.223 (0.223)\tLoss 2.9188 (2.9188)\tPrec@1 82.031 (82.031)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [28][10/97], lr: 0.01000\tTime 0.336 (0.345)\tData 0.000 (0.035)\tLoss 2.7025 (3.0775)\tPrec@1 85.156 (80.895)\tPrec@5 96.875 (97.656)\n",
      "Epoch: [28][20/97], lr: 0.01000\tTime 0.326 (0.338)\tData 0.000 (0.026)\tLoss 3.0362 (3.0244)\tPrec@1 77.344 (80.580)\tPrec@5 96.875 (97.954)\n",
      "Epoch: [28][30/97], lr: 0.01000\tTime 0.326 (0.335)\tData 0.000 (0.023)\tLoss 4.2088 (3.1368)\tPrec@1 74.219 (79.914)\tPrec@5 100.000 (98.009)\n",
      "Epoch: [28][40/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.022)\tLoss 2.5677 (3.1001)\tPrec@1 83.594 (79.821)\tPrec@5 99.219 (98.133)\n",
      "Epoch: [28][50/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.021)\tLoss 2.7805 (3.0988)\tPrec@1 82.812 (79.764)\tPrec@5 97.656 (98.146)\n",
      "Epoch: [28][60/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 3.1238 (3.1178)\tPrec@1 81.250 (79.752)\tPrec@5 99.219 (98.143)\n",
      "Epoch: [28][70/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 3.3426 (3.1086)\tPrec@1 78.125 (79.842)\tPrec@5 100.000 (98.173)\n",
      "Epoch: [28][80/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 3.0673 (3.0914)\tPrec@1 78.906 (79.977)\tPrec@5 98.438 (98.225)\n",
      "Epoch: [28][90/97], lr: 0.01000\tTime 0.328 (0.331)\tData 0.000 (0.019)\tLoss 2.8593 (3.0635)\tPrec@1 83.594 (80.143)\tPrec@5 97.656 (98.292)\n",
      "Epoch: [28][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 2.6172 (3.0615)\tPrec@1 83.051 (80.203)\tPrec@5 98.305 (98.315)\n",
      "Test: [0/100]\tTime 0.252 (0.252)\tLoss 13.2249 (13.2249)\tPrec@1 33.000 (33.000)\tPrec@5 88.000 (88.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 9.9717 (11.4554)\tPrec@1 46.000 (40.364)\tPrec@5 87.000 (90.273)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 9.9691 (11.4252)\tPrec@1 40.000 (39.857)\tPrec@5 93.000 (90.286)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 10.8696 (11.3099)\tPrec@1 41.000 (40.742)\tPrec@5 93.000 (90.290)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 11.6520 (11.3174)\tPrec@1 34.000 (40.707)\tPrec@5 87.000 (90.488)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 11.3082 (11.2252)\tPrec@1 44.000 (41.412)\tPrec@5 86.000 (90.745)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 9.0366 (11.2207)\tPrec@1 55.000 (41.213)\tPrec@5 91.000 (90.803)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 10.7523 (11.1838)\tPrec@1 42.000 (41.380)\tPrec@5 95.000 (90.930)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 10.2996 (11.1506)\tPrec@1 48.000 (41.556)\tPrec@5 91.000 (91.123)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.6824 (11.2276)\tPrec@1 46.000 (41.209)\tPrec@5 93.000 (91.033)\n",
      "val Results: Prec@1 41.120 Prec@5 90.990 Loss 11.26006\n",
      "val Class Accuracy: [0.952,0.970,0.831,0.600,0.421,0.288,0.034,0.013,0.002,0.001]\n",
      "Best Prec@1: 45.170\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [29][0/97], lr: 0.01000\tTime 0.391 (0.391)\tData 0.213 (0.213)\tLoss 2.5469 (2.5469)\tPrec@1 84.375 (84.375)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [29][10/97], lr: 0.01000\tTime 0.324 (0.340)\tData 0.000 (0.034)\tLoss 2.6860 (2.8275)\tPrec@1 83.594 (82.244)\tPrec@5 99.219 (98.722)\n",
      "Epoch: [29][20/97], lr: 0.01000\tTime 0.329 (0.335)\tData 0.000 (0.026)\tLoss 2.6430 (2.9857)\tPrec@1 82.812 (81.399)\tPrec@5 99.219 (98.251)\n",
      "Epoch: [29][30/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.023)\tLoss 2.6764 (3.0059)\tPrec@1 82.812 (81.149)\tPrec@5 98.438 (98.311)\n",
      "Epoch: [29][40/97], lr: 0.01000\tTime 0.329 (0.333)\tData 0.000 (0.022)\tLoss 3.1056 (3.0550)\tPrec@1 81.250 (80.716)\tPrec@5 98.438 (98.342)\n",
      "Epoch: [29][50/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.021)\tLoss 3.0747 (3.0137)\tPrec@1 78.906 (80.928)\tPrec@5 99.219 (98.453)\n",
      "Epoch: [29][60/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 3.9515 (3.0113)\tPrec@1 75.781 (80.930)\tPrec@5 97.656 (98.373)\n",
      "Epoch: [29][70/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 3.4182 (3.0203)\tPrec@1 76.562 (80.777)\tPrec@5 100.000 (98.404)\n",
      "Epoch: [29][80/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 3.3801 (3.0204)\tPrec@1 81.250 (80.720)\tPrec@5 98.438 (98.409)\n",
      "Epoch: [29][90/97], lr: 0.01000\tTime 0.328 (0.332)\tData 0.000 (0.019)\tLoss 3.7419 (3.0211)\tPrec@1 74.219 (80.632)\tPrec@5 99.219 (98.455)\n",
      "Epoch: [29][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 2.6666 (3.0211)\tPrec@1 83.051 (80.622)\tPrec@5 97.458 (98.477)\n",
      "Test: [0/100]\tTime 0.232 (0.232)\tLoss 11.0432 (11.0432)\tPrec@1 46.000 (46.000)\tPrec@5 88.000 (88.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 8.7118 (10.1543)\tPrec@1 56.000 (48.909)\tPrec@5 91.000 (87.545)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 9.0471 (9.9762)\tPrec@1 54.000 (50.048)\tPrec@5 93.000 (88.429)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.0312 (9.9380)\tPrec@1 51.000 (49.871)\tPrec@5 89.000 (88.226)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.0943 (9.9779)\tPrec@1 50.000 (49.244)\tPrec@5 90.000 (88.756)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.8974 (9.8876)\tPrec@1 48.000 (49.627)\tPrec@5 92.000 (88.980)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.9454 (9.8385)\tPrec@1 64.000 (49.623)\tPrec@5 93.000 (89.082)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.9779 (9.8329)\tPrec@1 58.000 (49.718)\tPrec@5 88.000 (88.761)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.0278 (9.7975)\tPrec@1 57.000 (49.938)\tPrec@5 89.000 (88.914)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.0631 (9.8729)\tPrec@1 50.000 (49.495)\tPrec@5 90.000 (88.692)\n",
      "val Results: Prec@1 49.320 Prec@5 88.680 Loss 9.91229\n",
      "val Class Accuracy: [0.825,0.912,0.779,0.533,0.744,0.426,0.444,0.261,0.006,0.002]\n",
      "Best Prec@1: 49.320\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [30][0/97], lr: 0.01000\tTime 0.417 (0.417)\tData 0.225 (0.225)\tLoss 2.8054 (2.8054)\tPrec@1 80.469 (80.469)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [30][10/97], lr: 0.01000\tTime 0.329 (0.341)\tData 0.000 (0.036)\tLoss 3.4863 (3.1248)\tPrec@1 78.906 (79.474)\tPrec@5 100.000 (98.366)\n",
      "Epoch: [30][20/97], lr: 0.01000\tTime 0.327 (0.337)\tData 0.000 (0.027)\tLoss 3.4729 (3.0593)\tPrec@1 77.344 (80.208)\tPrec@5 98.438 (98.661)\n",
      "Epoch: [30][30/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.024)\tLoss 2.3881 (3.0089)\tPrec@1 85.156 (80.796)\tPrec@5 96.875 (98.438)\n",
      "Epoch: [30][40/97], lr: 0.01000\tTime 0.331 (0.334)\tData 0.000 (0.022)\tLoss 3.2901 (3.0855)\tPrec@1 78.125 (80.259)\tPrec@5 98.438 (98.380)\n",
      "Epoch: [30][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 3.7532 (3.0913)\tPrec@1 76.562 (80.270)\tPrec@5 95.312 (98.284)\n",
      "Epoch: [30][60/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.021)\tLoss 2.8631 (3.0705)\tPrec@1 80.469 (80.341)\tPrec@5 98.438 (98.348)\n",
      "Epoch: [30][70/97], lr: 0.01000\tTime 0.331 (0.332)\tData 0.000 (0.020)\tLoss 3.2734 (3.0966)\tPrec@1 78.906 (80.172)\tPrec@5 97.656 (98.151)\n",
      "Epoch: [30][80/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 3.1093 (3.0856)\tPrec@1 82.031 (80.247)\tPrec@5 97.656 (98.119)\n",
      "Epoch: [30][90/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 2.9283 (3.0730)\tPrec@1 84.375 (80.314)\tPrec@5 98.438 (98.154)\n",
      "Epoch: [30][96/97], lr: 0.01000\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 3.1198 (3.0731)\tPrec@1 82.203 (80.364)\tPrec@5 98.305 (98.194)\n",
      "Test: [0/100]\tTime 0.236 (0.236)\tLoss 12.1322 (12.1322)\tPrec@1 36.000 (36.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 8.7133 (10.6665)\tPrec@1 53.000 (42.636)\tPrec@5 94.000 (94.545)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 9.6489 (10.7095)\tPrec@1 43.000 (41.524)\tPrec@5 96.000 (94.762)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.7578 (10.6083)\tPrec@1 46.000 (42.194)\tPrec@5 93.000 (94.516)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.9288 (10.6346)\tPrec@1 40.000 (42.415)\tPrec@5 92.000 (94.293)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 10.1537 (10.5198)\tPrec@1 48.000 (43.176)\tPrec@5 93.000 (94.392)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.8046 (10.5050)\tPrec@1 49.000 (42.787)\tPrec@5 94.000 (94.574)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 10.1727 (10.5005)\tPrec@1 44.000 (42.859)\tPrec@5 95.000 (94.507)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.3884 (10.4650)\tPrec@1 50.000 (43.086)\tPrec@5 93.000 (94.580)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.4678 (10.5376)\tPrec@1 43.000 (42.714)\tPrec@5 97.000 (94.593)\n",
      "val Results: Prec@1 42.680 Prec@5 94.540 Loss 10.56753\n",
      "val Class Accuracy: [0.975,0.979,0.674,0.535,0.416,0.449,0.138,0.062,0.039,0.001]\n",
      "Best Prec@1: 49.320\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [31][0/97], lr: 0.01000\tTime 0.436 (0.436)\tData 0.235 (0.235)\tLoss 2.8845 (2.8845)\tPrec@1 82.812 (82.812)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [31][10/97], lr: 0.01000\tTime 0.326 (0.343)\tData 0.000 (0.036)\tLoss 2.1316 (2.9298)\tPrec@1 85.938 (81.037)\tPrec@5 97.656 (98.509)\n",
      "Epoch: [31][20/97], lr: 0.01000\tTime 0.332 (0.337)\tData 0.000 (0.027)\tLoss 2.7416 (2.9682)\tPrec@1 85.156 (81.510)\tPrec@5 97.656 (98.065)\n",
      "Epoch: [31][30/97], lr: 0.01000\tTime 0.327 (0.335)\tData 0.000 (0.024)\tLoss 2.9574 (2.9919)\tPrec@1 82.031 (81.225)\tPrec@5 100.000 (98.085)\n",
      "Epoch: [31][40/97], lr: 0.01000\tTime 0.328 (0.334)\tData 0.000 (0.022)\tLoss 4.1022 (2.9948)\tPrec@1 72.656 (80.983)\tPrec@5 98.438 (98.190)\n",
      "Epoch: [31][50/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 2.5460 (3.0108)\tPrec@1 82.812 (80.913)\tPrec@5 96.875 (98.223)\n",
      "Epoch: [31][60/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.021)\tLoss 2.8800 (2.9842)\tPrec@1 81.250 (80.917)\tPrec@5 98.438 (98.271)\n",
      "Epoch: [31][70/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.020)\tLoss 3.3428 (2.9756)\tPrec@1 78.125 (81.008)\tPrec@5 98.438 (98.360)\n",
      "Epoch: [31][80/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.7072 (2.9732)\tPrec@1 82.812 (81.019)\tPrec@5 98.438 (98.428)\n",
      "Epoch: [31][90/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 4.0170 (2.9774)\tPrec@1 75.000 (81.044)\tPrec@5 99.219 (98.455)\n",
      "Epoch: [31][96/97], lr: 0.01000\tTime 0.316 (0.332)\tData 0.000 (0.020)\tLoss 3.8843 (2.9844)\tPrec@1 77.119 (81.049)\tPrec@5 98.305 (98.468)\n",
      "Test: [0/100]\tTime 0.273 (0.273)\tLoss 10.2368 (10.2368)\tPrec@1 46.000 (46.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 7.9580 (9.3552)\tPrec@1 56.000 (50.818)\tPrec@5 96.000 (95.909)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 9.2289 (9.3495)\tPrec@1 44.000 (49.857)\tPrec@5 95.000 (96.000)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.6049 (9.2609)\tPrec@1 54.000 (50.226)\tPrec@5 96.000 (96.194)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 10.2376 (9.2728)\tPrec@1 44.000 (50.195)\tPrec@5 95.000 (96.049)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 9.0162 (9.1924)\tPrec@1 56.000 (50.686)\tPrec@5 95.000 (96.196)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.4204 (9.1748)\tPrec@1 59.000 (50.623)\tPrec@5 96.000 (96.213)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.9096 (9.1674)\tPrec@1 53.000 (50.549)\tPrec@5 96.000 (96.211)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.4549 (9.1389)\tPrec@1 58.000 (50.778)\tPrec@5 94.000 (96.235)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.0339 (9.1927)\tPrec@1 55.000 (50.396)\tPrec@5 99.000 (96.297)\n",
      "val Results: Prec@1 50.170 Prec@5 96.270 Loss 9.22544\n",
      "val Class Accuracy: [0.926,0.966,0.846,0.595,0.640,0.234,0.518,0.046,0.116,0.130]\n",
      "Best Prec@1: 50.170\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [32][0/97], lr: 0.01000\tTime 0.402 (0.402)\tData 0.216 (0.216)\tLoss 2.9708 (2.9708)\tPrec@1 82.031 (82.031)\tPrec@5 96.875 (96.875)\n",
      "Epoch: [32][10/97], lr: 0.01000\tTime 0.326 (0.341)\tData 0.000 (0.034)\tLoss 3.5574 (2.8910)\tPrec@1 77.344 (81.321)\tPrec@5 99.219 (98.366)\n",
      "Epoch: [32][20/97], lr: 0.01000\tTime 0.329 (0.334)\tData 0.000 (0.026)\tLoss 2.6402 (2.9320)\tPrec@1 81.250 (81.064)\tPrec@5 99.219 (98.586)\n",
      "Epoch: [32][30/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.023)\tLoss 2.0706 (2.8846)\tPrec@1 86.719 (81.477)\tPrec@5 98.438 (98.740)\n",
      "Epoch: [32][40/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.022)\tLoss 3.4484 (2.8764)\tPrec@1 77.344 (81.612)\tPrec@5 99.219 (98.685)\n",
      "Epoch: [32][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 2.9418 (2.8810)\tPrec@1 80.469 (81.572)\tPrec@5 98.438 (98.698)\n",
      "Epoch: [32][60/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 2.3712 (2.8497)\tPrec@1 83.594 (81.801)\tPrec@5 98.438 (98.758)\n",
      "Epoch: [32][70/97], lr: 0.01000\tTime 0.327 (0.331)\tData 0.000 (0.020)\tLoss 3.2189 (2.8818)\tPrec@1 77.344 (81.558)\tPrec@5 99.219 (98.636)\n",
      "Epoch: [32][80/97], lr: 0.01000\tTime 0.329 (0.331)\tData 0.000 (0.020)\tLoss 3.3894 (2.9021)\tPrec@1 78.906 (81.453)\tPrec@5 99.219 (98.659)\n",
      "Epoch: [32][90/97], lr: 0.01000\tTime 0.322 (0.331)\tData 0.000 (0.019)\tLoss 3.0231 (2.8891)\tPrec@1 78.125 (81.482)\tPrec@5 98.438 (98.669)\n",
      "Epoch: [32][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 3.0291 (2.9038)\tPrec@1 83.898 (81.404)\tPrec@5 100.000 (98.622)\n",
      "Test: [0/100]\tTime 0.254 (0.254)\tLoss 11.2673 (11.2673)\tPrec@1 46.000 (46.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 8.1634 (10.8191)\tPrec@1 59.000 (45.091)\tPrec@5 92.000 (90.091)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 11.3809 (11.0468)\tPrec@1 39.000 (43.857)\tPrec@5 93.000 (90.619)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 10.3472 (11.0047)\tPrec@1 45.000 (44.194)\tPrec@5 86.000 (89.677)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 12.3124 (11.0854)\tPrec@1 36.000 (43.829)\tPrec@5 87.000 (89.390)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 11.2274 (11.0133)\tPrec@1 43.000 (44.196)\tPrec@5 90.000 (89.588)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 9.2712 (10.9818)\tPrec@1 51.000 (44.049)\tPrec@5 91.000 (89.393)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 10.2496 (10.9598)\tPrec@1 50.000 (44.155)\tPrec@5 88.000 (89.324)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 10.0051 (10.9329)\tPrec@1 52.000 (44.309)\tPrec@5 87.000 (89.321)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.9599 (10.9857)\tPrec@1 44.000 (44.011)\tPrec@5 89.000 (89.429)\n",
      "val Results: Prec@1 43.940 Prec@5 89.420 Loss 11.02712\n",
      "val Class Accuracy: [0.859,0.931,0.852,0.485,0.455,0.108,0.687,0.005,0.011,0.001]\n",
      "Best Prec@1: 50.170\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [33][0/97], lr: 0.01000\tTime 0.422 (0.422)\tData 0.238 (0.238)\tLoss 3.4190 (3.4190)\tPrec@1 81.250 (81.250)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [33][10/97], lr: 0.01000\tTime 0.329 (0.342)\tData 0.000 (0.037)\tLoss 2.1237 (2.7631)\tPrec@1 86.719 (82.102)\tPrec@5 100.000 (99.077)\n",
      "Epoch: [33][20/97], lr: 0.01000\tTime 0.325 (0.335)\tData 0.000 (0.027)\tLoss 3.1025 (2.8567)\tPrec@1 78.906 (81.659)\tPrec@5 100.000 (98.735)\n",
      "Epoch: [33][30/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.024)\tLoss 3.2395 (2.8430)\tPrec@1 78.906 (81.779)\tPrec@5 98.438 (98.690)\n",
      "Epoch: [33][40/97], lr: 0.01000\tTime 0.329 (0.333)\tData 0.000 (0.022)\tLoss 2.7860 (2.8240)\tPrec@1 81.250 (81.860)\tPrec@5 99.219 (98.628)\n",
      "Epoch: [33][50/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 3.1403 (2.8769)\tPrec@1 79.688 (81.526)\tPrec@5 97.656 (98.652)\n",
      "Epoch: [33][60/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 2.9632 (2.8834)\tPrec@1 81.250 (81.468)\tPrec@5 100.000 (98.617)\n",
      "Epoch: [33][70/97], lr: 0.01000\tTime 0.335 (0.332)\tData 0.000 (0.020)\tLoss 2.8967 (2.8837)\tPrec@1 80.469 (81.437)\tPrec@5 98.438 (98.636)\n",
      "Epoch: [33][80/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 3.5017 (2.9281)\tPrec@1 77.344 (81.240)\tPrec@5 99.219 (98.640)\n",
      "Epoch: [33][90/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 2.7345 (2.9299)\tPrec@1 82.812 (81.190)\tPrec@5 98.438 (98.609)\n",
      "Epoch: [33][96/97], lr: 0.01000\tTime 0.321 (0.331)\tData 0.000 (0.020)\tLoss 2.5163 (2.9310)\tPrec@1 84.746 (81.219)\tPrec@5 99.153 (98.565)\n",
      "Test: [0/100]\tTime 0.245 (0.245)\tLoss 12.4911 (12.4911)\tPrec@1 38.000 (38.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 8.7325 (11.4609)\tPrec@1 57.000 (41.636)\tPrec@5 96.000 (93.000)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 10.9424 (11.6313)\tPrec@1 43.000 (40.667)\tPrec@5 94.000 (92.952)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 10.7126 (11.5355)\tPrec@1 42.000 (41.129)\tPrec@5 89.000 (92.774)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 12.2788 (11.5882)\tPrec@1 36.000 (40.829)\tPrec@5 91.000 (92.805)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 11.0232 (11.4838)\tPrec@1 44.000 (41.333)\tPrec@5 95.000 (93.098)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 9.7992 (11.4867)\tPrec@1 48.000 (40.902)\tPrec@5 91.000 (92.803)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 11.2599 (11.4697)\tPrec@1 40.000 (41.014)\tPrec@5 95.000 (92.761)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 10.3315 (11.4479)\tPrec@1 49.000 (40.901)\tPrec@5 93.000 (92.963)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.5693 (11.5073)\tPrec@1 40.000 (40.758)\tPrec@5 93.000 (92.890)\n",
      "val Results: Prec@1 40.590 Prec@5 92.960 Loss 11.55051\n",
      "val Class Accuracy: [0.978,0.971,0.686,0.298,0.325,0.213,0.525,0.063,0.000,0.000]\n",
      "Best Prec@1: 50.170\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [34][0/97], lr: 0.01000\tTime 0.431 (0.431)\tData 0.228 (0.228)\tLoss 2.8648 (2.8648)\tPrec@1 82.812 (82.812)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [34][10/97], lr: 0.01000\tTime 0.328 (0.344)\tData 0.000 (0.036)\tLoss 2.9161 (2.9993)\tPrec@1 81.250 (80.753)\tPrec@5 100.000 (98.651)\n",
      "Epoch: [34][20/97], lr: 0.01000\tTime 0.345 (0.338)\tData 0.000 (0.027)\tLoss 2.9755 (2.8910)\tPrec@1 83.594 (81.808)\tPrec@5 98.438 (98.586)\n",
      "Epoch: [34][30/97], lr: 0.01000\tTime 0.325 (0.335)\tData 0.000 (0.024)\tLoss 2.2255 (2.9550)\tPrec@1 82.031 (81.048)\tPrec@5 99.219 (98.412)\n",
      "Epoch: [34][40/97], lr: 0.01000\tTime 0.327 (0.334)\tData 0.000 (0.022)\tLoss 2.9623 (2.9410)\tPrec@1 80.469 (81.002)\tPrec@5 98.438 (98.380)\n",
      "Epoch: [34][50/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.021)\tLoss 3.5409 (2.9403)\tPrec@1 74.219 (80.867)\tPrec@5 97.656 (98.438)\n",
      "Epoch: [34][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 3.4197 (2.9208)\tPrec@1 77.344 (81.019)\tPrec@5 98.438 (98.514)\n",
      "Epoch: [34][70/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 3.3272 (2.9364)\tPrec@1 79.688 (81.074)\tPrec@5 97.656 (98.570)\n",
      "Epoch: [34][80/97], lr: 0.01000\tTime 0.347 (0.333)\tData 0.000 (0.020)\tLoss 2.8179 (2.9373)\tPrec@1 82.031 (81.202)\tPrec@5 100.000 (98.582)\n",
      "Epoch: [34][90/97], lr: 0.01000\tTime 0.329 (0.334)\tData 0.000 (0.019)\tLoss 3.0113 (2.9430)\tPrec@1 79.688 (81.181)\tPrec@5 99.219 (98.558)\n",
      "Epoch: [34][96/97], lr: 0.01000\tTime 0.317 (0.333)\tData 0.000 (0.020)\tLoss 1.7618 (2.9203)\tPrec@1 89.831 (81.348)\tPrec@5 98.305 (98.557)\n",
      "Test: [0/100]\tTime 0.261 (0.261)\tLoss 11.2264 (11.2264)\tPrec@1 42.000 (42.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 8.1663 (10.1906)\tPrec@1 56.000 (46.545)\tPrec@5 93.000 (93.818)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 9.5915 (10.3298)\tPrec@1 46.000 (44.952)\tPrec@5 94.000 (94.190)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 9.0332 (10.2627)\tPrec@1 54.000 (45.484)\tPrec@5 94.000 (94.323)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 10.9799 (10.2941)\tPrec@1 42.000 (45.390)\tPrec@5 94.000 (94.220)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 9.7354 (10.1919)\tPrec@1 51.000 (46.020)\tPrec@5 93.000 (94.353)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.2344 (10.1453)\tPrec@1 55.000 (46.098)\tPrec@5 95.000 (94.410)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 9.6913 (10.1283)\tPrec@1 48.000 (46.141)\tPrec@5 96.000 (94.338)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.1490 (10.1041)\tPrec@1 54.000 (46.272)\tPrec@5 92.000 (94.481)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.3015 (10.1363)\tPrec@1 45.000 (46.220)\tPrec@5 96.000 (94.571)\n",
      "val Results: Prec@1 46.080 Prec@5 94.530 Loss 10.15392\n",
      "val Class Accuracy: [0.964,0.988,0.689,0.616,0.496,0.248,0.498,0.095,0.014,0.000]\n",
      "Best Prec@1: 50.170\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [35][0/97], lr: 0.01000\tTime 0.450 (0.450)\tData 0.234 (0.234)\tLoss 2.1859 (2.1859)\tPrec@1 89.062 (89.062)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [35][10/97], lr: 0.01000\tTime 0.327 (0.349)\tData 0.000 (0.036)\tLoss 2.5018 (2.7662)\tPrec@1 82.812 (82.244)\tPrec@5 100.000 (98.153)\n",
      "Epoch: [35][20/97], lr: 0.01000\tTime 0.337 (0.341)\tData 0.000 (0.027)\tLoss 2.7451 (2.7715)\tPrec@1 81.250 (82.106)\tPrec@5 100.000 (98.549)\n",
      "Epoch: [35][30/97], lr: 0.01000\tTime 0.326 (0.337)\tData 0.000 (0.024)\tLoss 2.8645 (2.8868)\tPrec@1 82.031 (81.477)\tPrec@5 99.219 (98.438)\n",
      "Epoch: [35][40/97], lr: 0.01000\tTime 0.326 (0.336)\tData 0.000 (0.022)\tLoss 2.9437 (2.9034)\tPrec@1 80.469 (81.250)\tPrec@5 97.656 (98.438)\n",
      "Epoch: [35][50/97], lr: 0.01000\tTime 0.326 (0.335)\tData 0.000 (0.021)\tLoss 3.0172 (2.8947)\tPrec@1 78.125 (81.541)\tPrec@5 98.438 (98.514)\n",
      "Epoch: [35][60/97], lr: 0.01000\tTime 0.323 (0.334)\tData 0.000 (0.021)\tLoss 2.4190 (2.8711)\tPrec@1 86.719 (81.660)\tPrec@5 99.219 (98.578)\n",
      "Epoch: [35][70/97], lr: 0.01000\tTime 0.327 (0.334)\tData 0.000 (0.020)\tLoss 2.4210 (2.8501)\tPrec@1 82.812 (81.877)\tPrec@5 98.438 (98.548)\n",
      "Epoch: [35][80/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.020)\tLoss 2.5988 (2.8547)\tPrec@1 82.812 (81.887)\tPrec@5 100.000 (98.582)\n",
      "Epoch: [35][90/97], lr: 0.01000\tTime 0.323 (0.333)\tData 0.000 (0.020)\tLoss 2.2580 (2.8666)\tPrec@1 83.594 (81.808)\tPrec@5 100.000 (98.541)\n",
      "Epoch: [35][96/97], lr: 0.01000\tTime 0.314 (0.332)\tData 0.000 (0.020)\tLoss 2.3869 (2.8545)\tPrec@1 84.746 (81.872)\tPrec@5 98.305 (98.557)\n",
      "Test: [0/100]\tTime 0.281 (0.281)\tLoss 10.9415 (10.9415)\tPrec@1 48.000 (48.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 8.2658 (9.9665)\tPrec@1 60.000 (50.636)\tPrec@5 96.000 (95.455)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 8.7792 (9.9956)\tPrec@1 53.000 (50.095)\tPrec@5 97.000 (95.143)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 8.6423 (9.9954)\tPrec@1 55.000 (49.710)\tPrec@5 95.000 (95.387)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 10.6466 (10.0580)\tPrec@1 45.000 (49.220)\tPrec@5 94.000 (95.244)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 9.1189 (9.9356)\tPrec@1 58.000 (49.824)\tPrec@5 92.000 (95.412)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.5025 (9.8615)\tPrec@1 64.000 (50.098)\tPrec@5 96.000 (95.361)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 9.5613 (9.8448)\tPrec@1 52.000 (50.099)\tPrec@5 96.000 (95.310)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 9.3196 (9.8032)\tPrec@1 51.000 (50.259)\tPrec@5 91.000 (95.321)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.5060 (9.8592)\tPrec@1 45.000 (49.989)\tPrec@5 95.000 (95.198)\n",
      "val Results: Prec@1 49.950 Prec@5 95.200 Loss 9.87974\n",
      "val Class Accuracy: [0.963,0.981,0.619,0.781,0.637,0.444,0.350,0.214,0.002,0.004]\n",
      "Best Prec@1: 50.170\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [36][0/97], lr: 0.01000\tTime 0.435 (0.435)\tData 0.223 (0.223)\tLoss 2.5954 (2.5954)\tPrec@1 85.156 (85.156)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [36][10/97], lr: 0.01000\tTime 0.329 (0.345)\tData 0.000 (0.035)\tLoss 2.1582 (2.6845)\tPrec@1 85.156 (82.812)\tPrec@5 98.438 (98.580)\n",
      "Epoch: [36][20/97], lr: 0.01000\tTime 0.325 (0.338)\tData 0.000 (0.027)\tLoss 3.5615 (2.7214)\tPrec@1 75.000 (82.068)\tPrec@5 99.219 (98.810)\n",
      "Epoch: [36][30/97], lr: 0.01000\tTime 0.325 (0.336)\tData 0.000 (0.024)\tLoss 3.0173 (2.7449)\tPrec@1 82.031 (82.182)\tPrec@5 98.438 (98.816)\n",
      "Epoch: [36][40/97], lr: 0.01000\tTime 0.329 (0.335)\tData 0.000 (0.022)\tLoss 1.9677 (2.7808)\tPrec@1 86.719 (81.726)\tPrec@5 99.219 (98.590)\n",
      "Epoch: [36][50/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.021)\tLoss 2.4945 (2.7954)\tPrec@1 85.938 (82.016)\tPrec@5 99.219 (98.529)\n",
      "Epoch: [36][60/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 2.5648 (2.7978)\tPrec@1 84.375 (82.185)\tPrec@5 98.438 (98.591)\n",
      "Epoch: [36][70/97], lr: 0.01000\tTime 0.333 (0.333)\tData 0.000 (0.020)\tLoss 3.3973 (2.8010)\tPrec@1 79.688 (82.273)\tPrec@5 96.875 (98.614)\n",
      "Epoch: [36][80/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.020)\tLoss 2.8892 (2.8487)\tPrec@1 82.031 (81.896)\tPrec@5 98.438 (98.505)\n",
      "Epoch: [36][90/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.019)\tLoss 2.8537 (2.8608)\tPrec@1 82.031 (81.825)\tPrec@5 98.438 (98.489)\n",
      "Epoch: [36][96/97], lr: 0.01000\tTime 0.321 (0.332)\tData 0.000 (0.020)\tLoss 3.8588 (2.8486)\tPrec@1 73.729 (81.839)\tPrec@5 99.153 (98.509)\n",
      "Test: [0/100]\tTime 0.269 (0.269)\tLoss 11.4623 (11.4623)\tPrec@1 45.000 (45.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 8.6011 (10.8282)\tPrec@1 58.000 (45.727)\tPrec@5 93.000 (93.727)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 9.5262 (10.8093)\tPrec@1 52.000 (45.952)\tPrec@5 97.000 (93.429)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 9.7816 (10.7195)\tPrec@1 46.000 (46.452)\tPrec@5 94.000 (93.290)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 11.2985 (10.7417)\tPrec@1 43.000 (46.732)\tPrec@5 90.000 (93.341)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 9.7796 (10.6258)\tPrec@1 54.000 (47.412)\tPrec@5 93.000 (93.490)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.2141 (10.5808)\tPrec@1 59.000 (47.377)\tPrec@5 96.000 (93.377)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 10.8200 (10.5793)\tPrec@1 46.000 (47.380)\tPrec@5 93.000 (93.423)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.7718 (10.5370)\tPrec@1 52.000 (47.568)\tPrec@5 89.000 (93.519)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.5076 (10.5730)\tPrec@1 51.000 (47.407)\tPrec@5 97.000 (93.495)\n",
      "val Results: Prec@1 47.380 Prec@5 93.540 Loss 10.59478\n",
      "val Class Accuracy: [0.900,0.996,0.767,0.439,0.466,0.559,0.573,0.032,0.006,0.000]\n",
      "Best Prec@1: 50.170\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [37][0/97], lr: 0.01000\tTime 0.447 (0.447)\tData 0.257 (0.257)\tLoss 2.8603 (2.8603)\tPrec@1 83.594 (83.594)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [37][10/97], lr: 0.01000\tTime 0.325 (0.346)\tData 0.000 (0.039)\tLoss 2.3771 (2.7514)\tPrec@1 85.938 (82.599)\tPrec@5 98.438 (98.580)\n",
      "Epoch: [37][20/97], lr: 0.01000\tTime 0.332 (0.338)\tData 0.000 (0.028)\tLoss 3.0043 (2.8255)\tPrec@1 82.031 (82.515)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [37][30/97], lr: 0.01000\tTime 0.327 (0.336)\tData 0.000 (0.025)\tLoss 3.2847 (2.8008)\tPrec@1 79.688 (82.712)\tPrec@5 99.219 (98.463)\n",
      "Epoch: [37][40/97], lr: 0.01000\tTime 0.329 (0.335)\tData 0.000 (0.023)\tLoss 3.0385 (2.8199)\tPrec@1 79.688 (82.393)\tPrec@5 96.094 (98.228)\n",
      "Epoch: [37][50/97], lr: 0.01000\tTime 0.326 (0.335)\tData 0.000 (0.022)\tLoss 4.1810 (2.8844)\tPrec@1 69.531 (81.924)\tPrec@5 96.875 (98.330)\n",
      "Epoch: [37][60/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.021)\tLoss 3.1894 (2.8961)\tPrec@1 79.688 (81.673)\tPrec@5 98.438 (98.361)\n",
      "Epoch: [37][70/97], lr: 0.01000\tTime 0.328 (0.334)\tData 0.000 (0.020)\tLoss 3.0405 (2.8839)\tPrec@1 79.688 (81.778)\tPrec@5 100.000 (98.438)\n",
      "Epoch: [37][80/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.020)\tLoss 1.8961 (2.8321)\tPrec@1 89.844 (82.108)\tPrec@5 99.219 (98.524)\n",
      "Epoch: [37][90/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.020)\tLoss 2.0597 (2.8401)\tPrec@1 89.062 (82.083)\tPrec@5 99.219 (98.549)\n",
      "Epoch: [37][96/97], lr: 0.01000\tTime 0.323 (0.333)\tData 0.000 (0.020)\tLoss 3.2513 (2.8539)\tPrec@1 77.966 (81.968)\tPrec@5 96.610 (98.501)\n",
      "Test: [0/100]\tTime 0.269 (0.269)\tLoss 11.7560 (11.7560)\tPrec@1 36.000 (36.000)\tPrec@5 84.000 (84.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 9.6863 (10.5741)\tPrec@1 47.000 (43.273)\tPrec@5 83.000 (88.091)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 9.0831 (10.6214)\tPrec@1 49.000 (42.286)\tPrec@5 89.000 (88.190)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 9.6886 (10.5533)\tPrec@1 46.000 (42.774)\tPrec@5 84.000 (87.774)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 11.2993 (10.5847)\tPrec@1 38.000 (42.732)\tPrec@5 81.000 (87.732)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 10.2681 (10.4663)\tPrec@1 45.000 (43.275)\tPrec@5 84.000 (87.824)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.9220 (10.4519)\tPrec@1 50.000 (43.049)\tPrec@5 89.000 (87.689)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 10.3457 (10.4134)\tPrec@1 40.000 (43.296)\tPrec@5 92.000 (87.873)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.6712 (10.3653)\tPrec@1 48.000 (43.494)\tPrec@5 89.000 (88.074)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.5770 (10.4329)\tPrec@1 42.000 (43.264)\tPrec@5 90.000 (87.901)\n",
      "val Results: Prec@1 43.240 Prec@5 87.900 Loss 10.44877\n",
      "val Class Accuracy: [0.980,0.932,0.552,0.589,0.489,0.620,0.007,0.076,0.042,0.037]\n",
      "Best Prec@1: 50.170\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [38][0/97], lr: 0.01000\tTime 0.486 (0.486)\tData 0.254 (0.254)\tLoss 3.3446 (3.3446)\tPrec@1 78.125 (78.125)\tPrec@5 96.094 (96.094)\n",
      "Epoch: [38][10/97], lr: 0.01000\tTime 0.327 (0.351)\tData 0.000 (0.038)\tLoss 2.5101 (2.8905)\tPrec@1 85.156 (81.889)\tPrec@5 99.219 (98.295)\n",
      "Epoch: [38][20/97], lr: 0.01000\tTime 0.331 (0.340)\tData 0.000 (0.028)\tLoss 3.0231 (3.0503)\tPrec@1 80.469 (80.729)\tPrec@5 98.438 (98.586)\n",
      "Epoch: [38][30/97], lr: 0.01000\tTime 0.326 (0.337)\tData 0.000 (0.025)\tLoss 3.1330 (2.8985)\tPrec@1 77.344 (81.502)\tPrec@5 99.219 (98.740)\n",
      "Epoch: [38][40/97], lr: 0.01000\tTime 0.327 (0.335)\tData 0.000 (0.023)\tLoss 3.6033 (2.9187)\tPrec@1 76.562 (81.307)\tPrec@5 98.438 (98.742)\n",
      "Epoch: [38][50/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.022)\tLoss 3.5991 (2.9284)\tPrec@1 75.781 (81.158)\tPrec@5 97.656 (98.575)\n",
      "Epoch: [38][60/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 2.3302 (2.9173)\tPrec@1 85.156 (81.263)\tPrec@5 99.219 (98.566)\n",
      "Epoch: [38][70/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.020)\tLoss 2.6026 (2.9150)\tPrec@1 83.594 (81.294)\tPrec@5 99.219 (98.537)\n",
      "Epoch: [38][80/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 3.2134 (2.9145)\tPrec@1 79.688 (81.250)\tPrec@5 97.656 (98.534)\n",
      "Epoch: [38][90/97], lr: 0.01000\tTime 0.333 (0.334)\tData 0.000 (0.020)\tLoss 2.1619 (2.8929)\tPrec@1 88.281 (81.465)\tPrec@5 99.219 (98.549)\n",
      "Epoch: [38][96/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.020)\tLoss 2.8779 (2.8906)\tPrec@1 79.661 (81.509)\tPrec@5 96.610 (98.525)\n",
      "Test: [0/100]\tTime 0.343 (0.343)\tLoss 11.2966 (11.2966)\tPrec@1 42.000 (42.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.098)\tLoss 8.3912 (10.3181)\tPrec@1 61.000 (48.000)\tPrec@5 92.000 (93.727)\n",
      "Test: [20/100]\tTime 0.073 (0.086)\tLoss 9.6624 (10.2210)\tPrec@1 49.000 (47.810)\tPrec@5 93.000 (93.333)\n",
      "Test: [30/100]\tTime 0.073 (0.082)\tLoss 8.6866 (10.1942)\tPrec@1 56.000 (47.935)\tPrec@5 94.000 (93.194)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 10.2039 (10.2477)\tPrec@1 45.000 (47.634)\tPrec@5 95.000 (93.512)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 9.3828 (10.1441)\tPrec@1 55.000 (48.294)\tPrec@5 91.000 (93.588)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 8.3329 (10.0887)\tPrec@1 57.000 (48.328)\tPrec@5 95.000 (93.623)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 10.0957 (10.0805)\tPrec@1 49.000 (48.310)\tPrec@5 97.000 (93.620)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 9.0624 (10.0351)\tPrec@1 55.000 (48.543)\tPrec@5 88.000 (93.667)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 10.6359 (10.1027)\tPrec@1 43.000 (48.143)\tPrec@5 95.000 (93.505)\n",
      "val Results: Prec@1 47.900 Prec@5 93.440 Loss 10.14406\n",
      "val Class Accuracy: [0.975,0.981,0.536,0.594,0.692,0.470,0.275,0.237,0.030,0.000]\n",
      "Best Prec@1: 50.170\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [39][0/97], lr: 0.01000\tTime 0.406 (0.406)\tData 0.205 (0.205)\tLoss 2.5362 (2.5362)\tPrec@1 84.375 (84.375)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [39][10/97], lr: 0.01000\tTime 0.328 (0.341)\tData 0.000 (0.033)\tLoss 3.0812 (2.8384)\tPrec@1 80.469 (81.960)\tPrec@5 99.219 (99.006)\n",
      "Epoch: [39][20/97], lr: 0.01000\tTime 0.325 (0.336)\tData 0.000 (0.025)\tLoss 2.0686 (2.8230)\tPrec@1 85.938 (81.845)\tPrec@5 100.000 (99.256)\n",
      "Epoch: [39][30/97], lr: 0.01000\tTime 0.324 (0.335)\tData 0.000 (0.023)\tLoss 2.6933 (2.8400)\tPrec@1 85.156 (82.082)\tPrec@5 98.438 (98.942)\n",
      "Epoch: [39][40/97], lr: 0.01000\tTime 0.332 (0.334)\tData 0.000 (0.021)\tLoss 2.8523 (2.7324)\tPrec@1 82.031 (82.717)\tPrec@5 99.219 (98.876)\n",
      "Epoch: [39][50/97], lr: 0.01000\tTime 0.329 (0.333)\tData 0.000 (0.021)\tLoss 2.9435 (2.7337)\tPrec@1 83.594 (82.843)\tPrec@5 96.875 (98.759)\n",
      "Epoch: [39][60/97], lr: 0.01000\tTime 0.334 (0.333)\tData 0.000 (0.020)\tLoss 3.5704 (2.7767)\tPrec@1 79.688 (82.659)\tPrec@5 99.219 (98.809)\n",
      "Epoch: [39][70/97], lr: 0.01000\tTime 0.330 (0.333)\tData 0.000 (0.020)\tLoss 3.0314 (2.7664)\tPrec@1 79.688 (82.647)\tPrec@5 96.094 (98.779)\n",
      "Epoch: [39][80/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.019)\tLoss 3.3220 (2.7798)\tPrec@1 79.688 (82.504)\tPrec@5 99.219 (98.736)\n",
      "Epoch: [39][90/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.019)\tLoss 3.4360 (2.7824)\tPrec@1 77.344 (82.512)\tPrec@5 99.219 (98.712)\n",
      "Epoch: [39][96/97], lr: 0.01000\tTime 0.318 (0.332)\tData 0.000 (0.020)\tLoss 1.9977 (2.7911)\tPrec@1 88.136 (82.444)\tPrec@5 99.153 (98.702)\n",
      "Test: [0/100]\tTime 0.245 (0.245)\tLoss 10.2170 (10.2170)\tPrec@1 51.000 (51.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.0194 (9.0718)\tPrec@1 67.000 (54.818)\tPrec@5 96.000 (96.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 8.5647 (9.0641)\tPrec@1 56.000 (55.000)\tPrec@5 95.000 (96.048)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.1836 (9.0338)\tPrec@1 59.000 (54.806)\tPrec@5 93.000 (95.677)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.1419 (9.0597)\tPrec@1 50.000 (54.756)\tPrec@5 95.000 (95.707)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.2219 (8.9507)\tPrec@1 59.000 (55.412)\tPrec@5 99.000 (95.843)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7439 (8.8678)\tPrec@1 68.000 (55.852)\tPrec@5 96.000 (95.836)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.2195 (8.8159)\tPrec@1 63.000 (56.352)\tPrec@5 98.000 (95.746)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.6318 (8.7625)\tPrec@1 58.000 (56.593)\tPrec@5 91.000 (95.728)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.2672 (8.8136)\tPrec@1 56.000 (56.418)\tPrec@5 94.000 (95.593)\n",
      "val Results: Prec@1 56.400 Prec@5 95.590 Loss 8.83417\n",
      "val Class Accuracy: [0.929,0.985,0.737,0.518,0.630,0.714,0.717,0.404,0.006,0.000]\n",
      "Best Prec@1: 56.400\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [40][0/97], lr: 0.01000\tTime 0.460 (0.460)\tData 0.237 (0.237)\tLoss 1.6474 (1.6474)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [40][10/97], lr: 0.01000\tTime 0.332 (0.348)\tData 0.000 (0.037)\tLoss 3.0358 (2.7096)\tPrec@1 80.469 (83.665)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [40][20/97], lr: 0.01000\tTime 0.326 (0.340)\tData 0.000 (0.027)\tLoss 3.5974 (2.7320)\tPrec@1 75.781 (82.887)\tPrec@5 98.438 (98.624)\n",
      "Epoch: [40][30/97], lr: 0.01000\tTime 0.327 (0.337)\tData 0.000 (0.024)\tLoss 3.0065 (2.7220)\tPrec@1 77.344 (82.636)\tPrec@5 96.875 (98.463)\n",
      "Epoch: [40][40/97], lr: 0.01000\tTime 0.330 (0.335)\tData 0.000 (0.022)\tLoss 3.4181 (2.6821)\tPrec@1 80.469 (82.812)\tPrec@5 94.531 (98.552)\n",
      "Epoch: [40][50/97], lr: 0.01000\tTime 0.327 (0.334)\tData 0.000 (0.021)\tLoss 2.8869 (2.7135)\tPrec@1 81.250 (82.736)\tPrec@5 97.656 (98.545)\n",
      "Epoch: [40][60/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.021)\tLoss 3.4477 (2.7027)\tPrec@1 78.906 (82.774)\tPrec@5 100.000 (98.642)\n",
      "Epoch: [40][70/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 3.1910 (2.7149)\tPrec@1 79.688 (82.757)\tPrec@5 97.656 (98.691)\n",
      "Epoch: [40][80/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 2.0399 (2.7093)\tPrec@1 86.719 (82.851)\tPrec@5 99.219 (98.688)\n",
      "Epoch: [40][90/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 4.3171 (2.7490)\tPrec@1 71.875 (82.667)\tPrec@5 97.656 (98.678)\n",
      "Epoch: [40][96/97], lr: 0.01000\tTime 0.319 (0.332)\tData 0.000 (0.020)\tLoss 3.4598 (2.7681)\tPrec@1 77.966 (82.468)\tPrec@5 98.305 (98.718)\n",
      "Test: [0/100]\tTime 0.246 (0.246)\tLoss 11.0673 (11.0673)\tPrec@1 46.000 (46.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 9.0366 (10.3945)\tPrec@1 54.000 (48.545)\tPrec@5 95.000 (94.000)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 9.2478 (10.5073)\tPrec@1 54.000 (47.095)\tPrec@5 92.000 (93.333)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 9.6960 (10.4243)\tPrec@1 51.000 (47.323)\tPrec@5 92.000 (93.226)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.7460 (10.4563)\tPrec@1 46.000 (47.585)\tPrec@5 89.000 (93.341)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.6472 (10.3501)\tPrec@1 48.000 (48.078)\tPrec@5 94.000 (93.608)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 9.0389 (10.3144)\tPrec@1 52.000 (48.115)\tPrec@5 95.000 (93.738)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.5942 (10.3216)\tPrec@1 57.000 (48.141)\tPrec@5 91.000 (93.563)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.5105 (10.2764)\tPrec@1 54.000 (48.531)\tPrec@5 94.000 (93.630)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.6802 (10.3204)\tPrec@1 50.000 (48.198)\tPrec@5 94.000 (93.560)\n",
      "val Results: Prec@1 48.250 Prec@5 93.620 Loss 10.33316\n",
      "val Class Accuracy: [0.782,0.999,0.659,0.485,0.380,0.584,0.447,0.468,0.021,0.000]\n",
      "Best Prec@1: 56.400\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [41][0/97], lr: 0.01000\tTime 0.464 (0.464)\tData 0.231 (0.231)\tLoss 2.8556 (2.8556)\tPrec@1 80.469 (80.469)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [41][10/97], lr: 0.01000\tTime 0.329 (0.350)\tData 0.000 (0.036)\tLoss 1.8535 (2.5832)\tPrec@1 90.625 (84.020)\tPrec@5 99.219 (99.006)\n",
      "Epoch: [41][20/97], lr: 0.01000\tTime 0.333 (0.341)\tData 0.000 (0.027)\tLoss 2.1228 (2.5492)\tPrec@1 85.938 (83.594)\tPrec@5 100.000 (99.107)\n",
      "Epoch: [41][30/97], lr: 0.01000\tTime 0.326 (0.337)\tData 0.000 (0.024)\tLoss 3.3067 (2.6888)\tPrec@1 77.344 (82.737)\tPrec@5 96.875 (98.790)\n",
      "Epoch: [41][40/97], lr: 0.01000\tTime 0.330 (0.336)\tData 0.000 (0.022)\tLoss 2.4748 (2.6660)\tPrec@1 84.375 (83.022)\tPrec@5 98.438 (98.742)\n",
      "Epoch: [41][50/97], lr: 0.01000\tTime 0.326 (0.335)\tData 0.000 (0.021)\tLoss 3.3024 (2.7343)\tPrec@1 78.125 (82.659)\tPrec@5 98.438 (98.683)\n",
      "Epoch: [41][60/97], lr: 0.01000\tTime 0.328 (0.334)\tData 0.000 (0.021)\tLoss 2.3706 (2.7128)\tPrec@1 85.156 (82.864)\tPrec@5 100.000 (98.681)\n",
      "Epoch: [41][70/97], lr: 0.01000\tTime 0.329 (0.334)\tData 0.000 (0.020)\tLoss 3.1486 (2.7352)\tPrec@1 78.906 (82.735)\tPrec@5 100.000 (98.702)\n",
      "Epoch: [41][80/97], lr: 0.01000\tTime 0.327 (0.334)\tData 0.000 (0.020)\tLoss 2.9621 (2.7502)\tPrec@1 79.688 (82.658)\tPrec@5 99.219 (98.688)\n",
      "Epoch: [41][90/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.020)\tLoss 2.8863 (2.7572)\tPrec@1 82.031 (82.564)\tPrec@5 98.438 (98.712)\n",
      "Epoch: [41][96/97], lr: 0.01000\tTime 0.319 (0.333)\tData 0.000 (0.020)\tLoss 2.4679 (2.7617)\tPrec@1 86.441 (82.541)\tPrec@5 98.305 (98.726)\n",
      "Test: [0/100]\tTime 0.261 (0.261)\tLoss 10.2064 (10.2064)\tPrec@1 51.000 (51.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 8.6629 (9.4203)\tPrec@1 51.000 (52.455)\tPrec@5 97.000 (97.091)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 9.1186 (9.2943)\tPrec@1 54.000 (52.714)\tPrec@5 95.000 (96.190)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.4385 (9.2940)\tPrec@1 56.000 (52.452)\tPrec@5 95.000 (95.742)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 10.0329 (9.3007)\tPrec@1 50.000 (52.561)\tPrec@5 93.000 (95.854)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 9.1353 (9.2009)\tPrec@1 56.000 (53.098)\tPrec@5 97.000 (96.000)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.9580 (9.1979)\tPrec@1 58.000 (52.984)\tPrec@5 96.000 (96.033)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.7409 (9.1780)\tPrec@1 55.000 (52.930)\tPrec@5 97.000 (96.169)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.4953 (9.1516)\tPrec@1 56.000 (53.062)\tPrec@5 98.000 (96.210)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.5578 (9.1998)\tPrec@1 51.000 (52.978)\tPrec@5 96.000 (96.055)\n",
      "val Results: Prec@1 52.830 Prec@5 96.020 Loss 9.22141\n",
      "val Class Accuracy: [0.914,0.986,0.547,0.787,0.817,0.269,0.423,0.232,0.268,0.040]\n",
      "Best Prec@1: 56.400\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [42][0/97], lr: 0.01000\tTime 0.470 (0.470)\tData 0.278 (0.278)\tLoss 2.9250 (2.9250)\tPrec@1 82.812 (82.812)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [42][10/97], lr: 0.01000\tTime 0.328 (0.349)\tData 0.000 (0.040)\tLoss 3.3811 (2.5985)\tPrec@1 78.906 (84.517)\tPrec@5 98.438 (98.935)\n",
      "Epoch: [42][20/97], lr: 0.01000\tTime 0.331 (0.340)\tData 0.000 (0.029)\tLoss 2.0940 (2.5872)\tPrec@1 85.938 (84.375)\tPrec@5 99.219 (98.921)\n",
      "Epoch: [42][30/97], lr: 0.01000\tTime 0.325 (0.337)\tData 0.000 (0.025)\tLoss 2.2755 (2.6426)\tPrec@1 86.719 (83.745)\tPrec@5 97.656 (98.639)\n",
      "Epoch: [42][40/97], lr: 0.01000\tTime 0.327 (0.336)\tData 0.000 (0.023)\tLoss 2.3163 (2.6507)\tPrec@1 83.594 (83.498)\tPrec@5 97.656 (98.590)\n",
      "Epoch: [42][50/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.022)\tLoss 2.3176 (2.6665)\tPrec@1 85.156 (83.318)\tPrec@5 99.219 (98.637)\n",
      "Epoch: [42][60/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.021)\tLoss 2.3727 (2.7012)\tPrec@1 84.375 (83.094)\tPrec@5 99.219 (98.553)\n",
      "Epoch: [42][70/97], lr: 0.01000\tTime 0.329 (0.334)\tData 0.000 (0.021)\tLoss 3.9495 (2.7635)\tPrec@1 72.656 (82.724)\tPrec@5 98.438 (98.504)\n",
      "Epoch: [42][80/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.020)\tLoss 2.0250 (2.7500)\tPrec@1 89.062 (82.870)\tPrec@5 99.219 (98.534)\n",
      "Epoch: [42][90/97], lr: 0.01000\tTime 0.322 (0.333)\tData 0.000 (0.020)\tLoss 2.4657 (2.7193)\tPrec@1 84.375 (82.993)\tPrec@5 100.000 (98.592)\n",
      "Epoch: [42][96/97], lr: 0.01000\tTime 0.318 (0.332)\tData 0.000 (0.021)\tLoss 2.8853 (2.7290)\tPrec@1 81.356 (82.960)\tPrec@5 100.000 (98.589)\n",
      "Test: [0/100]\tTime 0.278 (0.278)\tLoss 10.8414 (10.8414)\tPrec@1 41.000 (41.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 8.3413 (9.6680)\tPrec@1 56.000 (50.455)\tPrec@5 93.000 (96.000)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 7.9014 (9.5709)\tPrec@1 59.000 (51.000)\tPrec@5 93.000 (95.143)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 8.4078 (9.5338)\tPrec@1 50.000 (51.097)\tPrec@5 96.000 (95.065)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 9.3645 (9.5428)\tPrec@1 52.000 (51.463)\tPrec@5 94.000 (95.098)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 9.1349 (9.4246)\tPrec@1 57.000 (52.098)\tPrec@5 95.000 (95.196)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.8403 (9.3688)\tPrec@1 62.000 (52.410)\tPrec@5 94.000 (95.049)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 9.2845 (9.3578)\tPrec@1 49.000 (52.268)\tPrec@5 95.000 (95.127)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.0128 (9.3279)\tPrec@1 56.000 (52.407)\tPrec@5 92.000 (95.185)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.3358 (9.3856)\tPrec@1 51.000 (52.198)\tPrec@5 98.000 (95.055)\n",
      "val Results: Prec@1 52.180 Prec@5 95.050 Loss 9.40612\n",
      "val Class Accuracy: [0.975,0.987,0.632,0.708,0.582,0.580,0.151,0.417,0.168,0.018]\n",
      "Best Prec@1: 56.400\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [43][0/97], lr: 0.01000\tTime 0.456 (0.456)\tData 0.254 (0.254)\tLoss 2.9114 (2.9114)\tPrec@1 81.250 (81.250)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [43][10/97], lr: 0.01000\tTime 0.327 (0.346)\tData 0.000 (0.038)\tLoss 1.8804 (2.6927)\tPrec@1 89.062 (83.168)\tPrec@5 99.219 (98.366)\n",
      "Epoch: [43][20/97], lr: 0.01000\tTime 0.327 (0.338)\tData 0.000 (0.028)\tLoss 2.0757 (2.7693)\tPrec@1 88.281 (83.147)\tPrec@5 100.000 (98.661)\n",
      "Epoch: [43][30/97], lr: 0.01000\tTime 0.327 (0.336)\tData 0.000 (0.024)\tLoss 2.7887 (2.7193)\tPrec@1 81.250 (83.443)\tPrec@5 99.219 (98.715)\n",
      "Epoch: [43][40/97], lr: 0.01000\tTime 0.327 (0.334)\tData 0.000 (0.023)\tLoss 2.7386 (2.6997)\tPrec@1 83.594 (83.460)\tPrec@5 99.219 (98.761)\n",
      "Epoch: [43][50/97], lr: 0.01000\tTime 0.329 (0.334)\tData 0.000 (0.022)\tLoss 2.4482 (2.7034)\tPrec@1 85.938 (83.441)\tPrec@5 98.438 (98.744)\n",
      "Epoch: [43][60/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 2.9992 (2.6802)\tPrec@1 79.688 (83.466)\tPrec@5 98.438 (98.873)\n",
      "Epoch: [43][70/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.020)\tLoss 3.7351 (2.6857)\tPrec@1 75.781 (83.385)\tPrec@5 98.438 (98.856)\n",
      "Epoch: [43][80/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 2.0150 (2.6737)\tPrec@1 88.281 (83.497)\tPrec@5 100.000 (98.823)\n",
      "Epoch: [43][90/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.020)\tLoss 2.5260 (2.6593)\tPrec@1 84.375 (83.611)\tPrec@5 98.438 (98.721)\n",
      "Epoch: [43][96/97], lr: 0.01000\tTime 0.316 (0.332)\tData 0.000 (0.020)\tLoss 2.7769 (2.6669)\tPrec@1 80.508 (83.532)\tPrec@5 97.458 (98.702)\n",
      "Test: [0/100]\tTime 0.248 (0.248)\tLoss 10.7542 (10.7542)\tPrec@1 48.000 (48.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.5607 (9.5117)\tPrec@1 63.000 (53.909)\tPrec@5 98.000 (96.000)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 8.9383 (9.5741)\tPrec@1 52.000 (53.333)\tPrec@5 95.000 (95.619)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.1608 (9.5195)\tPrec@1 57.000 (53.387)\tPrec@5 95.000 (95.645)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.5336 (9.5462)\tPrec@1 52.000 (53.537)\tPrec@5 98.000 (95.610)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.8503 (9.4714)\tPrec@1 58.000 (53.961)\tPrec@5 95.000 (95.569)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.0252 (9.3652)\tPrec@1 66.000 (54.148)\tPrec@5 96.000 (95.623)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.1045 (9.3436)\tPrec@1 55.000 (54.183)\tPrec@5 98.000 (95.704)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.7630 (9.3178)\tPrec@1 58.000 (54.272)\tPrec@5 92.000 (95.704)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.7920 (9.3904)\tPrec@1 53.000 (53.868)\tPrec@5 99.000 (95.681)\n",
      "val Results: Prec@1 53.780 Prec@5 95.690 Loss 9.42383\n",
      "val Class Accuracy: [0.948,0.986,0.751,0.386,0.606,0.567,0.676,0.437,0.020,0.001]\n",
      "Best Prec@1: 56.400\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [44][0/97], lr: 0.01000\tTime 0.483 (0.483)\tData 0.288 (0.288)\tLoss 2.5136 (2.5136)\tPrec@1 82.031 (82.031)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [44][10/97], lr: 0.01000\tTime 0.328 (0.350)\tData 0.000 (0.041)\tLoss 2.7057 (2.6771)\tPrec@1 82.031 (82.528)\tPrec@5 98.438 (98.580)\n",
      "Epoch: [44][20/97], lr: 0.01000\tTime 0.326 (0.340)\tData 0.000 (0.030)\tLoss 2.1704 (2.7346)\tPrec@1 84.375 (82.254)\tPrec@5 97.656 (98.251)\n",
      "Epoch: [44][30/97], lr: 0.01000\tTime 0.326 (0.337)\tData 0.000 (0.026)\tLoss 2.3399 (2.7583)\tPrec@1 83.594 (82.258)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [44][40/97], lr: 0.01000\tTime 0.329 (0.335)\tData 0.000 (0.024)\tLoss 2.8276 (2.6926)\tPrec@1 82.031 (82.889)\tPrec@5 99.219 (98.552)\n",
      "Epoch: [44][50/97], lr: 0.01000\tTime 0.327 (0.334)\tData 0.000 (0.022)\tLoss 1.8736 (2.6877)\tPrec@1 88.281 (83.104)\tPrec@5 96.875 (98.499)\n",
      "Epoch: [44][60/97], lr: 0.01000\tTime 0.329 (0.334)\tData 0.000 (0.022)\tLoss 3.4899 (2.7018)\tPrec@1 77.344 (83.043)\tPrec@5 98.438 (98.578)\n",
      "Epoch: [44][70/97], lr: 0.01000\tTime 0.332 (0.333)\tData 0.000 (0.021)\tLoss 3.2212 (2.6626)\tPrec@1 79.688 (83.363)\tPrec@5 98.438 (98.603)\n",
      "Epoch: [44][80/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.020)\tLoss 2.4256 (2.6679)\tPrec@1 82.031 (83.169)\tPrec@5 98.438 (98.592)\n",
      "Epoch: [44][90/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.8177 (2.6889)\tPrec@1 82.031 (82.984)\tPrec@5 99.219 (98.566)\n",
      "Epoch: [44][96/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 3.1531 (2.7022)\tPrec@1 82.203 (82.911)\tPrec@5 96.610 (98.533)\n",
      "Test: [0/100]\tTime 0.229 (0.229)\tLoss 10.2465 (10.2465)\tPrec@1 45.000 (45.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 7.7705 (9.2697)\tPrec@1 56.000 (52.364)\tPrec@5 93.000 (95.364)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 8.7309 (9.3263)\tPrec@1 54.000 (52.476)\tPrec@5 96.000 (95.143)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 8.2195 (9.2949)\tPrec@1 55.000 (52.290)\tPrec@5 96.000 (95.032)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.7649 (9.3231)\tPrec@1 50.000 (52.512)\tPrec@5 94.000 (94.976)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.8116 (9.2307)\tPrec@1 58.000 (53.157)\tPrec@5 95.000 (95.020)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.0710 (9.1568)\tPrec@1 64.000 (53.426)\tPrec@5 95.000 (95.180)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.9090 (9.1707)\tPrec@1 51.000 (53.352)\tPrec@5 97.000 (95.239)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.8592 (9.1573)\tPrec@1 55.000 (53.457)\tPrec@5 92.000 (95.235)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.4968 (9.2335)\tPrec@1 50.000 (53.055)\tPrec@5 97.000 (95.187)\n",
      "val Results: Prec@1 52.830 Prec@5 95.200 Loss 9.27205\n",
      "val Class Accuracy: [0.976,0.979,0.637,0.616,0.713,0.413,0.596,0.296,0.055,0.002]\n",
      "Best Prec@1: 56.400\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [45][0/97], lr: 0.01000\tTime 0.469 (0.469)\tData 0.268 (0.268)\tLoss 2.0727 (2.0727)\tPrec@1 86.719 (86.719)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [45][10/97], lr: 0.01000\tTime 0.327 (0.349)\tData 0.000 (0.039)\tLoss 3.3473 (2.5812)\tPrec@1 78.906 (83.807)\tPrec@5 97.656 (98.580)\n",
      "Epoch: [45][20/97], lr: 0.01000\tTime 0.335 (0.341)\tData 0.000 (0.029)\tLoss 3.2589 (2.6142)\tPrec@1 78.906 (83.557)\tPrec@5 97.656 (98.438)\n",
      "Epoch: [45][30/97], lr: 0.01000\tTime 0.325 (0.338)\tData 0.000 (0.025)\tLoss 2.6478 (2.6489)\tPrec@1 82.031 (83.443)\tPrec@5 99.219 (98.639)\n",
      "Epoch: [45][40/97], lr: 0.01000\tTime 0.328 (0.336)\tData 0.000 (0.023)\tLoss 2.7202 (2.6420)\tPrec@1 83.594 (83.575)\tPrec@5 96.875 (98.552)\n",
      "Epoch: [45][50/97], lr: 0.01000\tTime 0.324 (0.335)\tData 0.000 (0.022)\tLoss 2.4267 (2.6515)\tPrec@1 83.594 (83.502)\tPrec@5 99.219 (98.621)\n",
      "Epoch: [45][60/97], lr: 0.01000\tTime 0.326 (0.335)\tData 0.000 (0.021)\tLoss 3.2715 (2.6618)\tPrec@1 78.906 (83.350)\tPrec@5 99.219 (98.617)\n",
      "Epoch: [45][70/97], lr: 0.01000\tTime 0.328 (0.334)\tData 0.000 (0.021)\tLoss 2.8733 (2.6414)\tPrec@1 82.031 (83.638)\tPrec@5 98.438 (98.603)\n",
      "Epoch: [45][80/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.020)\tLoss 2.2830 (2.6194)\tPrec@1 84.375 (83.719)\tPrec@5 100.000 (98.650)\n",
      "Epoch: [45][90/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.020)\tLoss 2.1918 (2.6177)\tPrec@1 86.719 (83.783)\tPrec@5 99.219 (98.661)\n",
      "Epoch: [45][96/97], lr: 0.01000\tTime 0.319 (0.333)\tData 0.000 (0.020)\tLoss 2.8443 (2.6423)\tPrec@1 81.356 (83.581)\tPrec@5 97.458 (98.638)\n",
      "Test: [0/100]\tTime 0.268 (0.268)\tLoss 12.4803 (12.4803)\tPrec@1 36.000 (36.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 8.8129 (10.7946)\tPrec@1 54.000 (44.273)\tPrec@5 98.000 (94.000)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 9.8760 (10.7311)\tPrec@1 46.000 (44.238)\tPrec@5 96.000 (94.333)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 9.9622 (10.7378)\tPrec@1 50.000 (44.355)\tPrec@5 94.000 (93.774)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 11.4326 (10.7467)\tPrec@1 42.000 (44.512)\tPrec@5 91.000 (93.561)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 10.8224 (10.6845)\tPrec@1 46.000 (44.941)\tPrec@5 92.000 (93.647)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.4857 (10.6457)\tPrec@1 57.000 (44.984)\tPrec@5 94.000 (93.607)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 10.7486 (10.6223)\tPrec@1 42.000 (45.141)\tPrec@5 96.000 (93.620)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.5161 (10.5901)\tPrec@1 50.000 (45.309)\tPrec@5 89.000 (93.704)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.4413 (10.6457)\tPrec@1 48.000 (45.044)\tPrec@5 96.000 (93.703)\n",
      "val Results: Prec@1 44.970 Prec@5 93.610 Loss 10.66097\n",
      "val Class Accuracy: [0.938,0.959,0.841,0.538,0.598,0.369,0.103,0.094,0.057,0.000]\n",
      "Best Prec@1: 56.400\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [46][0/97], lr: 0.01000\tTime 0.426 (0.426)\tData 0.228 (0.228)\tLoss 3.4221 (3.4221)\tPrec@1 78.125 (78.125)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [46][10/97], lr: 0.01000\tTime 0.328 (0.343)\tData 0.000 (0.036)\tLoss 2.1967 (2.7637)\tPrec@1 85.938 (82.955)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [46][20/97], lr: 0.01000\tTime 0.324 (0.336)\tData 0.000 (0.027)\tLoss 2.4961 (2.7085)\tPrec@1 82.031 (82.850)\tPrec@5 97.656 (99.033)\n",
      "Epoch: [46][30/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.024)\tLoss 2.9436 (2.7488)\tPrec@1 82.031 (82.712)\tPrec@5 96.875 (99.017)\n",
      "Epoch: [46][40/97], lr: 0.01000\tTime 0.332 (0.333)\tData 0.000 (0.022)\tLoss 2.4632 (2.7080)\tPrec@1 85.156 (82.832)\tPrec@5 98.438 (98.876)\n",
      "Epoch: [46][50/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 3.3228 (2.7255)\tPrec@1 76.562 (82.767)\tPrec@5 99.219 (98.790)\n",
      "Epoch: [46][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 2.4857 (2.6980)\tPrec@1 83.594 (83.005)\tPrec@5 100.000 (98.847)\n",
      "Epoch: [46][70/97], lr: 0.01000\tTime 0.330 (0.332)\tData 0.000 (0.020)\tLoss 2.9611 (2.7531)\tPrec@1 79.688 (82.592)\tPrec@5 98.438 (98.669)\n",
      "Epoch: [46][80/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 2.3625 (2.7503)\tPrec@1 85.156 (82.658)\tPrec@5 99.219 (98.698)\n",
      "Epoch: [46][90/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.019)\tLoss 3.4819 (2.7564)\tPrec@1 78.125 (82.641)\tPrec@5 98.438 (98.738)\n",
      "Epoch: [46][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 2.3881 (2.7305)\tPrec@1 85.593 (82.879)\tPrec@5 98.305 (98.775)\n",
      "Test: [0/100]\tTime 0.255 (0.255)\tLoss 10.9144 (10.9144)\tPrec@1 47.000 (47.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 9.0949 (10.4331)\tPrec@1 53.000 (48.273)\tPrec@5 92.000 (93.818)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 9.0552 (10.3284)\tPrec@1 53.000 (47.952)\tPrec@5 96.000 (94.000)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 9.8251 (10.3167)\tPrec@1 49.000 (47.742)\tPrec@5 95.000 (94.000)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.4743 (10.3165)\tPrec@1 46.000 (47.707)\tPrec@5 93.000 (93.951)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.8039 (10.2118)\tPrec@1 52.000 (48.353)\tPrec@5 93.000 (94.059)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.9598 (10.1701)\tPrec@1 57.000 (48.508)\tPrec@5 95.000 (94.115)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.0442 (10.1194)\tPrec@1 58.000 (48.803)\tPrec@5 99.000 (94.239)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.0074 (10.0439)\tPrec@1 58.000 (49.370)\tPrec@5 92.000 (94.284)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.0711 (10.0898)\tPrec@1 51.000 (49.165)\tPrec@5 94.000 (94.132)\n",
      "val Results: Prec@1 49.180 Prec@5 94.010 Loss 10.10237\n",
      "val Class Accuracy: [0.903,0.971,0.738,0.846,0.420,0.307,0.366,0.270,0.097,0.000]\n",
      "Best Prec@1: 56.400\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [47][0/97], lr: 0.01000\tTime 0.464 (0.464)\tData 0.258 (0.258)\tLoss 3.2075 (3.2075)\tPrec@1 78.906 (78.906)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [47][10/97], lr: 0.01000\tTime 0.328 (0.347)\tData 0.000 (0.039)\tLoss 3.4444 (2.7282)\tPrec@1 76.562 (83.097)\tPrec@5 98.438 (98.722)\n",
      "Epoch: [47][20/97], lr: 0.01000\tTime 0.332 (0.340)\tData 0.000 (0.028)\tLoss 2.3324 (2.7899)\tPrec@1 86.719 (82.478)\tPrec@5 99.219 (98.698)\n",
      "Epoch: [47][30/97], lr: 0.01000\tTime 0.327 (0.336)\tData 0.000 (0.025)\tLoss 2.5843 (2.7518)\tPrec@1 82.031 (82.686)\tPrec@5 97.656 (98.664)\n",
      "Epoch: [47][40/97], lr: 0.01000\tTime 0.331 (0.335)\tData 0.000 (0.023)\tLoss 2.7787 (2.7058)\tPrec@1 79.688 (82.870)\tPrec@5 99.219 (98.723)\n",
      "Epoch: [47][50/97], lr: 0.01000\tTime 0.329 (0.334)\tData 0.000 (0.022)\tLoss 2.9901 (2.6986)\tPrec@1 81.250 (82.935)\tPrec@5 100.000 (98.790)\n",
      "Epoch: [47][60/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.021)\tLoss 1.8344 (2.7193)\tPrec@1 87.500 (82.877)\tPrec@5 99.219 (98.668)\n",
      "Epoch: [47][70/97], lr: 0.01000\tTime 0.332 (0.334)\tData 0.000 (0.021)\tLoss 1.8003 (2.7180)\tPrec@1 89.062 (82.857)\tPrec@5 100.000 (98.724)\n",
      "Epoch: [47][80/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 2.2396 (2.7062)\tPrec@1 85.938 (82.928)\tPrec@5 100.000 (98.756)\n",
      "Epoch: [47][90/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 2.0978 (2.6979)\tPrec@1 85.938 (82.958)\tPrec@5 98.438 (98.738)\n",
      "Epoch: [47][96/97], lr: 0.01000\tTime 0.317 (0.333)\tData 0.000 (0.020)\tLoss 2.9187 (2.7088)\tPrec@1 81.356 (82.855)\tPrec@5 96.610 (98.710)\n",
      "Test: [0/100]\tTime 0.273 (0.273)\tLoss 11.7503 (11.7503)\tPrec@1 45.000 (45.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 9.2010 (10.8120)\tPrec@1 53.000 (46.636)\tPrec@5 97.000 (94.182)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 10.2407 (10.8275)\tPrec@1 46.000 (45.571)\tPrec@5 96.000 (94.286)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 10.3541 (10.8499)\tPrec@1 45.000 (45.323)\tPrec@5 93.000 (93.903)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 11.5962 (10.9251)\tPrec@1 43.000 (44.951)\tPrec@5 90.000 (93.805)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 10.8814 (10.7780)\tPrec@1 45.000 (45.569)\tPrec@5 91.000 (94.039)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.8019 (10.7645)\tPrec@1 56.000 (45.656)\tPrec@5 93.000 (94.082)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 9.7262 (10.7121)\tPrec@1 51.000 (45.986)\tPrec@5 97.000 (94.099)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 10.2764 (10.6864)\tPrec@1 49.000 (46.173)\tPrec@5 90.000 (94.123)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.2744 (10.7408)\tPrec@1 43.000 (45.945)\tPrec@5 97.000 (94.121)\n",
      "val Results: Prec@1 46.120 Prec@5 94.070 Loss 10.73485\n",
      "val Class Accuracy: [0.814,0.951,0.724,0.919,0.489,0.125,0.307,0.092,0.191,0.000]\n",
      "Best Prec@1: 56.400\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [48][0/97], lr: 0.01000\tTime 0.404 (0.404)\tData 0.215 (0.215)\tLoss 2.5727 (2.5727)\tPrec@1 82.031 (82.031)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [48][10/97], lr: 0.01000\tTime 0.329 (0.344)\tData 0.000 (0.034)\tLoss 2.5697 (2.5938)\tPrec@1 83.594 (83.452)\tPrec@5 98.438 (98.793)\n",
      "Epoch: [48][20/97], lr: 0.01000\tTime 0.330 (0.337)\tData 0.000 (0.026)\tLoss 2.5186 (2.5238)\tPrec@1 85.156 (84.040)\tPrec@5 99.219 (98.810)\n",
      "Epoch: [48][30/97], lr: 0.01000\tTime 0.325 (0.335)\tData 0.000 (0.023)\tLoss 2.5660 (2.5592)\tPrec@1 85.938 (83.871)\tPrec@5 97.656 (98.765)\n",
      "Epoch: [48][40/97], lr: 0.01000\tTime 0.327 (0.335)\tData 0.000 (0.022)\tLoss 1.7632 (2.5253)\tPrec@1 90.625 (84.127)\tPrec@5 99.219 (98.780)\n",
      "Epoch: [48][50/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 2.0770 (2.5587)\tPrec@1 86.719 (83.854)\tPrec@5 100.000 (98.713)\n",
      "Epoch: [48][60/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 2.6332 (2.6076)\tPrec@1 80.469 (83.491)\tPrec@5 96.875 (98.694)\n",
      "Epoch: [48][70/97], lr: 0.01000\tTime 0.329 (0.333)\tData 0.000 (0.020)\tLoss 2.8586 (2.6086)\tPrec@1 78.906 (83.572)\tPrec@5 98.438 (98.724)\n",
      "Epoch: [48][80/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 3.0380 (2.6436)\tPrec@1 80.469 (83.382)\tPrec@5 96.875 (98.650)\n",
      "Epoch: [48][90/97], lr: 0.01000\tTime 0.329 (0.333)\tData 0.000 (0.019)\tLoss 2.9127 (2.6505)\tPrec@1 80.469 (83.379)\tPrec@5 100.000 (98.755)\n",
      "Epoch: [48][96/97], lr: 0.01000\tTime 0.316 (0.332)\tData 0.000 (0.020)\tLoss 2.2812 (2.6450)\tPrec@1 84.746 (83.379)\tPrec@5 99.153 (98.751)\n",
      "Test: [0/100]\tTime 0.268 (0.268)\tLoss 10.0189 (10.0189)\tPrec@1 47.000 (47.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 6.4553 (9.1331)\tPrec@1 68.000 (54.545)\tPrec@5 99.000 (95.182)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 8.9247 (9.2198)\tPrec@1 54.000 (53.571)\tPrec@5 93.000 (94.571)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.9362 (9.2236)\tPrec@1 56.000 (53.355)\tPrec@5 93.000 (94.677)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 9.9069 (9.2600)\tPrec@1 48.000 (52.976)\tPrec@5 94.000 (94.780)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 8.6115 (9.1662)\tPrec@1 56.000 (53.294)\tPrec@5 94.000 (94.922)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.9030 (9.1182)\tPrec@1 60.000 (53.426)\tPrec@5 93.000 (94.705)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.5364 (9.1141)\tPrec@1 53.000 (53.324)\tPrec@5 97.000 (94.746)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.1924 (9.0690)\tPrec@1 58.000 (53.556)\tPrec@5 93.000 (94.815)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.7830 (9.1318)\tPrec@1 55.000 (53.297)\tPrec@5 94.000 (94.780)\n",
      "val Results: Prec@1 53.190 Prec@5 94.690 Loss 9.15073\n",
      "val Class Accuracy: [0.897,0.984,0.823,0.497,0.600,0.286,0.803,0.329,0.085,0.015]\n",
      "Best Prec@1: 56.400\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [49][0/97], lr: 0.01000\tTime 0.438 (0.438)\tData 0.233 (0.233)\tLoss 2.1758 (2.1758)\tPrec@1 85.156 (85.156)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [49][10/97], lr: 0.01000\tTime 0.327 (0.345)\tData 0.000 (0.036)\tLoss 3.1016 (2.5821)\tPrec@1 82.031 (83.026)\tPrec@5 98.438 (98.793)\n",
      "Epoch: [49][20/97], lr: 0.01000\tTime 0.338 (0.339)\tData 0.000 (0.027)\tLoss 2.8223 (2.6406)\tPrec@1 83.594 (83.036)\tPrec@5 98.438 (98.698)\n",
      "Epoch: [49][30/97], lr: 0.01000\tTime 0.359 (0.338)\tData 0.000 (0.024)\tLoss 2.6209 (2.6291)\tPrec@1 84.375 (82.964)\tPrec@5 99.219 (98.866)\n",
      "Epoch: [49][40/97], lr: 0.01000\tTime 0.356 (0.348)\tData 0.000 (0.022)\tLoss 2.2401 (2.6311)\tPrec@1 85.938 (83.079)\tPrec@5 99.219 (98.952)\n",
      "Epoch: [49][50/97], lr: 0.01000\tTime 0.329 (0.346)\tData 0.000 (0.021)\tLoss 2.7347 (2.6508)\tPrec@1 82.031 (82.920)\tPrec@5 98.438 (98.912)\n",
      "Epoch: [49][60/97], lr: 0.01000\tTime 0.325 (0.344)\tData 0.000 (0.020)\tLoss 3.7070 (2.6535)\tPrec@1 77.344 (82.915)\tPrec@5 97.656 (98.822)\n",
      "Epoch: [49][70/97], lr: 0.01000\tTime 0.327 (0.342)\tData 0.000 (0.020)\tLoss 3.2414 (2.6550)\tPrec@1 82.031 (83.000)\tPrec@5 98.438 (98.801)\n",
      "Epoch: [49][80/97], lr: 0.01000\tTime 0.334 (0.340)\tData 0.000 (0.020)\tLoss 3.0656 (2.6757)\tPrec@1 80.469 (82.861)\tPrec@5 100.000 (98.823)\n",
      "Epoch: [49][90/97], lr: 0.01000\tTime 0.323 (0.339)\tData 0.000 (0.019)\tLoss 4.0885 (2.6937)\tPrec@1 71.875 (82.744)\tPrec@5 99.219 (98.755)\n",
      "Epoch: [49][96/97], lr: 0.01000\tTime 0.346 (0.338)\tData 0.000 (0.020)\tLoss 3.2721 (2.6866)\tPrec@1 79.661 (82.807)\tPrec@5 98.305 (98.751)\n",
      "Test: [0/100]\tTime 0.242 (0.242)\tLoss 12.0000 (12.0000)\tPrec@1 40.000 (40.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 9.1127 (11.0547)\tPrec@1 55.000 (43.909)\tPrec@5 95.000 (92.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 10.3457 (11.0549)\tPrec@1 42.000 (43.238)\tPrec@5 96.000 (93.238)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 10.3617 (10.9967)\tPrec@1 43.000 (43.290)\tPrec@5 93.000 (93.097)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 11.4331 (11.0125)\tPrec@1 41.000 (43.537)\tPrec@5 90.000 (92.780)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 10.8431 (10.8546)\tPrec@1 48.000 (44.431)\tPrec@5 95.000 (92.941)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 9.0379 (10.8408)\tPrec@1 51.000 (44.393)\tPrec@5 97.000 (93.082)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 10.2410 (10.8306)\tPrec@1 49.000 (44.479)\tPrec@5 95.000 (93.099)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 10.9137 (10.7873)\tPrec@1 49.000 (44.852)\tPrec@5 88.000 (93.136)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.2934 (10.8437)\tPrec@1 40.000 (44.538)\tPrec@5 94.000 (93.022)\n",
      "val Results: Prec@1 44.610 Prec@5 93.070 Loss 10.85017\n",
      "val Class Accuracy: [0.876,0.953,0.712,0.915,0.330,0.098,0.324,0.158,0.095,0.000]\n",
      "Best Prec@1: 56.400\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [50][0/97], lr: 0.01000\tTime 0.428 (0.428)\tData 0.229 (0.229)\tLoss 2.7150 (2.7150)\tPrec@1 81.250 (81.250)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [50][10/97], lr: 0.01000\tTime 0.328 (0.343)\tData 0.000 (0.036)\tLoss 2.0337 (2.6996)\tPrec@1 89.062 (82.884)\tPrec@5 99.219 (98.864)\n",
      "Epoch: [50][20/97], lr: 0.01000\tTime 0.326 (0.337)\tData 0.000 (0.027)\tLoss 2.4299 (2.6271)\tPrec@1 87.500 (83.482)\tPrec@5 100.000 (99.033)\n",
      "Epoch: [50][30/97], lr: 0.01000\tTime 0.325 (0.335)\tData 0.000 (0.024)\tLoss 2.1352 (2.5587)\tPrec@1 85.156 (83.871)\tPrec@5 99.219 (99.017)\n",
      "Epoch: [50][40/97], lr: 0.01000\tTime 0.327 (0.334)\tData 0.000 (0.022)\tLoss 2.8524 (2.4658)\tPrec@1 82.812 (84.508)\tPrec@5 98.438 (98.990)\n",
      "Epoch: [50][50/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.021)\tLoss 2.6037 (2.5103)\tPrec@1 85.156 (84.314)\tPrec@5 98.438 (98.943)\n",
      "Epoch: [50][60/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.021)\tLoss 2.5493 (2.5588)\tPrec@1 84.375 (84.068)\tPrec@5 98.438 (98.886)\n",
      "Epoch: [50][70/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 2.4831 (2.5751)\tPrec@1 82.031 (83.858)\tPrec@5 98.438 (98.845)\n",
      "Epoch: [50][80/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.6170 (2.5741)\tPrec@1 83.594 (83.989)\tPrec@5 100.000 (98.891)\n",
      "Epoch: [50][90/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.019)\tLoss 2.4935 (2.5614)\tPrec@1 83.594 (84.100)\tPrec@5 99.219 (98.918)\n",
      "Epoch: [50][96/97], lr: 0.01000\tTime 0.319 (0.332)\tData 0.000 (0.020)\tLoss 2.8029 (2.5697)\tPrec@1 83.051 (84.048)\tPrec@5 98.305 (98.904)\n",
      "Test: [0/100]\tTime 0.237 (0.237)\tLoss 10.0598 (10.0598)\tPrec@1 48.000 (48.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 6.9894 (9.0213)\tPrec@1 64.000 (55.273)\tPrec@5 96.000 (94.909)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.7858 (9.0846)\tPrec@1 60.000 (54.095)\tPrec@5 95.000 (95.000)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 8.2324 (9.0782)\tPrec@1 60.000 (54.323)\tPrec@5 95.000 (95.000)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.4691 (9.0837)\tPrec@1 54.000 (54.659)\tPrec@5 92.000 (94.854)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.9043 (9.0020)\tPrec@1 56.000 (54.980)\tPrec@5 95.000 (95.196)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.4402 (8.9492)\tPrec@1 64.000 (55.049)\tPrec@5 95.000 (95.311)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.2422 (8.9352)\tPrec@1 54.000 (55.113)\tPrec@5 94.000 (95.352)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.8803 (8.8992)\tPrec@1 53.000 (55.296)\tPrec@5 92.000 (95.457)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.6528 (8.9567)\tPrec@1 53.000 (55.033)\tPrec@5 98.000 (95.516)\n",
      "val Results: Prec@1 54.780 Prec@5 95.480 Loss 8.99483\n",
      "val Class Accuracy: [0.950,0.968,0.820,0.611,0.586,0.376,0.620,0.435,0.097,0.015]\n",
      "Best Prec@1: 56.400\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [51][0/97], lr: 0.01000\tTime 0.426 (0.426)\tData 0.217 (0.217)\tLoss 2.6192 (2.6192)\tPrec@1 85.938 (85.938)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [51][10/97], lr: 0.01000\tTime 0.333 (0.344)\tData 0.000 (0.034)\tLoss 3.1000 (2.6605)\tPrec@1 80.469 (83.239)\tPrec@5 98.438 (98.366)\n",
      "Epoch: [51][20/97], lr: 0.01000\tTime 0.325 (0.338)\tData 0.000 (0.026)\tLoss 1.9187 (2.4364)\tPrec@1 87.500 (84.710)\tPrec@5 98.438 (98.772)\n",
      "Epoch: [51][30/97], lr: 0.01000\tTime 0.325 (0.336)\tData 0.000 (0.023)\tLoss 1.6741 (2.4371)\tPrec@1 89.062 (84.753)\tPrec@5 100.000 (98.765)\n",
      "Epoch: [51][40/97], lr: 0.01000\tTime 0.329 (0.334)\tData 0.000 (0.022)\tLoss 2.1514 (2.4656)\tPrec@1 85.938 (84.566)\tPrec@5 99.219 (98.761)\n",
      "Epoch: [51][50/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.021)\tLoss 2.0610 (2.4901)\tPrec@1 85.156 (84.360)\tPrec@5 100.000 (98.759)\n",
      "Epoch: [51][60/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.020)\tLoss 2.5487 (2.4797)\tPrec@1 85.156 (84.529)\tPrec@5 99.219 (98.835)\n",
      "Epoch: [51][70/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.020)\tLoss 2.9067 (2.5689)\tPrec@1 83.594 (83.990)\tPrec@5 98.438 (98.812)\n",
      "Epoch: [51][80/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.019)\tLoss 2.8507 (2.5919)\tPrec@1 82.031 (83.816)\tPrec@5 97.656 (98.756)\n",
      "Epoch: [51][90/97], lr: 0.01000\tTime 0.329 (0.332)\tData 0.000 (0.019)\tLoss 2.4514 (2.5896)\tPrec@1 82.812 (83.817)\tPrec@5 100.000 (98.712)\n",
      "Epoch: [51][96/97], lr: 0.01000\tTime 0.317 (0.332)\tData 0.000 (0.020)\tLoss 2.4456 (2.5853)\tPrec@1 85.593 (83.855)\tPrec@5 100.000 (98.751)\n",
      "Test: [0/100]\tTime 0.248 (0.248)\tLoss 10.9816 (10.9816)\tPrec@1 48.000 (48.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.6677 (9.7869)\tPrec@1 65.000 (52.636)\tPrec@5 96.000 (94.091)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 9.1665 (9.8552)\tPrec@1 51.000 (52.000)\tPrec@5 96.000 (93.429)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 9.0325 (9.8511)\tPrec@1 53.000 (51.548)\tPrec@5 92.000 (93.290)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.5571 (9.8615)\tPrec@1 45.000 (51.512)\tPrec@5 87.000 (93.220)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.6035 (9.7864)\tPrec@1 54.000 (51.922)\tPrec@5 92.000 (93.235)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.0944 (9.7643)\tPrec@1 60.000 (51.918)\tPrec@5 94.000 (93.230)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.3289 (9.7276)\tPrec@1 57.000 (52.028)\tPrec@5 93.000 (93.352)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.2926 (9.6749)\tPrec@1 55.000 (52.309)\tPrec@5 93.000 (93.531)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.8211 (9.7289)\tPrec@1 53.000 (52.187)\tPrec@5 94.000 (93.440)\n",
      "val Results: Prec@1 52.080 Prec@5 93.390 Loss 9.74567\n",
      "val Class Accuracy: [0.849,0.993,0.856,0.619,0.474,0.389,0.635,0.253,0.139,0.001]\n",
      "Best Prec@1: 56.400\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [52][0/97], lr: 0.01000\tTime 0.463 (0.463)\tData 0.242 (0.242)\tLoss 2.4233 (2.4233)\tPrec@1 84.375 (84.375)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [52][10/97], lr: 0.01000\tTime 0.329 (0.348)\tData 0.000 (0.037)\tLoss 3.0400 (2.4827)\tPrec@1 79.688 (84.375)\tPrec@5 99.219 (98.935)\n",
      "Epoch: [52][20/97], lr: 0.01000\tTime 0.335 (0.339)\tData 0.000 (0.027)\tLoss 2.5770 (2.5677)\tPrec@1 83.594 (83.594)\tPrec@5 96.875 (98.772)\n",
      "Epoch: [52][30/97], lr: 0.01000\tTime 0.325 (0.337)\tData 0.000 (0.024)\tLoss 1.4987 (2.5499)\tPrec@1 92.969 (83.896)\tPrec@5 100.000 (98.841)\n",
      "Epoch: [52][40/97], lr: 0.01000\tTime 0.328 (0.335)\tData 0.000 (0.022)\tLoss 2.8158 (2.5283)\tPrec@1 78.906 (83.899)\tPrec@5 98.438 (98.933)\n",
      "Epoch: [52][50/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.021)\tLoss 2.1639 (2.5347)\tPrec@1 87.500 (83.977)\tPrec@5 97.656 (98.958)\n",
      "Epoch: [52][60/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.021)\tLoss 1.8992 (2.5247)\tPrec@1 89.844 (84.183)\tPrec@5 99.219 (98.860)\n",
      "Epoch: [52][70/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.020)\tLoss 2.3350 (2.5394)\tPrec@1 83.594 (84.221)\tPrec@5 99.219 (98.768)\n",
      "Epoch: [52][80/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.8604 (2.5549)\tPrec@1 79.688 (84.086)\tPrec@5 100.000 (98.727)\n",
      "Epoch: [52][90/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.020)\tLoss 3.4114 (2.5845)\tPrec@1 78.125 (83.791)\tPrec@5 97.656 (98.686)\n",
      "Epoch: [52][96/97], lr: 0.01000\tTime 0.319 (0.331)\tData 0.000 (0.020)\tLoss 2.2002 (2.5880)\tPrec@1 88.136 (83.814)\tPrec@5 98.305 (98.678)\n",
      "Test: [0/100]\tTime 0.250 (0.250)\tLoss 9.6764 (9.6764)\tPrec@1 55.000 (55.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 6.8240 (8.9857)\tPrec@1 69.000 (55.455)\tPrec@5 95.000 (94.727)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 8.2551 (8.9929)\tPrec@1 52.000 (54.667)\tPrec@5 95.000 (95.286)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.2786 (9.0228)\tPrec@1 58.000 (54.613)\tPrec@5 96.000 (95.097)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.0839 (9.1016)\tPrec@1 55.000 (54.073)\tPrec@5 97.000 (95.146)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.7147 (8.9865)\tPrec@1 65.000 (54.725)\tPrec@5 97.000 (95.353)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.8211 (8.9297)\tPrec@1 57.000 (54.918)\tPrec@5 94.000 (95.361)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.5836 (8.9178)\tPrec@1 54.000 (54.944)\tPrec@5 100.000 (95.549)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.1757 (8.8811)\tPrec@1 61.000 (55.111)\tPrec@5 93.000 (95.556)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.8068 (8.9222)\tPrec@1 58.000 (54.967)\tPrec@5 96.000 (95.407)\n",
      "val Results: Prec@1 54.850 Prec@5 95.360 Loss 8.95306\n",
      "val Class Accuracy: [0.973,0.975,0.606,0.692,0.673,0.449,0.775,0.226,0.098,0.018]\n",
      "Best Prec@1: 56.400\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [53][0/97], lr: 0.01000\tTime 0.429 (0.429)\tData 0.230 (0.230)\tLoss 2.8349 (2.8349)\tPrec@1 83.594 (83.594)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [53][10/97], lr: 0.01000\tTime 0.333 (0.345)\tData 0.000 (0.036)\tLoss 2.1380 (2.7800)\tPrec@1 85.938 (82.599)\tPrec@5 100.000 (98.438)\n",
      "Epoch: [53][20/97], lr: 0.01000\tTime 0.326 (0.339)\tData 0.000 (0.027)\tLoss 3.1075 (2.6979)\tPrec@1 82.031 (83.185)\tPrec@5 97.656 (98.400)\n",
      "Epoch: [53][30/97], lr: 0.01000\tTime 0.327 (0.337)\tData 0.000 (0.024)\tLoss 1.7816 (2.5601)\tPrec@1 88.281 (83.795)\tPrec@5 98.438 (98.690)\n",
      "Epoch: [53][40/97], lr: 0.01000\tTime 0.335 (0.335)\tData 0.000 (0.022)\tLoss 3.3162 (2.5983)\tPrec@1 76.562 (83.479)\tPrec@5 97.656 (98.685)\n",
      "Epoch: [53][50/97], lr: 0.01000\tTime 0.334 (0.335)\tData 0.000 (0.021)\tLoss 3.0074 (2.5878)\tPrec@1 79.688 (83.594)\tPrec@5 96.875 (98.575)\n",
      "Epoch: [53][60/97], lr: 0.01000\tTime 0.357 (0.336)\tData 0.000 (0.021)\tLoss 2.1746 (2.5917)\tPrec@1 88.281 (83.594)\tPrec@5 100.000 (98.604)\n",
      "Epoch: [53][70/97], lr: 0.01000\tTime 0.344 (0.339)\tData 0.000 (0.020)\tLoss 2.4150 (2.5705)\tPrec@1 85.938 (83.803)\tPrec@5 100.000 (98.702)\n",
      "Epoch: [53][80/97], lr: 0.01000\tTime 0.357 (0.340)\tData 0.000 (0.020)\tLoss 2.6023 (2.5579)\tPrec@1 82.812 (83.787)\tPrec@5 98.438 (98.659)\n",
      "Epoch: [53][90/97], lr: 0.01000\tTime 0.340 (0.340)\tData 0.000 (0.019)\tLoss 2.3912 (2.5419)\tPrec@1 83.594 (83.954)\tPrec@5 99.219 (98.721)\n",
      "Epoch: [53][96/97], lr: 0.01000\tTime 0.334 (0.341)\tData 0.000 (0.020)\tLoss 2.6064 (2.5503)\tPrec@1 82.203 (83.911)\tPrec@5 97.458 (98.662)\n",
      "Test: [0/100]\tTime 0.313 (0.313)\tLoss 10.9414 (10.9414)\tPrec@1 50.000 (50.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.095)\tLoss 8.4893 (9.7452)\tPrec@1 61.000 (53.182)\tPrec@5 97.000 (95.727)\n",
      "Test: [20/100]\tTime 0.074 (0.085)\tLoss 8.6934 (9.8469)\tPrec@1 52.000 (51.238)\tPrec@5 95.000 (94.810)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 8.3673 (9.8074)\tPrec@1 59.000 (51.710)\tPrec@5 95.000 (94.419)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 10.2447 (9.7871)\tPrec@1 50.000 (52.024)\tPrec@5 95.000 (94.488)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 9.5431 (9.7180)\tPrec@1 57.000 (52.196)\tPrec@5 97.000 (94.667)\n",
      "Test: [60/100]\tTime 0.075 (0.077)\tLoss 8.1223 (9.6817)\tPrec@1 59.000 (52.213)\tPrec@5 92.000 (94.508)\n",
      "Test: [70/100]\tTime 0.075 (0.077)\tLoss 9.5631 (9.6741)\tPrec@1 48.000 (52.085)\tPrec@5 98.000 (94.662)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 8.8765 (9.6306)\tPrec@1 58.000 (52.173)\tPrec@5 94.000 (94.827)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 9.2956 (9.7003)\tPrec@1 55.000 (51.989)\tPrec@5 97.000 (94.736)\n",
      "val Results: Prec@1 52.010 Prec@5 94.780 Loss 9.71491\n",
      "val Class Accuracy: [0.925,0.992,0.787,0.634,0.558,0.431,0.225,0.464,0.178,0.007]\n",
      "Best Prec@1: 56.400\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [54][0/97], lr: 0.01000\tTime 1.078 (1.078)\tData 0.616 (0.616)\tLoss 2.8575 (2.8575)\tPrec@1 82.812 (82.812)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [54][10/97], lr: 0.01000\tTime 0.424 (0.542)\tData 0.000 (0.066)\tLoss 2.9896 (2.3981)\tPrec@1 82.812 (84.304)\tPrec@5 100.000 (99.148)\n",
      "Epoch: [54][20/97], lr: 0.01000\tTime 0.521 (0.490)\tData 0.000 (0.042)\tLoss 3.0478 (2.5147)\tPrec@1 82.031 (84.226)\tPrec@5 100.000 (98.810)\n",
      "Epoch: [54][30/97], lr: 0.01000\tTime 0.598 (0.500)\tData 0.001 (0.032)\tLoss 2.0756 (2.5100)\tPrec@1 86.719 (84.173)\tPrec@5 98.438 (98.916)\n",
      "Epoch: [54][40/97], lr: 0.01000\tTime 0.468 (0.520)\tData 0.000 (0.027)\tLoss 3.1324 (2.4813)\tPrec@1 78.906 (84.451)\tPrec@5 99.219 (98.952)\n",
      "Epoch: [54][50/97], lr: 0.01000\tTime 0.382 (0.504)\tData 0.001 (0.025)\tLoss 2.5163 (2.4847)\tPrec@1 84.375 (84.390)\tPrec@5 100.000 (98.882)\n",
      "Epoch: [54][60/97], lr: 0.01000\tTime 0.463 (0.501)\tData 0.000 (0.023)\tLoss 3.2698 (2.4941)\tPrec@1 76.562 (84.324)\tPrec@5 97.656 (98.873)\n",
      "Epoch: [54][70/97], lr: 0.01000\tTime 0.498 (0.507)\tData 0.000 (0.022)\tLoss 2.3657 (2.4956)\tPrec@1 85.938 (84.221)\tPrec@5 99.219 (98.944)\n",
      "Epoch: [54][80/97], lr: 0.01000\tTime 0.388 (0.504)\tData 0.000 (0.021)\tLoss 2.7033 (2.5209)\tPrec@1 83.594 (84.105)\tPrec@5 96.875 (98.843)\n",
      "Epoch: [54][90/97], lr: 0.01000\tTime 0.417 (0.493)\tData 0.000 (0.021)\tLoss 2.9595 (2.5418)\tPrec@1 80.469 (83.868)\tPrec@5 99.219 (98.815)\n",
      "Epoch: [54][96/97], lr: 0.01000\tTime 0.374 (0.488)\tData 0.000 (0.021)\tLoss 2.4962 (2.5540)\tPrec@1 84.746 (83.774)\tPrec@5 100.000 (98.775)\n",
      "Test: [0/100]\tTime 0.571 (0.571)\tLoss 9.6284 (9.6284)\tPrec@1 56.000 (56.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.076 (0.120)\tLoss 7.6125 (8.9521)\tPrec@1 60.000 (56.000)\tPrec@5 98.000 (96.727)\n",
      "Test: [20/100]\tTime 0.075 (0.099)\tLoss 7.4570 (8.9352)\tPrec@1 62.000 (56.190)\tPrec@5 94.000 (96.190)\n",
      "Test: [30/100]\tTime 0.074 (0.091)\tLoss 7.6113 (8.8983)\tPrec@1 61.000 (56.355)\tPrec@5 99.000 (96.258)\n",
      "Test: [40/100]\tTime 0.075 (0.087)\tLoss 8.9248 (8.8938)\tPrec@1 55.000 (56.561)\tPrec@5 96.000 (96.366)\n",
      "Test: [50/100]\tTime 0.121 (0.086)\tLoss 8.4159 (8.8412)\tPrec@1 59.000 (56.902)\tPrec@5 97.000 (96.490)\n",
      "Test: [60/100]\tTime 0.084 (0.085)\tLoss 7.1773 (8.7925)\tPrec@1 61.000 (56.902)\tPrec@5 97.000 (96.443)\n",
      "Test: [70/100]\tTime 0.107 (0.085)\tLoss 8.1472 (8.7719)\tPrec@1 61.000 (56.915)\tPrec@5 97.000 (96.507)\n",
      "Test: [80/100]\tTime 0.074 (0.084)\tLoss 8.2473 (8.7466)\tPrec@1 60.000 (56.975)\tPrec@5 95.000 (96.605)\n",
      "Test: [90/100]\tTime 0.074 (0.083)\tLoss 9.1310 (8.8212)\tPrec@1 56.000 (56.626)\tPrec@5 98.000 (96.527)\n",
      "val Results: Prec@1 56.580 Prec@5 96.520 Loss 8.84586\n",
      "val Class Accuracy: [0.959,0.987,0.731,0.646,0.694,0.487,0.446,0.410,0.297,0.001]\n",
      "Best Prec@1: 56.580\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [55][0/97], lr: 0.01000\tTime 1.751 (1.751)\tData 0.891 (0.891)\tLoss 2.0514 (2.0514)\tPrec@1 85.156 (85.156)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [55][10/97], lr: 0.01000\tTime 0.388 (0.572)\tData 0.000 (0.089)\tLoss 2.2953 (2.5098)\tPrec@1 86.719 (83.594)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [55][20/97], lr: 0.01000\tTime 0.455 (0.524)\tData 0.000 (0.054)\tLoss 3.0179 (2.5182)\tPrec@1 81.250 (83.817)\tPrec@5 97.656 (99.144)\n",
      "Epoch: [55][30/97], lr: 0.01000\tTime 0.393 (0.499)\tData 0.001 (0.042)\tLoss 2.3044 (2.5437)\tPrec@1 84.375 (83.795)\tPrec@5 99.219 (99.118)\n",
      "Epoch: [55][40/97], lr: 0.01000\tTime 0.407 (0.483)\tData 0.001 (0.035)\tLoss 2.3599 (2.6001)\tPrec@1 84.375 (83.327)\tPrec@5 100.000 (99.123)\n",
      "Epoch: [55][50/97], lr: 0.01000\tTime 0.489 (0.480)\tData 0.001 (0.032)\tLoss 2.6470 (2.5368)\tPrec@1 82.812 (83.854)\tPrec@5 99.219 (99.096)\n",
      "Epoch: [55][60/97], lr: 0.01000\tTime 0.502 (0.483)\tData 0.000 (0.029)\tLoss 2.6418 (2.5360)\tPrec@1 84.375 (83.952)\tPrec@5 98.438 (99.091)\n",
      "Epoch: [55][70/97], lr: 0.01000\tTime 0.476 (0.488)\tData 0.000 (0.026)\tLoss 2.2866 (2.5081)\tPrec@1 83.594 (84.056)\tPrec@5 99.219 (99.076)\n",
      "Epoch: [55][80/97], lr: 0.01000\tTime 0.345 (0.476)\tData 0.000 (0.025)\tLoss 2.8893 (2.5323)\tPrec@1 81.250 (83.864)\tPrec@5 98.438 (98.958)\n",
      "Epoch: [55][90/97], lr: 0.01000\tTime 0.325 (0.461)\tData 0.000 (0.024)\tLoss 2.5382 (2.5178)\tPrec@1 84.375 (84.032)\tPrec@5 96.875 (98.901)\n",
      "Epoch: [55][96/97], lr: 0.01000\tTime 0.314 (0.452)\tData 0.000 (0.024)\tLoss 1.7050 (2.5021)\tPrec@1 88.136 (84.145)\tPrec@5 100.000 (98.912)\n",
      "Test: [0/100]\tTime 0.263 (0.263)\tLoss 10.8491 (10.8491)\tPrec@1 49.000 (49.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 9.1057 (10.9633)\tPrec@1 55.000 (46.455)\tPrec@5 93.000 (91.636)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 10.3886 (11.0787)\tPrec@1 46.000 (45.190)\tPrec@5 90.000 (90.619)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 10.8486 (11.0546)\tPrec@1 44.000 (45.387)\tPrec@5 88.000 (90.419)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 12.3292 (11.1147)\tPrec@1 40.000 (45.195)\tPrec@5 87.000 (90.073)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 10.2922 (11.0369)\tPrec@1 52.000 (45.588)\tPrec@5 87.000 (90.078)\n",
      "Test: [60/100]\tTime 0.074 (0.076)\tLoss 9.7816 (11.0486)\tPrec@1 51.000 (45.328)\tPrec@5 92.000 (89.738)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 10.8721 (10.9976)\tPrec@1 45.000 (45.493)\tPrec@5 90.000 (89.746)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 9.8845 (10.9548)\tPrec@1 52.000 (45.617)\tPrec@5 89.000 (89.840)\n",
      "Test: [90/100]\tTime 0.075 (0.076)\tLoss 10.7442 (10.9801)\tPrec@1 44.000 (45.527)\tPrec@5 96.000 (89.879)\n",
      "val Results: Prec@1 45.400 Prec@5 89.930 Loss 11.01273\n",
      "val Class Accuracy: [0.962,0.979,0.789,0.664,0.191,0.118,0.651,0.027,0.144,0.015]\n",
      "Best Prec@1: 56.580\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [56][0/97], lr: 0.01000\tTime 0.806 (0.806)\tData 0.469 (0.469)\tLoss 2.4453 (2.4453)\tPrec@1 85.156 (85.156)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [56][10/97], lr: 0.01000\tTime 0.388 (0.473)\tData 0.000 (0.054)\tLoss 2.2663 (2.4650)\tPrec@1 85.938 (84.943)\tPrec@5 98.438 (98.580)\n",
      "Epoch: [56][20/97], lr: 0.01000\tTime 0.412 (0.439)\tData 0.000 (0.036)\tLoss 2.5531 (2.3797)\tPrec@1 84.375 (85.454)\tPrec@5 96.094 (98.512)\n",
      "Epoch: [56][30/97], lr: 0.01000\tTime 0.436 (0.430)\tData 0.000 (0.029)\tLoss 3.2997 (2.5351)\tPrec@1 78.125 (84.451)\tPrec@5 100.000 (98.387)\n",
      "Epoch: [56][40/97], lr: 0.01000\tTime 0.440 (0.437)\tData 0.000 (0.026)\tLoss 2.4773 (2.5201)\tPrec@1 84.375 (84.394)\tPrec@5 100.000 (98.552)\n",
      "Epoch: [56][50/97], lr: 0.01000\tTime 0.433 (0.450)\tData 0.001 (0.024)\tLoss 2.6115 (2.5880)\tPrec@1 85.156 (84.099)\tPrec@5 99.219 (98.575)\n",
      "Epoch: [56][60/97], lr: 0.01000\tTime 0.397 (0.451)\tData 0.000 (0.022)\tLoss 2.3018 (2.5643)\tPrec@1 89.062 (84.362)\tPrec@5 99.219 (98.604)\n",
      "Epoch: [56][70/97], lr: 0.01000\tTime 0.425 (0.452)\tData 0.000 (0.021)\tLoss 1.9320 (2.5437)\tPrec@1 86.719 (84.419)\tPrec@5 99.219 (98.702)\n",
      "Epoch: [56][80/97], lr: 0.01000\tTime 0.533 (0.450)\tData 0.000 (0.021)\tLoss 2.9228 (2.5781)\tPrec@1 84.375 (84.221)\tPrec@5 97.656 (98.736)\n",
      "Epoch: [56][90/97], lr: 0.01000\tTime 0.385 (0.449)\tData 0.000 (0.020)\tLoss 2.4583 (2.5545)\tPrec@1 84.375 (84.332)\tPrec@5 99.219 (98.772)\n",
      "Epoch: [56][96/97], lr: 0.01000\tTime 0.369 (0.446)\tData 0.000 (0.020)\tLoss 2.5481 (2.5425)\tPrec@1 81.356 (84.362)\tPrec@5 100.000 (98.799)\n",
      "Test: [0/100]\tTime 0.406 (0.406)\tLoss 11.4175 (11.4175)\tPrec@1 46.000 (46.000)\tPrec@5 88.000 (88.000)\n",
      "Test: [10/100]\tTime 0.074 (0.104)\tLoss 8.1039 (10.3495)\tPrec@1 63.000 (49.636)\tPrec@5 93.000 (89.818)\n",
      "Test: [20/100]\tTime 0.074 (0.090)\tLoss 8.8203 (10.1934)\tPrec@1 55.000 (49.857)\tPrec@5 96.000 (90.476)\n",
      "Test: [30/100]\tTime 0.075 (0.085)\tLoss 9.7924 (10.1787)\tPrec@1 46.000 (49.677)\tPrec@5 90.000 (90.323)\n",
      "Test: [40/100]\tTime 0.074 (0.082)\tLoss 10.9170 (10.2164)\tPrec@1 48.000 (49.488)\tPrec@5 90.000 (90.366)\n",
      "Test: [50/100]\tTime 0.074 (0.081)\tLoss 9.2546 (10.0396)\tPrec@1 56.000 (50.529)\tPrec@5 93.000 (90.255)\n",
      "Test: [60/100]\tTime 0.073 (0.079)\tLoss 7.8970 (10.0116)\tPrec@1 65.000 (50.557)\tPrec@5 89.000 (90.279)\n",
      "Test: [70/100]\tTime 0.074 (0.079)\tLoss 9.0044 (9.9500)\tPrec@1 58.000 (50.944)\tPrec@5 95.000 (90.225)\n",
      "Test: [80/100]\tTime 0.074 (0.078)\tLoss 9.1869 (9.8758)\tPrec@1 55.000 (51.358)\tPrec@5 87.000 (90.395)\n",
      "Test: [90/100]\tTime 0.073 (0.078)\tLoss 9.6929 (9.9248)\tPrec@1 51.000 (51.198)\tPrec@5 92.000 (90.352)\n",
      "val Results: Prec@1 51.270 Prec@5 90.250 Loss 9.92067\n",
      "val Class Accuracy: [0.816,0.979,0.704,0.766,0.530,0.676,0.485,0.138,0.019,0.014]\n",
      "Best Prec@1: 56.580\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [57][0/97], lr: 0.01000\tTime 1.017 (1.017)\tData 0.575 (0.575)\tLoss 2.0300 (2.0300)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [57][10/97], lr: 0.01000\tTime 0.409 (0.516)\tData 0.000 (0.062)\tLoss 2.1767 (2.4531)\tPrec@1 86.719 (84.517)\tPrec@5 97.656 (98.651)\n",
      "Epoch: [57][20/97], lr: 0.01000\tTime 0.376 (0.461)\tData 0.000 (0.040)\tLoss 3.2632 (2.4924)\tPrec@1 76.562 (84.189)\tPrec@5 97.656 (98.735)\n",
      "Epoch: [57][30/97], lr: 0.01000\tTime 0.371 (0.438)\tData 0.000 (0.032)\tLoss 2.7307 (2.5090)\tPrec@1 82.812 (84.375)\tPrec@5 100.000 (98.664)\n",
      "Epoch: [57][40/97], lr: 0.01000\tTime 0.420 (0.428)\tData 0.000 (0.028)\tLoss 2.0938 (2.5475)\tPrec@1 85.938 (84.108)\tPrec@5 100.000 (98.761)\n",
      "Epoch: [57][50/97], lr: 0.01000\tTime 0.533 (0.439)\tData 0.001 (0.026)\tLoss 2.6662 (2.5447)\tPrec@1 85.938 (84.161)\tPrec@5 97.656 (98.836)\n",
      "Epoch: [57][60/97], lr: 0.01000\tTime 0.486 (0.445)\tData 0.001 (0.024)\tLoss 2.6621 (2.5203)\tPrec@1 79.688 (84.337)\tPrec@5 98.438 (98.860)\n",
      "Epoch: [57][70/97], lr: 0.01000\tTime 0.398 (0.442)\tData 0.000 (0.022)\tLoss 2.1972 (2.5202)\tPrec@1 84.375 (84.287)\tPrec@5 98.438 (98.878)\n",
      "Epoch: [57][80/97], lr: 0.01000\tTime 0.468 (0.444)\tData 0.000 (0.022)\tLoss 2.1525 (2.5221)\tPrec@1 87.500 (84.279)\tPrec@5 99.219 (98.910)\n",
      "Epoch: [57][90/97], lr: 0.01000\tTime 0.423 (0.444)\tData 0.000 (0.021)\tLoss 2.3188 (2.5214)\tPrec@1 87.500 (84.289)\tPrec@5 99.219 (98.935)\n",
      "Epoch: [57][96/97], lr: 0.01000\tTime 0.374 (0.442)\tData 0.000 (0.021)\tLoss 2.2354 (2.5191)\tPrec@1 88.136 (84.306)\tPrec@5 96.610 (98.904)\n",
      "Test: [0/100]\tTime 0.464 (0.464)\tLoss 12.3057 (12.3057)\tPrec@1 39.000 (39.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.109)\tLoss 10.0086 (11.7338)\tPrec@1 52.000 (42.636)\tPrec@5 95.000 (92.818)\n",
      "Test: [20/100]\tTime 0.074 (0.093)\tLoss 10.1744 (11.6314)\tPrec@1 51.000 (43.000)\tPrec@5 97.000 (92.952)\n",
      "Test: [30/100]\tTime 0.075 (0.087)\tLoss 10.8852 (11.5188)\tPrec@1 46.000 (43.387)\tPrec@5 96.000 (92.710)\n",
      "Test: [40/100]\tTime 0.074 (0.084)\tLoss 12.6643 (11.6002)\tPrec@1 38.000 (43.000)\tPrec@5 89.000 (92.390)\n",
      "Test: [50/100]\tTime 0.074 (0.082)\tLoss 11.4791 (11.4985)\tPrec@1 43.000 (43.608)\tPrec@5 93.000 (92.549)\n",
      "Test: [60/100]\tTime 0.074 (0.081)\tLoss 9.6895 (11.4910)\tPrec@1 51.000 (43.393)\tPrec@5 93.000 (92.607)\n",
      "Test: [70/100]\tTime 0.073 (0.080)\tLoss 10.5482 (11.4628)\tPrec@1 51.000 (43.634)\tPrec@5 95.000 (92.592)\n",
      "Test: [80/100]\tTime 0.081 (0.079)\tLoss 10.4234 (11.3957)\tPrec@1 53.000 (43.951)\tPrec@5 87.000 (92.642)\n",
      "Test: [90/100]\tTime 0.074 (0.079)\tLoss 10.8374 (11.4498)\tPrec@1 47.000 (43.593)\tPrec@5 93.000 (92.659)\n",
      "val Results: Prec@1 43.520 Prec@5 92.610 Loss 11.46602\n",
      "val Class Accuracy: [0.852,0.974,0.919,0.784,0.243,0.317,0.112,0.027,0.124,0.000]\n",
      "Best Prec@1: 56.580\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [58][0/97], lr: 0.01000\tTime 1.127 (1.127)\tData 0.569 (0.569)\tLoss 2.5154 (2.5154)\tPrec@1 83.594 (83.594)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [58][10/97], lr: 0.01000\tTime 0.402 (0.557)\tData 0.000 (0.059)\tLoss 2.3570 (2.4186)\tPrec@1 86.719 (85.227)\tPrec@5 99.219 (98.935)\n",
      "Epoch: [58][20/97], lr: 0.01000\tTime 0.325 (0.455)\tData 0.000 (0.038)\tLoss 3.0797 (2.3989)\tPrec@1 79.688 (85.119)\tPrec@5 98.438 (98.958)\n",
      "Epoch: [58][30/97], lr: 0.01000\tTime 0.322 (0.414)\tData 0.000 (0.032)\tLoss 2.7612 (2.4531)\tPrec@1 81.250 (84.879)\tPrec@5 97.656 (98.891)\n",
      "Epoch: [58][40/97], lr: 0.01000\tTime 0.322 (0.392)\tData 0.000 (0.028)\tLoss 2.3256 (2.4611)\tPrec@1 85.156 (84.737)\tPrec@5 99.219 (98.800)\n",
      "Epoch: [58][50/97], lr: 0.01000\tTime 0.326 (0.379)\tData 0.000 (0.026)\tLoss 2.5564 (2.4828)\tPrec@1 83.594 (84.635)\tPrec@5 100.000 (98.775)\n",
      "Epoch: [58][60/97], lr: 0.01000\tTime 0.324 (0.371)\tData 0.000 (0.025)\tLoss 2.3270 (2.5166)\tPrec@1 85.938 (84.452)\tPrec@5 100.000 (98.835)\n",
      "Epoch: [58][70/97], lr: 0.01000\tTime 0.324 (0.365)\tData 0.000 (0.024)\tLoss 2.2447 (2.5315)\tPrec@1 84.375 (84.243)\tPrec@5 100.000 (98.823)\n",
      "Epoch: [58][80/97], lr: 0.01000\tTime 0.328 (0.360)\tData 0.000 (0.023)\tLoss 2.1763 (2.5344)\tPrec@1 86.719 (84.221)\tPrec@5 99.219 (98.804)\n",
      "Epoch: [58][90/97], lr: 0.01000\tTime 0.323 (0.357)\tData 0.000 (0.022)\tLoss 2.7973 (2.5433)\tPrec@1 82.812 (84.143)\tPrec@5 99.219 (98.807)\n",
      "Epoch: [58][96/97], lr: 0.01000\tTime 0.321 (0.355)\tData 0.000 (0.023)\tLoss 2.6064 (2.5419)\tPrec@1 84.746 (84.129)\tPrec@5 99.153 (98.815)\n",
      "Test: [0/100]\tTime 0.254 (0.254)\tLoss 10.8277 (10.8277)\tPrec@1 48.000 (48.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.7772 (9.5592)\tPrec@1 61.000 (52.818)\tPrec@5 98.000 (95.273)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 9.1184 (9.5805)\tPrec@1 49.000 (52.143)\tPrec@5 95.000 (95.714)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.5482 (9.6250)\tPrec@1 56.000 (51.677)\tPrec@5 93.000 (95.484)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.1600 (9.5807)\tPrec@1 47.000 (52.171)\tPrec@5 94.000 (95.341)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.3539 (9.5004)\tPrec@1 56.000 (52.647)\tPrec@5 94.000 (95.412)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.9088 (9.4431)\tPrec@1 63.000 (52.803)\tPrec@5 94.000 (95.328)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.5725 (9.4284)\tPrec@1 55.000 (52.789)\tPrec@5 96.000 (95.324)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.8202 (9.3776)\tPrec@1 55.000 (53.099)\tPrec@5 92.000 (95.370)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.2384 (9.4564)\tPrec@1 56.000 (52.758)\tPrec@5 98.000 (95.297)\n",
      "val Results: Prec@1 52.520 Prec@5 95.130 Loss 9.50346\n",
      "val Class Accuracy: [0.851,0.949,0.893,0.390,0.758,0.405,0.528,0.333,0.137,0.008]\n",
      "Best Prec@1: 56.580\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [59][0/97], lr: 0.01000\tTime 0.411 (0.411)\tData 0.200 (0.200)\tLoss 2.4178 (2.4178)\tPrec@1 82.031 (82.031)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [59][10/97], lr: 0.01000\tTime 0.325 (0.339)\tData 0.000 (0.033)\tLoss 2.2702 (2.4893)\tPrec@1 82.031 (84.517)\tPrec@5 100.000 (98.722)\n",
      "Epoch: [59][20/97], lr: 0.01000\tTime 0.322 (0.334)\tData 0.000 (0.026)\tLoss 2.5672 (2.4962)\tPrec@1 83.594 (84.226)\tPrec@5 100.000 (98.772)\n",
      "Epoch: [59][30/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.023)\tLoss 2.8731 (2.4610)\tPrec@1 84.375 (84.501)\tPrec@5 97.656 (98.866)\n",
      "Epoch: [59][40/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.022)\tLoss 1.9488 (2.4722)\tPrec@1 85.938 (84.394)\tPrec@5 98.438 (98.857)\n",
      "Epoch: [59][50/97], lr: 0.01000\tTime 0.328 (0.331)\tData 0.000 (0.021)\tLoss 2.5829 (2.4992)\tPrec@1 85.938 (84.298)\tPrec@5 99.219 (98.897)\n",
      "Epoch: [59][60/97], lr: 0.01000\tTime 0.330 (0.330)\tData 0.000 (0.020)\tLoss 2.8286 (2.5231)\tPrec@1 85.156 (84.144)\tPrec@5 96.875 (98.835)\n",
      "Epoch: [59][70/97], lr: 0.01000\tTime 0.328 (0.330)\tData 0.000 (0.020)\tLoss 2.2617 (2.5179)\tPrec@1 85.156 (84.221)\tPrec@5 98.438 (98.779)\n",
      "Epoch: [59][80/97], lr: 0.01000\tTime 0.324 (0.330)\tData 0.000 (0.019)\tLoss 2.2125 (2.5280)\tPrec@1 87.500 (84.192)\tPrec@5 100.000 (98.765)\n",
      "Epoch: [59][90/97], lr: 0.01000\tTime 0.323 (0.330)\tData 0.000 (0.019)\tLoss 2.8584 (2.5190)\tPrec@1 82.031 (84.246)\tPrec@5 97.656 (98.729)\n",
      "Epoch: [59][96/97], lr: 0.01000\tTime 0.317 (0.329)\tData 0.000 (0.020)\tLoss 2.4130 (2.5166)\tPrec@1 84.746 (84.225)\tPrec@5 100.000 (98.759)\n",
      "Test: [0/100]\tTime 0.259 (0.259)\tLoss 8.9982 (8.9982)\tPrec@1 55.000 (55.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 7.0916 (8.4641)\tPrec@1 64.000 (56.818)\tPrec@5 95.000 (96.182)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 8.2444 (8.5507)\tPrec@1 56.000 (55.952)\tPrec@5 96.000 (96.286)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.7272 (8.5630)\tPrec@1 60.000 (55.903)\tPrec@5 97.000 (96.323)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.4643 (8.5519)\tPrec@1 61.000 (56.341)\tPrec@5 98.000 (96.220)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 8.0752 (8.4709)\tPrec@1 59.000 (56.863)\tPrec@5 98.000 (96.412)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.2273 (8.4283)\tPrec@1 61.000 (57.049)\tPrec@5 98.000 (96.426)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.9061 (8.4231)\tPrec@1 62.000 (57.197)\tPrec@5 97.000 (96.423)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.4039 (8.3763)\tPrec@1 60.000 (57.395)\tPrec@5 97.000 (96.432)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.7134 (8.4549)\tPrec@1 56.000 (56.956)\tPrec@5 96.000 (96.429)\n",
      "val Results: Prec@1 56.970 Prec@5 96.420 Loss 8.45820\n",
      "val Class Accuracy: [0.928,0.984,0.618,0.784,0.512,0.420,0.414,0.758,0.215,0.064]\n",
      "Best Prec@1: 56.970\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [60][0/97], lr: 0.01000\tTime 0.475 (0.475)\tData 0.263 (0.263)\tLoss 2.3936 (2.3936)\tPrec@1 85.938 (85.938)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [60][10/97], lr: 0.01000\tTime 0.336 (0.349)\tData 0.000 (0.039)\tLoss 2.2535 (2.4145)\tPrec@1 85.938 (84.943)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [60][20/97], lr: 0.01000\tTime 0.324 (0.340)\tData 0.000 (0.028)\tLoss 1.9469 (2.2774)\tPrec@1 85.938 (85.640)\tPrec@5 100.000 (99.256)\n",
      "Epoch: [60][30/97], lr: 0.01000\tTime 0.328 (0.336)\tData 0.000 (0.025)\tLoss 3.4730 (2.4210)\tPrec@1 80.469 (84.929)\tPrec@5 96.875 (98.816)\n",
      "Epoch: [60][40/97], lr: 0.01000\tTime 0.334 (0.334)\tData 0.000 (0.023)\tLoss 2.3267 (2.4242)\tPrec@1 83.594 (84.947)\tPrec@5 98.438 (98.876)\n",
      "Epoch: [60][50/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.022)\tLoss 2.6266 (2.5062)\tPrec@1 83.594 (84.559)\tPrec@5 99.219 (98.775)\n",
      "Epoch: [60][60/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 2.1128 (2.4846)\tPrec@1 85.938 (84.708)\tPrec@5 99.219 (98.796)\n",
      "Epoch: [60][70/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 1.9255 (2.4493)\tPrec@1 87.500 (85.002)\tPrec@5 99.219 (98.823)\n",
      "Epoch: [60][80/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.020)\tLoss 1.9681 (2.4510)\tPrec@1 88.281 (84.992)\tPrec@5 99.219 (98.872)\n",
      "Epoch: [60][90/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 2.4605 (2.4450)\tPrec@1 82.031 (85.036)\tPrec@5 99.219 (98.841)\n",
      "Epoch: [60][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 2.3616 (2.4488)\tPrec@1 86.441 (85.015)\tPrec@5 100.000 (98.839)\n",
      "Test: [0/100]\tTime 0.247 (0.247)\tLoss 12.3562 (12.3562)\tPrec@1 39.000 (39.000)\tPrec@5 88.000 (88.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 9.0001 (10.8704)\tPrec@1 53.000 (46.000)\tPrec@5 93.000 (88.636)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 9.7089 (10.8431)\tPrec@1 49.000 (46.095)\tPrec@5 92.000 (89.476)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 10.2756 (10.8524)\tPrec@1 49.000 (46.129)\tPrec@5 87.000 (89.194)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 11.9002 (10.8370)\tPrec@1 43.000 (46.268)\tPrec@5 87.000 (89.268)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 10.3750 (10.7159)\tPrec@1 48.000 (46.686)\tPrec@5 88.000 (89.529)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.7788 (10.6902)\tPrec@1 57.000 (46.623)\tPrec@5 93.000 (89.459)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 10.3764 (10.6811)\tPrec@1 49.000 (46.620)\tPrec@5 91.000 (89.408)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 10.1729 (10.6238)\tPrec@1 51.000 (46.975)\tPrec@5 86.000 (89.630)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.0555 (10.6893)\tPrec@1 54.000 (46.670)\tPrec@5 91.000 (89.758)\n",
      "val Results: Prec@1 46.780 Prec@5 89.610 Loss 10.69943\n",
      "val Class Accuracy: [0.919,0.936,0.852,0.758,0.501,0.208,0.182,0.279,0.043,0.000]\n",
      "Best Prec@1: 56.970\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [61][0/97], lr: 0.01000\tTime 0.448 (0.448)\tData 0.197 (0.197)\tLoss 2.2378 (2.2378)\tPrec@1 87.500 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [61][10/97], lr: 0.01000\tTime 0.325 (0.344)\tData 0.000 (0.032)\tLoss 2.0782 (2.1751)\tPrec@1 87.500 (86.364)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [61][20/97], lr: 0.01000\tTime 0.327 (0.337)\tData 0.000 (0.025)\tLoss 2.0514 (2.3212)\tPrec@1 85.156 (85.305)\tPrec@5 99.219 (99.182)\n",
      "Epoch: [61][30/97], lr: 0.01000\tTime 0.330 (0.334)\tData 0.000 (0.023)\tLoss 3.4209 (2.4000)\tPrec@1 80.469 (84.955)\tPrec@5 97.656 (98.790)\n",
      "Epoch: [61][40/97], lr: 0.01000\tTime 0.331 (0.334)\tData 0.000 (0.021)\tLoss 3.4375 (2.4615)\tPrec@1 78.125 (84.699)\tPrec@5 99.219 (98.895)\n",
      "Epoch: [61][50/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.021)\tLoss 2.5939 (2.4524)\tPrec@1 82.031 (84.926)\tPrec@5 98.438 (98.836)\n",
      "Epoch: [61][60/97], lr: 0.01000\tTime 0.330 (0.332)\tData 0.000 (0.020)\tLoss 2.2401 (2.4455)\tPrec@1 88.281 (85.015)\tPrec@5 99.219 (98.745)\n",
      "Epoch: [61][70/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.6248 (2.4834)\tPrec@1 83.594 (84.683)\tPrec@5 97.656 (98.702)\n",
      "Epoch: [61][80/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.019)\tLoss 2.9442 (2.4818)\tPrec@1 82.031 (84.693)\tPrec@5 99.219 (98.794)\n",
      "Epoch: [61][90/97], lr: 0.01000\tTime 0.328 (0.331)\tData 0.000 (0.019)\tLoss 2.5461 (2.4987)\tPrec@1 83.594 (84.521)\tPrec@5 100.000 (98.858)\n",
      "Epoch: [61][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 1.5779 (2.4930)\tPrec@1 91.525 (84.588)\tPrec@5 99.153 (98.831)\n",
      "Test: [0/100]\tTime 0.260 (0.260)\tLoss 10.3460 (10.3460)\tPrec@1 51.000 (51.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 7.2433 (9.4572)\tPrec@1 61.000 (53.455)\tPrec@5 95.000 (92.545)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 8.3793 (9.4262)\tPrec@1 58.000 (52.952)\tPrec@5 93.000 (93.095)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.4175 (9.3789)\tPrec@1 58.000 (53.323)\tPrec@5 96.000 (93.161)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.6738 (9.4008)\tPrec@1 51.000 (53.171)\tPrec@5 92.000 (93.366)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 8.4110 (9.2952)\tPrec@1 57.000 (53.588)\tPrec@5 97.000 (93.569)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.7438 (9.2656)\tPrec@1 61.000 (53.639)\tPrec@5 94.000 (93.639)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.5444 (9.2535)\tPrec@1 61.000 (53.746)\tPrec@5 95.000 (93.592)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.2898 (9.2023)\tPrec@1 61.000 (54.136)\tPrec@5 92.000 (93.765)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.4544 (9.2655)\tPrec@1 59.000 (53.813)\tPrec@5 93.000 (93.626)\n",
      "val Results: Prec@1 53.840 Prec@5 93.530 Loss 9.29352\n",
      "val Class Accuracy: [0.873,0.982,0.817,0.614,0.623,0.467,0.582,0.288,0.128,0.010]\n",
      "Best Prec@1: 56.970\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [62][0/97], lr: 0.01000\tTime 0.424 (0.424)\tData 0.214 (0.214)\tLoss 3.1086 (3.1086)\tPrec@1 79.688 (79.688)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [62][10/97], lr: 0.01000\tTime 0.325 (0.344)\tData 0.000 (0.035)\tLoss 2.9666 (2.7071)\tPrec@1 82.031 (82.955)\tPrec@5 97.656 (98.509)\n",
      "Epoch: [62][20/97], lr: 0.01000\tTime 0.334 (0.337)\tData 0.000 (0.026)\tLoss 2.1471 (2.5000)\tPrec@1 86.719 (84.598)\tPrec@5 100.000 (98.810)\n",
      "Epoch: [62][30/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.023)\tLoss 2.7469 (2.5150)\tPrec@1 81.250 (84.627)\tPrec@5 97.656 (98.715)\n",
      "Epoch: [62][40/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.022)\tLoss 2.5673 (2.4799)\tPrec@1 83.594 (84.489)\tPrec@5 97.656 (98.685)\n",
      "Epoch: [62][50/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.021)\tLoss 1.7646 (2.4846)\tPrec@1 89.062 (84.375)\tPrec@5 100.000 (98.836)\n",
      "Epoch: [62][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.1303 (2.4922)\tPrec@1 86.719 (84.362)\tPrec@5 99.219 (98.809)\n",
      "Epoch: [62][70/97], lr: 0.01000\tTime 0.322 (0.332)\tData 0.000 (0.020)\tLoss 2.1654 (2.4900)\tPrec@1 87.500 (84.430)\tPrec@5 99.219 (98.834)\n",
      "Epoch: [62][80/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.020)\tLoss 2.1175 (2.4713)\tPrec@1 85.938 (84.404)\tPrec@5 100.000 (98.891)\n",
      "Epoch: [62][90/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.019)\tLoss 2.5182 (2.4764)\tPrec@1 85.156 (84.375)\tPrec@5 99.219 (98.893)\n",
      "Epoch: [62][96/97], lr: 0.01000\tTime 0.317 (0.330)\tData 0.000 (0.020)\tLoss 2.1145 (2.4553)\tPrec@1 88.136 (84.491)\tPrec@5 100.000 (98.952)\n",
      "Test: [0/100]\tTime 0.255 (0.255)\tLoss 9.5212 (9.5212)\tPrec@1 55.000 (55.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.0569 (8.8250)\tPrec@1 65.000 (56.818)\tPrec@5 96.000 (95.727)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.9390 (8.8478)\tPrec@1 58.000 (56.000)\tPrec@5 98.000 (95.667)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.5908 (8.8366)\tPrec@1 62.000 (56.097)\tPrec@5 97.000 (95.839)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.3529 (8.8902)\tPrec@1 54.000 (55.927)\tPrec@5 92.000 (95.707)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.3839 (8.8104)\tPrec@1 58.000 (56.353)\tPrec@5 94.000 (95.804)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.5323 (8.7606)\tPrec@1 66.000 (56.492)\tPrec@5 96.000 (95.754)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.9680 (8.7256)\tPrec@1 55.000 (56.775)\tPrec@5 98.000 (95.887)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.1038 (8.7112)\tPrec@1 51.000 (56.593)\tPrec@5 94.000 (95.926)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.3004 (8.7342)\tPrec@1 52.000 (56.407)\tPrec@5 100.000 (95.923)\n",
      "val Results: Prec@1 56.310 Prec@5 95.870 Loss 8.77051\n",
      "val Class Accuracy: [0.970,0.978,0.623,0.803,0.587,0.442,0.678,0.118,0.411,0.021]\n",
      "Best Prec@1: 56.970\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [63][0/97], lr: 0.01000\tTime 0.510 (0.510)\tData 0.274 (0.274)\tLoss 1.8201 (1.8201)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [63][10/97], lr: 0.01000\tTime 0.327 (0.350)\tData 0.000 (0.040)\tLoss 2.0063 (2.3107)\tPrec@1 87.500 (85.653)\tPrec@5 97.656 (99.077)\n",
      "Epoch: [63][20/97], lr: 0.01000\tTime 0.322 (0.340)\tData 0.000 (0.029)\tLoss 2.7611 (2.3057)\tPrec@1 82.812 (85.119)\tPrec@5 100.000 (99.033)\n",
      "Epoch: [63][30/97], lr: 0.01000\tTime 0.324 (0.336)\tData 0.000 (0.025)\tLoss 2.8991 (2.3617)\tPrec@1 82.031 (84.955)\tPrec@5 99.219 (98.891)\n",
      "Epoch: [63][40/97], lr: 0.01000\tTime 0.327 (0.335)\tData 0.000 (0.023)\tLoss 2.5269 (2.3415)\tPrec@1 86.719 (85.232)\tPrec@5 100.000 (98.990)\n",
      "Epoch: [63][50/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.022)\tLoss 2.5779 (2.3108)\tPrec@1 83.594 (85.524)\tPrec@5 100.000 (99.050)\n",
      "Epoch: [63][60/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 2.4843 (2.3123)\tPrec@1 83.594 (85.566)\tPrec@5 100.000 (99.001)\n",
      "Epoch: [63][70/97], lr: 0.01000\tTime 0.331 (0.332)\tData 0.000 (0.021)\tLoss 2.8875 (2.3618)\tPrec@1 82.031 (85.343)\tPrec@5 100.000 (98.977)\n",
      "Epoch: [63][80/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.6806 (2.3877)\tPrec@1 83.594 (85.214)\tPrec@5 98.438 (98.920)\n",
      "Epoch: [63][90/97], lr: 0.01000\tTime 0.322 (0.332)\tData 0.000 (0.020)\tLoss 2.7233 (2.4142)\tPrec@1 82.031 (85.027)\tPrec@5 100.000 (98.910)\n",
      "Epoch: [63][96/97], lr: 0.01000\tTime 0.317 (0.331)\tData 0.000 (0.021)\tLoss 2.6481 (2.4577)\tPrec@1 82.203 (84.757)\tPrec@5 98.305 (98.863)\n",
      "Test: [0/100]\tTime 0.249 (0.249)\tLoss 9.8768 (9.8768)\tPrec@1 54.000 (54.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.6081 (8.9174)\tPrec@1 57.000 (56.091)\tPrec@5 92.000 (94.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.3554 (8.8400)\tPrec@1 58.000 (55.762)\tPrec@5 94.000 (94.571)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.7527 (8.8406)\tPrec@1 62.000 (55.935)\tPrec@5 96.000 (94.774)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.1738 (8.8467)\tPrec@1 54.000 (55.780)\tPrec@5 93.000 (94.634)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.1614 (8.7637)\tPrec@1 58.000 (56.176)\tPrec@5 93.000 (94.588)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.3692 (8.7087)\tPrec@1 69.000 (56.459)\tPrec@5 97.000 (94.459)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.8011 (8.6986)\tPrec@1 57.000 (56.394)\tPrec@5 95.000 (94.577)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.9689 (8.6641)\tPrec@1 60.000 (56.494)\tPrec@5 94.000 (94.728)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.2796 (8.7258)\tPrec@1 62.000 (56.396)\tPrec@5 99.000 (94.648)\n",
      "val Results: Prec@1 56.440 Prec@5 94.670 Loss 8.73089\n",
      "val Class Accuracy: [0.948,0.990,0.702,0.413,0.565,0.752,0.315,0.624,0.332,0.003]\n",
      "Best Prec@1: 56.970\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [64][0/97], lr: 0.01000\tTime 0.383 (0.383)\tData 0.185 (0.185)\tLoss 2.4410 (2.4410)\tPrec@1 85.156 (85.156)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [64][10/97], lr: 0.01000\tTime 0.325 (0.339)\tData 0.000 (0.032)\tLoss 2.8688 (2.5126)\tPrec@1 82.031 (84.091)\tPrec@5 97.656 (98.651)\n",
      "Epoch: [64][20/97], lr: 0.01000\tTime 0.321 (0.334)\tData 0.000 (0.025)\tLoss 1.9614 (2.5100)\tPrec@1 88.281 (84.338)\tPrec@5 99.219 (98.958)\n",
      "Epoch: [64][30/97], lr: 0.01000\tTime 0.329 (0.333)\tData 0.000 (0.022)\tLoss 2.2230 (2.4419)\tPrec@1 85.938 (84.652)\tPrec@5 98.438 (98.816)\n",
      "Epoch: [64][40/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 2.4461 (2.4672)\tPrec@1 83.594 (84.546)\tPrec@5 99.219 (98.895)\n",
      "Epoch: [64][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 3.4645 (2.4823)\tPrec@1 78.125 (84.344)\tPrec@5 97.656 (98.805)\n",
      "Epoch: [64][60/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 2.6944 (2.4566)\tPrec@1 86.719 (84.695)\tPrec@5 99.219 (98.847)\n",
      "Epoch: [64][70/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 2.4251 (2.4234)\tPrec@1 83.594 (84.848)\tPrec@5 98.438 (98.922)\n",
      "Epoch: [64][80/97], lr: 0.01000\tTime 0.324 (0.330)\tData 0.000 (0.019)\tLoss 2.6323 (2.4254)\tPrec@1 83.594 (84.799)\tPrec@5 98.438 (98.968)\n",
      "Epoch: [64][90/97], lr: 0.01000\tTime 0.329 (0.330)\tData 0.000 (0.019)\tLoss 2.0709 (2.4340)\tPrec@1 85.938 (84.761)\tPrec@5 100.000 (98.961)\n",
      "Epoch: [64][96/97], lr: 0.01000\tTime 0.316 (0.330)\tData 0.000 (0.020)\tLoss 1.9236 (2.4253)\tPrec@1 88.983 (84.806)\tPrec@5 99.153 (98.984)\n",
      "Test: [0/100]\tTime 0.234 (0.234)\tLoss 9.7995 (9.7995)\tPrec@1 53.000 (53.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 7.6029 (8.9979)\tPrec@1 67.000 (56.455)\tPrec@5 97.000 (97.364)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.6793 (8.8975)\tPrec@1 57.000 (56.048)\tPrec@5 100.000 (97.048)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.8566 (8.8481)\tPrec@1 59.000 (55.935)\tPrec@5 97.000 (96.935)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.6495 (8.8596)\tPrec@1 53.000 (55.829)\tPrec@5 96.000 (96.780)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.1733 (8.7425)\tPrec@1 60.000 (56.314)\tPrec@5 97.000 (96.980)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.4401 (8.6876)\tPrec@1 67.000 (56.590)\tPrec@5 97.000 (97.066)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.2089 (8.6788)\tPrec@1 59.000 (56.577)\tPrec@5 98.000 (97.141)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.4827 (8.6227)\tPrec@1 60.000 (57.012)\tPrec@5 97.000 (97.210)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.8957 (8.7217)\tPrec@1 56.000 (56.440)\tPrec@5 99.000 (97.099)\n",
      "val Results: Prec@1 56.490 Prec@5 97.100 Loss 8.71225\n",
      "val Class Accuracy: [0.920,0.966,0.815,0.816,0.548,0.521,0.369,0.505,0.123,0.066]\n",
      "Best Prec@1: 56.970\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [65][0/97], lr: 0.01000\tTime 0.418 (0.418)\tData 0.182 (0.182)\tLoss 2.2643 (2.2643)\tPrec@1 85.938 (85.938)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [65][10/97], lr: 0.01000\tTime 0.321 (0.341)\tData 0.000 (0.031)\tLoss 2.1092 (2.3101)\tPrec@1 88.281 (85.227)\tPrec@5 98.438 (98.722)\n",
      "Epoch: [65][20/97], lr: 0.01000\tTime 0.329 (0.335)\tData 0.000 (0.025)\tLoss 2.0244 (2.3146)\tPrec@1 88.281 (85.528)\tPrec@5 99.219 (98.735)\n",
      "Epoch: [65][30/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.022)\tLoss 2.3131 (2.3254)\tPrec@1 88.281 (85.610)\tPrec@5 99.219 (98.816)\n",
      "Epoch: [65][40/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 2.8801 (2.4107)\tPrec@1 80.469 (85.118)\tPrec@5 99.219 (98.742)\n",
      "Epoch: [65][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.2358 (2.4202)\tPrec@1 87.500 (84.926)\tPrec@5 100.000 (98.836)\n",
      "Epoch: [65][60/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 2.9947 (2.4292)\tPrec@1 79.688 (84.785)\tPrec@5 99.219 (98.847)\n",
      "Epoch: [65][70/97], lr: 0.01000\tTime 0.328 (0.331)\tData 0.000 (0.019)\tLoss 2.8102 (2.4111)\tPrec@1 82.812 (84.881)\tPrec@5 97.656 (98.889)\n",
      "Epoch: [65][80/97], lr: 0.01000\tTime 0.327 (0.331)\tData 0.000 (0.019)\tLoss 3.4776 (2.4227)\tPrec@1 77.344 (84.780)\tPrec@5 99.219 (98.900)\n",
      "Epoch: [65][90/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.019)\tLoss 3.0129 (2.4223)\tPrec@1 80.469 (84.899)\tPrec@5 99.219 (98.927)\n",
      "Epoch: [65][96/97], lr: 0.01000\tTime 0.319 (0.331)\tData 0.000 (0.020)\tLoss 2.0050 (2.4267)\tPrec@1 89.831 (84.886)\tPrec@5 98.305 (98.944)\n",
      "Test: [0/100]\tTime 0.286 (0.286)\tLoss 8.3415 (8.3415)\tPrec@1 62.000 (62.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 6.3174 (7.9498)\tPrec@1 70.000 (61.091)\tPrec@5 94.000 (95.909)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 7.1587 (7.9502)\tPrec@1 62.000 (60.762)\tPrec@5 98.000 (96.381)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 7.0647 (7.9014)\tPrec@1 66.000 (61.258)\tPrec@5 97.000 (96.226)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 7.7540 (7.8954)\tPrec@1 59.000 (61.390)\tPrec@5 96.000 (96.098)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.8342 (7.8534)\tPrec@1 64.000 (61.510)\tPrec@5 97.000 (96.098)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.5317 (7.8006)\tPrec@1 71.000 (61.885)\tPrec@5 96.000 (96.148)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 7.8368 (7.8128)\tPrec@1 61.000 (61.775)\tPrec@5 99.000 (96.113)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 7.6874 (7.7613)\tPrec@1 59.000 (61.988)\tPrec@5 95.000 (96.235)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.9920 (7.8328)\tPrec@1 68.000 (61.681)\tPrec@5 98.000 (96.209)\n",
      "val Results: Prec@1 61.620 Prec@5 96.190 Loss 7.84779\n",
      "val Class Accuracy: [0.902,0.983,0.765,0.678,0.809,0.514,0.457,0.623,0.417,0.014]\n",
      "Best Prec@1: 61.620\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [66][0/97], lr: 0.01000\tTime 0.414 (0.414)\tData 0.213 (0.213)\tLoss 3.0017 (3.0017)\tPrec@1 81.250 (81.250)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [66][10/97], lr: 0.01000\tTime 0.330 (0.342)\tData 0.000 (0.033)\tLoss 2.5183 (2.3744)\tPrec@1 83.594 (85.724)\tPrec@5 99.219 (99.148)\n",
      "Epoch: [66][20/97], lr: 0.01000\tTime 0.324 (0.335)\tData 0.000 (0.026)\tLoss 3.1031 (2.3976)\tPrec@1 83.594 (85.528)\tPrec@5 98.438 (99.182)\n",
      "Epoch: [66][30/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.023)\tLoss 2.8418 (2.3408)\tPrec@1 83.594 (85.660)\tPrec@5 98.438 (99.244)\n",
      "Epoch: [66][40/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.022)\tLoss 1.9471 (2.3695)\tPrec@1 86.719 (85.518)\tPrec@5 99.219 (99.200)\n",
      "Epoch: [66][50/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.021)\tLoss 2.1846 (2.3609)\tPrec@1 89.062 (85.539)\tPrec@5 98.438 (99.112)\n",
      "Epoch: [66][60/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 3.8411 (2.4072)\tPrec@1 75.781 (85.182)\tPrec@5 96.094 (99.039)\n",
      "Epoch: [66][70/97], lr: 0.01000\tTime 0.324 (0.330)\tData 0.000 (0.020)\tLoss 2.2502 (2.4144)\tPrec@1 84.375 (85.178)\tPrec@5 100.000 (99.010)\n",
      "Epoch: [66][80/97], lr: 0.01000\tTime 0.324 (0.330)\tData 0.000 (0.019)\tLoss 2.1499 (2.4067)\tPrec@1 88.281 (85.176)\tPrec@5 99.219 (98.997)\n",
      "Epoch: [66][90/97], lr: 0.01000\tTime 0.322 (0.330)\tData 0.000 (0.019)\tLoss 3.5932 (2.4622)\tPrec@1 76.562 (84.736)\tPrec@5 100.000 (98.970)\n",
      "Epoch: [66][96/97], lr: 0.01000\tTime 0.317 (0.330)\tData 0.000 (0.020)\tLoss 2.7830 (2.4517)\tPrec@1 79.661 (84.806)\tPrec@5 99.153 (98.984)\n",
      "Test: [0/100]\tTime 0.234 (0.234)\tLoss 10.5363 (10.5363)\tPrec@1 47.000 (47.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 7.7262 (9.5758)\tPrec@1 60.000 (51.909)\tPrec@5 94.000 (93.000)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 8.6185 (9.3655)\tPrec@1 53.000 (53.048)\tPrec@5 96.000 (93.048)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.4623 (9.4221)\tPrec@1 48.000 (52.516)\tPrec@5 92.000 (92.677)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.6073 (9.4403)\tPrec@1 53.000 (52.707)\tPrec@5 93.000 (92.537)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.2951 (9.3302)\tPrec@1 52.000 (53.392)\tPrec@5 92.000 (92.529)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.4695 (9.3261)\tPrec@1 66.000 (53.393)\tPrec@5 94.000 (92.475)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.4311 (9.2854)\tPrec@1 58.000 (53.592)\tPrec@5 92.000 (92.451)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.6943 (9.2589)\tPrec@1 58.000 (53.605)\tPrec@5 89.000 (92.494)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.1622 (9.3004)\tPrec@1 53.000 (53.473)\tPrec@5 92.000 (92.363)\n",
      "val Results: Prec@1 53.550 Prec@5 92.240 Loss 9.31624\n",
      "val Class Accuracy: [0.843,0.971,0.834,0.680,0.731,0.436,0.287,0.197,0.362,0.014]\n",
      "Best Prec@1: 61.620\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [67][0/97], lr: 0.01000\tTime 0.486 (0.486)\tData 0.216 (0.216)\tLoss 3.0200 (3.0200)\tPrec@1 82.031 (82.031)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [67][10/97], lr: 0.01000\tTime 0.325 (0.346)\tData 0.000 (0.035)\tLoss 2.2461 (2.3532)\tPrec@1 84.375 (85.511)\tPrec@5 99.219 (98.935)\n",
      "Epoch: [67][20/97], lr: 0.01000\tTime 0.325 (0.338)\tData 0.000 (0.026)\tLoss 2.0149 (2.2905)\tPrec@1 85.156 (85.565)\tPrec@5 97.656 (98.996)\n",
      "Epoch: [67][30/97], lr: 0.01000\tTime 0.328 (0.335)\tData 0.000 (0.024)\tLoss 2.1394 (2.2799)\tPrec@1 87.500 (85.711)\tPrec@5 100.000 (99.118)\n",
      "Epoch: [67][40/97], lr: 0.01000\tTime 0.327 (0.334)\tData 0.000 (0.022)\tLoss 2.4559 (2.2550)\tPrec@1 83.594 (85.861)\tPrec@5 100.000 (99.276)\n",
      "Epoch: [67][50/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 2.7458 (2.3044)\tPrec@1 82.031 (85.478)\tPrec@5 99.219 (99.295)\n",
      "Epoch: [67][60/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 2.7566 (2.3629)\tPrec@1 85.938 (85.169)\tPrec@5 97.656 (99.219)\n",
      "Epoch: [67][70/97], lr: 0.01000\tTime 0.330 (0.332)\tData 0.000 (0.020)\tLoss 2.3822 (2.3719)\tPrec@1 85.156 (85.101)\tPrec@5 99.219 (99.164)\n",
      "Epoch: [67][80/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.1878 (2.3707)\tPrec@1 85.938 (85.166)\tPrec@5 97.656 (99.084)\n",
      "Epoch: [67][90/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.019)\tLoss 2.2877 (2.3666)\tPrec@1 85.156 (85.199)\tPrec@5 100.000 (99.107)\n",
      "Epoch: [67][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 2.5002 (2.3558)\tPrec@1 84.746 (85.233)\tPrec@5 99.153 (99.121)\n",
      "Test: [0/100]\tTime 0.251 (0.251)\tLoss 10.8471 (10.8471)\tPrec@1 51.000 (51.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 8.2968 (10.1609)\tPrec@1 59.000 (52.182)\tPrec@5 96.000 (95.000)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 8.6300 (10.2048)\tPrec@1 56.000 (51.095)\tPrec@5 94.000 (94.905)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 9.6501 (10.1708)\tPrec@1 52.000 (51.032)\tPrec@5 93.000 (94.903)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.5392 (10.1234)\tPrec@1 49.000 (51.512)\tPrec@5 92.000 (94.780)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.5015 (10.0348)\tPrec@1 57.000 (52.078)\tPrec@5 95.000 (94.863)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.1562 (10.0165)\tPrec@1 59.000 (51.869)\tPrec@5 95.000 (94.738)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.2010 (9.9728)\tPrec@1 55.000 (52.070)\tPrec@5 97.000 (94.915)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.9780 (9.9346)\tPrec@1 52.000 (52.296)\tPrec@5 90.000 (94.988)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.1508 (10.0088)\tPrec@1 52.000 (51.879)\tPrec@5 97.000 (95.033)\n",
      "val Results: Prec@1 51.910 Prec@5 94.930 Loss 10.01507\n",
      "val Class Accuracy: [0.946,0.990,0.816,0.768,0.702,0.306,0.166,0.303,0.194,0.000]\n",
      "Best Prec@1: 61.620\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [68][0/97], lr: 0.01000\tTime 0.404 (0.404)\tData 0.204 (0.204)\tLoss 3.5386 (3.5386)\tPrec@1 78.125 (78.125)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [68][10/97], lr: 0.01000\tTime 0.329 (0.341)\tData 0.000 (0.033)\tLoss 2.9779 (2.5529)\tPrec@1 83.594 (84.943)\tPrec@5 98.438 (99.077)\n",
      "Epoch: [68][20/97], lr: 0.01000\tTime 0.330 (0.335)\tData 0.000 (0.025)\tLoss 1.9734 (2.3189)\tPrec@1 84.375 (86.272)\tPrec@5 96.875 (98.996)\n",
      "Epoch: [68][30/97], lr: 0.01000\tTime 0.323 (0.333)\tData 0.000 (0.023)\tLoss 1.8298 (2.3307)\tPrec@1 87.500 (86.038)\tPrec@5 100.000 (98.992)\n",
      "Epoch: [68][40/97], lr: 0.01000\tTime 0.329 (0.332)\tData 0.000 (0.021)\tLoss 3.1564 (2.3811)\tPrec@1 78.125 (85.461)\tPrec@5 99.219 (99.009)\n",
      "Epoch: [68][50/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.021)\tLoss 2.9833 (2.3670)\tPrec@1 79.688 (85.478)\tPrec@5 98.438 (99.066)\n",
      "Epoch: [68][60/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.020)\tLoss 2.7486 (2.3723)\tPrec@1 85.938 (85.553)\tPrec@5 97.656 (98.963)\n",
      "Epoch: [68][70/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 2.4037 (2.3901)\tPrec@1 82.812 (85.310)\tPrec@5 97.656 (98.889)\n",
      "Epoch: [68][80/97], lr: 0.01000\tTime 0.324 (0.330)\tData 0.000 (0.019)\tLoss 3.2275 (2.3659)\tPrec@1 80.469 (85.552)\tPrec@5 100.000 (98.949)\n",
      "Epoch: [68][90/97], lr: 0.01000\tTime 0.324 (0.330)\tData 0.000 (0.019)\tLoss 2.7764 (2.3708)\tPrec@1 80.469 (85.474)\tPrec@5 99.219 (98.953)\n",
      "Epoch: [68][96/97], lr: 0.01000\tTime 0.317 (0.330)\tData 0.000 (0.020)\tLoss 2.4556 (2.3870)\tPrec@1 84.746 (85.370)\tPrec@5 99.153 (98.888)\n",
      "Test: [0/100]\tTime 0.250 (0.250)\tLoss 8.3075 (8.3075)\tPrec@1 62.000 (62.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.3584 (8.1874)\tPrec@1 62.000 (59.727)\tPrec@5 95.000 (96.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.3592 (8.2512)\tPrec@1 61.000 (58.857)\tPrec@5 98.000 (96.333)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.5656 (8.2872)\tPrec@1 61.000 (58.548)\tPrec@5 98.000 (96.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.0047 (8.3887)\tPrec@1 59.000 (57.878)\tPrec@5 94.000 (96.171)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.5411 (8.2950)\tPrec@1 54.000 (58.431)\tPrec@5 97.000 (96.216)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.6411 (8.2972)\tPrec@1 59.000 (58.246)\tPrec@5 98.000 (96.164)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.6641 (8.2571)\tPrec@1 63.000 (58.577)\tPrec@5 98.000 (96.254)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.3894 (8.2200)\tPrec@1 58.000 (58.556)\tPrec@5 93.000 (96.259)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.2397 (8.2726)\tPrec@1 58.000 (58.275)\tPrec@5 99.000 (96.198)\n",
      "val Results: Prec@1 58.450 Prec@5 96.220 Loss 8.26704\n",
      "val Class Accuracy: [0.908,0.980,0.572,0.892,0.469,0.367,0.491,0.485,0.482,0.199]\n",
      "Best Prec@1: 61.620\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [69][0/97], lr: 0.01000\tTime 0.423 (0.423)\tData 0.224 (0.224)\tLoss 2.5100 (2.5100)\tPrec@1 82.031 (82.031)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [69][10/97], lr: 0.01000\tTime 0.327 (0.345)\tData 0.000 (0.034)\tLoss 2.7944 (2.4886)\tPrec@1 84.375 (84.446)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [69][20/97], lr: 0.01000\tTime 0.324 (0.337)\tData 0.000 (0.026)\tLoss 2.4994 (2.4098)\tPrec@1 87.500 (85.491)\tPrec@5 98.438 (99.182)\n",
      "Epoch: [69][30/97], lr: 0.01000\tTime 0.325 (0.335)\tData 0.000 (0.023)\tLoss 2.6616 (2.3610)\tPrec@1 81.250 (85.635)\tPrec@5 100.000 (99.143)\n",
      "Epoch: [69][40/97], lr: 0.01000\tTime 0.328 (0.333)\tData 0.000 (0.022)\tLoss 2.2709 (2.3476)\tPrec@1 86.719 (85.461)\tPrec@5 99.219 (99.009)\n",
      "Epoch: [69][50/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 2.7403 (2.3593)\tPrec@1 82.812 (85.600)\tPrec@5 97.656 (98.958)\n",
      "Epoch: [69][60/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.020)\tLoss 2.1950 (2.3684)\tPrec@1 85.156 (85.451)\tPrec@5 100.000 (98.963)\n",
      "Epoch: [69][70/97], lr: 0.01000\tTime 0.329 (0.332)\tData 0.000 (0.020)\tLoss 2.0367 (2.3805)\tPrec@1 88.281 (85.299)\tPrec@5 100.000 (98.988)\n",
      "Epoch: [69][80/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 3.5285 (2.4078)\tPrec@1 78.906 (85.118)\tPrec@5 100.000 (99.026)\n",
      "Epoch: [69][90/97], lr: 0.01000\tTime 0.319 (0.331)\tData 0.000 (0.019)\tLoss 2.5536 (2.4044)\tPrec@1 84.375 (85.053)\tPrec@5 100.000 (99.056)\n",
      "Epoch: [69][96/97], lr: 0.01000\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 2.8347 (2.4179)\tPrec@1 81.356 (84.935)\tPrec@5 97.458 (99.025)\n",
      "Test: [0/100]\tTime 0.258 (0.258)\tLoss 11.0264 (11.0264)\tPrec@1 46.000 (46.000)\tPrec@5 90.000 (90.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 9.3820 (10.5012)\tPrec@1 48.000 (48.000)\tPrec@5 96.000 (91.727)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 8.4765 (10.3178)\tPrec@1 56.000 (48.667)\tPrec@5 96.000 (92.619)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 10.0557 (10.2508)\tPrec@1 49.000 (49.226)\tPrec@5 93.000 (92.548)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.8961 (10.2909)\tPrec@1 43.000 (49.195)\tPrec@5 93.000 (92.195)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 10.0898 (10.1940)\tPrec@1 51.000 (49.588)\tPrec@5 94.000 (92.255)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.0167 (10.1892)\tPrec@1 59.000 (49.492)\tPrec@5 96.000 (92.295)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 10.1880 (10.1694)\tPrec@1 51.000 (49.521)\tPrec@5 91.000 (92.408)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.9725 (10.1001)\tPrec@1 54.000 (49.802)\tPrec@5 85.000 (92.457)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.7399 (10.1477)\tPrec@1 52.000 (49.516)\tPrec@5 94.000 (92.473)\n",
      "val Results: Prec@1 49.590 Prec@5 92.400 Loss 10.15268\n",
      "val Class Accuracy: [0.927,0.966,0.838,0.747,0.387,0.486,0.212,0.091,0.305,0.000]\n",
      "Best Prec@1: 61.620\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [70][0/97], lr: 0.01000\tTime 0.454 (0.454)\tData 0.224 (0.224)\tLoss 2.3870 (2.3870)\tPrec@1 86.719 (86.719)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [70][10/97], lr: 0.01000\tTime 0.324 (0.344)\tData 0.000 (0.035)\tLoss 2.2916 (2.3165)\tPrec@1 87.500 (85.653)\tPrec@5 98.438 (99.077)\n",
      "Epoch: [70][20/97], lr: 0.01000\tTime 0.328 (0.338)\tData 0.000 (0.027)\tLoss 2.4796 (2.4289)\tPrec@1 85.938 (85.045)\tPrec@5 99.219 (99.182)\n",
      "Epoch: [70][30/97], lr: 0.01000\tTime 0.327 (0.334)\tData 0.000 (0.024)\tLoss 1.5317 (2.3573)\tPrec@1 89.062 (85.408)\tPrec@5 98.438 (99.093)\n",
      "Epoch: [70][40/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.022)\tLoss 2.1035 (2.3232)\tPrec@1 89.062 (85.671)\tPrec@5 99.219 (99.104)\n",
      "Epoch: [70][50/97], lr: 0.01000\tTime 0.328 (0.332)\tData 0.000 (0.021)\tLoss 2.0542 (2.3371)\tPrec@1 85.938 (85.539)\tPrec@5 96.875 (99.066)\n",
      "Epoch: [70][60/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 3.1971 (2.3864)\tPrec@1 81.250 (85.143)\tPrec@5 99.219 (99.027)\n",
      "Epoch: [70][70/97], lr: 0.01000\tTime 0.329 (0.332)\tData 0.000 (0.020)\tLoss 2.0818 (2.3698)\tPrec@1 87.500 (85.332)\tPrec@5 99.219 (99.032)\n",
      "Epoch: [70][80/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 2.6469 (2.3721)\tPrec@1 82.031 (85.301)\tPrec@5 98.438 (99.026)\n",
      "Epoch: [70][90/97], lr: 0.01000\tTime 0.329 (0.331)\tData 0.000 (0.019)\tLoss 2.2543 (2.3598)\tPrec@1 87.500 (85.388)\tPrec@5 100.000 (99.030)\n",
      "Epoch: [70][96/97], lr: 0.01000\tTime 0.319 (0.331)\tData 0.000 (0.020)\tLoss 2.0750 (2.3591)\tPrec@1 88.136 (85.418)\tPrec@5 98.305 (98.968)\n",
      "Test: [0/100]\tTime 0.252 (0.252)\tLoss 10.4395 (10.4395)\tPrec@1 50.000 (50.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 8.3138 (9.6219)\tPrec@1 57.000 (53.000)\tPrec@5 98.000 (92.818)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 8.7527 (9.5616)\tPrec@1 55.000 (52.524)\tPrec@5 93.000 (93.143)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.9699 (9.4491)\tPrec@1 63.000 (53.000)\tPrec@5 93.000 (93.484)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.9950 (9.4186)\tPrec@1 53.000 (53.366)\tPrec@5 93.000 (93.341)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.3158 (9.3786)\tPrec@1 56.000 (53.490)\tPrec@5 96.000 (93.529)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.0639 (9.3189)\tPrec@1 58.000 (53.623)\tPrec@5 94.000 (93.672)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.2002 (9.3042)\tPrec@1 60.000 (53.746)\tPrec@5 97.000 (93.718)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.5018 (9.2721)\tPrec@1 61.000 (53.790)\tPrec@5 92.000 (93.716)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.7879 (9.3320)\tPrec@1 51.000 (53.659)\tPrec@5 94.000 (93.659)\n",
      "val Results: Prec@1 53.690 Prec@5 93.510 Loss 9.35048\n",
      "val Class Accuracy: [0.961,0.984,0.780,0.599,0.712,0.135,0.536,0.580,0.076,0.006]\n",
      "Best Prec@1: 61.620\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [71][0/97], lr: 0.01000\tTime 0.441 (0.441)\tData 0.217 (0.217)\tLoss 2.5424 (2.5424)\tPrec@1 85.156 (85.156)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [71][10/97], lr: 0.01000\tTime 0.324 (0.344)\tData 0.000 (0.034)\tLoss 1.8129 (2.1667)\tPrec@1 89.062 (86.932)\tPrec@5 99.219 (98.935)\n",
      "Epoch: [71][20/97], lr: 0.01000\tTime 0.328 (0.337)\tData 0.000 (0.026)\tLoss 2.1089 (2.2994)\tPrec@1 88.281 (86.049)\tPrec@5 99.219 (99.033)\n",
      "Epoch: [71][30/97], lr: 0.01000\tTime 0.326 (0.335)\tData 0.000 (0.023)\tLoss 1.8848 (2.2906)\tPrec@1 89.844 (85.963)\tPrec@5 100.000 (99.068)\n",
      "Epoch: [71][40/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.022)\tLoss 2.8284 (2.3001)\tPrec@1 82.812 (85.842)\tPrec@5 100.000 (99.085)\n",
      "Epoch: [71][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 2.2019 (2.2899)\tPrec@1 87.500 (85.907)\tPrec@5 100.000 (99.112)\n",
      "Epoch: [71][60/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 2.1938 (2.3275)\tPrec@1 86.719 (85.707)\tPrec@5 99.219 (99.091)\n",
      "Epoch: [71][70/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 2.4179 (2.3461)\tPrec@1 84.375 (85.673)\tPrec@5 97.656 (99.032)\n",
      "Epoch: [71][80/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.020)\tLoss 2.4156 (2.3657)\tPrec@1 85.156 (85.494)\tPrec@5 99.219 (98.968)\n",
      "Epoch: [71][90/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.019)\tLoss 3.2290 (2.3497)\tPrec@1 82.812 (85.706)\tPrec@5 100.000 (99.013)\n",
      "Epoch: [71][96/97], lr: 0.01000\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 1.1803 (2.3334)\tPrec@1 94.068 (85.846)\tPrec@5 99.153 (99.017)\n",
      "Test: [0/100]\tTime 0.244 (0.244)\tLoss 9.0803 (9.0803)\tPrec@1 55.000 (55.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 6.4675 (8.9982)\tPrec@1 69.000 (57.000)\tPrec@5 98.000 (95.091)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 8.6076 (9.0647)\tPrec@1 56.000 (56.762)\tPrec@5 96.000 (94.524)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 8.7477 (9.0652)\tPrec@1 57.000 (56.806)\tPrec@5 94.000 (94.452)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.8060 (9.1400)\tPrec@1 54.000 (56.561)\tPrec@5 90.000 (94.195)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.8262 (9.0679)\tPrec@1 57.000 (56.784)\tPrec@5 98.000 (94.373)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.0072 (9.0647)\tPrec@1 62.000 (56.738)\tPrec@5 93.000 (94.295)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.5582 (9.0039)\tPrec@1 58.000 (56.986)\tPrec@5 97.000 (94.563)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.7218 (8.9619)\tPrec@1 59.000 (57.173)\tPrec@5 93.000 (94.593)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.2056 (8.9906)\tPrec@1 51.000 (56.967)\tPrec@5 97.000 (94.593)\n",
      "val Results: Prec@1 56.950 Prec@5 94.530 Loss 9.00856\n",
      "val Class Accuracy: [0.907,0.987,0.779,0.720,0.560,0.376,0.849,0.288,0.226,0.003]\n",
      "Best Prec@1: 61.620\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [72][0/97], lr: 0.01000\tTime 0.479 (0.479)\tData 0.257 (0.257)\tLoss 1.4606 (1.4606)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [72][10/97], lr: 0.01000\tTime 0.331 (0.348)\tData 0.000 (0.038)\tLoss 2.1033 (2.2150)\tPrec@1 87.500 (86.790)\tPrec@5 98.438 (99.148)\n",
      "Epoch: [72][20/97], lr: 0.01000\tTime 0.324 (0.338)\tData 0.000 (0.028)\tLoss 3.1387 (2.4315)\tPrec@1 82.812 (85.305)\tPrec@5 98.438 (98.921)\n",
      "Epoch: [72][30/97], lr: 0.01000\tTime 0.325 (0.335)\tData 0.000 (0.025)\tLoss 3.4912 (2.4619)\tPrec@1 78.906 (84.929)\tPrec@5 98.438 (98.841)\n",
      "Epoch: [72][40/97], lr: 0.01000\tTime 0.327 (0.334)\tData 0.000 (0.023)\tLoss 2.4313 (2.4553)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (98.819)\n",
      "Epoch: [72][50/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.022)\tLoss 2.7402 (2.4125)\tPrec@1 82.812 (85.401)\tPrec@5 99.219 (98.882)\n",
      "Epoch: [72][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 1.8064 (2.3970)\tPrec@1 89.062 (85.425)\tPrec@5 98.438 (98.937)\n",
      "Epoch: [72][70/97], lr: 0.01000\tTime 0.330 (0.331)\tData 0.000 (0.021)\tLoss 1.9293 (2.3884)\tPrec@1 88.281 (85.354)\tPrec@5 100.000 (98.966)\n",
      "Epoch: [72][80/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 2.2716 (2.3613)\tPrec@1 85.156 (85.513)\tPrec@5 100.000 (99.026)\n",
      "Epoch: [72][90/97], lr: 0.01000\tTime 0.322 (0.331)\tData 0.000 (0.020)\tLoss 1.9186 (2.3428)\tPrec@1 87.500 (85.551)\tPrec@5 100.000 (99.004)\n",
      "Epoch: [72][96/97], lr: 0.01000\tTime 0.317 (0.330)\tData 0.000 (0.020)\tLoss 2.1245 (2.3322)\tPrec@1 87.288 (85.668)\tPrec@5 98.305 (99.000)\n",
      "Test: [0/100]\tTime 0.240 (0.240)\tLoss 10.3712 (10.3712)\tPrec@1 49.000 (49.000)\tPrec@5 90.000 (90.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 7.4811 (9.0870)\tPrec@1 64.000 (57.000)\tPrec@5 95.000 (92.091)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.0797 (8.9225)\tPrec@1 64.000 (57.238)\tPrec@5 96.000 (92.286)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 8.2788 (8.9645)\tPrec@1 61.000 (56.871)\tPrec@5 95.000 (92.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.8949 (9.0031)\tPrec@1 53.000 (56.829)\tPrec@5 89.000 (92.415)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.9407 (8.9150)\tPrec@1 64.000 (57.412)\tPrec@5 93.000 (92.706)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7245 (8.8614)\tPrec@1 68.000 (57.525)\tPrec@5 93.000 (92.705)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.6707 (8.8146)\tPrec@1 59.000 (57.873)\tPrec@5 95.000 (92.732)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.3178 (8.7697)\tPrec@1 60.000 (58.012)\tPrec@5 89.000 (92.765)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.2113 (8.8301)\tPrec@1 55.000 (57.637)\tPrec@5 95.000 (92.769)\n",
      "val Results: Prec@1 57.670 Prec@5 92.700 Loss 8.84695\n",
      "val Class Accuracy: [0.949,0.965,0.661,0.667,0.728,0.782,0.413,0.274,0.327,0.001]\n",
      "Best Prec@1: 61.620\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [73][0/97], lr: 0.01000\tTime 0.500 (0.500)\tData 0.257 (0.257)\tLoss 2.4602 (2.4602)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [73][10/97], lr: 0.01000\tTime 0.330 (0.349)\tData 0.000 (0.038)\tLoss 2.2812 (2.3001)\tPrec@1 85.156 (86.151)\tPrec@5 98.438 (98.864)\n",
      "Epoch: [73][20/97], lr: 0.01000\tTime 0.327 (0.340)\tData 0.000 (0.028)\tLoss 1.9231 (2.3765)\tPrec@1 89.844 (85.603)\tPrec@5 99.219 (98.921)\n",
      "Epoch: [73][30/97], lr: 0.01000\tTime 0.329 (0.337)\tData 0.000 (0.025)\tLoss 2.3686 (2.4316)\tPrec@1 85.938 (85.081)\tPrec@5 99.219 (99.042)\n",
      "Epoch: [73][40/97], lr: 0.01000\tTime 0.325 (0.336)\tData 0.000 (0.023)\tLoss 2.3047 (2.3925)\tPrec@1 85.156 (85.423)\tPrec@5 100.000 (99.085)\n",
      "Epoch: [73][50/97], lr: 0.01000\tTime 0.325 (0.335)\tData 0.000 (0.022)\tLoss 3.3708 (2.4198)\tPrec@1 82.031 (85.294)\tPrec@5 100.000 (99.066)\n",
      "Epoch: [73][60/97], lr: 0.01000\tTime 0.334 (0.334)\tData 0.000 (0.021)\tLoss 2.3103 (2.4037)\tPrec@1 85.156 (85.323)\tPrec@5 100.000 (99.014)\n",
      "Epoch: [73][70/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 2.3049 (2.4014)\tPrec@1 85.938 (85.266)\tPrec@5 98.438 (99.043)\n",
      "Epoch: [73][80/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 2.2119 (2.3944)\tPrec@1 85.938 (85.233)\tPrec@5 99.219 (99.093)\n",
      "Epoch: [73][90/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 2.8856 (2.3942)\tPrec@1 82.031 (85.319)\tPrec@5 99.219 (99.107)\n",
      "Epoch: [73][96/97], lr: 0.01000\tTime 0.319 (0.332)\tData 0.000 (0.020)\tLoss 1.8371 (2.3832)\tPrec@1 89.831 (85.402)\tPrec@5 100.000 (99.113)\n",
      "Test: [0/100]\tTime 0.237 (0.237)\tLoss 8.2433 (8.2433)\tPrec@1 63.000 (63.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 6.2505 (8.0093)\tPrec@1 71.000 (61.727)\tPrec@5 99.000 (95.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.8772 (8.0173)\tPrec@1 60.000 (61.571)\tPrec@5 96.000 (95.095)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 6.6908 (8.0441)\tPrec@1 67.000 (61.194)\tPrec@5 97.000 (95.258)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.6475 (8.0201)\tPrec@1 65.000 (61.073)\tPrec@5 96.000 (95.268)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.7908 (7.9244)\tPrec@1 66.000 (61.569)\tPrec@5 99.000 (95.353)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.4014 (7.8432)\tPrec@1 66.000 (61.836)\tPrec@5 95.000 (95.443)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.9899 (7.8316)\tPrec@1 62.000 (62.127)\tPrec@5 96.000 (95.408)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.2368 (7.7894)\tPrec@1 63.000 (62.272)\tPrec@5 94.000 (95.420)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.4801 (7.8497)\tPrec@1 56.000 (62.066)\tPrec@5 94.000 (95.352)\n",
      "val Results: Prec@1 62.010 Prec@5 95.270 Loss 7.87196\n",
      "val Class Accuracy: [0.931,0.959,0.551,0.446,0.736,0.811,0.801,0.729,0.194,0.043]\n",
      "Best Prec@1: 62.010\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [74][0/97], lr: 0.01000\tTime 0.466 (0.466)\tData 0.241 (0.241)\tLoss 2.7276 (2.7276)\tPrec@1 82.031 (82.031)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [74][10/97], lr: 0.01000\tTime 0.324 (0.346)\tData 0.000 (0.037)\tLoss 2.2643 (2.0911)\tPrec@1 84.375 (87.358)\tPrec@5 98.438 (99.148)\n",
      "Epoch: [74][20/97], lr: 0.01000\tTime 0.326 (0.337)\tData 0.000 (0.027)\tLoss 2.3254 (2.2111)\tPrec@1 85.156 (86.421)\tPrec@5 97.656 (99.107)\n",
      "Epoch: [74][30/97], lr: 0.01000\tTime 0.330 (0.335)\tData 0.000 (0.024)\tLoss 2.1355 (2.3738)\tPrec@1 84.375 (85.484)\tPrec@5 99.219 (98.866)\n",
      "Epoch: [74][40/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.022)\tLoss 2.0252 (2.3744)\tPrec@1 87.500 (85.461)\tPrec@5 100.000 (98.838)\n",
      "Epoch: [74][50/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.021)\tLoss 2.8649 (2.3758)\tPrec@1 81.250 (85.447)\tPrec@5 97.656 (98.912)\n",
      "Epoch: [74][60/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.021)\tLoss 1.9364 (2.3524)\tPrec@1 90.625 (85.707)\tPrec@5 99.219 (98.873)\n",
      "Epoch: [74][70/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 3.5387 (2.3782)\tPrec@1 77.344 (85.541)\tPrec@5 100.000 (98.856)\n",
      "Epoch: [74][80/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 2.4587 (2.4020)\tPrec@1 85.938 (85.330)\tPrec@5 97.656 (98.862)\n",
      "Epoch: [74][90/97], lr: 0.01000\tTime 0.332 (0.334)\tData 0.000 (0.020)\tLoss 1.6303 (2.3717)\tPrec@1 90.625 (85.586)\tPrec@5 100.000 (98.875)\n",
      "Epoch: [74][96/97], lr: 0.01000\tTime 0.322 (0.333)\tData 0.000 (0.020)\tLoss 2.3236 (2.3628)\tPrec@1 85.593 (85.644)\tPrec@5 98.305 (98.880)\n",
      "Test: [0/100]\tTime 0.263 (0.263)\tLoss 10.3674 (10.3674)\tPrec@1 49.000 (49.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 7.8923 (9.1644)\tPrec@1 59.000 (55.091)\tPrec@5 97.000 (94.909)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 8.0511 (9.0515)\tPrec@1 57.000 (55.571)\tPrec@5 95.000 (95.143)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.8925 (9.0085)\tPrec@1 61.000 (56.065)\tPrec@5 94.000 (94.935)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 9.5260 (9.0212)\tPrec@1 55.000 (55.976)\tPrec@5 91.000 (94.854)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 9.4699 (8.9350)\tPrec@1 53.000 (56.333)\tPrec@5 93.000 (94.745)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.3471 (8.9222)\tPrec@1 62.000 (56.328)\tPrec@5 96.000 (94.721)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.5720 (8.8737)\tPrec@1 59.000 (56.465)\tPrec@5 97.000 (94.930)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.2689 (8.8338)\tPrec@1 59.000 (56.617)\tPrec@5 95.000 (95.086)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.5824 (8.9008)\tPrec@1 55.000 (56.363)\tPrec@5 99.000 (95.044)\n",
      "val Results: Prec@1 56.270 Prec@5 94.950 Loss 8.93664\n",
      "val Class Accuracy: [0.967,0.969,0.836,0.535,0.749,0.419,0.429,0.240,0.457,0.026]\n",
      "Best Prec@1: 62.010\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [75][0/97], lr: 0.01000\tTime 0.407 (0.407)\tData 0.205 (0.205)\tLoss 1.8416 (1.8416)\tPrec@1 88.281 (88.281)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [75][10/97], lr: 0.01000\tTime 0.328 (0.347)\tData 0.000 (0.034)\tLoss 1.9982 (2.1528)\tPrec@1 85.938 (86.790)\tPrec@5 99.219 (99.148)\n",
      "Epoch: [75][20/97], lr: 0.01000\tTime 0.338 (0.341)\tData 0.000 (0.026)\tLoss 2.3382 (2.1957)\tPrec@1 86.719 (86.570)\tPrec@5 98.438 (99.070)\n",
      "Epoch: [75][30/97], lr: 0.01000\tTime 0.324 (0.337)\tData 0.000 (0.023)\tLoss 2.9278 (2.2399)\tPrec@1 82.031 (86.114)\tPrec@5 100.000 (98.992)\n",
      "Epoch: [75][40/97], lr: 0.01000\tTime 0.324 (0.335)\tData 0.000 (0.022)\tLoss 2.1693 (2.2556)\tPrec@1 85.156 (85.995)\tPrec@5 99.219 (99.047)\n",
      "Epoch: [75][50/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.021)\tLoss 2.5903 (2.2553)\tPrec@1 82.812 (85.938)\tPrec@5 99.219 (99.050)\n",
      "Epoch: [75][60/97], lr: 0.01000\tTime 0.323 (0.333)\tData 0.000 (0.020)\tLoss 1.7008 (2.2850)\tPrec@1 90.625 (85.707)\tPrec@5 99.219 (99.103)\n",
      "Epoch: [75][70/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.4352 (2.2962)\tPrec@1 86.719 (85.805)\tPrec@5 99.219 (99.098)\n",
      "Epoch: [75][80/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.019)\tLoss 1.7297 (2.2793)\tPrec@1 89.844 (85.889)\tPrec@5 99.219 (99.122)\n",
      "Epoch: [75][90/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.019)\tLoss 3.0852 (2.2874)\tPrec@1 82.031 (85.869)\tPrec@5 99.219 (99.116)\n",
      "Epoch: [75][96/97], lr: 0.01000\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 1.4106 (2.2734)\tPrec@1 90.678 (85.910)\tPrec@5 99.153 (99.089)\n",
      "Test: [0/100]\tTime 0.254 (0.254)\tLoss 9.7454 (9.7454)\tPrec@1 54.000 (54.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.3141 (8.6046)\tPrec@1 60.000 (56.909)\tPrec@5 98.000 (96.273)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.7389 (8.3883)\tPrec@1 62.000 (57.952)\tPrec@5 96.000 (96.190)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.0897 (8.3854)\tPrec@1 66.000 (58.516)\tPrec@5 95.000 (95.774)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.4463 (8.4408)\tPrec@1 54.000 (58.366)\tPrec@5 95.000 (95.610)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.7834 (8.3610)\tPrec@1 61.000 (58.588)\tPrec@5 98.000 (95.843)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.5044 (8.3336)\tPrec@1 70.000 (58.607)\tPrec@5 97.000 (95.918)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.4067 (8.3417)\tPrec@1 58.000 (58.648)\tPrec@5 96.000 (96.070)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.2456 (8.3169)\tPrec@1 58.000 (58.630)\tPrec@5 92.000 (96.111)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.5494 (8.3764)\tPrec@1 57.000 (58.330)\tPrec@5 97.000 (96.044)\n",
      "val Results: Prec@1 58.180 Prec@5 96.000 Loss 8.41235\n",
      "val Class Accuracy: [0.982,0.956,0.750,0.734,0.785,0.602,0.356,0.231,0.373,0.049]\n",
      "Best Prec@1: 62.010\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [76][0/97], lr: 0.01000\tTime 0.427 (0.427)\tData 0.207 (0.207)\tLoss 1.6577 (1.6577)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [76][10/97], lr: 0.01000\tTime 0.328 (0.346)\tData 0.000 (0.033)\tLoss 2.1148 (2.0840)\tPrec@1 85.156 (86.719)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [76][20/97], lr: 0.01000\tTime 0.329 (0.339)\tData 0.000 (0.026)\tLoss 2.4777 (2.0995)\tPrec@1 81.250 (86.570)\tPrec@5 99.219 (99.479)\n",
      "Epoch: [76][30/97], lr: 0.01000\tTime 0.326 (0.337)\tData 0.000 (0.023)\tLoss 2.4576 (2.1358)\tPrec@1 84.375 (86.416)\tPrec@5 98.438 (99.194)\n",
      "Epoch: [76][40/97], lr: 0.01000\tTime 0.331 (0.335)\tData 0.000 (0.022)\tLoss 2.6132 (2.1851)\tPrec@1 84.375 (86.338)\tPrec@5 100.000 (99.123)\n",
      "Epoch: [76][50/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.021)\tLoss 2.2367 (2.1776)\tPrec@1 86.719 (86.366)\tPrec@5 97.656 (99.157)\n",
      "Epoch: [76][60/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.020)\tLoss 2.4120 (2.2360)\tPrec@1 84.375 (86.002)\tPrec@5 99.219 (99.014)\n",
      "Epoch: [76][70/97], lr: 0.01000\tTime 0.330 (0.332)\tData 0.000 (0.020)\tLoss 2.1223 (2.2911)\tPrec@1 85.156 (85.706)\tPrec@5 97.656 (98.834)\n",
      "Epoch: [76][80/97], lr: 0.01000\tTime 0.330 (0.332)\tData 0.000 (0.019)\tLoss 2.7395 (2.3331)\tPrec@1 82.031 (85.417)\tPrec@5 96.094 (98.833)\n",
      "Epoch: [76][90/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.019)\tLoss 2.4176 (2.3361)\tPrec@1 83.594 (85.345)\tPrec@5 100.000 (98.875)\n",
      "Epoch: [76][96/97], lr: 0.01000\tTime 0.319 (0.332)\tData 0.000 (0.020)\tLoss 2.2153 (2.3395)\tPrec@1 87.288 (85.354)\tPrec@5 99.153 (98.896)\n",
      "Test: [0/100]\tTime 0.235 (0.235)\tLoss 9.0787 (9.0787)\tPrec@1 57.000 (57.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 7.3075 (8.4744)\tPrec@1 66.000 (59.000)\tPrec@5 97.000 (95.000)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.3284 (8.4170)\tPrec@1 63.000 (58.905)\tPrec@5 95.000 (95.095)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.3530 (8.4570)\tPrec@1 68.000 (58.839)\tPrec@5 96.000 (95.032)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.1035 (8.5186)\tPrec@1 56.000 (58.659)\tPrec@5 92.000 (94.780)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.9906 (8.4128)\tPrec@1 62.000 (59.157)\tPrec@5 96.000 (94.961)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.0345 (8.4001)\tPrec@1 61.000 (58.885)\tPrec@5 91.000 (94.672)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.8403 (8.4105)\tPrec@1 64.000 (58.803)\tPrec@5 94.000 (94.761)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.5489 (8.3933)\tPrec@1 59.000 (58.852)\tPrec@5 92.000 (94.852)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.6378 (8.4449)\tPrec@1 57.000 (58.484)\tPrec@5 93.000 (94.868)\n",
      "val Results: Prec@1 58.550 Prec@5 94.830 Loss 8.46546\n",
      "val Class Accuracy: [0.972,0.964,0.686,0.775,0.752,0.344,0.596,0.449,0.308,0.009]\n",
      "Best Prec@1: 62.010\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [77][0/97], lr: 0.01000\tTime 0.473 (0.473)\tData 0.192 (0.192)\tLoss 2.2382 (2.2382)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [77][10/97], lr: 0.01000\tTime 0.322 (0.349)\tData 0.000 (0.032)\tLoss 2.3576 (2.1184)\tPrec@1 86.719 (86.506)\tPrec@5 99.219 (99.361)\n",
      "Epoch: [77][20/97], lr: 0.01000\tTime 0.330 (0.339)\tData 0.000 (0.025)\tLoss 2.7151 (2.3358)\tPrec@1 83.594 (85.045)\tPrec@5 100.000 (99.107)\n",
      "Epoch: [77][30/97], lr: 0.01000\tTime 0.325 (0.336)\tData 0.000 (0.023)\tLoss 2.6013 (2.2400)\tPrec@1 82.812 (85.711)\tPrec@5 97.656 (99.194)\n",
      "Epoch: [77][40/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.021)\tLoss 2.6474 (2.3136)\tPrec@1 84.375 (85.309)\tPrec@5 100.000 (99.181)\n",
      "Epoch: [77][50/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.021)\tLoss 1.8602 (2.2660)\tPrec@1 90.625 (85.708)\tPrec@5 100.000 (99.203)\n",
      "Epoch: [77][60/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 1.7211 (2.2872)\tPrec@1 89.844 (85.630)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [77][70/97], lr: 0.01000\tTime 0.328 (0.333)\tData 0.000 (0.020)\tLoss 2.1356 (2.3153)\tPrec@1 88.281 (85.552)\tPrec@5 97.656 (99.142)\n",
      "Epoch: [77][80/97], lr: 0.01000\tTime 0.321 (0.332)\tData 0.000 (0.019)\tLoss 2.6743 (2.3118)\tPrec@1 82.812 (85.494)\tPrec@5 100.000 (99.122)\n",
      "Epoch: [77][90/97], lr: 0.01000\tTime 0.322 (0.332)\tData 0.000 (0.019)\tLoss 2.4087 (2.3227)\tPrec@1 83.594 (85.362)\tPrec@5 98.438 (99.116)\n",
      "Epoch: [77][96/97], lr: 0.01000\tTime 0.319 (0.332)\tData 0.000 (0.020)\tLoss 2.4427 (2.3223)\tPrec@1 83.898 (85.354)\tPrec@5 97.458 (99.097)\n",
      "Test: [0/100]\tTime 0.227 (0.227)\tLoss 9.4214 (9.4214)\tPrec@1 57.000 (57.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 7.2792 (8.6068)\tPrec@1 64.000 (57.909)\tPrec@5 96.000 (95.364)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 6.4125 (8.4094)\tPrec@1 67.000 (58.667)\tPrec@5 98.000 (95.286)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.1155 (8.3679)\tPrec@1 66.000 (58.968)\tPrec@5 93.000 (95.129)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.1845 (8.3667)\tPrec@1 60.000 (59.098)\tPrec@5 98.000 (95.000)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.3523 (8.2594)\tPrec@1 64.000 (59.745)\tPrec@5 95.000 (94.882)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 6.6203 (8.2369)\tPrec@1 67.000 (59.770)\tPrec@5 95.000 (95.000)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.4792 (8.2156)\tPrec@1 65.000 (59.887)\tPrec@5 97.000 (94.930)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.9545 (8.1716)\tPrec@1 59.000 (60.049)\tPrec@5 93.000 (95.000)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.2087 (8.2601)\tPrec@1 59.000 (59.692)\tPrec@5 97.000 (94.923)\n",
      "val Results: Prec@1 59.860 Prec@5 94.940 Loss 8.25638\n",
      "val Class Accuracy: [0.874,0.981,0.762,0.680,0.663,0.779,0.389,0.547,0.304,0.007]\n",
      "Best Prec@1: 62.010\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [78][0/97], lr: 0.01000\tTime 0.450 (0.450)\tData 0.217 (0.217)\tLoss 1.9683 (1.9683)\tPrec@1 89.062 (89.062)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [78][10/97], lr: 0.01000\tTime 0.328 (0.346)\tData 0.000 (0.034)\tLoss 2.0112 (2.4435)\tPrec@1 89.062 (84.872)\tPrec@5 98.438 (99.361)\n",
      "Epoch: [78][20/97], lr: 0.01000\tTime 0.325 (0.337)\tData 0.000 (0.026)\tLoss 2.2567 (2.3361)\tPrec@1 85.938 (85.565)\tPrec@5 99.219 (99.293)\n",
      "Epoch: [78][30/97], lr: 0.01000\tTime 0.324 (0.335)\tData 0.000 (0.023)\tLoss 2.0604 (2.2805)\tPrec@1 87.500 (85.761)\tPrec@5 100.000 (99.320)\n",
      "Epoch: [78][40/97], lr: 0.01000\tTime 0.329 (0.333)\tData 0.000 (0.022)\tLoss 1.8818 (2.2505)\tPrec@1 89.062 (86.109)\tPrec@5 99.219 (99.295)\n",
      "Epoch: [78][50/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 2.2212 (2.2116)\tPrec@1 88.281 (86.397)\tPrec@5 99.219 (99.280)\n",
      "Epoch: [78][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.4132 (2.2369)\tPrec@1 85.938 (86.206)\tPrec@5 99.219 (99.206)\n",
      "Epoch: [78][70/97], lr: 0.01000\tTime 0.338 (0.332)\tData 0.000 (0.020)\tLoss 1.6166 (2.2182)\tPrec@1 92.188 (86.389)\tPrec@5 100.000 (99.186)\n",
      "Epoch: [78][80/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.019)\tLoss 2.7582 (2.2183)\tPrec@1 82.812 (86.333)\tPrec@5 99.219 (99.171)\n",
      "Epoch: [78][90/97], lr: 0.01000\tTime 0.322 (0.331)\tData 0.000 (0.019)\tLoss 2.2569 (2.2444)\tPrec@1 83.594 (86.135)\tPrec@5 99.219 (99.133)\n",
      "Epoch: [78][96/97], lr: 0.01000\tTime 0.317 (0.330)\tData 0.000 (0.020)\tLoss 2.6077 (2.2790)\tPrec@1 80.508 (85.942)\tPrec@5 100.000 (99.113)\n",
      "Test: [0/100]\tTime 0.254 (0.254)\tLoss 9.4071 (9.4071)\tPrec@1 55.000 (55.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 6.9674 (8.5941)\tPrec@1 64.000 (58.364)\tPrec@5 97.000 (95.636)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.7324 (8.4074)\tPrec@1 65.000 (58.905)\tPrec@5 96.000 (95.143)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.0697 (8.4820)\tPrec@1 59.000 (58.226)\tPrec@5 94.000 (95.097)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.6696 (8.5499)\tPrec@1 54.000 (58.024)\tPrec@5 90.000 (94.512)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.5785 (8.4551)\tPrec@1 61.000 (58.333)\tPrec@5 94.000 (94.647)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.6387 (8.4143)\tPrec@1 68.000 (58.475)\tPrec@5 94.000 (94.525)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.8719 (8.4008)\tPrec@1 61.000 (58.465)\tPrec@5 97.000 (94.634)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.4740 (8.3301)\tPrec@1 65.000 (59.000)\tPrec@5 93.000 (94.691)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.0944 (8.3965)\tPrec@1 61.000 (58.791)\tPrec@5 97.000 (94.703)\n",
      "val Results: Prec@1 58.800 Prec@5 94.670 Loss 8.39986\n",
      "val Class Accuracy: [0.930,0.983,0.767,0.489,0.739,0.834,0.523,0.300,0.258,0.057]\n",
      "Best Prec@1: 62.010\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [79][0/97], lr: 0.01000\tTime 0.620 (0.620)\tData 0.355 (0.355)\tLoss 3.3829 (3.3829)\tPrec@1 82.031 (82.031)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [79][10/97], lr: 0.01000\tTime 0.332 (0.373)\tData 0.000 (0.046)\tLoss 2.9899 (2.4953)\tPrec@1 82.812 (85.369)\tPrec@5 98.438 (98.935)\n",
      "Epoch: [79][20/97], lr: 0.01000\tTime 0.327 (0.353)\tData 0.000 (0.032)\tLoss 1.7348 (2.3720)\tPrec@1 88.281 (85.751)\tPrec@5 99.219 (99.070)\n",
      "Epoch: [79][30/97], lr: 0.01000\tTime 0.327 (0.345)\tData 0.000 (0.028)\tLoss 2.5443 (2.4220)\tPrec@1 84.375 (85.181)\tPrec@5 99.219 (99.093)\n",
      "Epoch: [79][40/97], lr: 0.01000\tTime 0.325 (0.341)\tData 0.000 (0.025)\tLoss 2.2688 (2.4251)\tPrec@1 85.156 (85.252)\tPrec@5 99.219 (99.009)\n",
      "Epoch: [79][50/97], lr: 0.01000\tTime 0.327 (0.339)\tData 0.000 (0.024)\tLoss 2.4891 (2.3633)\tPrec@1 85.156 (85.539)\tPrec@5 100.000 (99.096)\n",
      "Epoch: [79][60/97], lr: 0.01000\tTime 0.334 (0.337)\tData 0.000 (0.023)\tLoss 2.0541 (2.3674)\tPrec@1 89.062 (85.464)\tPrec@5 99.219 (99.116)\n",
      "Epoch: [79][70/97], lr: 0.01000\tTime 0.325 (0.336)\tData 0.000 (0.022)\tLoss 2.9998 (2.3839)\tPrec@1 81.250 (85.387)\tPrec@5 99.219 (99.087)\n",
      "Epoch: [79][80/97], lr: 0.01000\tTime 0.328 (0.335)\tData 0.000 (0.021)\tLoss 1.9755 (2.3751)\tPrec@1 89.062 (85.465)\tPrec@5 99.219 (99.074)\n",
      "Epoch: [79][90/97], lr: 0.01000\tTime 0.332 (0.335)\tData 0.000 (0.021)\tLoss 2.0904 (2.3567)\tPrec@1 87.500 (85.482)\tPrec@5 98.438 (99.056)\n",
      "Epoch: [79][96/97], lr: 0.01000\tTime 0.319 (0.334)\tData 0.000 (0.021)\tLoss 2.6750 (2.3523)\tPrec@1 83.051 (85.475)\tPrec@5 98.305 (99.081)\n",
      "Test: [0/100]\tTime 0.247 (0.247)\tLoss 9.3903 (9.3903)\tPrec@1 58.000 (58.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.3992 (8.5336)\tPrec@1 62.000 (58.727)\tPrec@5 97.000 (96.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.8669 (8.5782)\tPrec@1 58.000 (58.333)\tPrec@5 95.000 (95.952)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.4189 (8.6171)\tPrec@1 62.000 (58.355)\tPrec@5 96.000 (96.129)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.5853 (8.6347)\tPrec@1 65.000 (58.244)\tPrec@5 97.000 (96.073)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.1438 (8.5458)\tPrec@1 61.000 (58.706)\tPrec@5 93.000 (96.157)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.6514 (8.4677)\tPrec@1 67.000 (58.852)\tPrec@5 97.000 (96.131)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.6755 (8.4463)\tPrec@1 65.000 (58.887)\tPrec@5 99.000 (96.296)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.5940 (8.4136)\tPrec@1 63.000 (58.815)\tPrec@5 99.000 (96.481)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.9150 (8.4853)\tPrec@1 57.000 (58.538)\tPrec@5 99.000 (96.462)\n",
      "val Results: Prec@1 58.460 Prec@5 96.440 Loss 8.51324\n",
      "val Class Accuracy: [0.942,0.986,0.578,0.581,0.823,0.556,0.660,0.494,0.218,0.008]\n",
      "Best Prec@1: 62.010\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [80][0/97], lr: 0.01000\tTime 0.487 (0.487)\tData 0.275 (0.275)\tLoss 2.5945 (2.5945)\tPrec@1 86.719 (86.719)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [80][10/97], lr: 0.01000\tTime 0.325 (0.349)\tData 0.000 (0.040)\tLoss 2.5225 (2.2991)\tPrec@1 83.594 (86.506)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [80][20/97], lr: 0.01000\tTime 0.331 (0.339)\tData 0.000 (0.029)\tLoss 2.4277 (2.2341)\tPrec@1 85.938 (86.458)\tPrec@5 97.656 (99.256)\n",
      "Epoch: [80][30/97], lr: 0.01000\tTime 0.325 (0.336)\tData 0.000 (0.025)\tLoss 2.4109 (2.2637)\tPrec@1 82.812 (86.064)\tPrec@5 99.219 (99.269)\n",
      "Epoch: [80][40/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.023)\tLoss 2.6929 (2.2578)\tPrec@1 83.594 (85.938)\tPrec@5 99.219 (99.295)\n",
      "Epoch: [80][50/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.022)\tLoss 2.7138 (2.2982)\tPrec@1 85.156 (85.662)\tPrec@5 100.000 (99.387)\n",
      "Epoch: [80][60/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.021)\tLoss 2.0838 (2.2756)\tPrec@1 87.500 (85.809)\tPrec@5 98.438 (99.411)\n",
      "Epoch: [80][70/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.021)\tLoss 2.0833 (2.2783)\tPrec@1 88.281 (85.783)\tPrec@5 100.000 (99.417)\n",
      "Epoch: [80][80/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.7653 (2.2665)\tPrec@1 85.938 (85.986)\tPrec@5 96.094 (99.354)\n",
      "Epoch: [80][90/97], lr: 0.01000\tTime 0.320 (0.333)\tData 0.000 (0.020)\tLoss 2.6049 (2.2703)\tPrec@1 84.375 (85.938)\tPrec@5 99.219 (99.348)\n",
      "Epoch: [80][96/97], lr: 0.01000\tTime 0.318 (0.332)\tData 0.000 (0.021)\tLoss 2.1036 (2.2573)\tPrec@1 86.441 (86.063)\tPrec@5 100.000 (99.323)\n",
      "Test: [0/100]\tTime 0.251 (0.251)\tLoss 8.9086 (8.9086)\tPrec@1 60.000 (60.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 6.3177 (8.1277)\tPrec@1 69.000 (62.091)\tPrec@5 99.000 (96.545)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.6993 (7.9549)\tPrec@1 65.000 (61.905)\tPrec@5 97.000 (96.429)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.0149 (8.0005)\tPrec@1 62.000 (61.484)\tPrec@5 96.000 (96.290)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.1777 (8.0057)\tPrec@1 62.000 (61.537)\tPrec@5 95.000 (96.220)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.0550 (7.9246)\tPrec@1 65.000 (62.059)\tPrec@5 95.000 (96.275)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.0617 (7.8527)\tPrec@1 65.000 (62.508)\tPrec@5 96.000 (96.295)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.7160 (7.8160)\tPrec@1 66.000 (62.718)\tPrec@5 98.000 (96.437)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.5985 (7.7823)\tPrec@1 62.000 (62.765)\tPrec@5 97.000 (96.543)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.4775 (7.8605)\tPrec@1 58.000 (62.286)\tPrec@5 98.000 (96.648)\n",
      "val Results: Prec@1 62.140 Prec@5 96.650 Loss 7.90365\n",
      "val Class Accuracy: [0.963,0.986,0.581,0.553,0.763,0.745,0.792,0.509,0.274,0.048]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [81][0/97], lr: 0.01000\tTime 0.446 (0.446)\tData 0.228 (0.228)\tLoss 2.1920 (2.1920)\tPrec@1 86.719 (86.719)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [81][10/97], lr: 0.01000\tTime 0.327 (0.343)\tData 0.000 (0.036)\tLoss 2.5774 (2.3177)\tPrec@1 83.594 (86.506)\tPrec@5 98.438 (98.864)\n",
      "Epoch: [81][20/97], lr: 0.01000\tTime 0.322 (0.335)\tData 0.000 (0.027)\tLoss 2.3616 (2.4265)\tPrec@1 87.500 (85.491)\tPrec@5 98.438 (98.698)\n",
      "Epoch: [81][30/97], lr: 0.01000\tTime 0.330 (0.334)\tData 0.000 (0.024)\tLoss 1.5145 (2.3590)\tPrec@1 92.188 (85.559)\tPrec@5 99.219 (98.664)\n",
      "Epoch: [81][40/97], lr: 0.01000\tTime 0.328 (0.333)\tData 0.000 (0.022)\tLoss 2.6781 (2.3652)\tPrec@1 82.031 (85.595)\tPrec@5 99.219 (98.876)\n",
      "Epoch: [81][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 2.4954 (2.3958)\tPrec@1 82.812 (85.371)\tPrec@5 99.219 (98.912)\n",
      "Epoch: [81][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 2.0103 (2.3519)\tPrec@1 87.500 (85.464)\tPrec@5 99.219 (98.963)\n",
      "Epoch: [81][70/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 2.0678 (2.3102)\tPrec@1 87.500 (85.706)\tPrec@5 97.656 (98.955)\n",
      "Epoch: [81][80/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 2.5356 (2.3071)\tPrec@1 82.031 (85.725)\tPrec@5 98.438 (98.987)\n",
      "Epoch: [81][90/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.020)\tLoss 1.9859 (2.2884)\tPrec@1 86.719 (85.800)\tPrec@5 100.000 (99.038)\n",
      "Epoch: [81][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 1.9997 (2.2922)\tPrec@1 85.593 (85.700)\tPrec@5 99.153 (99.017)\n",
      "Test: [0/100]\tTime 0.241 (0.241)\tLoss 8.7980 (8.7980)\tPrec@1 61.000 (61.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 6.9216 (8.1529)\tPrec@1 65.000 (59.909)\tPrec@5 94.000 (96.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.4113 (7.8889)\tPrec@1 68.000 (61.333)\tPrec@5 97.000 (96.476)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.1377 (7.9142)\tPrec@1 67.000 (61.194)\tPrec@5 99.000 (96.581)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.4652 (7.8931)\tPrec@1 66.000 (61.561)\tPrec@5 96.000 (96.488)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.1115 (7.8313)\tPrec@1 67.000 (61.863)\tPrec@5 98.000 (96.706)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.1084 (7.8275)\tPrec@1 67.000 (61.623)\tPrec@5 96.000 (96.738)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.2904 (7.7963)\tPrec@1 68.000 (61.887)\tPrec@5 97.000 (96.732)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.3908 (7.7413)\tPrec@1 63.000 (62.049)\tPrec@5 94.000 (96.840)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.5743 (7.7844)\tPrec@1 63.000 (61.890)\tPrec@5 99.000 (96.912)\n",
      "val Results: Prec@1 61.880 Prec@5 96.860 Loss 7.79748\n",
      "val Class Accuracy: [0.921,0.981,0.836,0.578,0.714,0.665,0.402,0.536,0.542,0.013]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [82][0/97], lr: 0.01000\tTime 0.513 (0.513)\tData 0.299 (0.299)\tLoss 1.9409 (1.9409)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [82][10/97], lr: 0.01000\tTime 0.324 (0.352)\tData 0.000 (0.042)\tLoss 1.8880 (2.3296)\tPrec@1 90.625 (85.653)\tPrec@5 99.219 (98.864)\n",
      "Epoch: [82][20/97], lr: 0.01000\tTime 0.325 (0.341)\tData 0.000 (0.030)\tLoss 2.3991 (2.1660)\tPrec@1 87.500 (86.979)\tPrec@5 98.438 (98.847)\n",
      "Epoch: [82][30/97], lr: 0.01000\tTime 0.326 (0.337)\tData 0.000 (0.026)\tLoss 2.6169 (2.1533)\tPrec@1 84.375 (86.870)\tPrec@5 99.219 (98.942)\n",
      "Epoch: [82][40/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.024)\tLoss 2.4227 (2.1804)\tPrec@1 86.719 (86.738)\tPrec@5 99.219 (98.933)\n",
      "Epoch: [82][50/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.023)\tLoss 2.3085 (2.1782)\tPrec@1 87.500 (86.719)\tPrec@5 99.219 (98.989)\n",
      "Epoch: [82][60/97], lr: 0.01000\tTime 0.333 (0.332)\tData 0.000 (0.022)\tLoss 2.3874 (2.2228)\tPrec@1 85.156 (86.603)\tPrec@5 98.438 (99.014)\n",
      "Epoch: [82][70/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.021)\tLoss 2.1503 (2.2634)\tPrec@1 84.375 (86.114)\tPrec@5 99.219 (99.054)\n",
      "Epoch: [82][80/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 2.3720 (2.2759)\tPrec@1 82.812 (86.044)\tPrec@5 99.219 (99.055)\n",
      "Epoch: [82][90/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 2.1409 (2.2972)\tPrec@1 87.500 (85.826)\tPrec@5 100.000 (99.090)\n",
      "Epoch: [82][96/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.021)\tLoss 2.8223 (2.2850)\tPrec@1 83.898 (85.910)\tPrec@5 100.000 (99.097)\n",
      "Test: [0/100]\tTime 0.256 (0.256)\tLoss 8.7403 (8.7403)\tPrec@1 59.000 (59.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 7.3531 (8.6916)\tPrec@1 62.000 (59.545)\tPrec@5 95.000 (95.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.9072 (8.4520)\tPrec@1 62.000 (60.524)\tPrec@5 97.000 (95.571)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.4880 (8.4151)\tPrec@1 64.000 (60.581)\tPrec@5 97.000 (95.484)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.4743 (8.4835)\tPrec@1 59.000 (60.439)\tPrec@5 96.000 (95.390)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.2387 (8.4121)\tPrec@1 59.000 (60.588)\tPrec@5 96.000 (95.608)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.0989 (8.3910)\tPrec@1 64.000 (60.623)\tPrec@5 97.000 (95.787)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.5369 (8.3533)\tPrec@1 60.000 (60.831)\tPrec@5 96.000 (95.775)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.0352 (8.2768)\tPrec@1 62.000 (61.235)\tPrec@5 91.000 (95.778)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.3997 (8.3095)\tPrec@1 60.000 (60.934)\tPrec@5 97.000 (95.659)\n",
      "val Results: Prec@1 61.020 Prec@5 95.630 Loss 8.32263\n",
      "val Class Accuracy: [0.875,0.991,0.763,0.738,0.281,0.691,0.706,0.616,0.437,0.004]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [83][0/97], lr: 0.01000\tTime 0.472 (0.472)\tData 0.273 (0.273)\tLoss 2.3251 (2.3251)\tPrec@1 86.719 (86.719)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [83][10/97], lr: 0.01000\tTime 0.326 (0.347)\tData 0.000 (0.040)\tLoss 2.1443 (2.1880)\tPrec@1 87.500 (86.435)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [83][20/97], lr: 0.01000\tTime 0.331 (0.338)\tData 0.000 (0.029)\tLoss 2.4006 (2.1822)\tPrec@1 84.375 (86.793)\tPrec@5 99.219 (99.405)\n",
      "Epoch: [83][30/97], lr: 0.01000\tTime 0.327 (0.336)\tData 0.000 (0.025)\tLoss 1.6424 (2.1808)\tPrec@1 89.062 (86.643)\tPrec@5 98.438 (99.345)\n",
      "Epoch: [83][40/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.023)\tLoss 2.0096 (2.1691)\tPrec@1 85.938 (86.471)\tPrec@5 98.438 (99.314)\n",
      "Epoch: [83][50/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.022)\tLoss 2.6833 (2.2384)\tPrec@1 82.812 (86.152)\tPrec@5 99.219 (99.234)\n",
      "Epoch: [83][60/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 2.2248 (2.2120)\tPrec@1 85.156 (86.322)\tPrec@5 96.094 (99.065)\n",
      "Epoch: [83][70/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 2.5137 (2.2664)\tPrec@1 82.812 (85.915)\tPrec@5 100.000 (99.131)\n",
      "Epoch: [83][80/97], lr: 0.01000\tTime 0.328 (0.332)\tData 0.000 (0.020)\tLoss 2.2521 (2.2577)\tPrec@1 86.719 (86.053)\tPrec@5 99.219 (99.171)\n",
      "Epoch: [83][90/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.020)\tLoss 1.6141 (2.2559)\tPrec@1 88.281 (86.118)\tPrec@5 100.000 (99.159)\n",
      "Epoch: [83][96/97], lr: 0.01000\tTime 0.320 (0.331)\tData 0.000 (0.021)\tLoss 2.6445 (2.2701)\tPrec@1 84.746 (86.071)\tPrec@5 98.305 (99.105)\n",
      "Test: [0/100]\tTime 0.245 (0.245)\tLoss 8.2434 (8.2434)\tPrec@1 57.000 (57.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 6.5761 (7.6389)\tPrec@1 65.000 (60.909)\tPrec@5 96.000 (96.182)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.7079 (7.7230)\tPrec@1 64.000 (60.095)\tPrec@5 95.000 (95.857)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.0719 (7.7214)\tPrec@1 60.000 (60.000)\tPrec@5 97.000 (95.677)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.1596 (7.7613)\tPrec@1 66.000 (59.951)\tPrec@5 93.000 (95.537)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.4644 (7.6928)\tPrec@1 62.000 (60.490)\tPrec@5 94.000 (95.667)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.8367 (7.7016)\tPrec@1 62.000 (60.295)\tPrec@5 96.000 (95.721)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.6646 (7.6842)\tPrec@1 55.000 (60.352)\tPrec@5 97.000 (95.831)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.3785 (7.6533)\tPrec@1 64.000 (60.519)\tPrec@5 96.000 (95.951)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.3124 (7.7020)\tPrec@1 64.000 (60.352)\tPrec@5 98.000 (96.044)\n",
      "val Results: Prec@1 60.480 Prec@5 95.980 Loss 7.72051\n",
      "val Class Accuracy: [0.976,0.957,0.660,0.510,0.702,0.428,0.675,0.458,0.595,0.087]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [84][0/97], lr: 0.01000\tTime 0.467 (0.467)\tData 0.252 (0.252)\tLoss 3.0104 (3.0104)\tPrec@1 80.469 (80.469)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [84][10/97], lr: 0.01000\tTime 0.327 (0.347)\tData 0.000 (0.037)\tLoss 2.0233 (2.1581)\tPrec@1 85.938 (86.861)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [84][20/97], lr: 0.01000\tTime 0.325 (0.338)\tData 0.000 (0.028)\tLoss 2.3654 (2.1641)\tPrec@1 82.031 (86.533)\tPrec@5 99.219 (99.479)\n",
      "Epoch: [84][30/97], lr: 0.01000\tTime 0.322 (0.336)\tData 0.000 (0.024)\tLoss 2.5598 (2.2706)\tPrec@1 86.719 (85.963)\tPrec@5 99.219 (99.471)\n",
      "Epoch: [84][40/97], lr: 0.01000\tTime 0.328 (0.334)\tData 0.000 (0.023)\tLoss 1.6989 (2.2988)\tPrec@1 89.062 (85.823)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [84][50/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.022)\tLoss 2.9748 (2.2831)\tPrec@1 82.031 (85.922)\tPrec@5 96.875 (99.295)\n",
      "Epoch: [84][60/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 2.6071 (2.2737)\tPrec@1 84.375 (86.040)\tPrec@5 98.438 (99.232)\n",
      "Epoch: [84][70/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 2.1244 (2.2598)\tPrec@1 88.281 (86.158)\tPrec@5 100.000 (99.263)\n",
      "Epoch: [84][80/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 2.8068 (2.2588)\tPrec@1 79.688 (86.101)\tPrec@5 100.000 (99.277)\n",
      "Epoch: [84][90/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.020)\tLoss 2.4210 (2.2863)\tPrec@1 84.375 (85.869)\tPrec@5 99.219 (99.270)\n",
      "Epoch: [84][96/97], lr: 0.01000\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 2.7806 (2.2868)\tPrec@1 82.203 (85.870)\tPrec@5 99.153 (99.226)\n",
      "Test: [0/100]\tTime 0.219 (0.219)\tLoss 8.6590 (8.6590)\tPrec@1 60.000 (60.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.086)\tLoss 7.8305 (8.4684)\tPrec@1 62.000 (59.182)\tPrec@5 95.000 (96.000)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 7.2632 (8.3070)\tPrec@1 60.000 (59.143)\tPrec@5 97.000 (95.714)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.4901 (8.3482)\tPrec@1 64.000 (59.000)\tPrec@5 96.000 (95.935)\n",
      "Test: [40/100]\tTime 0.073 (0.076)\tLoss 8.4913 (8.4145)\tPrec@1 62.000 (58.927)\tPrec@5 92.000 (95.634)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.9413 (8.3449)\tPrec@1 62.000 (59.235)\tPrec@5 90.000 (95.608)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 7.1179 (8.3283)\tPrec@1 63.000 (59.180)\tPrec@5 97.000 (95.574)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.1581 (8.3019)\tPrec@1 64.000 (59.507)\tPrec@5 97.000 (95.634)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.0287 (8.2654)\tPrec@1 61.000 (59.642)\tPrec@5 97.000 (95.741)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.9433 (8.3654)\tPrec@1 56.000 (59.099)\tPrec@5 99.000 (95.813)\n",
      "val Results: Prec@1 59.050 Prec@5 95.800 Loss 8.39338\n",
      "val Class Accuracy: [0.912,0.996,0.665,0.690,0.838,0.470,0.431,0.441,0.380,0.082]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [85][0/97], lr: 0.01000\tTime 0.529 (0.529)\tData 0.303 (0.303)\tLoss 2.2420 (2.2420)\tPrec@1 83.594 (83.594)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [85][10/97], lr: 0.01000\tTime 0.328 (0.353)\tData 0.000 (0.042)\tLoss 2.3148 (2.1858)\tPrec@1 85.156 (85.866)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [85][20/97], lr: 0.01000\tTime 0.325 (0.343)\tData 0.000 (0.030)\tLoss 2.7834 (2.2823)\tPrec@1 82.812 (85.789)\tPrec@5 99.219 (99.107)\n",
      "Epoch: [85][30/97], lr: 0.01000\tTime 0.322 (0.339)\tData 0.000 (0.026)\tLoss 2.0286 (2.1936)\tPrec@1 86.719 (86.215)\tPrec@5 100.000 (99.194)\n",
      "Epoch: [85][40/97], lr: 0.01000\tTime 0.325 (0.337)\tData 0.000 (0.024)\tLoss 1.8179 (2.1863)\tPrec@1 90.625 (86.414)\tPrec@5 99.219 (99.238)\n",
      "Epoch: [85][50/97], lr: 0.01000\tTime 0.327 (0.336)\tData 0.000 (0.023)\tLoss 2.5406 (2.2092)\tPrec@1 85.938 (86.275)\tPrec@5 99.219 (99.265)\n",
      "Epoch: [85][60/97], lr: 0.01000\tTime 0.358 (0.335)\tData 0.000 (0.022)\tLoss 2.6055 (2.2522)\tPrec@1 85.156 (86.091)\tPrec@5 99.219 (99.193)\n",
      "Epoch: [85][70/97], lr: 0.01000\tTime 0.328 (0.334)\tData 0.000 (0.021)\tLoss 2.4460 (2.2384)\tPrec@1 85.938 (86.158)\tPrec@5 100.000 (99.230)\n",
      "Epoch: [85][80/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.021)\tLoss 2.6709 (2.2341)\tPrec@1 82.031 (86.227)\tPrec@5 99.219 (99.257)\n",
      "Epoch: [85][90/97], lr: 0.01000\tTime 0.323 (0.333)\tData 0.000 (0.020)\tLoss 1.8708 (2.2649)\tPrec@1 89.062 (86.041)\tPrec@5 99.219 (99.253)\n",
      "Epoch: [85][96/97], lr: 0.01000\tTime 0.321 (0.333)\tData 0.000 (0.021)\tLoss 1.6730 (2.2542)\tPrec@1 90.678 (86.152)\tPrec@5 99.153 (99.242)\n",
      "Test: [0/100]\tTime 0.247 (0.247)\tLoss 10.4062 (10.4062)\tPrec@1 53.000 (53.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 8.2651 (9.5863)\tPrec@1 58.000 (53.727)\tPrec@5 88.000 (93.273)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.8008 (9.4047)\tPrec@1 61.000 (54.905)\tPrec@5 96.000 (93.190)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.8101 (9.2593)\tPrec@1 61.000 (55.742)\tPrec@5 97.000 (93.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.8751 (9.2667)\tPrec@1 59.000 (55.805)\tPrec@5 97.000 (93.537)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.8547 (9.2274)\tPrec@1 59.000 (56.118)\tPrec@5 97.000 (93.745)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.9059 (9.2138)\tPrec@1 67.000 (56.033)\tPrec@5 93.000 (93.820)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.7345 (9.2004)\tPrec@1 59.000 (56.113)\tPrec@5 98.000 (93.901)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.6644 (9.1701)\tPrec@1 61.000 (56.025)\tPrec@5 92.000 (93.963)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.5844 (9.2775)\tPrec@1 63.000 (55.593)\tPrec@5 96.000 (93.725)\n",
      "val Results: Prec@1 55.440 Prec@5 93.710 Loss 9.31279\n",
      "val Class Accuracy: [0.955,0.974,0.858,0.546,0.564,0.549,0.316,0.495,0.287,0.000]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [86][0/97], lr: 0.01000\tTime 0.420 (0.420)\tData 0.199 (0.199)\tLoss 2.4580 (2.4580)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [86][10/97], lr: 0.01000\tTime 0.327 (0.344)\tData 0.000 (0.033)\tLoss 1.7367 (2.1719)\tPrec@1 90.625 (86.719)\tPrec@5 99.219 (99.361)\n",
      "Epoch: [86][20/97], lr: 0.01000\tTime 0.366 (0.339)\tData 0.000 (0.025)\tLoss 2.8528 (2.2550)\tPrec@1 84.375 (86.124)\tPrec@5 99.219 (99.330)\n",
      "Epoch: [86][30/97], lr: 0.01000\tTime 0.325 (0.336)\tData 0.000 (0.023)\tLoss 2.8333 (2.3145)\tPrec@1 81.250 (85.484)\tPrec@5 97.656 (99.320)\n",
      "Epoch: [86][40/97], lr: 0.01000\tTime 0.324 (0.335)\tData 0.000 (0.021)\tLoss 2.2235 (2.2481)\tPrec@1 83.594 (85.804)\tPrec@5 99.219 (99.238)\n",
      "Epoch: [86][50/97], lr: 0.01000\tTime 0.323 (0.334)\tData 0.000 (0.021)\tLoss 1.2801 (2.2543)\tPrec@1 92.969 (85.800)\tPrec@5 99.219 (99.249)\n",
      "Epoch: [86][60/97], lr: 0.01000\tTime 0.329 (0.333)\tData 0.000 (0.020)\tLoss 3.0076 (2.2051)\tPrec@1 80.469 (86.091)\tPrec@5 96.875 (99.232)\n",
      "Epoch: [86][70/97], lr: 0.01000\tTime 0.328 (0.333)\tData 0.000 (0.020)\tLoss 1.4257 (2.1817)\tPrec@1 93.750 (86.334)\tPrec@5 100.000 (99.230)\n",
      "Epoch: [86][80/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.019)\tLoss 2.7536 (2.2035)\tPrec@1 84.375 (86.236)\tPrec@5 98.438 (99.180)\n",
      "Epoch: [86][90/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.019)\tLoss 2.5822 (2.2011)\tPrec@1 84.375 (86.221)\tPrec@5 100.000 (99.176)\n",
      "Epoch: [86][96/97], lr: 0.01000\tTime 0.318 (0.332)\tData 0.000 (0.020)\tLoss 2.2668 (2.2174)\tPrec@1 85.593 (86.128)\tPrec@5 99.153 (99.162)\n",
      "Test: [0/100]\tTime 0.282 (0.282)\tLoss 10.8020 (10.8020)\tPrec@1 50.000 (50.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 7.3798 (10.5583)\tPrec@1 62.000 (49.727)\tPrec@5 97.000 (92.455)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 10.1409 (10.5140)\tPrec@1 52.000 (50.571)\tPrec@5 94.000 (92.762)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 10.3909 (10.5034)\tPrec@1 48.000 (50.323)\tPrec@5 95.000 (92.387)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 11.3756 (10.5333)\tPrec@1 51.000 (50.366)\tPrec@5 91.000 (92.268)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 10.3489 (10.4664)\tPrec@1 51.000 (50.392)\tPrec@5 95.000 (92.569)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.8836 (10.4478)\tPrec@1 56.000 (50.230)\tPrec@5 90.000 (92.508)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 9.9506 (10.4048)\tPrec@1 55.000 (50.577)\tPrec@5 94.000 (92.408)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.8469 (10.3715)\tPrec@1 55.000 (50.765)\tPrec@5 87.000 (92.494)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.0289 (10.4282)\tPrec@1 51.000 (50.484)\tPrec@5 96.000 (92.505)\n",
      "val Results: Prec@1 50.430 Prec@5 92.380 Loss 10.45058\n",
      "val Class Accuracy: [0.931,0.936,0.824,0.403,0.388,0.315,0.864,0.282,0.100,0.000]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [87][0/97], lr: 0.01000\tTime 0.503 (0.503)\tData 0.267 (0.267)\tLoss 2.1157 (2.1157)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [87][10/97], lr: 0.01000\tTime 0.327 (0.349)\tData 0.000 (0.039)\tLoss 2.2271 (2.2868)\tPrec@1 88.281 (86.364)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [87][20/97], lr: 0.01000\tTime 0.324 (0.339)\tData 0.000 (0.029)\tLoss 2.1038 (2.3325)\tPrec@1 88.281 (85.975)\tPrec@5 100.000 (99.293)\n",
      "Epoch: [87][30/97], lr: 0.01000\tTime 0.326 (0.336)\tData 0.000 (0.025)\tLoss 2.1278 (2.3201)\tPrec@1 86.719 (85.912)\tPrec@5 100.000 (99.244)\n",
      "Epoch: [87][40/97], lr: 0.01000\tTime 0.332 (0.334)\tData 0.000 (0.023)\tLoss 2.3157 (2.3025)\tPrec@1 85.938 (85.899)\tPrec@5 99.219 (99.238)\n",
      "Epoch: [87][50/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.022)\tLoss 1.9457 (2.3360)\tPrec@1 89.062 (85.692)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [87][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 1.9137 (2.3206)\tPrec@1 87.500 (85.937)\tPrec@5 96.875 (99.129)\n",
      "Epoch: [87][70/97], lr: 0.01000\tTime 0.330 (0.332)\tData 0.000 (0.021)\tLoss 2.1868 (2.2762)\tPrec@1 85.938 (86.070)\tPrec@5 99.219 (99.120)\n",
      "Epoch: [87][80/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 3.3866 (2.2984)\tPrec@1 80.469 (85.880)\tPrec@5 96.094 (99.045)\n",
      "Epoch: [87][90/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.020)\tLoss 2.0584 (2.3098)\tPrec@1 86.719 (85.792)\tPrec@5 99.219 (99.056)\n",
      "Epoch: [87][96/97], lr: 0.01000\tTime 0.322 (0.331)\tData 0.000 (0.020)\tLoss 2.3666 (2.2977)\tPrec@1 87.288 (85.942)\tPrec@5 99.153 (99.089)\n",
      "Test: [0/100]\tTime 0.251 (0.251)\tLoss 8.8902 (8.8902)\tPrec@1 56.000 (56.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.2995 (8.2059)\tPrec@1 62.000 (58.818)\tPrec@5 95.000 (96.545)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.7776 (8.1454)\tPrec@1 63.000 (58.810)\tPrec@5 99.000 (96.714)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.1929 (8.1399)\tPrec@1 60.000 (58.968)\tPrec@5 98.000 (96.871)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.2943 (8.1502)\tPrec@1 62.000 (59.171)\tPrec@5 97.000 (96.829)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.6078 (8.0956)\tPrec@1 67.000 (59.451)\tPrec@5 96.000 (96.922)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.9527 (8.0783)\tPrec@1 59.000 (59.262)\tPrec@5 97.000 (96.934)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.4084 (8.0706)\tPrec@1 60.000 (59.324)\tPrec@5 99.000 (96.958)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.6606 (8.0348)\tPrec@1 63.000 (59.370)\tPrec@5 93.000 (96.975)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.1201 (8.1032)\tPrec@1 62.000 (59.099)\tPrec@5 97.000 (96.923)\n",
      "val Results: Prec@1 58.960 Prec@5 96.930 Loss 8.13093\n",
      "val Class Accuracy: [0.979,0.960,0.727,0.586,0.642,0.491,0.566,0.559,0.339,0.047]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [88][0/97], lr: 0.01000\tTime 0.516 (0.516)\tData 0.290 (0.290)\tLoss 2.2078 (2.2078)\tPrec@1 87.500 (87.500)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [88][10/97], lr: 0.01000\tTime 0.340 (0.351)\tData 0.000 (0.041)\tLoss 2.1816 (1.9805)\tPrec@1 88.281 (87.997)\tPrec@5 98.438 (99.503)\n",
      "Epoch: [88][20/97], lr: 0.01000\tTime 0.325 (0.341)\tData 0.000 (0.030)\tLoss 1.8396 (2.0904)\tPrec@1 88.281 (87.128)\tPrec@5 99.219 (99.330)\n",
      "Epoch: [88][30/97], lr: 0.01000\tTime 0.325 (0.339)\tData 0.000 (0.026)\tLoss 1.7672 (2.0148)\tPrec@1 90.625 (88.004)\tPrec@5 100.000 (99.370)\n",
      "Epoch: [88][40/97], lr: 0.01000\tTime 0.329 (0.336)\tData 0.000 (0.024)\tLoss 2.2983 (2.0949)\tPrec@1 85.156 (87.538)\tPrec@5 96.875 (99.257)\n",
      "Epoch: [88][50/97], lr: 0.01000\tTime 0.324 (0.335)\tData 0.000 (0.022)\tLoss 2.7088 (2.1766)\tPrec@1 84.375 (86.857)\tPrec@5 98.438 (99.203)\n",
      "Epoch: [88][60/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.022)\tLoss 2.3669 (2.1570)\tPrec@1 83.594 (86.808)\tPrec@5 100.000 (99.244)\n",
      "Epoch: [88][70/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.021)\tLoss 3.0802 (2.1914)\tPrec@1 82.031 (86.620)\tPrec@5 97.656 (99.164)\n",
      "Epoch: [88][80/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 1.5103 (2.1891)\tPrec@1 89.844 (86.699)\tPrec@5 99.219 (99.142)\n",
      "Epoch: [88][90/97], lr: 0.01000\tTime 0.322 (0.332)\tData 0.000 (0.020)\tLoss 2.3188 (2.2139)\tPrec@1 82.812 (86.427)\tPrec@5 100.000 (99.107)\n",
      "Epoch: [88][96/97], lr: 0.01000\tTime 0.320 (0.332)\tData 0.000 (0.021)\tLoss 2.8716 (2.2075)\tPrec@1 81.356 (86.474)\tPrec@5 98.305 (99.113)\n",
      "Test: [0/100]\tTime 0.249 (0.249)\tLoss 10.2390 (10.2390)\tPrec@1 55.000 (55.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.7761 (10.1403)\tPrec@1 61.000 (52.818)\tPrec@5 97.000 (96.545)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 8.5638 (10.0194)\tPrec@1 58.000 (53.429)\tPrec@5 95.000 (95.952)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 9.7851 (9.9691)\tPrec@1 50.000 (53.645)\tPrec@5 98.000 (96.032)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.4776 (9.9752)\tPrec@1 49.000 (53.854)\tPrec@5 95.000 (96.049)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 10.1495 (9.8806)\tPrec@1 52.000 (54.176)\tPrec@5 97.000 (96.118)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.7144 (9.8732)\tPrec@1 62.000 (54.098)\tPrec@5 98.000 (96.213)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.3810 (9.8434)\tPrec@1 59.000 (54.268)\tPrec@5 97.000 (96.338)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.0357 (9.7914)\tPrec@1 57.000 (54.617)\tPrec@5 97.000 (96.407)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.6132 (9.8571)\tPrec@1 58.000 (54.319)\tPrec@5 100.000 (96.396)\n",
      "val Results: Prec@1 54.520 Prec@5 96.410 Loss 9.85906\n",
      "val Class Accuracy: [0.862,0.997,0.871,0.724,0.226,0.390,0.590,0.482,0.310,0.000]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [89][0/97], lr: 0.01000\tTime 0.500 (0.500)\tData 0.281 (0.281)\tLoss 1.8765 (1.8765)\tPrec@1 92.969 (92.969)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [89][10/97], lr: 0.01000\tTime 0.325 (0.350)\tData 0.000 (0.041)\tLoss 2.3681 (2.3134)\tPrec@1 85.156 (86.151)\tPrec@5 98.438 (98.935)\n",
      "Epoch: [89][20/97], lr: 0.01000\tTime 0.331 (0.340)\tData 0.000 (0.030)\tLoss 2.4857 (2.3881)\tPrec@1 85.156 (85.640)\tPrec@5 100.000 (99.107)\n",
      "Epoch: [89][30/97], lr: 0.01000\tTime 0.324 (0.337)\tData 0.000 (0.026)\tLoss 2.0509 (2.4012)\tPrec@1 89.062 (85.459)\tPrec@5 100.000 (98.942)\n",
      "Epoch: [89][40/97], lr: 0.01000\tTime 0.326 (0.335)\tData 0.000 (0.024)\tLoss 2.0732 (2.3516)\tPrec@1 87.500 (85.728)\tPrec@5 99.219 (98.990)\n",
      "Epoch: [89][50/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.022)\tLoss 2.4191 (2.2818)\tPrec@1 86.719 (86.183)\tPrec@5 100.000 (99.081)\n",
      "Epoch: [89][60/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.022)\tLoss 2.1072 (2.2569)\tPrec@1 85.938 (86.245)\tPrec@5 99.219 (99.155)\n",
      "Epoch: [89][70/97], lr: 0.01000\tTime 0.328 (0.333)\tData 0.000 (0.021)\tLoss 2.0362 (2.3034)\tPrec@1 85.938 (85.750)\tPrec@5 100.000 (99.076)\n",
      "Epoch: [89][80/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 2.3414 (2.2828)\tPrec@1 85.938 (85.986)\tPrec@5 96.875 (99.055)\n",
      "Epoch: [89][90/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 1.6976 (2.2704)\tPrec@1 89.062 (86.058)\tPrec@5 100.000 (99.090)\n",
      "Epoch: [89][96/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.021)\tLoss 2.8215 (2.2650)\tPrec@1 80.508 (86.112)\tPrec@5 100.000 (99.121)\n",
      "Test: [0/100]\tTime 0.257 (0.257)\tLoss 11.2056 (11.2056)\tPrec@1 42.000 (42.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 8.3897 (9.9128)\tPrec@1 57.000 (50.091)\tPrec@5 89.000 (94.000)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.6738 (9.8373)\tPrec@1 59.000 (51.048)\tPrec@5 98.000 (94.381)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.4327 (9.8040)\tPrec@1 57.000 (52.000)\tPrec@5 97.000 (94.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.8910 (9.7505)\tPrec@1 54.000 (52.366)\tPrec@5 93.000 (94.415)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.4891 (9.6328)\tPrec@1 59.000 (53.059)\tPrec@5 96.000 (94.706)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.9597 (9.6126)\tPrec@1 60.000 (52.934)\tPrec@5 94.000 (94.689)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.2993 (9.6046)\tPrec@1 54.000 (53.014)\tPrec@5 99.000 (94.718)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.9146 (9.5642)\tPrec@1 61.000 (53.321)\tPrec@5 92.000 (94.790)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.5177 (9.6634)\tPrec@1 56.000 (52.780)\tPrec@5 97.000 (94.670)\n",
      "val Results: Prec@1 52.750 Prec@5 94.660 Loss 9.68881\n",
      "val Class Accuracy: [0.991,0.953,0.711,0.632,0.778,0.427,0.266,0.373,0.131,0.013]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [90][0/97], lr: 0.01000\tTime 0.492 (0.492)\tData 0.257 (0.257)\tLoss 1.9420 (1.9420)\tPrec@1 89.062 (89.062)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [90][10/97], lr: 0.01000\tTime 0.326 (0.348)\tData 0.000 (0.038)\tLoss 1.8066 (1.9279)\tPrec@1 91.406 (88.352)\tPrec@5 99.219 (98.864)\n",
      "Epoch: [90][20/97], lr: 0.01000\tTime 0.323 (0.337)\tData 0.000 (0.028)\tLoss 2.7732 (2.2644)\tPrec@1 83.594 (85.603)\tPrec@5 100.000 (98.884)\n",
      "Epoch: [90][30/97], lr: 0.01000\tTime 0.321 (0.334)\tData 0.000 (0.025)\tLoss 2.4096 (2.2299)\tPrec@1 84.375 (85.963)\tPrec@5 100.000 (99.017)\n",
      "Epoch: [90][40/97], lr: 0.01000\tTime 0.328 (0.333)\tData 0.000 (0.023)\tLoss 2.0460 (2.1932)\tPrec@1 89.062 (86.223)\tPrec@5 99.219 (99.123)\n",
      "Epoch: [90][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.022)\tLoss 1.3625 (2.2030)\tPrec@1 91.406 (86.366)\tPrec@5 100.000 (99.020)\n",
      "Epoch: [90][60/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.021)\tLoss 1.7339 (2.1821)\tPrec@1 89.062 (86.565)\tPrec@5 99.219 (99.103)\n",
      "Epoch: [90][70/97], lr: 0.01000\tTime 0.342 (0.331)\tData 0.000 (0.020)\tLoss 2.6769 (2.1982)\tPrec@1 82.812 (86.466)\tPrec@5 99.219 (99.065)\n",
      "Epoch: [90][80/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 2.9567 (2.2034)\tPrec@1 82.031 (86.478)\tPrec@5 97.656 (99.074)\n",
      "Epoch: [90][90/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 2.8980 (2.2540)\tPrec@1 80.469 (86.204)\tPrec@5 98.438 (99.081)\n",
      "Epoch: [90][96/97], lr: 0.01000\tTime 0.317 (0.330)\tData 0.000 (0.020)\tLoss 2.4954 (2.2485)\tPrec@1 83.898 (86.232)\tPrec@5 98.305 (99.041)\n",
      "Test: [0/100]\tTime 0.251 (0.251)\tLoss 9.7412 (9.7412)\tPrec@1 52.000 (52.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.0692 (8.7949)\tPrec@1 66.000 (57.455)\tPrec@5 94.000 (93.636)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.6865 (8.5290)\tPrec@1 66.000 (58.810)\tPrec@5 97.000 (93.857)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.2319 (8.6232)\tPrec@1 60.000 (58.032)\tPrec@5 96.000 (93.581)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.2477 (8.6231)\tPrec@1 59.000 (58.146)\tPrec@5 91.000 (93.390)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.7136 (8.5017)\tPrec@1 64.000 (58.804)\tPrec@5 93.000 (93.314)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.5281 (8.4335)\tPrec@1 66.000 (59.213)\tPrec@5 94.000 (93.295)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.9172 (8.4220)\tPrec@1 63.000 (59.366)\tPrec@5 95.000 (93.408)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.8073 (8.3831)\tPrec@1 59.000 (59.494)\tPrec@5 91.000 (93.543)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.7999 (8.4492)\tPrec@1 54.000 (59.154)\tPrec@5 97.000 (93.615)\n",
      "val Results: Prec@1 59.100 Prec@5 93.650 Loss 8.45778\n",
      "val Class Accuracy: [0.884,0.927,0.623,0.784,0.705,0.763,0.473,0.415,0.291,0.045]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [91][0/97], lr: 0.01000\tTime 0.438 (0.438)\tData 0.235 (0.235)\tLoss 2.4059 (2.4059)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [91][10/97], lr: 0.01000\tTime 0.331 (0.345)\tData 0.000 (0.036)\tLoss 2.6003 (2.2356)\tPrec@1 86.719 (86.364)\tPrec@5 100.000 (99.077)\n",
      "Epoch: [91][20/97], lr: 0.01000\tTime 0.330 (0.338)\tData 0.000 (0.027)\tLoss 2.2079 (2.2741)\tPrec@1 85.156 (86.161)\tPrec@5 99.219 (99.033)\n",
      "Epoch: [91][30/97], lr: 0.01000\tTime 0.330 (0.334)\tData 0.000 (0.024)\tLoss 2.4335 (2.2034)\tPrec@1 82.812 (86.719)\tPrec@5 99.219 (98.992)\n",
      "Epoch: [91][40/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.022)\tLoss 1.9360 (2.1854)\tPrec@1 88.281 (86.585)\tPrec@5 99.219 (99.085)\n",
      "Epoch: [91][50/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.021)\tLoss 1.3155 (2.1360)\tPrec@1 88.281 (86.841)\tPrec@5 100.000 (99.127)\n",
      "Epoch: [91][60/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.021)\tLoss 2.6690 (2.1903)\tPrec@1 84.375 (86.501)\tPrec@5 99.219 (99.103)\n",
      "Epoch: [91][70/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 2.6644 (2.2020)\tPrec@1 84.375 (86.510)\tPrec@5 100.000 (99.142)\n",
      "Epoch: [91][80/97], lr: 0.01000\tTime 0.329 (0.331)\tData 0.000 (0.020)\tLoss 1.8050 (2.2249)\tPrec@1 89.844 (86.343)\tPrec@5 98.438 (99.074)\n",
      "Epoch: [91][90/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.019)\tLoss 2.3472 (2.2152)\tPrec@1 86.719 (86.410)\tPrec@5 97.656 (99.064)\n",
      "Epoch: [91][96/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.020)\tLoss 2.3104 (2.2077)\tPrec@1 87.288 (86.442)\tPrec@5 97.458 (99.081)\n",
      "Test: [0/100]\tTime 0.255 (0.255)\tLoss 10.1168 (10.1168)\tPrec@1 53.000 (53.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.2028 (8.7063)\tPrec@1 63.000 (57.636)\tPrec@5 96.000 (96.364)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.5461 (8.7197)\tPrec@1 60.000 (56.952)\tPrec@5 97.000 (96.333)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.7721 (8.6752)\tPrec@1 60.000 (57.129)\tPrec@5 94.000 (96.129)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.3377 (8.6633)\tPrec@1 58.000 (57.366)\tPrec@5 98.000 (96.341)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.5413 (8.5895)\tPrec@1 59.000 (57.843)\tPrec@5 98.000 (96.510)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7868 (8.5270)\tPrec@1 68.000 (58.262)\tPrec@5 95.000 (96.557)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.6230 (8.5167)\tPrec@1 63.000 (58.380)\tPrec@5 97.000 (96.634)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.8437 (8.4724)\tPrec@1 64.000 (58.556)\tPrec@5 95.000 (96.568)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.0122 (8.5240)\tPrec@1 63.000 (58.484)\tPrec@5 96.000 (96.516)\n",
      "val Results: Prec@1 58.490 Prec@5 96.420 Loss 8.53553\n",
      "val Class Accuracy: [0.969,0.987,0.659,0.803,0.610,0.464,0.487,0.652,0.167,0.051]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [92][0/97], lr: 0.01000\tTime 0.469 (0.469)\tData 0.233 (0.233)\tLoss 1.5328 (1.5328)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [92][10/97], lr: 0.01000\tTime 0.325 (0.347)\tData 0.000 (0.036)\tLoss 1.9102 (2.1045)\tPrec@1 89.844 (86.435)\tPrec@5 99.219 (99.503)\n",
      "Epoch: [92][20/97], lr: 0.01000\tTime 0.335 (0.338)\tData 0.000 (0.027)\tLoss 2.1151 (2.1009)\tPrec@1 84.375 (86.719)\tPrec@5 99.219 (99.516)\n",
      "Epoch: [92][30/97], lr: 0.01000\tTime 0.324 (0.335)\tData 0.000 (0.024)\tLoss 2.4036 (2.1222)\tPrec@1 85.938 (86.719)\tPrec@5 97.656 (99.395)\n",
      "Epoch: [92][40/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.022)\tLoss 2.0612 (2.1285)\tPrec@1 88.281 (86.604)\tPrec@5 99.219 (99.352)\n",
      "Epoch: [92][50/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.021)\tLoss 2.4659 (2.1914)\tPrec@1 85.938 (86.305)\tPrec@5 100.000 (99.326)\n",
      "Epoch: [92][60/97], lr: 0.01000\tTime 0.328 (0.332)\tData 0.000 (0.021)\tLoss 2.2665 (2.1900)\tPrec@1 85.156 (86.399)\tPrec@5 100.000 (99.334)\n",
      "Epoch: [92][70/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 2.6326 (2.1669)\tPrec@1 82.812 (86.554)\tPrec@5 99.219 (99.318)\n",
      "Epoch: [92][80/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.020)\tLoss 2.1096 (2.1345)\tPrec@1 85.938 (86.757)\tPrec@5 100.000 (99.325)\n",
      "Epoch: [92][90/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 2.6321 (2.1502)\tPrec@1 85.938 (86.787)\tPrec@5 96.875 (99.296)\n",
      "Epoch: [92][96/97], lr: 0.01000\tTime 0.314 (0.330)\tData 0.000 (0.020)\tLoss 3.0097 (2.1501)\tPrec@1 80.508 (86.732)\tPrec@5 98.305 (99.275)\n",
      "Test: [0/100]\tTime 0.218 (0.218)\tLoss 11.0219 (11.0219)\tPrec@1 45.000 (45.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.086)\tLoss 8.1114 (9.8530)\tPrec@1 60.000 (51.727)\tPrec@5 94.000 (95.818)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 8.9439 (9.7749)\tPrec@1 55.000 (51.762)\tPrec@5 97.000 (95.667)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 8.7741 (9.7351)\tPrec@1 58.000 (52.000)\tPrec@5 97.000 (95.548)\n",
      "Test: [40/100]\tTime 0.073 (0.076)\tLoss 10.3947 (9.7237)\tPrec@1 47.000 (52.146)\tPrec@5 96.000 (95.341)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 10.3159 (9.7200)\tPrec@1 49.000 (52.275)\tPrec@5 94.000 (95.235)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 8.4667 (9.7408)\tPrec@1 58.000 (52.082)\tPrec@5 97.000 (95.246)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.6391 (9.7155)\tPrec@1 57.000 (52.155)\tPrec@5 97.000 (95.211)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.6033 (9.6928)\tPrec@1 52.000 (52.333)\tPrec@5 93.000 (95.222)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.8152 (9.7482)\tPrec@1 58.000 (52.011)\tPrec@5 99.000 (95.242)\n",
      "val Results: Prec@1 52.130 Prec@5 95.220 Loss 9.76198\n",
      "val Class Accuracy: [0.891,0.940,0.926,0.507,0.459,0.273,0.529,0.253,0.368,0.067]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [93][0/97], lr: 0.01000\tTime 0.504 (0.504)\tData 0.281 (0.281)\tLoss 2.6241 (2.6241)\tPrec@1 82.812 (82.812)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [93][10/97], lr: 0.01000\tTime 0.326 (0.351)\tData 0.000 (0.040)\tLoss 2.2252 (2.0657)\tPrec@1 87.500 (87.145)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [93][20/97], lr: 0.01000\tTime 0.328 (0.340)\tData 0.000 (0.029)\tLoss 3.1221 (2.2554)\tPrec@1 83.594 (86.086)\tPrec@5 99.219 (99.293)\n",
      "Epoch: [93][30/97], lr: 0.01000\tTime 0.324 (0.337)\tData 0.000 (0.025)\tLoss 2.9783 (2.2693)\tPrec@1 83.594 (85.988)\tPrec@5 99.219 (99.370)\n",
      "Epoch: [93][40/97], lr: 0.01000\tTime 0.330 (0.335)\tData 0.000 (0.023)\tLoss 2.4736 (2.2498)\tPrec@1 83.594 (86.109)\tPrec@5 98.438 (99.295)\n",
      "Epoch: [93][50/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.022)\tLoss 2.5667 (2.2704)\tPrec@1 83.594 (86.045)\tPrec@5 100.000 (99.357)\n",
      "Epoch: [93][60/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 2.1357 (2.2327)\tPrec@1 87.500 (86.296)\tPrec@5 98.438 (99.411)\n",
      "Epoch: [93][70/97], lr: 0.01000\tTime 0.332 (0.332)\tData 0.000 (0.021)\tLoss 1.7634 (2.2120)\tPrec@1 90.625 (86.510)\tPrec@5 98.438 (99.373)\n",
      "Epoch: [93][80/97], lr: 0.01000\tTime 0.330 (0.332)\tData 0.000 (0.020)\tLoss 2.5000 (2.2064)\tPrec@1 84.375 (86.497)\tPrec@5 99.219 (99.383)\n",
      "Epoch: [93][90/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 2.6649 (2.1979)\tPrec@1 83.594 (86.521)\tPrec@5 99.219 (99.390)\n",
      "Epoch: [93][96/97], lr: 0.01000\tTime 0.316 (0.332)\tData 0.000 (0.021)\tLoss 1.6955 (2.1854)\tPrec@1 87.288 (86.539)\tPrec@5 100.000 (99.395)\n",
      "Test: [0/100]\tTime 0.277 (0.277)\tLoss 8.2108 (8.2108)\tPrec@1 61.000 (61.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 7.1585 (8.0013)\tPrec@1 63.000 (60.091)\tPrec@5 95.000 (96.636)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.4394 (7.8044)\tPrec@1 66.000 (60.857)\tPrec@5 97.000 (96.619)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 7.2423 (7.8149)\tPrec@1 65.000 (60.903)\tPrec@5 98.000 (96.323)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 7.1779 (7.8036)\tPrec@1 67.000 (61.073)\tPrec@5 99.000 (96.268)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.9800 (7.7915)\tPrec@1 68.000 (61.235)\tPrec@5 97.000 (96.333)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.0349 (7.7985)\tPrec@1 65.000 (61.098)\tPrec@5 96.000 (96.197)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 6.7952 (7.7816)\tPrec@1 67.000 (61.352)\tPrec@5 98.000 (96.254)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 7.6417 (7.7723)\tPrec@1 63.000 (61.481)\tPrec@5 92.000 (96.272)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.0784 (7.8495)\tPrec@1 59.000 (61.099)\tPrec@5 99.000 (96.275)\n",
      "val Results: Prec@1 61.080 Prec@5 96.270 Loss 7.85983\n",
      "val Class Accuracy: [0.883,0.941,0.687,0.626,0.937,0.513,0.279,0.567,0.539,0.136]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [94][0/97], lr: 0.01000\tTime 0.470 (0.470)\tData 0.234 (0.234)\tLoss 2.2112 (2.2112)\tPrec@1 86.719 (86.719)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [94][10/97], lr: 0.01000\tTime 0.330 (0.356)\tData 0.000 (0.036)\tLoss 1.5399 (2.2275)\tPrec@1 92.188 (86.080)\tPrec@5 99.219 (99.148)\n",
      "Epoch: [94][20/97], lr: 0.01000\tTime 0.335 (0.344)\tData 0.000 (0.027)\tLoss 2.1823 (2.1839)\tPrec@1 87.500 (86.793)\tPrec@5 99.219 (99.070)\n",
      "Epoch: [94][30/97], lr: 0.01000\tTime 0.328 (0.339)\tData 0.000 (0.024)\tLoss 3.0102 (2.2066)\tPrec@1 78.906 (86.416)\tPrec@5 98.438 (99.118)\n",
      "Epoch: [94][40/97], lr: 0.01000\tTime 0.327 (0.337)\tData 0.000 (0.022)\tLoss 2.6434 (2.1945)\tPrec@1 82.031 (86.185)\tPrec@5 99.219 (99.200)\n",
      "Epoch: [94][50/97], lr: 0.01000\tTime 0.323 (0.335)\tData 0.000 (0.021)\tLoss 1.8743 (2.1993)\tPrec@1 87.500 (86.351)\tPrec@5 100.000 (99.188)\n",
      "Epoch: [94][60/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.021)\tLoss 1.8121 (2.1998)\tPrec@1 85.938 (86.219)\tPrec@5 98.438 (99.180)\n",
      "Epoch: [94][70/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.020)\tLoss 2.2540 (2.2063)\tPrec@1 85.938 (86.103)\tPrec@5 98.438 (99.175)\n",
      "Epoch: [94][80/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 1.4643 (2.2158)\tPrec@1 91.406 (86.044)\tPrec@5 100.000 (99.209)\n",
      "Epoch: [94][90/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.020)\tLoss 2.4057 (2.2441)\tPrec@1 84.375 (85.912)\tPrec@5 98.438 (99.210)\n",
      "Epoch: [94][96/97], lr: 0.01000\tTime 0.317 (0.332)\tData 0.000 (0.020)\tLoss 2.3167 (2.2460)\tPrec@1 84.746 (85.934)\tPrec@5 99.153 (99.162)\n",
      "Test: [0/100]\tTime 0.250 (0.250)\tLoss 8.8332 (8.8332)\tPrec@1 58.000 (58.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 6.1316 (7.7115)\tPrec@1 76.000 (62.909)\tPrec@5 95.000 (96.091)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.4336 (7.7212)\tPrec@1 65.000 (62.238)\tPrec@5 99.000 (95.952)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.2821 (7.7787)\tPrec@1 65.000 (61.871)\tPrec@5 97.000 (96.161)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.0394 (7.7875)\tPrec@1 59.000 (61.683)\tPrec@5 97.000 (96.024)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.0618 (7.7465)\tPrec@1 67.000 (61.941)\tPrec@5 97.000 (96.196)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.1326 (7.7087)\tPrec@1 70.000 (61.984)\tPrec@5 97.000 (96.197)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.2261 (7.6933)\tPrec@1 66.000 (62.169)\tPrec@5 98.000 (96.310)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.9275 (7.6718)\tPrec@1 69.000 (62.185)\tPrec@5 94.000 (96.370)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.9914 (7.7157)\tPrec@1 61.000 (62.022)\tPrec@5 99.000 (96.330)\n",
      "val Results: Prec@1 62.040 Prec@5 96.230 Loss 7.73462\n",
      "val Class Accuracy: [0.951,0.984,0.780,0.737,0.704,0.545,0.621,0.572,0.298,0.012]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [95][0/97], lr: 0.01000\tTime 0.461 (0.461)\tData 0.231 (0.231)\tLoss 1.9243 (1.9243)\tPrec@1 88.281 (88.281)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [95][10/97], lr: 0.01000\tTime 0.332 (0.348)\tData 0.000 (0.036)\tLoss 2.4969 (2.0817)\tPrec@1 83.594 (87.287)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [95][20/97], lr: 0.01000\tTime 0.325 (0.338)\tData 0.000 (0.027)\tLoss 1.8546 (2.0893)\tPrec@1 89.062 (87.240)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [95][30/97], lr: 0.01000\tTime 0.322 (0.335)\tData 0.000 (0.024)\tLoss 2.3142 (2.0934)\tPrec@1 86.719 (87.576)\tPrec@5 100.000 (99.420)\n",
      "Epoch: [95][40/97], lr: 0.01000\tTime 0.332 (0.334)\tData 0.000 (0.022)\tLoss 2.6479 (2.0648)\tPrec@1 82.812 (87.576)\tPrec@5 99.219 (99.390)\n",
      "Epoch: [95][50/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 1.8735 (2.0883)\tPrec@1 87.500 (87.485)\tPrec@5 99.219 (99.387)\n",
      "Epoch: [95][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 2.2269 (2.1176)\tPrec@1 85.938 (87.244)\tPrec@5 97.656 (99.321)\n",
      "Epoch: [95][70/97], lr: 0.01000\tTime 0.333 (0.332)\tData 0.000 (0.020)\tLoss 2.8546 (2.1490)\tPrec@1 82.812 (87.049)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [95][80/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 1.4857 (2.1500)\tPrec@1 90.625 (87.047)\tPrec@5 99.219 (99.190)\n",
      "Epoch: [95][90/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.4507 (2.1704)\tPrec@1 84.375 (86.890)\tPrec@5 98.438 (99.176)\n",
      "Epoch: [95][96/97], lr: 0.01000\tTime 0.317 (0.332)\tData 0.000 (0.020)\tLoss 2.2857 (2.1802)\tPrec@1 84.746 (86.748)\tPrec@5 98.305 (99.146)\n",
      "Test: [0/100]\tTime 0.252 (0.252)\tLoss 8.5396 (8.5396)\tPrec@1 60.000 (60.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 6.4511 (8.0496)\tPrec@1 70.000 (61.364)\tPrec@5 96.000 (93.727)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.1618 (7.9108)\tPrec@1 64.000 (61.810)\tPrec@5 97.000 (94.238)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.2621 (7.9571)\tPrec@1 63.000 (61.065)\tPrec@5 98.000 (94.226)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.8724 (8.0268)\tPrec@1 59.000 (60.415)\tPrec@5 95.000 (94.415)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.3264 (7.9571)\tPrec@1 65.000 (60.824)\tPrec@5 96.000 (94.412)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.1554 (7.9443)\tPrec@1 69.000 (60.672)\tPrec@5 98.000 (94.459)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.0832 (7.9447)\tPrec@1 57.000 (60.676)\tPrec@5 98.000 (94.437)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.9129 (7.9014)\tPrec@1 59.000 (60.840)\tPrec@5 93.000 (94.568)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.4019 (7.9550)\tPrec@1 58.000 (60.604)\tPrec@5 95.000 (94.462)\n",
      "val Results: Prec@1 60.690 Prec@5 94.480 Loss 7.96253\n",
      "val Class Accuracy: [0.964,0.949,0.653,0.782,0.565,0.428,0.736,0.546,0.401,0.045]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [96][0/97], lr: 0.01000\tTime 0.432 (0.432)\tData 0.238 (0.238)\tLoss 1.8272 (1.8272)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [96][10/97], lr: 0.01000\tTime 0.325 (0.344)\tData 0.000 (0.037)\tLoss 2.0780 (2.0076)\tPrec@1 87.500 (87.074)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [96][20/97], lr: 0.01000\tTime 0.326 (0.336)\tData 0.000 (0.027)\tLoss 2.8775 (2.0801)\tPrec@1 83.594 (87.016)\tPrec@5 97.656 (99.144)\n",
      "Epoch: [96][30/97], lr: 0.01000\tTime 0.330 (0.335)\tData 0.000 (0.024)\tLoss 2.3551 (2.1246)\tPrec@1 85.938 (86.845)\tPrec@5 99.219 (99.168)\n",
      "Epoch: [96][40/97], lr: 0.01000\tTime 0.325 (0.335)\tData 0.000 (0.022)\tLoss 2.5450 (2.1603)\tPrec@1 86.719 (86.795)\tPrec@5 100.000 (99.123)\n",
      "Epoch: [96][50/97], lr: 0.01000\tTime 0.323 (0.333)\tData 0.000 (0.021)\tLoss 2.2592 (2.1939)\tPrec@1 85.938 (86.566)\tPrec@5 98.438 (99.081)\n",
      "Epoch: [96][60/97], lr: 0.01000\tTime 0.330 (0.333)\tData 0.000 (0.021)\tLoss 2.3234 (2.2080)\tPrec@1 87.500 (86.514)\tPrec@5 98.438 (99.065)\n",
      "Epoch: [96][70/97], lr: 0.01000\tTime 0.330 (0.333)\tData 0.000 (0.020)\tLoss 2.5641 (2.2085)\tPrec@1 84.375 (86.521)\tPrec@5 98.438 (98.955)\n",
      "Epoch: [96][80/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 2.7792 (2.2044)\tPrec@1 85.156 (86.584)\tPrec@5 100.000 (99.016)\n",
      "Epoch: [96][90/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.2523 (2.2049)\tPrec@1 87.500 (86.590)\tPrec@5 96.875 (98.996)\n",
      "Epoch: [96][96/97], lr: 0.01000\tTime 0.322 (0.331)\tData 0.000 (0.020)\tLoss 2.6041 (2.1961)\tPrec@1 83.898 (86.668)\tPrec@5 97.458 (98.992)\n",
      "Test: [0/100]\tTime 0.258 (0.258)\tLoss 8.9644 (8.9644)\tPrec@1 59.000 (59.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 7.4997 (8.5718)\tPrec@1 64.000 (57.455)\tPrec@5 97.000 (96.727)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.3278 (8.4834)\tPrec@1 63.000 (57.810)\tPrec@5 97.000 (96.381)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.3684 (8.5737)\tPrec@1 55.000 (57.581)\tPrec@5 97.000 (96.290)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.5572 (8.5591)\tPrec@1 60.000 (57.805)\tPrec@5 92.000 (95.878)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 8.0042 (8.4823)\tPrec@1 60.000 (58.235)\tPrec@5 97.000 (96.020)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.8232 (8.4871)\tPrec@1 65.000 (57.918)\tPrec@5 96.000 (95.984)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.2458 (8.4452)\tPrec@1 57.000 (58.197)\tPrec@5 98.000 (96.127)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.9544 (8.4404)\tPrec@1 53.000 (58.062)\tPrec@5 93.000 (96.074)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.6103 (8.4788)\tPrec@1 60.000 (57.923)\tPrec@5 96.000 (96.000)\n",
      "val Results: Prec@1 57.910 Prec@5 95.980 Loss 8.48461\n",
      "val Class Accuracy: [0.933,0.985,0.724,0.828,0.541,0.383,0.463,0.319,0.607,0.008]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [97][0/97], lr: 0.01000\tTime 0.407 (0.407)\tData 0.211 (0.211)\tLoss 2.0114 (2.0114)\tPrec@1 85.938 (85.938)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [97][10/97], lr: 0.01000\tTime 0.328 (0.340)\tData 0.000 (0.034)\tLoss 2.3075 (2.1479)\tPrec@1 82.812 (85.938)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [97][20/97], lr: 0.01000\tTime 0.326 (0.335)\tData 0.000 (0.026)\tLoss 2.2763 (2.0904)\tPrec@1 85.938 (86.942)\tPrec@5 99.219 (99.405)\n",
      "Epoch: [97][30/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.023)\tLoss 1.9965 (2.1736)\tPrec@1 88.281 (86.215)\tPrec@5 100.000 (99.269)\n",
      "Epoch: [97][40/97], lr: 0.01000\tTime 0.328 (0.332)\tData 0.000 (0.022)\tLoss 3.8171 (2.1843)\tPrec@1 77.344 (86.357)\tPrec@5 97.656 (99.257)\n",
      "Epoch: [97][50/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.021)\tLoss 2.4153 (2.1714)\tPrec@1 85.938 (86.489)\tPrec@5 99.219 (99.326)\n",
      "Epoch: [97][60/97], lr: 0.01000\tTime 0.328 (0.332)\tData 0.000 (0.020)\tLoss 3.0196 (2.2225)\tPrec@1 83.594 (86.155)\tPrec@5 99.219 (99.321)\n",
      "Epoch: [97][70/97], lr: 0.01000\tTime 0.332 (0.331)\tData 0.000 (0.020)\tLoss 2.1853 (2.2150)\tPrec@1 87.500 (86.268)\tPrec@5 99.219 (99.296)\n",
      "Epoch: [97][80/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 2.0807 (2.2140)\tPrec@1 88.281 (86.246)\tPrec@5 100.000 (99.257)\n",
      "Epoch: [97][90/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.019)\tLoss 2.6036 (2.2162)\tPrec@1 85.156 (86.264)\tPrec@5 99.219 (99.245)\n",
      "Epoch: [97][96/97], lr: 0.01000\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 2.3511 (2.2120)\tPrec@1 87.288 (86.305)\tPrec@5 96.610 (99.218)\n",
      "Test: [0/100]\tTime 0.260 (0.260)\tLoss 10.1354 (10.1354)\tPrec@1 47.000 (47.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.8342 (9.0040)\tPrec@1 69.000 (56.636)\tPrec@5 97.000 (96.364)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.8567 (8.9719)\tPrec@1 59.000 (56.333)\tPrec@5 98.000 (95.857)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.4749 (9.0489)\tPrec@1 57.000 (55.806)\tPrec@5 97.000 (95.903)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.0788 (8.9992)\tPrec@1 57.000 (56.415)\tPrec@5 90.000 (95.488)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 8.7823 (8.9111)\tPrec@1 55.000 (56.804)\tPrec@5 95.000 (95.412)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.3122 (8.8562)\tPrec@1 65.000 (56.967)\tPrec@5 92.000 (95.344)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.6899 (8.8749)\tPrec@1 60.000 (56.817)\tPrec@5 94.000 (95.296)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.0164 (8.8204)\tPrec@1 63.000 (57.123)\tPrec@5 93.000 (95.383)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.2500 (8.8703)\tPrec@1 58.000 (56.857)\tPrec@5 98.000 (95.374)\n",
      "val Results: Prec@1 56.820 Prec@5 95.390 Loss 8.87943\n",
      "val Class Accuracy: [0.945,0.971,0.866,0.768,0.496,0.477,0.479,0.421,0.188,0.071]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [98][0/97], lr: 0.01000\tTime 0.430 (0.430)\tData 0.210 (0.210)\tLoss 1.9987 (1.9987)\tPrec@1 85.938 (85.938)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [98][10/97], lr: 0.01000\tTime 0.326 (0.344)\tData 0.000 (0.034)\tLoss 1.9082 (1.9828)\tPrec@1 89.062 (87.997)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [98][20/97], lr: 0.01000\tTime 0.329 (0.337)\tData 0.000 (0.026)\tLoss 2.6454 (2.0662)\tPrec@1 85.156 (87.612)\tPrec@5 98.438 (99.070)\n",
      "Epoch: [98][30/97], lr: 0.01000\tTime 0.325 (0.335)\tData 0.000 (0.023)\tLoss 1.4060 (2.0651)\tPrec@1 92.188 (87.424)\tPrec@5 99.219 (99.168)\n",
      "Epoch: [98][40/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.022)\tLoss 2.6924 (2.1480)\tPrec@1 84.375 (87.024)\tPrec@5 99.219 (99.238)\n",
      "Epoch: [98][50/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.021)\tLoss 2.9703 (2.1990)\tPrec@1 82.031 (86.749)\tPrec@5 99.219 (99.188)\n",
      "Epoch: [98][60/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 2.0407 (2.1830)\tPrec@1 85.938 (86.693)\tPrec@5 99.219 (99.155)\n",
      "Epoch: [98][70/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 2.6705 (2.1856)\tPrec@1 80.469 (86.521)\tPrec@5 99.219 (99.153)\n",
      "Epoch: [98][80/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 1.7873 (2.1822)\tPrec@1 90.625 (86.613)\tPrec@5 99.219 (99.171)\n",
      "Epoch: [98][90/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.019)\tLoss 2.3421 (2.1620)\tPrec@1 89.844 (86.796)\tPrec@5 99.219 (99.176)\n",
      "Epoch: [98][96/97], lr: 0.01000\tTime 0.319 (0.331)\tData 0.000 (0.020)\tLoss 2.2970 (2.1795)\tPrec@1 85.593 (86.603)\tPrec@5 99.153 (99.194)\n",
      "Test: [0/100]\tTime 0.274 (0.274)\tLoss 10.2470 (10.2470)\tPrec@1 55.000 (55.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 7.0154 (9.0019)\tPrec@1 66.000 (58.091)\tPrec@5 95.000 (93.455)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 7.3381 (8.8844)\tPrec@1 63.000 (58.905)\tPrec@5 96.000 (93.143)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.1617 (8.9744)\tPrec@1 61.000 (58.355)\tPrec@5 93.000 (93.161)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 8.9649 (9.0257)\tPrec@1 56.000 (58.512)\tPrec@5 92.000 (93.122)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 8.6079 (8.9435)\tPrec@1 61.000 (59.020)\tPrec@5 93.000 (93.255)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7923 (8.8620)\tPrec@1 67.000 (59.295)\tPrec@5 95.000 (93.443)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.9212 (8.8336)\tPrec@1 61.000 (59.479)\tPrec@5 96.000 (93.352)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.4870 (8.7823)\tPrec@1 58.000 (59.642)\tPrec@5 93.000 (93.444)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.5692 (8.8357)\tPrec@1 58.000 (59.440)\tPrec@5 93.000 (93.286)\n",
      "val Results: Prec@1 59.430 Prec@5 93.150 Loss 8.86185\n",
      "val Class Accuracy: [0.859,0.987,0.655,0.795,0.724,0.635,0.716,0.402,0.170,0.000]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [99][0/97], lr: 0.01000\tTime 0.421 (0.421)\tData 0.226 (0.226)\tLoss 2.0218 (2.0218)\tPrec@1 85.156 (85.156)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [99][10/97], lr: 0.01000\tTime 0.325 (0.343)\tData 0.000 (0.035)\tLoss 2.2022 (2.1887)\tPrec@1 88.281 (86.293)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [99][20/97], lr: 0.01000\tTime 0.334 (0.336)\tData 0.000 (0.027)\tLoss 2.3795 (2.3042)\tPrec@1 87.500 (85.863)\tPrec@5 96.094 (98.958)\n",
      "Epoch: [99][30/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.024)\tLoss 2.0470 (2.2446)\tPrec@1 87.500 (85.912)\tPrec@5 100.000 (99.194)\n",
      "Epoch: [99][40/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.022)\tLoss 2.4578 (2.2508)\tPrec@1 84.375 (85.918)\tPrec@5 99.219 (99.143)\n",
      "Epoch: [99][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 2.6617 (2.2120)\tPrec@1 84.375 (86.275)\tPrec@5 97.656 (99.066)\n",
      "Epoch: [99][60/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 2.4143 (2.2107)\tPrec@1 85.156 (86.130)\tPrec@5 100.000 (99.129)\n",
      "Epoch: [99][70/97], lr: 0.01000\tTime 0.327 (0.331)\tData 0.000 (0.020)\tLoss 2.4960 (2.1981)\tPrec@1 87.500 (86.224)\tPrec@5 100.000 (99.175)\n",
      "Epoch: [99][80/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 2.0521 (2.1959)\tPrec@1 88.281 (86.227)\tPrec@5 100.000 (99.171)\n",
      "Epoch: [99][90/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.019)\tLoss 1.7540 (2.1803)\tPrec@1 89.844 (86.324)\tPrec@5 100.000 (99.176)\n",
      "Epoch: [99][96/97], lr: 0.01000\tTime 0.319 (0.330)\tData 0.000 (0.020)\tLoss 1.8486 (2.1780)\tPrec@1 88.136 (86.321)\tPrec@5 100.000 (99.186)\n",
      "Test: [0/100]\tTime 0.235 (0.235)\tLoss 10.7108 (10.7108)\tPrec@1 45.000 (45.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 8.0963 (9.6068)\tPrec@1 59.000 (52.818)\tPrec@5 97.000 (95.364)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.8873 (9.5430)\tPrec@1 57.000 (52.952)\tPrec@5 93.000 (95.095)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.3388 (9.5886)\tPrec@1 54.000 (52.645)\tPrec@5 94.000 (95.097)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.9351 (9.5893)\tPrec@1 51.000 (52.610)\tPrec@5 93.000 (94.732)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.5322 (9.5201)\tPrec@1 52.000 (53.020)\tPrec@5 94.000 (94.824)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.3599 (9.4955)\tPrec@1 66.000 (52.984)\tPrec@5 95.000 (94.623)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.0080 (9.4922)\tPrec@1 56.000 (53.028)\tPrec@5 96.000 (94.662)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.2431 (9.4383)\tPrec@1 52.000 (53.173)\tPrec@5 93.000 (94.815)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.7187 (9.5109)\tPrec@1 51.000 (52.824)\tPrec@5 94.000 (94.725)\n",
      "val Results: Prec@1 52.710 Prec@5 94.700 Loss 9.55123\n",
      "val Class Accuracy: [0.963,0.975,0.762,0.690,0.789,0.354,0.105,0.307,0.303,0.023]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [100][0/97], lr: 0.01000\tTime 0.398 (0.398)\tData 0.201 (0.201)\tLoss 2.1371 (2.1371)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [100][10/97], lr: 0.01000\tTime 0.328 (0.345)\tData 0.000 (0.033)\tLoss 2.3153 (2.3557)\tPrec@1 86.719 (85.369)\tPrec@5 97.656 (98.722)\n",
      "Epoch: [100][20/97], lr: 0.01000\tTime 0.325 (0.337)\tData 0.000 (0.025)\tLoss 1.8129 (2.1569)\tPrec@1 86.719 (86.719)\tPrec@5 100.000 (98.996)\n",
      "Epoch: [100][30/97], lr: 0.01000\tTime 0.332 (0.336)\tData 0.000 (0.023)\tLoss 2.9600 (2.2218)\tPrec@1 82.031 (86.316)\tPrec@5 100.000 (99.093)\n",
      "Epoch: [100][40/97], lr: 0.01000\tTime 0.331 (0.335)\tData 0.000 (0.021)\tLoss 2.8070 (2.2238)\tPrec@1 82.812 (86.147)\tPrec@5 99.219 (99.238)\n",
      "Epoch: [100][50/97], lr: 0.01000\tTime 0.344 (0.335)\tData 0.000 (0.021)\tLoss 3.3552 (2.2527)\tPrec@1 78.906 (85.938)\tPrec@5 98.438 (99.234)\n",
      "Epoch: [100][60/97], lr: 0.01000\tTime 0.369 (0.339)\tData 0.000 (0.020)\tLoss 2.5962 (2.2664)\tPrec@1 82.812 (85.925)\tPrec@5 98.438 (99.129)\n",
      "Epoch: [100][70/97], lr: 0.01000\tTime 0.350 (0.345)\tData 0.000 (0.019)\tLoss 2.1416 (2.2492)\tPrec@1 87.500 (85.982)\tPrec@5 100.000 (99.098)\n",
      "Epoch: [100][80/97], lr: 0.01000\tTime 0.331 (0.348)\tData 0.000 (0.019)\tLoss 2.0855 (2.2198)\tPrec@1 87.500 (86.236)\tPrec@5 100.000 (99.113)\n",
      "Epoch: [100][90/97], lr: 0.01000\tTime 0.330 (0.347)\tData 0.000 (0.019)\tLoss 2.1512 (2.2178)\tPrec@1 86.719 (86.324)\tPrec@5 98.438 (99.064)\n",
      "Epoch: [100][96/97], lr: 0.01000\tTime 0.341 (0.347)\tData 0.000 (0.019)\tLoss 2.5724 (2.2081)\tPrec@1 83.898 (86.418)\tPrec@5 100.000 (99.089)\n",
      "Test: [0/100]\tTime 0.315 (0.315)\tLoss 9.1564 (9.1564)\tPrec@1 57.000 (57.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.095)\tLoss 6.3197 (8.2600)\tPrec@1 69.000 (60.182)\tPrec@5 98.000 (96.818)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 7.5924 (8.1602)\tPrec@1 60.000 (60.333)\tPrec@5 93.000 (96.524)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 7.7937 (8.1254)\tPrec@1 62.000 (60.935)\tPrec@5 98.000 (96.548)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 8.5368 (8.0891)\tPrec@1 58.000 (61.073)\tPrec@5 93.000 (96.390)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 7.7289 (8.0587)\tPrec@1 61.000 (61.098)\tPrec@5 97.000 (96.490)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 6.7831 (8.0473)\tPrec@1 66.000 (60.902)\tPrec@5 97.000 (96.459)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 7.0675 (8.0389)\tPrec@1 66.000 (60.915)\tPrec@5 99.000 (96.592)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 8.0175 (8.0281)\tPrec@1 61.000 (60.975)\tPrec@5 96.000 (96.679)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 8.2885 (8.0776)\tPrec@1 57.000 (60.670)\tPrec@5 98.000 (96.725)\n",
      "val Results: Prec@1 60.550 Prec@5 96.740 Loss 8.10603\n",
      "val Class Accuracy: [0.948,0.990,0.830,0.668,0.685,0.263,0.661,0.454,0.491,0.065]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [101][0/97], lr: 0.01000\tTime 1.073 (1.073)\tData 0.587 (0.587)\tLoss 2.4262 (2.4262)\tPrec@1 85.938 (85.938)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [101][10/97], lr: 0.01000\tTime 0.343 (0.469)\tData 0.000 (0.064)\tLoss 1.9552 (2.1549)\tPrec@1 88.281 (86.932)\tPrec@5 98.438 (99.006)\n",
      "Epoch: [101][20/97], lr: 0.01000\tTime 0.326 (0.407)\tData 0.000 (0.042)\tLoss 1.8665 (2.3201)\tPrec@1 86.719 (85.826)\tPrec@5 100.000 (99.033)\n",
      "Epoch: [101][30/97], lr: 0.01000\tTime 0.347 (0.390)\tData 0.000 (0.034)\tLoss 2.5254 (2.2671)\tPrec@1 85.156 (86.114)\tPrec@5 99.219 (99.042)\n",
      "Epoch: [101][40/97], lr: 0.01000\tTime 0.369 (0.385)\tData 0.000 (0.030)\tLoss 1.7267 (2.1900)\tPrec@1 89.062 (86.509)\tPrec@5 100.000 (99.066)\n",
      "Epoch: [101][50/97], lr: 0.01000\tTime 0.370 (0.385)\tData 0.000 (0.027)\tLoss 2.3983 (2.1857)\tPrec@1 89.062 (86.765)\tPrec@5 100.000 (99.112)\n",
      "Epoch: [101][60/97], lr: 0.01000\tTime 0.381 (0.387)\tData 0.001 (0.025)\tLoss 2.2156 (2.1665)\tPrec@1 87.500 (86.834)\tPrec@5 97.656 (99.129)\n",
      "Epoch: [101][70/97], lr: 0.01000\tTime 0.343 (0.385)\tData 0.000 (0.024)\tLoss 1.5456 (2.1507)\tPrec@1 88.281 (86.851)\tPrec@5 99.219 (99.109)\n",
      "Epoch: [101][80/97], lr: 0.01000\tTime 0.364 (0.383)\tData 0.000 (0.023)\tLoss 2.2062 (2.1751)\tPrec@1 87.500 (86.767)\tPrec@5 100.000 (99.122)\n",
      "Epoch: [101][90/97], lr: 0.01000\tTime 0.337 (0.383)\tData 0.000 (0.022)\tLoss 2.7282 (2.1715)\tPrec@1 82.812 (86.787)\tPrec@5 100.000 (99.133)\n",
      "Epoch: [101][96/97], lr: 0.01000\tTime 0.376 (0.382)\tData 0.000 (0.023)\tLoss 1.7278 (2.1772)\tPrec@1 89.831 (86.724)\tPrec@5 98.305 (99.129)\n",
      "Test: [0/100]\tTime 0.364 (0.364)\tLoss 10.5141 (10.5141)\tPrec@1 48.000 (48.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.074 (0.100)\tLoss 6.9743 (8.7803)\tPrec@1 65.000 (56.727)\tPrec@5 95.000 (97.000)\n",
      "Test: [20/100]\tTime 0.074 (0.088)\tLoss 7.7207 (8.7823)\tPrec@1 60.000 (56.762)\tPrec@5 99.000 (97.286)\n",
      "Test: [30/100]\tTime 0.074 (0.083)\tLoss 8.1773 (8.7909)\tPrec@1 58.000 (56.903)\tPrec@5 96.000 (97.032)\n",
      "Test: [40/100]\tTime 0.074 (0.081)\tLoss 8.4399 (8.7622)\tPrec@1 61.000 (57.171)\tPrec@5 98.000 (97.049)\n",
      "Test: [50/100]\tTime 0.074 (0.080)\tLoss 8.9278 (8.7411)\tPrec@1 54.000 (57.196)\tPrec@5 98.000 (97.039)\n",
      "Test: [60/100]\tTime 0.073 (0.079)\tLoss 6.8920 (8.7257)\tPrec@1 66.000 (57.131)\tPrec@5 97.000 (97.049)\n",
      "Test: [70/100]\tTime 0.073 (0.078)\tLoss 8.0501 (8.7219)\tPrec@1 64.000 (57.056)\tPrec@5 99.000 (97.042)\n",
      "Test: [80/100]\tTime 0.073 (0.078)\tLoss 8.4212 (8.6844)\tPrec@1 59.000 (57.136)\tPrec@5 93.000 (97.049)\n",
      "Test: [90/100]\tTime 0.073 (0.077)\tLoss 9.2128 (8.7405)\tPrec@1 54.000 (56.857)\tPrec@5 99.000 (97.022)\n",
      "val Results: Prec@1 56.890 Prec@5 97.000 Loss 8.73323\n",
      "val Class Accuracy: [0.938,0.956,0.816,0.844,0.533,0.309,0.537,0.515,0.230,0.011]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [102][0/97], lr: 0.01000\tTime 0.493 (0.493)\tData 0.250 (0.250)\tLoss 2.0793 (2.0793)\tPrec@1 85.156 (85.156)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [102][10/97], lr: 0.01000\tTime 0.327 (0.356)\tData 0.000 (0.037)\tLoss 1.6631 (1.8995)\tPrec@1 91.406 (88.494)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [102][20/97], lr: 0.01000\tTime 0.341 (0.357)\tData 0.000 (0.028)\tLoss 2.4872 (2.0731)\tPrec@1 83.594 (87.277)\tPrec@5 100.000 (99.330)\n",
      "Epoch: [102][30/97], lr: 0.01000\tTime 0.343 (0.350)\tData 0.000 (0.024)\tLoss 2.2540 (2.1466)\tPrec@1 87.500 (86.719)\tPrec@5 98.438 (99.320)\n",
      "Epoch: [102][40/97], lr: 0.01000\tTime 0.323 (0.346)\tData 0.000 (0.023)\tLoss 1.8633 (2.1566)\tPrec@1 89.062 (86.833)\tPrec@5 100.000 (99.276)\n",
      "Epoch: [102][50/97], lr: 0.01000\tTime 0.324 (0.343)\tData 0.000 (0.022)\tLoss 2.7103 (2.1713)\tPrec@1 85.938 (86.749)\tPrec@5 96.875 (99.188)\n",
      "Epoch: [102][60/97], lr: 0.01000\tTime 0.324 (0.340)\tData 0.000 (0.021)\tLoss 2.3120 (2.1757)\tPrec@1 84.375 (86.770)\tPrec@5 98.438 (99.168)\n",
      "Epoch: [102][70/97], lr: 0.01000\tTime 0.328 (0.339)\tData 0.000 (0.020)\tLoss 2.8451 (2.1652)\tPrec@1 83.594 (86.741)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [102][80/97], lr: 0.01000\tTime 0.327 (0.338)\tData 0.000 (0.020)\tLoss 2.0634 (2.1680)\tPrec@1 83.594 (86.661)\tPrec@5 100.000 (99.209)\n",
      "Epoch: [102][90/97], lr: 0.01000\tTime 0.323 (0.337)\tData 0.000 (0.020)\tLoss 2.0920 (2.1743)\tPrec@1 86.719 (86.616)\tPrec@5 99.219 (99.227)\n",
      "Epoch: [102][96/97], lr: 0.01000\tTime 0.321 (0.336)\tData 0.000 (0.020)\tLoss 2.2098 (2.1553)\tPrec@1 83.898 (86.716)\tPrec@5 100.000 (99.226)\n",
      "Test: [0/100]\tTime 0.235 (0.235)\tLoss 8.8368 (8.8368)\tPrec@1 57.000 (57.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 7.0342 (8.4827)\tPrec@1 68.000 (58.182)\tPrec@5 94.000 (95.182)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.5540 (8.2812)\tPrec@1 61.000 (58.952)\tPrec@5 93.000 (94.810)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.6358 (8.2594)\tPrec@1 61.000 (59.032)\tPrec@5 97.000 (94.968)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.9054 (8.2849)\tPrec@1 57.000 (59.049)\tPrec@5 93.000 (95.073)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.8313 (8.2331)\tPrec@1 56.000 (59.216)\tPrec@5 94.000 (95.137)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.5529 (8.2405)\tPrec@1 67.000 (58.934)\tPrec@5 94.000 (94.902)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.0196 (8.2012)\tPrec@1 67.000 (59.141)\tPrec@5 97.000 (95.085)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.2687 (8.1700)\tPrec@1 58.000 (59.247)\tPrec@5 96.000 (95.148)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.5099 (8.1921)\tPrec@1 50.000 (59.110)\tPrec@5 98.000 (95.231)\n",
      "val Results: Prec@1 59.000 Prec@5 95.270 Loss 8.21213\n",
      "val Class Accuracy: [0.950,0.990,0.667,0.802,0.660,0.270,0.371,0.420,0.622,0.148]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [103][0/97], lr: 0.01000\tTime 0.403 (0.403)\tData 0.215 (0.215)\tLoss 1.6186 (1.6186)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [103][10/97], lr: 0.01000\tTime 0.325 (0.342)\tData 0.000 (0.035)\tLoss 2.1793 (2.0293)\tPrec@1 86.719 (87.713)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [103][20/97], lr: 0.01000\tTime 0.350 (0.336)\tData 0.000 (0.026)\tLoss 2.4472 (2.0045)\tPrec@1 84.375 (87.612)\tPrec@5 100.000 (99.368)\n",
      "Epoch: [103][30/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.023)\tLoss 2.5267 (2.0743)\tPrec@1 85.938 (87.072)\tPrec@5 99.219 (99.345)\n",
      "Epoch: [103][40/97], lr: 0.01000\tTime 0.331 (0.334)\tData 0.000 (0.022)\tLoss 2.4577 (2.0935)\tPrec@1 84.375 (87.062)\tPrec@5 99.219 (99.352)\n",
      "Epoch: [103][50/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 2.1123 (2.0999)\tPrec@1 83.594 (87.086)\tPrec@5 100.000 (99.433)\n",
      "Epoch: [103][60/97], lr: 0.01000\tTime 0.329 (0.333)\tData 0.000 (0.020)\tLoss 1.4672 (2.0861)\tPrec@1 89.844 (87.129)\tPrec@5 100.000 (99.372)\n",
      "Epoch: [103][70/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 1.4442 (2.0990)\tPrec@1 90.625 (87.049)\tPrec@5 100.000 (99.329)\n",
      "Epoch: [103][80/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 2.1640 (2.1131)\tPrec@1 86.719 (86.863)\tPrec@5 100.000 (99.325)\n",
      "Epoch: [103][90/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.019)\tLoss 2.7617 (2.1246)\tPrec@1 83.594 (86.822)\tPrec@5 97.656 (99.287)\n",
      "Epoch: [103][96/97], lr: 0.01000\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 2.1144 (2.1298)\tPrec@1 87.288 (86.781)\tPrec@5 99.153 (99.250)\n",
      "Test: [0/100]\tTime 0.256 (0.256)\tLoss 9.0056 (9.0056)\tPrec@1 58.000 (58.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.1838 (8.4994)\tPrec@1 72.000 (58.364)\tPrec@5 92.000 (93.182)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.1518 (8.4040)\tPrec@1 66.000 (58.571)\tPrec@5 96.000 (93.429)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.1937 (8.4308)\tPrec@1 59.000 (58.323)\tPrec@5 93.000 (93.000)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.1723 (8.4767)\tPrec@1 48.000 (58.098)\tPrec@5 96.000 (93.098)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.6532 (8.4169)\tPrec@1 54.000 (58.392)\tPrec@5 95.000 (93.196)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 5.7789 (8.3687)\tPrec@1 71.000 (58.541)\tPrec@5 98.000 (93.410)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.5529 (8.3513)\tPrec@1 58.000 (58.606)\tPrec@5 96.000 (93.324)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.2590 (8.3417)\tPrec@1 58.000 (58.556)\tPrec@5 91.000 (93.272)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.1364 (8.3810)\tPrec@1 60.000 (58.396)\tPrec@5 94.000 (93.187)\n",
      "val Results: Prec@1 58.460 Prec@5 93.230 Loss 8.37328\n",
      "val Class Accuracy: [0.852,0.886,0.708,0.639,0.541,0.739,0.847,0.314,0.268,0.052]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [104][0/97], lr: 0.01000\tTime 0.473 (0.473)\tData 0.258 (0.258)\tLoss 1.9333 (1.9333)\tPrec@1 85.938 (85.938)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [104][10/97], lr: 0.01000\tTime 0.326 (0.346)\tData 0.000 (0.038)\tLoss 1.7236 (2.0636)\tPrec@1 87.500 (87.287)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [104][20/97], lr: 0.01000\tTime 0.323 (0.337)\tData 0.000 (0.028)\tLoss 2.0396 (2.0517)\tPrec@1 89.844 (87.574)\tPrec@5 100.000 (99.330)\n",
      "Epoch: [104][30/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.025)\tLoss 1.7313 (2.0351)\tPrec@1 88.281 (87.702)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [104][40/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.023)\tLoss 1.6794 (2.0773)\tPrec@1 86.719 (87.405)\tPrec@5 100.000 (99.257)\n",
      "Epoch: [104][50/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.022)\tLoss 2.1756 (2.1174)\tPrec@1 85.156 (86.994)\tPrec@5 100.000 (99.188)\n",
      "Epoch: [104][60/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.021)\tLoss 2.6964 (2.1752)\tPrec@1 83.594 (86.629)\tPrec@5 97.656 (99.180)\n",
      "Epoch: [104][70/97], lr: 0.01000\tTime 0.336 (0.331)\tData 0.000 (0.020)\tLoss 2.3273 (2.1316)\tPrec@1 85.156 (86.972)\tPrec@5 99.219 (99.263)\n",
      "Epoch: [104][80/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 1.5900 (2.1544)\tPrec@1 91.406 (86.863)\tPrec@5 99.219 (99.228)\n",
      "Epoch: [104][90/97], lr: 0.01000\tTime 0.322 (0.331)\tData 0.000 (0.020)\tLoss 1.9771 (2.1448)\tPrec@1 90.625 (86.976)\tPrec@5 99.219 (99.253)\n",
      "Epoch: [104][96/97], lr: 0.01000\tTime 0.323 (0.330)\tData 0.000 (0.020)\tLoss 2.0974 (2.1554)\tPrec@1 87.288 (86.877)\tPrec@5 98.305 (99.275)\n",
      "Test: [0/100]\tTime 0.243 (0.243)\tLoss 9.3070 (9.3070)\tPrec@1 55.000 (55.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 6.8258 (8.6916)\tPrec@1 65.000 (56.182)\tPrec@5 97.000 (95.182)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 8.4403 (8.5921)\tPrec@1 58.000 (57.190)\tPrec@5 97.000 (95.524)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 8.3845 (8.6780)\tPrec@1 57.000 (56.742)\tPrec@5 94.000 (95.484)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.3812 (8.6956)\tPrec@1 59.000 (56.976)\tPrec@5 95.000 (95.341)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.2746 (8.5866)\tPrec@1 59.000 (57.373)\tPrec@5 95.000 (95.392)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7076 (8.5365)\tPrec@1 67.000 (57.508)\tPrec@5 95.000 (95.525)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.2425 (8.5228)\tPrec@1 63.000 (57.577)\tPrec@5 97.000 (95.352)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.6851 (8.4577)\tPrec@1 67.000 (58.025)\tPrec@5 96.000 (95.407)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.7650 (8.4969)\tPrec@1 63.000 (57.824)\tPrec@5 96.000 (95.308)\n",
      "val Results: Prec@1 57.680 Prec@5 95.280 Loss 8.53554\n",
      "val Class Accuracy: [0.818,0.976,0.778,0.725,0.856,0.390,0.716,0.296,0.129,0.084]\n",
      "Best Prec@1: 62.140\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [105][0/97], lr: 0.01000\tTime 0.435 (0.435)\tData 0.205 (0.205)\tLoss 1.6956 (1.6956)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [105][10/97], lr: 0.01000\tTime 0.331 (0.345)\tData 0.000 (0.034)\tLoss 2.3908 (1.9248)\tPrec@1 85.156 (87.997)\tPrec@5 97.656 (99.006)\n",
      "Epoch: [105][20/97], lr: 0.01000\tTime 0.348 (0.339)\tData 0.000 (0.026)\tLoss 2.1470 (1.9538)\tPrec@1 86.719 (87.649)\tPrec@5 100.000 (99.070)\n",
      "Epoch: [105][30/97], lr: 0.01000\tTime 0.326 (0.336)\tData 0.000 (0.023)\tLoss 1.7665 (1.9217)\tPrec@1 89.844 (88.130)\tPrec@5 99.219 (99.042)\n",
      "Epoch: [105][40/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.022)\tLoss 1.6712 (1.9873)\tPrec@1 89.844 (87.957)\tPrec@5 99.219 (99.085)\n",
      "Epoch: [105][50/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 1.4866 (2.0738)\tPrec@1 92.188 (87.454)\tPrec@5 99.219 (99.020)\n",
      "Epoch: [105][60/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.020)\tLoss 2.0973 (2.1266)\tPrec@1 85.156 (87.167)\tPrec@5 100.000 (99.052)\n",
      "Epoch: [105][70/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 2.3177 (2.1185)\tPrec@1 85.156 (87.192)\tPrec@5 98.438 (99.087)\n",
      "Epoch: [105][80/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 2.1531 (2.1474)\tPrec@1 87.500 (87.037)\tPrec@5 99.219 (99.122)\n",
      "Epoch: [105][90/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.019)\tLoss 2.2896 (2.1414)\tPrec@1 85.156 (86.985)\tPrec@5 99.219 (99.141)\n",
      "Epoch: [105][96/97], lr: 0.01000\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 3.1555 (2.1643)\tPrec@1 82.203 (86.869)\tPrec@5 99.153 (99.146)\n",
      "Test: [0/100]\tTime 0.254 (0.254)\tLoss 7.6803 (7.6803)\tPrec@1 62.000 (62.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 5.6714 (7.5118)\tPrec@1 72.000 (63.182)\tPrec@5 97.000 (97.545)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.7947 (7.3401)\tPrec@1 71.000 (63.952)\tPrec@5 96.000 (97.238)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.4443 (7.3291)\tPrec@1 69.000 (64.161)\tPrec@5 96.000 (97.161)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.4032 (7.3492)\tPrec@1 63.000 (64.000)\tPrec@5 97.000 (97.049)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.7835 (7.2689)\tPrec@1 67.000 (64.294)\tPrec@5 98.000 (97.216)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 5.7508 (7.2590)\tPrec@1 71.000 (64.131)\tPrec@5 97.000 (97.131)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.5020 (7.2379)\tPrec@1 63.000 (64.296)\tPrec@5 100.000 (97.155)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.6567 (7.1975)\tPrec@1 67.000 (64.580)\tPrec@5 98.000 (97.160)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.2999 (7.2660)\tPrec@1 65.000 (64.143)\tPrec@5 95.000 (97.154)\n",
      "val Results: Prec@1 64.030 Prec@5 97.120 Loss 7.29075\n",
      "val Class Accuracy: [0.947,0.993,0.651,0.599,0.646,0.787,0.647,0.519,0.509,0.105]\n",
      "Best Prec@1: 64.030\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [106][0/97], lr: 0.01000\tTime 0.427 (0.427)\tData 0.231 (0.231)\tLoss 2.6109 (2.6109)\tPrec@1 79.688 (79.688)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [106][10/97], lr: 0.01000\tTime 0.327 (0.343)\tData 0.000 (0.036)\tLoss 2.1700 (2.0829)\tPrec@1 86.719 (86.719)\tPrec@5 98.438 (99.574)\n",
      "Epoch: [106][20/97], lr: 0.01000\tTime 0.324 (0.336)\tData 0.000 (0.027)\tLoss 2.5891 (2.1463)\tPrec@1 82.812 (86.570)\tPrec@5 99.219 (99.293)\n",
      "Epoch: [106][30/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.024)\tLoss 1.9927 (2.1205)\tPrec@1 87.500 (86.971)\tPrec@5 100.000 (99.294)\n",
      "Epoch: [106][40/97], lr: 0.01000\tTime 0.332 (0.333)\tData 0.000 (0.022)\tLoss 1.6839 (2.0642)\tPrec@1 88.281 (87.290)\tPrec@5 100.000 (99.276)\n",
      "Epoch: [106][50/97], lr: 0.01000\tTime 0.328 (0.333)\tData 0.000 (0.021)\tLoss 1.7458 (2.0751)\tPrec@1 89.062 (87.255)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [106][60/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.021)\tLoss 1.8409 (2.0590)\tPrec@1 85.938 (87.282)\tPrec@5 100.000 (99.257)\n",
      "Epoch: [106][70/97], lr: 0.01000\tTime 0.322 (0.331)\tData 0.000 (0.020)\tLoss 2.5740 (2.0851)\tPrec@1 86.719 (87.082)\tPrec@5 98.438 (99.263)\n",
      "Epoch: [106][80/97], lr: 0.01000\tTime 0.328 (0.331)\tData 0.000 (0.020)\tLoss 2.6031 (2.1003)\tPrec@1 82.812 (86.960)\tPrec@5 100.000 (99.277)\n",
      "Epoch: [106][90/97], lr: 0.01000\tTime 0.322 (0.331)\tData 0.000 (0.019)\tLoss 2.4473 (2.0922)\tPrec@1 85.156 (87.028)\tPrec@5 99.219 (99.279)\n",
      "Epoch: [106][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 2.9181 (2.1109)\tPrec@1 82.203 (86.926)\tPrec@5 100.000 (99.266)\n",
      "Test: [0/100]\tTime 0.259 (0.259)\tLoss 10.8344 (10.8344)\tPrec@1 49.000 (49.000)\tPrec@5 90.000 (90.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 7.0257 (9.4851)\tPrec@1 68.000 (56.545)\tPrec@5 94.000 (92.364)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.7697 (9.3069)\tPrec@1 64.000 (56.952)\tPrec@5 97.000 (92.000)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.5097 (9.2849)\tPrec@1 60.000 (57.161)\tPrec@5 91.000 (91.903)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.9794 (9.3413)\tPrec@1 57.000 (56.756)\tPrec@5 89.000 (91.366)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 8.7507 (9.2044)\tPrec@1 59.000 (57.255)\tPrec@5 91.000 (91.431)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.9331 (9.1518)\tPrec@1 67.000 (57.443)\tPrec@5 95.000 (91.754)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.9158 (9.1464)\tPrec@1 57.000 (57.521)\tPrec@5 95.000 (91.803)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.1667 (9.0899)\tPrec@1 55.000 (57.691)\tPrec@5 90.000 (91.975)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.4806 (9.1689)\tPrec@1 61.000 (57.363)\tPrec@5 94.000 (91.978)\n",
      "val Results: Prec@1 57.230 Prec@5 91.840 Loss 9.21599\n",
      "val Class Accuracy: [0.977,0.921,0.779,0.656,0.758,0.709,0.474,0.142,0.147,0.160]\n",
      "Best Prec@1: 64.030\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [107][0/97], lr: 0.01000\tTime 0.407 (0.407)\tData 0.211 (0.211)\tLoss 2.4367 (2.4367)\tPrec@1 84.375 (84.375)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [107][10/97], lr: 0.01000\tTime 0.326 (0.343)\tData 0.000 (0.034)\tLoss 2.1452 (2.1624)\tPrec@1 86.719 (87.358)\tPrec@5 99.219 (99.006)\n",
      "Epoch: [107][20/97], lr: 0.01000\tTime 0.338 (0.337)\tData 0.000 (0.026)\tLoss 2.1559 (2.1441)\tPrec@1 85.938 (86.830)\tPrec@5 98.438 (99.107)\n",
      "Epoch: [107][30/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.023)\tLoss 2.4469 (2.1264)\tPrec@1 85.938 (86.996)\tPrec@5 97.656 (98.992)\n",
      "Epoch: [107][40/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.022)\tLoss 2.6507 (2.1219)\tPrec@1 84.375 (87.081)\tPrec@5 99.219 (99.028)\n",
      "Epoch: [107][50/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.021)\tLoss 1.5810 (2.1023)\tPrec@1 91.406 (87.194)\tPrec@5 99.219 (99.173)\n",
      "Epoch: [107][60/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 1.7520 (2.0979)\tPrec@1 88.281 (87.205)\tPrec@5 99.219 (99.193)\n",
      "Epoch: [107][70/97], lr: 0.01000\tTime 0.329 (0.332)\tData 0.000 (0.020)\tLoss 3.1484 (2.1265)\tPrec@1 82.031 (87.060)\tPrec@5 99.219 (99.208)\n",
      "Epoch: [107][80/97], lr: 0.01000\tTime 0.332 (0.331)\tData 0.000 (0.020)\tLoss 2.6232 (2.1284)\tPrec@1 86.719 (86.998)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [107][90/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.019)\tLoss 2.1914 (2.1404)\tPrec@1 85.938 (86.848)\tPrec@5 98.438 (99.167)\n",
      "Epoch: [107][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 2.5379 (2.1473)\tPrec@1 84.746 (86.837)\tPrec@5 98.305 (99.097)\n",
      "Test: [0/100]\tTime 0.237 (0.237)\tLoss 12.0333 (12.0333)\tPrec@1 41.000 (41.000)\tPrec@5 87.000 (87.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 9.2858 (11.0652)\tPrec@1 54.000 (48.636)\tPrec@5 88.000 (83.364)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 9.1375 (10.7908)\tPrec@1 54.000 (49.286)\tPrec@5 89.000 (84.714)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 10.3636 (10.7441)\tPrec@1 47.000 (49.516)\tPrec@5 87.000 (84.419)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 11.2297 (10.7416)\tPrec@1 45.000 (49.610)\tPrec@5 86.000 (84.537)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 10.7536 (10.6269)\tPrec@1 49.000 (50.118)\tPrec@5 86.000 (84.745)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.5094 (10.5958)\tPrec@1 57.000 (49.934)\tPrec@5 92.000 (85.033)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 10.2589 (10.6056)\tPrec@1 52.000 (49.944)\tPrec@5 84.000 (84.775)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.9720 (10.5348)\tPrec@1 54.000 (50.235)\tPrec@5 87.000 (84.975)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.7034 (10.6156)\tPrec@1 51.000 (49.813)\tPrec@5 86.000 (84.736)\n",
      "val Results: Prec@1 49.870 Prec@5 84.690 Loss 10.61557\n",
      "val Class Accuracy: [0.850,0.900,0.820,0.879,0.519,0.361,0.117,0.478,0.060,0.003]\n",
      "Best Prec@1: 64.030\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [108][0/97], lr: 0.01000\tTime 0.439 (0.439)\tData 0.241 (0.241)\tLoss 2.0748 (2.0748)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [108][10/97], lr: 0.01000\tTime 0.328 (0.342)\tData 0.000 (0.037)\tLoss 1.8134 (1.8915)\tPrec@1 87.500 (89.134)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [108][20/97], lr: 0.01000\tTime 0.323 (0.335)\tData 0.000 (0.027)\tLoss 2.0299 (2.0212)\tPrec@1 85.938 (87.946)\tPrec@5 99.219 (99.442)\n",
      "Epoch: [108][30/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.024)\tLoss 2.7278 (2.0081)\tPrec@1 82.812 (88.004)\tPrec@5 99.219 (99.420)\n",
      "Epoch: [108][40/97], lr: 0.01000\tTime 0.331 (0.332)\tData 0.000 (0.023)\tLoss 2.0672 (2.0147)\tPrec@1 87.500 (87.881)\tPrec@5 99.219 (99.371)\n",
      "Epoch: [108][50/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.021)\tLoss 2.5722 (2.0488)\tPrec@1 84.375 (87.623)\tPrec@5 100.000 (99.403)\n",
      "Epoch: [108][60/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.021)\tLoss 1.2373 (2.0791)\tPrec@1 92.188 (87.359)\tPrec@5 100.000 (99.385)\n",
      "Epoch: [108][70/97], lr: 0.01000\tTime 0.331 (0.330)\tData 0.000 (0.020)\tLoss 1.7190 (2.0887)\tPrec@1 89.062 (87.192)\tPrec@5 100.000 (99.384)\n",
      "Epoch: [108][80/97], lr: 0.01000\tTime 0.324 (0.330)\tData 0.000 (0.020)\tLoss 2.1008 (2.1061)\tPrec@1 87.500 (87.124)\tPrec@5 98.438 (99.334)\n",
      "Epoch: [108][90/97], lr: 0.01000\tTime 0.324 (0.330)\tData 0.000 (0.020)\tLoss 1.7940 (2.1017)\tPrec@1 89.844 (87.139)\tPrec@5 99.219 (99.313)\n",
      "Epoch: [108][96/97], lr: 0.01000\tTime 0.318 (0.330)\tData 0.000 (0.020)\tLoss 2.1014 (2.1287)\tPrec@1 86.441 (86.998)\tPrec@5 97.458 (99.283)\n",
      "Test: [0/100]\tTime 0.230 (0.230)\tLoss 8.0147 (8.0147)\tPrec@1 62.000 (62.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 6.2048 (7.3248)\tPrec@1 68.000 (63.909)\tPrec@5 96.000 (96.364)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 7.2680 (7.2901)\tPrec@1 61.000 (64.190)\tPrec@5 100.000 (96.286)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 6.5001 (7.2685)\tPrec@1 69.000 (64.548)\tPrec@5 97.000 (96.387)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 6.6538 (7.2340)\tPrec@1 71.000 (64.976)\tPrec@5 95.000 (96.463)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.3758 (7.1577)\tPrec@1 70.000 (65.392)\tPrec@5 96.000 (96.588)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 6.0960 (7.0825)\tPrec@1 73.000 (65.803)\tPrec@5 94.000 (96.607)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 6.8301 (7.0980)\tPrec@1 67.000 (65.775)\tPrec@5 98.000 (96.732)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.9367 (7.0577)\tPrec@1 70.000 (66.074)\tPrec@5 95.000 (96.778)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.3633 (7.1404)\tPrec@1 60.000 (65.396)\tPrec@5 98.000 (96.725)\n",
      "val Results: Prec@1 65.380 Prec@5 96.680 Loss 7.16017\n",
      "val Class Accuracy: [0.934,0.943,0.683,0.539,0.867,0.555,0.654,0.684,0.306,0.373]\n",
      "Best Prec@1: 65.380\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [109][0/97], lr: 0.01000\tTime 0.445 (0.445)\tData 0.245 (0.245)\tLoss 2.1137 (2.1137)\tPrec@1 87.500 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [109][10/97], lr: 0.01000\tTime 0.324 (0.344)\tData 0.000 (0.037)\tLoss 2.4751 (2.0617)\tPrec@1 86.719 (88.210)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [109][20/97], lr: 0.01000\tTime 0.334 (0.336)\tData 0.000 (0.028)\tLoss 1.7495 (2.0303)\tPrec@1 87.500 (88.095)\tPrec@5 99.219 (99.107)\n",
      "Epoch: [109][30/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.024)\tLoss 1.8640 (2.0153)\tPrec@1 88.281 (87.878)\tPrec@5 100.000 (99.269)\n",
      "Epoch: [109][40/97], lr: 0.01000\tTime 0.329 (0.333)\tData 0.000 (0.023)\tLoss 2.9064 (2.0942)\tPrec@1 82.812 (87.348)\tPrec@5 99.219 (99.276)\n",
      "Epoch: [109][50/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.022)\tLoss 1.3712 (2.0870)\tPrec@1 92.188 (87.209)\tPrec@5 99.219 (99.311)\n",
      "Epoch: [109][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 2.4377 (2.1406)\tPrec@1 84.375 (86.911)\tPrec@5 99.219 (99.270)\n",
      "Epoch: [109][70/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 2.5993 (2.1488)\tPrec@1 82.031 (86.829)\tPrec@5 99.219 (99.285)\n",
      "Epoch: [109][80/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 3.1530 (2.1441)\tPrec@1 78.906 (86.834)\tPrec@5 98.438 (99.315)\n",
      "Epoch: [109][90/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 2.5912 (2.1376)\tPrec@1 82.031 (86.822)\tPrec@5 99.219 (99.348)\n",
      "Epoch: [109][96/97], lr: 0.01000\tTime 0.322 (0.331)\tData 0.000 (0.020)\tLoss 2.0391 (2.1405)\tPrec@1 87.288 (86.805)\tPrec@5 97.458 (99.299)\n",
      "Test: [0/100]\tTime 0.280 (0.280)\tLoss 9.5745 (9.5745)\tPrec@1 53.000 (53.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 6.8300 (9.1080)\tPrec@1 65.000 (55.727)\tPrec@5 97.000 (93.727)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 8.0743 (9.1532)\tPrec@1 60.000 (55.429)\tPrec@5 95.000 (93.429)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 8.6144 (9.1951)\tPrec@1 58.000 (55.419)\tPrec@5 95.000 (93.129)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 9.5916 (9.2719)\tPrec@1 56.000 (55.195)\tPrec@5 88.000 (93.098)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 8.5469 (9.1971)\tPrec@1 62.000 (55.667)\tPrec@5 97.000 (93.216)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.8558 (9.1211)\tPrec@1 62.000 (55.984)\tPrec@5 94.000 (93.344)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.7285 (9.1189)\tPrec@1 59.000 (55.958)\tPrec@5 95.000 (93.366)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.2823 (9.0718)\tPrec@1 60.000 (56.148)\tPrec@5 96.000 (93.457)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.3436 (9.1310)\tPrec@1 58.000 (55.989)\tPrec@5 96.000 (93.484)\n",
      "val Results: Prec@1 55.830 Prec@5 93.470 Loss 9.15459\n",
      "val Class Accuracy: [0.981,0.973,0.602,0.395,0.514,0.587,0.814,0.635,0.041,0.041]\n",
      "Best Prec@1: 65.380\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [110][0/97], lr: 0.01000\tTime 0.425 (0.425)\tData 0.222 (0.222)\tLoss 2.1141 (2.1141)\tPrec@1 88.281 (88.281)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [110][10/97], lr: 0.01000\tTime 0.327 (0.342)\tData 0.000 (0.035)\tLoss 1.7412 (1.9783)\tPrec@1 91.406 (87.429)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [110][20/97], lr: 0.01000\tTime 0.323 (0.335)\tData 0.000 (0.027)\tLoss 2.2687 (2.1253)\tPrec@1 85.156 (86.682)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [110][30/97], lr: 0.01000\tTime 0.328 (0.334)\tData 0.000 (0.024)\tLoss 2.8935 (2.1555)\tPrec@1 81.250 (86.618)\tPrec@5 96.875 (99.194)\n",
      "Epoch: [110][40/97], lr: 0.01000\tTime 0.329 (0.333)\tData 0.000 (0.022)\tLoss 2.3590 (2.1709)\tPrec@1 87.500 (86.433)\tPrec@5 98.438 (99.200)\n",
      "Epoch: [110][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 1.9446 (2.1796)\tPrec@1 87.500 (86.382)\tPrec@5 100.000 (99.081)\n",
      "Epoch: [110][60/97], lr: 0.01000\tTime 0.329 (0.332)\tData 0.000 (0.021)\tLoss 2.5760 (2.1597)\tPrec@1 84.375 (86.629)\tPrec@5 100.000 (99.116)\n",
      "Epoch: [110][70/97], lr: 0.01000\tTime 0.328 (0.332)\tData 0.000 (0.020)\tLoss 1.8359 (2.1519)\tPrec@1 87.500 (86.576)\tPrec@5 98.438 (99.153)\n",
      "Epoch: [110][80/97], lr: 0.01000\tTime 0.328 (0.331)\tData 0.000 (0.020)\tLoss 1.7383 (2.1488)\tPrec@1 89.844 (86.593)\tPrec@5 99.219 (99.142)\n",
      "Epoch: [110][90/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.019)\tLoss 2.2561 (2.1669)\tPrec@1 85.938 (86.513)\tPrec@5 99.219 (99.124)\n",
      "Epoch: [110][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 2.0282 (2.1651)\tPrec@1 88.136 (86.547)\tPrec@5 99.153 (99.138)\n",
      "Test: [0/100]\tTime 0.235 (0.235)\tLoss 10.6744 (10.6744)\tPrec@1 48.000 (48.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 8.5840 (9.5375)\tPrec@1 58.000 (54.000)\tPrec@5 88.000 (94.545)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.8530 (9.3814)\tPrec@1 65.000 (54.952)\tPrec@5 95.000 (94.714)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.1224 (9.3318)\tPrec@1 53.000 (54.935)\tPrec@5 97.000 (95.000)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.6202 (9.3413)\tPrec@1 49.000 (54.756)\tPrec@5 92.000 (94.927)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.1099 (9.2686)\tPrec@1 64.000 (55.412)\tPrec@5 99.000 (95.078)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.5031 (9.2463)\tPrec@1 58.000 (55.033)\tPrec@5 94.000 (95.016)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.1602 (9.2421)\tPrec@1 56.000 (55.141)\tPrec@5 97.000 (95.099)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.6053 (9.1968)\tPrec@1 61.000 (55.568)\tPrec@5 95.000 (95.222)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.5792 (9.3004)\tPrec@1 53.000 (55.055)\tPrec@5 95.000 (95.275)\n",
      "val Results: Prec@1 55.090 Prec@5 95.210 Loss 9.30712\n",
      "val Class Accuracy: [0.927,0.991,0.831,0.700,0.363,0.704,0.188,0.561,0.228,0.016]\n",
      "Best Prec@1: 65.380\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [111][0/97], lr: 0.01000\tTime 0.399 (0.399)\tData 0.192 (0.192)\tLoss 1.9989 (1.9989)\tPrec@1 90.625 (90.625)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [111][10/97], lr: 0.01000\tTime 0.324 (0.342)\tData 0.000 (0.033)\tLoss 1.6753 (2.0535)\tPrec@1 89.844 (87.216)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [111][20/97], lr: 0.01000\tTime 0.329 (0.335)\tData 0.000 (0.025)\tLoss 3.0886 (2.1221)\tPrec@1 81.250 (87.165)\tPrec@5 97.656 (99.256)\n",
      "Epoch: [111][30/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.023)\tLoss 1.4258 (2.1238)\tPrec@1 90.625 (86.895)\tPrec@5 99.219 (99.320)\n",
      "Epoch: [111][40/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 2.1509 (2.1413)\tPrec@1 85.156 (86.738)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [111][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 2.1728 (2.1577)\tPrec@1 88.281 (86.719)\tPrec@5 98.438 (99.203)\n",
      "Epoch: [111][60/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 1.8340 (2.1223)\tPrec@1 89.844 (86.898)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [111][70/97], lr: 0.01000\tTime 0.328 (0.331)\tData 0.000 (0.020)\tLoss 2.3217 (2.1004)\tPrec@1 85.156 (87.159)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [111][80/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.019)\tLoss 2.9513 (2.1140)\tPrec@1 81.250 (87.047)\tPrec@5 97.656 (99.190)\n",
      "Epoch: [111][90/97], lr: 0.01000\tTime 0.327 (0.331)\tData 0.000 (0.019)\tLoss 3.0256 (2.1191)\tPrec@1 82.812 (87.011)\tPrec@5 97.656 (99.159)\n",
      "Epoch: [111][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 2.0875 (2.1232)\tPrec@1 86.441 (86.966)\tPrec@5 99.153 (99.178)\n",
      "Test: [0/100]\tTime 0.243 (0.243)\tLoss 10.4104 (10.4104)\tPrec@1 51.000 (51.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 7.5708 (9.3540)\tPrec@1 64.000 (55.636)\tPrec@5 93.000 (96.000)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.2271 (9.2935)\tPrec@1 60.000 (54.619)\tPrec@5 94.000 (95.476)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 8.1950 (9.3678)\tPrec@1 58.000 (54.032)\tPrec@5 94.000 (95.484)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.2162 (9.4208)\tPrec@1 52.000 (53.707)\tPrec@5 93.000 (95.488)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.6742 (9.3142)\tPrec@1 57.000 (54.118)\tPrec@5 95.000 (95.667)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.0204 (9.2680)\tPrec@1 58.000 (54.230)\tPrec@5 98.000 (95.639)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.9863 (9.2167)\tPrec@1 55.000 (54.493)\tPrec@5 97.000 (95.746)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.6135 (9.1423)\tPrec@1 55.000 (54.790)\tPrec@5 94.000 (95.827)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.3668 (9.2307)\tPrec@1 53.000 (54.407)\tPrec@5 98.000 (95.857)\n",
      "val Results: Prec@1 54.420 Prec@5 95.880 Loss 9.25771\n",
      "val Class Accuracy: [0.981,0.986,0.580,0.714,0.429,0.684,0.491,0.472,0.099,0.006]\n",
      "Best Prec@1: 65.380\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [112][0/97], lr: 0.01000\tTime 0.420 (0.420)\tData 0.228 (0.228)\tLoss 1.6726 (1.6726)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [112][10/97], lr: 0.01000\tTime 0.327 (0.342)\tData 0.000 (0.035)\tLoss 2.3389 (2.0865)\tPrec@1 84.375 (86.435)\tPrec@5 99.219 (98.935)\n",
      "Epoch: [112][20/97], lr: 0.01000\tTime 0.326 (0.335)\tData 0.000 (0.027)\tLoss 1.7045 (2.0740)\tPrec@1 88.281 (86.979)\tPrec@5 100.000 (99.107)\n",
      "Epoch: [112][30/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.024)\tLoss 1.8153 (2.0470)\tPrec@1 88.281 (87.399)\tPrec@5 100.000 (99.143)\n",
      "Epoch: [112][40/97], lr: 0.01000\tTime 0.330 (0.333)\tData 0.000 (0.022)\tLoss 1.8080 (2.0550)\tPrec@1 89.062 (87.576)\tPrec@5 100.000 (99.181)\n",
      "Epoch: [112][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 2.2824 (2.0449)\tPrec@1 85.938 (87.607)\tPrec@5 99.219 (99.203)\n",
      "Epoch: [112][60/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 1.8698 (2.0702)\tPrec@1 90.625 (87.410)\tPrec@5 100.000 (99.232)\n",
      "Epoch: [112][70/97], lr: 0.01000\tTime 0.336 (0.332)\tData 0.000 (0.020)\tLoss 2.2260 (2.0681)\tPrec@1 85.938 (87.445)\tPrec@5 100.000 (99.252)\n",
      "Epoch: [112][80/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 2.5831 (2.0696)\tPrec@1 82.812 (87.481)\tPrec@5 98.438 (99.228)\n",
      "Epoch: [112][90/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.019)\tLoss 1.4441 (2.0591)\tPrec@1 92.969 (87.491)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [112][96/97], lr: 0.01000\tTime 0.321 (0.331)\tData 0.000 (0.020)\tLoss 1.5933 (2.0740)\tPrec@1 89.831 (87.425)\tPrec@5 100.000 (99.210)\n",
      "Test: [0/100]\tTime 0.264 (0.264)\tLoss 9.7117 (9.7117)\tPrec@1 52.000 (52.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.9880 (8.8749)\tPrec@1 64.000 (56.818)\tPrec@5 98.000 (97.545)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.7093 (8.8939)\tPrec@1 62.000 (56.429)\tPrec@5 97.000 (96.952)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.5823 (8.9306)\tPrec@1 54.000 (56.387)\tPrec@5 98.000 (96.839)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 9.3219 (8.9670)\tPrec@1 56.000 (56.512)\tPrec@5 94.000 (96.634)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 8.5482 (8.8554)\tPrec@1 60.000 (57.118)\tPrec@5 99.000 (96.725)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.1837 (8.8519)\tPrec@1 65.000 (56.967)\tPrec@5 97.000 (96.754)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.9673 (8.7985)\tPrec@1 54.000 (57.169)\tPrec@5 98.000 (96.873)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.7864 (8.7732)\tPrec@1 58.000 (57.247)\tPrec@5 93.000 (96.778)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.2678 (8.8179)\tPrec@1 60.000 (57.011)\tPrec@5 98.000 (96.813)\n",
      "val Results: Prec@1 57.080 Prec@5 96.780 Loss 8.82296\n",
      "val Class Accuracy: [0.958,0.991,0.610,0.888,0.584,0.395,0.601,0.299,0.312,0.070]\n",
      "Best Prec@1: 65.380\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [113][0/97], lr: 0.01000\tTime 0.406 (0.406)\tData 0.202 (0.202)\tLoss 2.1590 (2.1590)\tPrec@1 87.500 (87.500)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [113][10/97], lr: 0.01000\tTime 0.331 (0.345)\tData 0.000 (0.033)\tLoss 1.8391 (1.9291)\tPrec@1 90.625 (88.565)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [113][20/97], lr: 0.01000\tTime 0.337 (0.337)\tData 0.000 (0.026)\tLoss 1.7030 (2.0408)\tPrec@1 88.281 (87.649)\tPrec@5 100.000 (99.293)\n",
      "Epoch: [113][30/97], lr: 0.01000\tTime 0.325 (0.335)\tData 0.000 (0.023)\tLoss 1.9445 (2.0769)\tPrec@1 90.625 (87.450)\tPrec@5 99.219 (99.370)\n",
      "Epoch: [113][40/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.022)\tLoss 2.6903 (2.0965)\tPrec@1 82.812 (87.348)\tPrec@5 98.438 (99.333)\n",
      "Epoch: [113][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 1.7488 (2.1127)\tPrec@1 90.625 (87.240)\tPrec@5 100.000 (99.372)\n",
      "Epoch: [113][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.1484 (2.1188)\tPrec@1 84.375 (87.116)\tPrec@5 99.219 (99.385)\n",
      "Epoch: [113][70/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 1.4907 (2.0976)\tPrec@1 91.406 (87.159)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [113][80/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.019)\tLoss 1.4526 (2.0890)\tPrec@1 92.188 (87.259)\tPrec@5 99.219 (99.363)\n",
      "Epoch: [113][90/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.019)\tLoss 2.6178 (2.0805)\tPrec@1 83.594 (87.191)\tPrec@5 98.438 (99.339)\n",
      "Epoch: [113][96/97], lr: 0.01000\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 2.4237 (2.0968)\tPrec@1 83.898 (87.071)\tPrec@5 100.000 (99.347)\n",
      "Test: [0/100]\tTime 0.243 (0.243)\tLoss 8.8579 (8.8579)\tPrec@1 56.000 (56.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 6.4123 (8.1367)\tPrec@1 66.000 (60.091)\tPrec@5 96.000 (96.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.6086 (7.9892)\tPrec@1 67.000 (61.143)\tPrec@5 99.000 (96.762)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 6.9911 (7.9804)\tPrec@1 63.000 (61.161)\tPrec@5 99.000 (96.613)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.1818 (7.9640)\tPrec@1 61.000 (61.341)\tPrec@5 96.000 (96.439)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.2264 (7.8655)\tPrec@1 64.000 (61.765)\tPrec@5 97.000 (96.549)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.3753 (7.8090)\tPrec@1 69.000 (61.967)\tPrec@5 98.000 (96.639)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.5970 (7.8022)\tPrec@1 61.000 (61.986)\tPrec@5 96.000 (96.746)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.9709 (7.7596)\tPrec@1 62.000 (62.099)\tPrec@5 98.000 (96.852)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.8243 (7.8208)\tPrec@1 61.000 (61.769)\tPrec@5 99.000 (96.890)\n",
      "val Results: Prec@1 61.660 Prec@5 96.950 Loss 7.83624\n",
      "val Class Accuracy: [0.983,0.947,0.686,0.743,0.705,0.590,0.566,0.586,0.343,0.017]\n",
      "Best Prec@1: 65.380\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [114][0/97], lr: 0.01000\tTime 0.458 (0.458)\tData 0.222 (0.222)\tLoss 2.3181 (2.3181)\tPrec@1 82.812 (82.812)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [114][10/97], lr: 0.01000\tTime 0.328 (0.346)\tData 0.000 (0.035)\tLoss 2.2588 (1.9298)\tPrec@1 85.938 (88.139)\tPrec@5 99.219 (99.574)\n",
      "Epoch: [114][20/97], lr: 0.01000\tTime 0.325 (0.337)\tData 0.000 (0.027)\tLoss 1.8114 (2.0643)\tPrec@1 88.281 (87.500)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [114][30/97], lr: 0.01000\tTime 0.325 (0.335)\tData 0.000 (0.024)\tLoss 1.7503 (2.0286)\tPrec@1 90.625 (87.727)\tPrec@5 100.000 (99.244)\n",
      "Epoch: [114][40/97], lr: 0.01000\tTime 0.329 (0.333)\tData 0.000 (0.022)\tLoss 2.6518 (2.0483)\tPrec@1 83.594 (87.290)\tPrec@5 100.000 (99.276)\n",
      "Epoch: [114][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 2.1865 (2.0619)\tPrec@1 85.938 (87.224)\tPrec@5 99.219 (99.173)\n",
      "Epoch: [114][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 1.7331 (2.0371)\tPrec@1 89.062 (87.487)\tPrec@5 100.000 (99.180)\n",
      "Epoch: [114][70/97], lr: 0.01000\tTime 0.327 (0.331)\tData 0.000 (0.020)\tLoss 2.6427 (2.0958)\tPrec@1 83.594 (87.082)\tPrec@5 100.000 (99.197)\n",
      "Epoch: [114][80/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 2.6685 (2.1038)\tPrec@1 82.812 (87.047)\tPrec@5 98.438 (99.209)\n",
      "Epoch: [114][90/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.019)\tLoss 1.4187 (2.0885)\tPrec@1 92.188 (87.122)\tPrec@5 100.000 (99.253)\n",
      "Epoch: [114][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 1.3192 (2.0808)\tPrec@1 93.220 (87.192)\tPrec@5 99.153 (99.266)\n",
      "Test: [0/100]\tTime 0.238 (0.238)\tLoss 9.9508 (9.9508)\tPrec@1 49.000 (49.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 6.6659 (8.3709)\tPrec@1 66.000 (58.091)\tPrec@5 99.000 (96.091)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.1339 (8.3352)\tPrec@1 61.000 (58.762)\tPrec@5 96.000 (95.905)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.9605 (8.3604)\tPrec@1 58.000 (58.645)\tPrec@5 96.000 (95.774)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.0231 (8.3503)\tPrec@1 56.000 (58.659)\tPrec@5 91.000 (95.537)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.6742 (8.2623)\tPrec@1 62.000 (59.157)\tPrec@5 94.000 (95.667)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.3816 (8.2302)\tPrec@1 66.000 (59.066)\tPrec@5 96.000 (95.525)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.8036 (8.2326)\tPrec@1 60.000 (58.972)\tPrec@5 96.000 (95.535)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.1762 (8.1880)\tPrec@1 62.000 (59.358)\tPrec@5 92.000 (95.580)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.1585 (8.2411)\tPrec@1 61.000 (59.220)\tPrec@5 96.000 (95.571)\n",
      "val Results: Prec@1 59.260 Prec@5 95.450 Loss 8.25293\n",
      "val Class Accuracy: [0.976,0.980,0.813,0.574,0.699,0.497,0.555,0.403,0.295,0.134]\n",
      "Best Prec@1: 65.380\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [115][0/97], lr: 0.01000\tTime 0.435 (0.435)\tData 0.206 (0.206)\tLoss 1.8598 (1.8598)\tPrec@1 90.625 (90.625)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [115][10/97], lr: 0.01000\tTime 0.324 (0.342)\tData 0.000 (0.034)\tLoss 2.3918 (1.7685)\tPrec@1 88.281 (89.631)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [115][20/97], lr: 0.01000\tTime 0.324 (0.335)\tData 0.000 (0.026)\tLoss 2.5317 (1.9140)\tPrec@1 84.375 (88.356)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [115][30/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.023)\tLoss 1.8738 (2.0545)\tPrec@1 88.281 (87.349)\tPrec@5 100.000 (99.345)\n",
      "Epoch: [115][40/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.022)\tLoss 2.8291 (2.1056)\tPrec@1 84.375 (86.966)\tPrec@5 100.000 (99.333)\n",
      "Epoch: [115][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 2.1890 (2.1045)\tPrec@1 88.281 (86.994)\tPrec@5 99.219 (99.357)\n",
      "Epoch: [115][60/97], lr: 0.01000\tTime 0.328 (0.331)\tData 0.000 (0.020)\tLoss 2.0480 (2.1106)\tPrec@1 86.719 (86.962)\tPrec@5 100.000 (99.347)\n",
      "Epoch: [115][70/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 2.6453 (2.1408)\tPrec@1 81.250 (86.752)\tPrec@5 99.219 (99.263)\n",
      "Epoch: [115][80/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 2.4199 (2.1474)\tPrec@1 83.594 (86.825)\tPrec@5 100.000 (99.238)\n",
      "Epoch: [115][90/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.019)\tLoss 2.0902 (2.1865)\tPrec@1 86.719 (86.530)\tPrec@5 100.000 (99.245)\n",
      "Epoch: [115][96/97], lr: 0.01000\tTime 0.317 (0.330)\tData 0.000 (0.020)\tLoss 2.0935 (2.1869)\tPrec@1 88.136 (86.515)\tPrec@5 99.153 (99.234)\n",
      "Test: [0/100]\tTime 0.266 (0.266)\tLoss 7.2773 (7.2773)\tPrec@1 66.000 (66.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 5.2690 (7.3901)\tPrec@1 75.000 (64.182)\tPrec@5 97.000 (97.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.8524 (7.4021)\tPrec@1 61.000 (63.571)\tPrec@5 98.000 (97.095)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.7777 (7.3300)\tPrec@1 66.000 (64.355)\tPrec@5 98.000 (96.710)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 7.3005 (7.3887)\tPrec@1 65.000 (64.293)\tPrec@5 96.000 (96.805)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.9251 (7.3032)\tPrec@1 66.000 (64.765)\tPrec@5 97.000 (96.804)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.3364 (7.3381)\tPrec@1 68.000 (64.607)\tPrec@5 96.000 (96.738)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 6.7594 (7.2999)\tPrec@1 69.000 (64.887)\tPrec@5 98.000 (96.789)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.8676 (7.2633)\tPrec@1 67.000 (65.062)\tPrec@5 96.000 (96.840)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.4410 (7.3095)\tPrec@1 67.000 (64.846)\tPrec@5 98.000 (96.824)\n",
      "val Results: Prec@1 64.730 Prec@5 96.900 Loss 7.32656\n",
      "val Class Accuracy: [0.842,0.995,0.798,0.586,0.629,0.552,0.842,0.591,0.594,0.044]\n",
      "Best Prec@1: 65.380\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [116][0/97], lr: 0.01000\tTime 0.415 (0.415)\tData 0.198 (0.198)\tLoss 1.6835 (1.6835)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [116][10/97], lr: 0.01000\tTime 0.325 (0.340)\tData 0.000 (0.033)\tLoss 1.9254 (2.2852)\tPrec@1 89.062 (86.009)\tPrec@5 100.000 (99.077)\n",
      "Epoch: [116][20/97], lr: 0.01000\tTime 0.340 (0.335)\tData 0.000 (0.026)\tLoss 2.2058 (2.2490)\tPrec@1 87.500 (86.347)\tPrec@5 98.438 (99.144)\n",
      "Epoch: [116][30/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.023)\tLoss 2.4214 (2.2037)\tPrec@1 85.938 (86.794)\tPrec@5 98.438 (99.143)\n",
      "Epoch: [116][40/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.022)\tLoss 1.2507 (2.1477)\tPrec@1 91.406 (87.138)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [116][50/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.021)\tLoss 2.7462 (2.1653)\tPrec@1 82.812 (86.903)\tPrec@5 98.438 (99.157)\n",
      "Epoch: [116][60/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 1.4369 (2.1456)\tPrec@1 89.844 (87.077)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [116][70/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 1.7056 (2.1388)\tPrec@1 89.062 (87.038)\tPrec@5 98.438 (99.252)\n",
      "Epoch: [116][80/97], lr: 0.01000\tTime 0.323 (0.330)\tData 0.000 (0.019)\tLoss 2.9414 (2.1525)\tPrec@1 81.250 (87.037)\tPrec@5 99.219 (99.257)\n",
      "Epoch: [116][90/97], lr: 0.01000\tTime 0.323 (0.330)\tData 0.000 (0.019)\tLoss 2.6125 (2.1584)\tPrec@1 82.812 (86.916)\tPrec@5 99.219 (99.236)\n",
      "Epoch: [116][96/97], lr: 0.01000\tTime 0.317 (0.330)\tData 0.000 (0.020)\tLoss 2.3226 (2.1601)\tPrec@1 85.593 (86.869)\tPrec@5 99.153 (99.210)\n",
      "Test: [0/100]\tTime 0.253 (0.253)\tLoss 10.8150 (10.8150)\tPrec@1 49.000 (49.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 8.8725 (9.7372)\tPrec@1 56.000 (52.182)\tPrec@5 93.000 (95.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.4185 (9.6321)\tPrec@1 61.000 (52.286)\tPrec@5 92.000 (94.857)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 9.4290 (9.6219)\tPrec@1 50.000 (52.548)\tPrec@5 96.000 (94.677)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.5587 (9.5689)\tPrec@1 57.000 (52.878)\tPrec@5 91.000 (95.000)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.8696 (9.4961)\tPrec@1 57.000 (53.451)\tPrec@5 97.000 (95.196)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.9419 (9.5237)\tPrec@1 60.000 (53.082)\tPrec@5 92.000 (95.016)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.0500 (9.5295)\tPrec@1 56.000 (53.085)\tPrec@5 98.000 (95.042)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.0550 (9.4883)\tPrec@1 57.000 (53.235)\tPrec@5 94.000 (95.012)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.4420 (9.5734)\tPrec@1 53.000 (52.923)\tPrec@5 97.000 (94.802)\n",
      "val Results: Prec@1 52.860 Prec@5 94.840 Loss 9.59973\n",
      "val Class Accuracy: [0.981,0.984,0.728,0.493,0.564,0.484,0.246,0.567,0.230,0.009]\n",
      "Best Prec@1: 65.380\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [117][0/97], lr: 0.01000\tTime 0.456 (0.456)\tData 0.240 (0.240)\tLoss 2.2666 (2.2666)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [117][10/97], lr: 0.01000\tTime 0.327 (0.350)\tData 0.000 (0.036)\tLoss 1.4152 (1.9502)\tPrec@1 91.406 (88.707)\tPrec@5 98.438 (99.503)\n",
      "Epoch: [117][20/97], lr: 0.01000\tTime 0.327 (0.341)\tData 0.000 (0.027)\tLoss 2.6707 (2.0579)\tPrec@1 82.031 (87.909)\tPrec@5 100.000 (99.293)\n",
      "Epoch: [117][30/97], lr: 0.01000\tTime 0.331 (0.340)\tData 0.000 (0.024)\tLoss 2.0688 (2.0690)\tPrec@1 84.375 (87.550)\tPrec@5 100.000 (99.118)\n",
      "Epoch: [117][40/97], lr: 0.01000\tTime 0.336 (0.339)\tData 0.000 (0.022)\tLoss 2.0287 (2.0059)\tPrec@1 89.844 (87.881)\tPrec@5 99.219 (99.200)\n",
      "Epoch: [117][50/97], lr: 0.01000\tTime 0.325 (0.338)\tData 0.000 (0.021)\tLoss 1.9307 (1.9936)\tPrec@1 86.719 (87.975)\tPrec@5 99.219 (99.265)\n",
      "Epoch: [117][60/97], lr: 0.01000\tTime 0.327 (0.337)\tData 0.000 (0.021)\tLoss 2.0146 (2.0037)\tPrec@1 89.062 (87.961)\tPrec@5 97.656 (99.232)\n",
      "Epoch: [117][70/97], lr: 0.01000\tTime 0.328 (0.336)\tData 0.000 (0.020)\tLoss 2.1962 (2.0605)\tPrec@1 84.375 (87.500)\tPrec@5 99.219 (99.175)\n",
      "Epoch: [117][80/97], lr: 0.01000\tTime 0.325 (0.335)\tData 0.000 (0.020)\tLoss 1.8124 (2.0668)\tPrec@1 89.062 (87.384)\tPrec@5 99.219 (99.171)\n",
      "Epoch: [117][90/97], lr: 0.01000\tTime 0.322 (0.334)\tData 0.000 (0.020)\tLoss 2.6475 (2.0829)\tPrec@1 84.375 (87.328)\tPrec@5 100.000 (99.210)\n",
      "Epoch: [117][96/97], lr: 0.01000\tTime 0.317 (0.333)\tData 0.000 (0.020)\tLoss 2.8889 (2.0965)\tPrec@1 81.356 (87.224)\tPrec@5 100.000 (99.210)\n",
      "Test: [0/100]\tTime 0.249 (0.249)\tLoss 7.1663 (7.1663)\tPrec@1 68.000 (68.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 5.6846 (7.2369)\tPrec@1 69.000 (64.455)\tPrec@5 99.000 (96.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.3227 (7.1024)\tPrec@1 67.000 (64.762)\tPrec@5 99.000 (96.381)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.2034 (7.1257)\tPrec@1 63.000 (64.581)\tPrec@5 96.000 (96.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 6.6260 (7.0801)\tPrec@1 71.000 (65.463)\tPrec@5 95.000 (96.195)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.1701 (6.9984)\tPrec@1 74.000 (65.941)\tPrec@5 96.000 (96.353)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.4662 (6.9634)\tPrec@1 68.000 (66.213)\tPrec@5 94.000 (96.361)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 6.7463 (6.9528)\tPrec@1 67.000 (66.451)\tPrec@5 98.000 (96.324)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.6841 (6.9300)\tPrec@1 70.000 (66.691)\tPrec@5 93.000 (96.407)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.2247 (6.9995)\tPrec@1 65.000 (66.418)\tPrec@5 98.000 (96.473)\n",
      "val Results: Prec@1 66.240 Prec@5 96.410 Loss 7.03269\n",
      "val Class Accuracy: [0.888,0.979,0.739,0.595,0.847,0.670,0.834,0.495,0.439,0.138]\n",
      "Best Prec@1: 66.240\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [118][0/97], lr: 0.01000\tTime 0.426 (0.426)\tData 0.206 (0.206)\tLoss 1.8925 (1.8925)\tPrec@1 87.500 (87.500)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [118][10/97], lr: 0.01000\tTime 0.329 (0.341)\tData 0.000 (0.034)\tLoss 1.5234 (1.9054)\tPrec@1 89.844 (88.565)\tPrec@5 100.000 (98.651)\n",
      "Epoch: [118][20/97], lr: 0.01000\tTime 0.324 (0.335)\tData 0.000 (0.026)\tLoss 2.3483 (2.0411)\tPrec@1 87.500 (87.574)\tPrec@5 99.219 (98.884)\n",
      "Epoch: [118][30/97], lr: 0.01000\tTime 0.318 (0.333)\tData 0.000 (0.023)\tLoss 2.8617 (2.0478)\tPrec@1 83.594 (87.550)\tPrec@5 98.438 (99.093)\n",
      "Epoch: [118][40/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.022)\tLoss 1.9477 (2.0554)\tPrec@1 89.062 (87.367)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [118][50/97], lr: 0.01000\tTime 0.329 (0.331)\tData 0.000 (0.021)\tLoss 2.4585 (2.0275)\tPrec@1 86.719 (87.638)\tPrec@5 100.000 (99.311)\n",
      "Epoch: [118][60/97], lr: 0.01000\tTime 0.327 (0.331)\tData 0.000 (0.020)\tLoss 2.5821 (2.0552)\tPrec@1 83.594 (87.398)\tPrec@5 100.000 (99.283)\n",
      "Epoch: [118][70/97], lr: 0.01000\tTime 0.322 (0.330)\tData 0.000 (0.020)\tLoss 2.4314 (2.0635)\tPrec@1 83.594 (87.302)\tPrec@5 99.219 (99.307)\n",
      "Epoch: [118][80/97], lr: 0.01000\tTime 0.323 (0.330)\tData 0.000 (0.019)\tLoss 1.9811 (2.0853)\tPrec@1 86.719 (87.133)\tPrec@5 98.438 (99.238)\n",
      "Epoch: [118][90/97], lr: 0.01000\tTime 0.327 (0.330)\tData 0.000 (0.019)\tLoss 2.1174 (2.0935)\tPrec@1 86.719 (86.993)\tPrec@5 99.219 (99.236)\n",
      "Epoch: [118][96/97], lr: 0.01000\tTime 0.318 (0.330)\tData 0.000 (0.020)\tLoss 2.1170 (2.0788)\tPrec@1 91.525 (87.176)\tPrec@5 98.305 (99.242)\n",
      "Test: [0/100]\tTime 0.245 (0.245)\tLoss 9.0247 (9.0247)\tPrec@1 55.000 (55.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.3100 (8.5421)\tPrec@1 64.000 (58.273)\tPrec@5 96.000 (96.273)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.0661 (8.4502)\tPrec@1 65.000 (58.810)\tPrec@5 96.000 (95.952)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.7940 (8.4470)\tPrec@1 61.000 (59.000)\tPrec@5 98.000 (95.871)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.0630 (8.4296)\tPrec@1 64.000 (59.390)\tPrec@5 95.000 (96.073)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.5014 (8.3653)\tPrec@1 66.000 (59.843)\tPrec@5 95.000 (96.255)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.9301 (8.3627)\tPrec@1 66.000 (59.557)\tPrec@5 96.000 (96.180)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.1970 (8.3715)\tPrec@1 61.000 (59.451)\tPrec@5 96.000 (96.225)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.6884 (8.3256)\tPrec@1 60.000 (59.691)\tPrec@5 97.000 (96.272)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.8603 (8.3979)\tPrec@1 64.000 (59.451)\tPrec@5 99.000 (96.308)\n",
      "val Results: Prec@1 59.390 Prec@5 96.300 Loss 8.43011\n",
      "val Class Accuracy: [0.904,0.995,0.777,0.419,0.527,0.589,0.741,0.642,0.304,0.041]\n",
      "Best Prec@1: 66.240\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [119][0/97], lr: 0.01000\tTime 0.448 (0.448)\tData 0.221 (0.221)\tLoss 2.2867 (2.2867)\tPrec@1 86.719 (86.719)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [119][10/97], lr: 0.01000\tTime 0.326 (0.346)\tData 0.000 (0.035)\tLoss 1.6827 (1.9581)\tPrec@1 90.625 (88.423)\tPrec@5 99.219 (99.148)\n",
      "Epoch: [119][20/97], lr: 0.01000\tTime 0.328 (0.337)\tData 0.000 (0.027)\tLoss 2.1806 (1.9941)\tPrec@1 88.281 (88.170)\tPrec@5 99.219 (99.256)\n",
      "Epoch: [119][30/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.024)\tLoss 1.6564 (1.9989)\tPrec@1 88.281 (87.979)\tPrec@5 99.219 (99.194)\n",
      "Epoch: [119][40/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.022)\tLoss 1.9599 (2.0422)\tPrec@1 88.281 (87.691)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [119][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 1.8633 (2.0408)\tPrec@1 89.062 (87.714)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [119][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 1.7579 (2.0741)\tPrec@1 89.844 (87.449)\tPrec@5 100.000 (99.270)\n",
      "Epoch: [119][70/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 1.8459 (2.0923)\tPrec@1 90.625 (87.313)\tPrec@5 98.438 (99.241)\n",
      "Epoch: [119][80/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 2.3509 (2.0892)\tPrec@1 84.375 (87.384)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [119][90/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.019)\tLoss 1.9773 (2.0657)\tPrec@1 87.500 (87.577)\tPrec@5 99.219 (99.227)\n",
      "Epoch: [119][96/97], lr: 0.01000\tTime 0.322 (0.331)\tData 0.000 (0.020)\tLoss 2.1122 (2.0830)\tPrec@1 86.441 (87.474)\tPrec@5 98.305 (99.210)\n",
      "Test: [0/100]\tTime 0.224 (0.224)\tLoss 8.0621 (8.0621)\tPrec@1 61.000 (61.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 5.7424 (7.7496)\tPrec@1 74.000 (63.182)\tPrec@5 96.000 (95.455)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 6.6418 (7.5476)\tPrec@1 65.000 (64.000)\tPrec@5 97.000 (95.905)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 6.5463 (7.5712)\tPrec@1 70.000 (64.000)\tPrec@5 95.000 (95.645)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 6.8496 (7.6064)\tPrec@1 64.000 (63.902)\tPrec@5 99.000 (95.585)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.6020 (7.5672)\tPrec@1 64.000 (64.098)\tPrec@5 96.000 (95.647)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 5.9402 (7.5332)\tPrec@1 68.000 (63.836)\tPrec@5 99.000 (95.869)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 6.4766 (7.5162)\tPrec@1 70.000 (63.930)\tPrec@5 98.000 (95.817)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.6958 (7.4985)\tPrec@1 64.000 (64.037)\tPrec@5 94.000 (95.877)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.3270 (7.5659)\tPrec@1 65.000 (63.637)\tPrec@5 99.000 (95.780)\n",
      "val Results: Prec@1 63.620 Prec@5 95.830 Loss 7.58745\n",
      "val Class Accuracy: [0.786,0.989,0.848,0.628,0.813,0.565,0.714,0.440,0.564,0.015]\n",
      "Best Prec@1: 66.240\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [120][0/97], lr: 0.01000\tTime 0.429 (0.429)\tData 0.236 (0.236)\tLoss 2.0854 (2.0854)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [120][10/97], lr: 0.01000\tTime 0.329 (0.341)\tData 0.000 (0.036)\tLoss 2.1675 (2.0215)\tPrec@1 85.938 (88.210)\tPrec@5 99.219 (99.361)\n",
      "Epoch: [120][20/97], lr: 0.01000\tTime 0.327 (0.335)\tData 0.000 (0.027)\tLoss 2.0754 (2.0844)\tPrec@1 85.156 (87.537)\tPrec@5 100.000 (99.330)\n",
      "Epoch: [120][30/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.024)\tLoss 1.5613 (2.1210)\tPrec@1 90.625 (87.248)\tPrec@5 99.219 (99.294)\n",
      "Epoch: [120][40/97], lr: 0.01000\tTime 0.330 (0.332)\tData 0.000 (0.022)\tLoss 1.0533 (2.0606)\tPrec@1 94.531 (87.500)\tPrec@5 100.000 (99.352)\n",
      "Epoch: [120][50/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.021)\tLoss 2.0481 (2.0737)\tPrec@1 85.938 (87.224)\tPrec@5 99.219 (99.311)\n",
      "Epoch: [120][60/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.021)\tLoss 1.6147 (2.0591)\tPrec@1 89.844 (87.359)\tPrec@5 100.000 (99.308)\n",
      "Epoch: [120][70/97], lr: 0.01000\tTime 0.338 (0.331)\tData 0.000 (0.020)\tLoss 1.6829 (2.0768)\tPrec@1 92.969 (87.280)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [120][80/97], lr: 0.01000\tTime 0.328 (0.331)\tData 0.000 (0.020)\tLoss 1.9583 (2.0957)\tPrec@1 87.500 (87.105)\tPrec@5 98.438 (99.248)\n",
      "Epoch: [120][90/97], lr: 0.01000\tTime 0.322 (0.330)\tData 0.000 (0.019)\tLoss 2.1878 (2.1130)\tPrec@1 85.938 (87.011)\tPrec@5 98.438 (99.184)\n",
      "Epoch: [120][96/97], lr: 0.01000\tTime 0.317 (0.330)\tData 0.000 (0.020)\tLoss 1.4785 (2.1097)\tPrec@1 90.678 (86.990)\tPrec@5 97.458 (99.178)\n",
      "Test: [0/100]\tTime 0.246 (0.246)\tLoss 8.3631 (8.3631)\tPrec@1 64.000 (64.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 6.5962 (8.2428)\tPrec@1 70.000 (61.091)\tPrec@5 96.000 (97.000)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.1782 (8.0214)\tPrec@1 69.000 (62.190)\tPrec@5 99.000 (96.952)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 8.1951 (8.0741)\tPrec@1 59.000 (61.645)\tPrec@5 96.000 (96.935)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.7178 (8.0642)\tPrec@1 62.000 (61.610)\tPrec@5 98.000 (96.829)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.9954 (8.0119)\tPrec@1 58.000 (61.706)\tPrec@5 97.000 (96.804)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.8166 (8.0327)\tPrec@1 64.000 (61.361)\tPrec@5 97.000 (96.754)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.7560 (8.0066)\tPrec@1 59.000 (61.507)\tPrec@5 96.000 (96.648)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.2601 (7.9571)\tPrec@1 59.000 (61.852)\tPrec@5 95.000 (96.654)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.2076 (8.0338)\tPrec@1 61.000 (61.418)\tPrec@5 97.000 (96.637)\n",
      "val Results: Prec@1 61.560 Prec@5 96.640 Loss 8.01489\n",
      "val Class Accuracy: [0.880,0.991,0.751,0.884,0.653,0.496,0.532,0.399,0.426,0.144]\n",
      "Best Prec@1: 66.240\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [121][0/97], lr: 0.01000\tTime 0.442 (0.442)\tData 0.217 (0.217)\tLoss 1.4424 (1.4424)\tPrec@1 92.969 (92.969)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [121][10/97], lr: 0.01000\tTime 0.326 (0.342)\tData 0.000 (0.034)\tLoss 1.8654 (1.8218)\tPrec@1 86.719 (89.347)\tPrec@5 100.000 (99.716)\n",
      "Epoch: [121][20/97], lr: 0.01000\tTime 0.326 (0.336)\tData 0.000 (0.026)\tLoss 1.9234 (1.8800)\tPrec@1 87.500 (88.802)\tPrec@5 100.000 (99.628)\n",
      "Epoch: [121][30/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.023)\tLoss 1.6014 (1.9029)\tPrec@1 88.281 (88.458)\tPrec@5 99.219 (99.471)\n",
      "Epoch: [121][40/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.022)\tLoss 2.1717 (1.9453)\tPrec@1 84.375 (88.034)\tPrec@5 100.000 (99.447)\n",
      "Epoch: [121][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 2.4178 (1.9736)\tPrec@1 85.156 (87.898)\tPrec@5 99.219 (99.403)\n",
      "Epoch: [121][60/97], lr: 0.01000\tTime 0.334 (0.332)\tData 0.000 (0.020)\tLoss 2.9727 (2.0214)\tPrec@1 82.812 (87.436)\tPrec@5 98.438 (99.424)\n",
      "Epoch: [121][70/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.020)\tLoss 2.5260 (2.0323)\tPrec@1 87.500 (87.412)\tPrec@5 99.219 (99.406)\n",
      "Epoch: [121][80/97], lr: 0.01000\tTime 0.329 (0.331)\tData 0.000 (0.020)\tLoss 1.5272 (2.0207)\tPrec@1 91.406 (87.490)\tPrec@5 99.219 (99.392)\n",
      "Epoch: [121][90/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.019)\tLoss 2.3869 (2.0284)\tPrec@1 85.156 (87.380)\tPrec@5 97.656 (99.408)\n",
      "Epoch: [121][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 2.3790 (2.0499)\tPrec@1 83.898 (87.200)\tPrec@5 100.000 (99.395)\n",
      "Test: [0/100]\tTime 0.239 (0.239)\tLoss 6.8546 (6.8546)\tPrec@1 67.000 (67.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 6.0021 (7.1060)\tPrec@1 68.000 (64.818)\tPrec@5 98.000 (98.273)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.7843 (6.9632)\tPrec@1 72.000 (65.714)\tPrec@5 99.000 (98.048)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 5.8787 (6.9460)\tPrec@1 74.000 (65.903)\tPrec@5 97.000 (97.871)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 6.6029 (6.9741)\tPrec@1 67.000 (65.780)\tPrec@5 98.000 (97.707)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.6880 (6.9089)\tPrec@1 65.000 (65.902)\tPrec@5 97.000 (97.706)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 5.9297 (6.8964)\tPrec@1 70.000 (65.967)\tPrec@5 96.000 (97.705)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 6.8382 (6.8628)\tPrec@1 64.000 (66.141)\tPrec@5 99.000 (97.732)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.2765 (6.8358)\tPrec@1 64.000 (66.185)\tPrec@5 96.000 (97.679)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.4162 (6.8597)\tPrec@1 72.000 (66.209)\tPrec@5 100.000 (97.747)\n",
      "val Results: Prec@1 66.330 Prec@5 97.740 Loss 6.85122\n",
      "val Class Accuracy: [0.960,0.991,0.684,0.839,0.674,0.475,0.745,0.564,0.581,0.120]\n",
      "Best Prec@1: 66.330\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [122][0/97], lr: 0.01000\tTime 0.462 (0.462)\tData 0.262 (0.262)\tLoss 1.7754 (1.7754)\tPrec@1 88.281 (88.281)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [122][10/97], lr: 0.01000\tTime 0.328 (0.353)\tData 0.000 (0.039)\tLoss 1.4007 (1.8589)\tPrec@1 92.188 (88.778)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [122][20/97], lr: 0.01000\tTime 0.333 (0.341)\tData 0.000 (0.028)\tLoss 2.2673 (1.9257)\tPrec@1 88.281 (88.244)\tPrec@5 99.219 (99.554)\n",
      "Epoch: [122][30/97], lr: 0.01000\tTime 0.324 (0.337)\tData 0.000 (0.025)\tLoss 1.5714 (2.0033)\tPrec@1 92.969 (87.777)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [122][40/97], lr: 0.01000\tTime 0.324 (0.335)\tData 0.000 (0.023)\tLoss 2.5339 (1.9788)\tPrec@1 86.719 (88.148)\tPrec@5 100.000 (99.352)\n",
      "Epoch: [122][50/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.022)\tLoss 2.5482 (1.9983)\tPrec@1 83.594 (87.990)\tPrec@5 99.219 (99.357)\n",
      "Epoch: [122][60/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 1.8108 (2.0202)\tPrec@1 89.844 (87.948)\tPrec@5 100.000 (99.347)\n",
      "Epoch: [122][70/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 1.7172 (2.0578)\tPrec@1 88.281 (87.577)\tPrec@5 100.000 (99.318)\n",
      "Epoch: [122][80/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 1.3653 (2.0450)\tPrec@1 92.188 (87.548)\tPrec@5 99.219 (99.344)\n",
      "Epoch: [122][90/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 1.7464 (2.0485)\tPrec@1 89.062 (87.560)\tPrec@5 99.219 (99.313)\n",
      "Epoch: [122][96/97], lr: 0.01000\tTime 0.320 (0.331)\tData 0.000 (0.020)\tLoss 1.7651 (2.0592)\tPrec@1 88.136 (87.522)\tPrec@5 99.153 (99.307)\n",
      "Test: [0/100]\tTime 0.249 (0.249)\tLoss 8.0378 (8.0378)\tPrec@1 62.000 (62.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 5.8848 (7.0344)\tPrec@1 74.000 (66.000)\tPrec@5 96.000 (97.091)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.8397 (6.8790)\tPrec@1 68.000 (66.952)\tPrec@5 99.000 (97.333)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.1423 (6.8885)\tPrec@1 71.000 (67.258)\tPrec@5 97.000 (97.387)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 6.0863 (6.8708)\tPrec@1 69.000 (67.146)\tPrec@5 98.000 (97.220)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.2091 (6.8271)\tPrec@1 71.000 (67.255)\tPrec@5 99.000 (97.275)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.1878 (6.8211)\tPrec@1 68.000 (67.098)\tPrec@5 98.000 (97.377)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 6.0597 (6.8190)\tPrec@1 70.000 (67.056)\tPrec@5 99.000 (97.324)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.5156 (6.7751)\tPrec@1 73.000 (67.296)\tPrec@5 95.000 (97.395)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.9461 (6.8520)\tPrec@1 68.000 (66.901)\tPrec@5 99.000 (97.385)\n",
      "val Results: Prec@1 66.830 Prec@5 97.420 Loss 6.86384\n",
      "val Class Accuracy: [0.923,0.989,0.833,0.606,0.795,0.649,0.558,0.659,0.456,0.215]\n",
      "Best Prec@1: 66.830\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [123][0/97], lr: 0.01000\tTime 0.432 (0.432)\tData 0.218 (0.218)\tLoss 2.1627 (2.1627)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [123][10/97], lr: 0.01000\tTime 0.327 (0.345)\tData 0.000 (0.034)\tLoss 1.6732 (2.1111)\tPrec@1 89.844 (86.364)\tPrec@5 98.438 (99.290)\n",
      "Epoch: [123][20/97], lr: 0.01000\tTime 0.326 (0.336)\tData 0.000 (0.026)\tLoss 2.7059 (2.1105)\tPrec@1 78.906 (86.421)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [123][30/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.023)\tLoss 2.9682 (2.0760)\tPrec@1 83.594 (86.920)\tPrec@5 98.438 (99.395)\n",
      "Epoch: [123][40/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.022)\tLoss 2.4026 (2.0575)\tPrec@1 84.375 (86.947)\tPrec@5 100.000 (99.352)\n",
      "Epoch: [123][50/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 2.4229 (2.0653)\tPrec@1 84.375 (86.964)\tPrec@5 99.219 (99.403)\n",
      "Epoch: [123][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 1.7628 (2.0294)\tPrec@1 89.844 (87.282)\tPrec@5 97.656 (99.398)\n",
      "Epoch: [123][70/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 2.3490 (2.0463)\tPrec@1 85.938 (87.302)\tPrec@5 98.438 (99.307)\n",
      "Epoch: [123][80/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 2.1037 (2.0517)\tPrec@1 87.500 (87.201)\tPrec@5 100.000 (99.306)\n",
      "Epoch: [123][90/97], lr: 0.01000\tTime 0.329 (0.331)\tData 0.000 (0.019)\tLoss 2.7844 (2.0696)\tPrec@1 82.812 (87.062)\tPrec@5 96.875 (99.270)\n",
      "Epoch: [123][96/97], lr: 0.01000\tTime 0.319 (0.331)\tData 0.000 (0.020)\tLoss 1.5442 (2.0779)\tPrec@1 92.373 (87.030)\tPrec@5 100.000 (99.234)\n",
      "Test: [0/100]\tTime 0.229 (0.229)\tLoss 9.8431 (9.8431)\tPrec@1 54.000 (54.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 7.9465 (9.2768)\tPrec@1 62.000 (55.364)\tPrec@5 95.000 (92.818)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.3195 (9.2535)\tPrec@1 65.000 (55.143)\tPrec@5 95.000 (93.143)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.2657 (9.2080)\tPrec@1 52.000 (55.613)\tPrec@5 96.000 (92.935)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.9811 (9.1982)\tPrec@1 58.000 (55.805)\tPrec@5 96.000 (93.122)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.2373 (9.1537)\tPrec@1 53.000 (55.824)\tPrec@5 95.000 (93.294)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.8411 (9.1170)\tPrec@1 66.000 (55.803)\tPrec@5 95.000 (93.295)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.1858 (9.0769)\tPrec@1 56.000 (56.000)\tPrec@5 95.000 (93.169)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.8724 (9.0231)\tPrec@1 58.000 (56.222)\tPrec@5 90.000 (93.235)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.9320 (9.1021)\tPrec@1 57.000 (55.780)\tPrec@5 97.000 (93.209)\n",
      "val Results: Prec@1 55.830 Prec@5 93.130 Loss 9.10088\n",
      "val Class Accuracy: [0.949,0.982,0.757,0.796,0.339,0.708,0.208,0.424,0.400,0.020]\n",
      "Best Prec@1: 66.830\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [124][0/97], lr: 0.01000\tTime 0.457 (0.457)\tData 0.228 (0.228)\tLoss 1.6211 (1.6211)\tPrec@1 88.281 (88.281)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [124][10/97], lr: 0.01000\tTime 0.327 (0.344)\tData 0.000 (0.036)\tLoss 1.6211 (1.9764)\tPrec@1 89.844 (88.139)\tPrec@5 100.000 (99.716)\n",
      "Epoch: [124][20/97], lr: 0.01000\tTime 0.337 (0.336)\tData 0.000 (0.027)\tLoss 2.1704 (1.8931)\tPrec@1 86.719 (88.653)\tPrec@5 100.000 (99.628)\n",
      "Epoch: [124][30/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.024)\tLoss 1.7983 (1.8963)\tPrec@1 91.406 (88.760)\tPrec@5 99.219 (99.572)\n",
      "Epoch: [124][40/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.022)\tLoss 2.3558 (1.9249)\tPrec@1 84.375 (88.720)\tPrec@5 97.656 (99.371)\n",
      "Epoch: [124][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 2.2876 (1.9676)\tPrec@1 87.500 (88.343)\tPrec@5 99.219 (99.357)\n",
      "Epoch: [124][60/97], lr: 0.01000\tTime 0.327 (0.331)\tData 0.000 (0.021)\tLoss 1.4773 (1.9759)\tPrec@1 92.188 (88.358)\tPrec@5 99.219 (99.283)\n",
      "Epoch: [124][70/97], lr: 0.01000\tTime 0.327 (0.331)\tData 0.000 (0.020)\tLoss 1.5391 (1.9922)\tPrec@1 90.625 (88.072)\tPrec@5 100.000 (99.318)\n",
      "Epoch: [124][80/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 2.5805 (2.0188)\tPrec@1 84.375 (87.857)\tPrec@5 99.219 (99.296)\n",
      "Epoch: [124][90/97], lr: 0.01000\tTime 0.327 (0.331)\tData 0.000 (0.019)\tLoss 1.7934 (2.0135)\tPrec@1 89.844 (87.843)\tPrec@5 99.219 (99.305)\n",
      "Epoch: [124][96/97], lr: 0.01000\tTime 0.333 (0.331)\tData 0.000 (0.020)\tLoss 2.2824 (2.0404)\tPrec@1 85.593 (87.683)\tPrec@5 100.000 (99.299)\n",
      "Test: [0/100]\tTime 0.257 (0.257)\tLoss 10.1022 (10.1022)\tPrec@1 52.000 (52.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 7.0060 (8.8339)\tPrec@1 66.000 (57.909)\tPrec@5 96.000 (95.545)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.3629 (8.8563)\tPrec@1 61.000 (58.143)\tPrec@5 100.000 (95.571)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.8955 (8.8142)\tPrec@1 63.000 (58.194)\tPrec@5 95.000 (95.516)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.4596 (8.7970)\tPrec@1 56.000 (58.439)\tPrec@5 93.000 (95.415)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.6387 (8.6815)\tPrec@1 66.000 (59.000)\tPrec@5 95.000 (95.392)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.5305 (8.6281)\tPrec@1 71.000 (58.951)\tPrec@5 99.000 (95.525)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.4986 (8.6047)\tPrec@1 59.000 (59.099)\tPrec@5 96.000 (95.563)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.1551 (8.5710)\tPrec@1 60.000 (59.247)\tPrec@5 91.000 (95.642)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.4902 (8.6529)\tPrec@1 59.000 (58.868)\tPrec@5 98.000 (95.549)\n",
      "val Results: Prec@1 59.000 Prec@5 95.480 Loss 8.66606\n",
      "val Class Accuracy: [0.975,0.979,0.775,0.742,0.707,0.615,0.545,0.412,0.066,0.084]\n",
      "Best Prec@1: 66.830\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [125][0/97], lr: 0.01000\tTime 0.375 (0.375)\tData 0.180 (0.180)\tLoss 1.9543 (1.9543)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [125][10/97], lr: 0.01000\tTime 0.326 (0.339)\tData 0.000 (0.031)\tLoss 2.4036 (2.0567)\tPrec@1 85.938 (87.074)\tPrec@5 97.656 (98.935)\n",
      "Epoch: [125][20/97], lr: 0.01000\tTime 0.368 (0.336)\tData 0.000 (0.025)\tLoss 2.1075 (2.0927)\tPrec@1 87.500 (87.351)\tPrec@5 99.219 (99.107)\n",
      "Epoch: [125][30/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.022)\tLoss 1.0945 (2.0553)\tPrec@1 93.750 (87.676)\tPrec@5 100.000 (99.168)\n",
      "Epoch: [125][40/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.021)\tLoss 1.7560 (1.9965)\tPrec@1 89.062 (87.919)\tPrec@5 99.219 (99.143)\n",
      "Epoch: [125][50/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.020)\tLoss 2.5342 (2.0120)\tPrec@1 85.156 (87.822)\tPrec@5 99.219 (99.280)\n",
      "Epoch: [125][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.0535 (2.0299)\tPrec@1 86.719 (87.666)\tPrec@5 98.438 (99.232)\n",
      "Epoch: [125][70/97], lr: 0.01000\tTime 0.331 (0.332)\tData 0.000 (0.019)\tLoss 2.0852 (2.0364)\tPrec@1 87.500 (87.577)\tPrec@5 98.438 (99.230)\n",
      "Epoch: [125][80/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.019)\tLoss 1.8296 (2.0179)\tPrec@1 87.500 (87.683)\tPrec@5 100.000 (99.199)\n",
      "Epoch: [125][90/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.019)\tLoss 2.2309 (2.0315)\tPrec@1 85.938 (87.577)\tPrec@5 99.219 (99.184)\n",
      "Epoch: [125][96/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.020)\tLoss 2.5547 (2.0490)\tPrec@1 84.746 (87.450)\tPrec@5 99.153 (99.194)\n",
      "Test: [0/100]\tTime 0.268 (0.268)\tLoss 10.2248 (10.2248)\tPrec@1 52.000 (52.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 6.8435 (8.5791)\tPrec@1 70.000 (60.182)\tPrec@5 95.000 (95.909)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.6837 (8.4804)\tPrec@1 67.000 (60.143)\tPrec@5 100.000 (96.238)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.4686 (8.3662)\tPrec@1 63.000 (60.839)\tPrec@5 97.000 (96.290)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 7.9271 (8.3064)\tPrec@1 63.000 (61.220)\tPrec@5 94.000 (95.951)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.5238 (8.2354)\tPrec@1 66.000 (61.725)\tPrec@5 96.000 (95.961)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.2169 (8.1325)\tPrec@1 71.000 (62.148)\tPrec@5 97.000 (95.951)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.2258 (8.1819)\tPrec@1 65.000 (61.803)\tPrec@5 98.000 (96.014)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.8490 (8.1499)\tPrec@1 64.000 (61.988)\tPrec@5 95.000 (95.963)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.3205 (8.2585)\tPrec@1 61.000 (61.341)\tPrec@5 97.000 (95.780)\n",
      "val Results: Prec@1 61.300 Prec@5 95.740 Loss 8.27567\n",
      "val Class Accuracy: [0.956,0.985,0.685,0.447,0.887,0.748,0.472,0.642,0.197,0.111]\n",
      "Best Prec@1: 66.830\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [126][0/97], lr: 0.01000\tTime 0.414 (0.414)\tData 0.194 (0.194)\tLoss 1.5796 (1.5796)\tPrec@1 92.188 (92.188)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [126][10/97], lr: 0.01000\tTime 0.325 (0.339)\tData 0.000 (0.033)\tLoss 1.9289 (1.8858)\tPrec@1 89.062 (88.849)\tPrec@5 99.219 (99.574)\n",
      "Epoch: [126][20/97], lr: 0.01000\tTime 0.322 (0.333)\tData 0.000 (0.025)\tLoss 2.3935 (1.9590)\tPrec@1 85.156 (88.504)\tPrec@5 98.438 (99.330)\n",
      "Epoch: [126][30/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.023)\tLoss 1.9491 (1.9409)\tPrec@1 85.156 (88.281)\tPrec@5 99.219 (99.244)\n",
      "Epoch: [126][40/97], lr: 0.01000\tTime 0.327 (0.331)\tData 0.000 (0.021)\tLoss 2.6567 (1.9774)\tPrec@1 83.594 (87.957)\tPrec@5 99.219 (99.200)\n",
      "Epoch: [126][50/97], lr: 0.01000\tTime 0.323 (0.330)\tData 0.000 (0.021)\tLoss 1.9160 (1.9683)\tPrec@1 86.719 (88.021)\tPrec@5 99.219 (99.249)\n",
      "Epoch: [126][60/97], lr: 0.01000\tTime 0.325 (0.330)\tData 0.000 (0.020)\tLoss 2.2332 (1.9756)\tPrec@1 87.500 (87.987)\tPrec@5 99.219 (99.283)\n",
      "Epoch: [126][70/97], lr: 0.01000\tTime 0.353 (0.330)\tData 0.000 (0.020)\tLoss 1.7571 (1.9640)\tPrec@1 89.844 (87.995)\tPrec@5 99.219 (99.274)\n",
      "Epoch: [126][80/97], lr: 0.01000\tTime 0.322 (0.330)\tData 0.000 (0.019)\tLoss 2.4170 (2.0039)\tPrec@1 85.156 (87.712)\tPrec@5 98.438 (99.277)\n",
      "Epoch: [126][90/97], lr: 0.01000\tTime 0.322 (0.330)\tData 0.000 (0.019)\tLoss 2.1688 (2.0477)\tPrec@1 87.500 (87.448)\tPrec@5 100.000 (99.227)\n",
      "Epoch: [126][96/97], lr: 0.01000\tTime 0.318 (0.329)\tData 0.000 (0.020)\tLoss 2.4037 (2.0593)\tPrec@1 85.593 (87.353)\tPrec@5 99.153 (99.226)\n",
      "Test: [0/100]\tTime 0.259 (0.259)\tLoss 8.8082 (8.8082)\tPrec@1 57.000 (57.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.7499 (8.2630)\tPrec@1 68.000 (60.364)\tPrec@5 96.000 (96.182)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.2458 (8.0972)\tPrec@1 69.000 (61.000)\tPrec@5 96.000 (95.619)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.0823 (8.0481)\tPrec@1 67.000 (61.548)\tPrec@5 97.000 (95.710)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.2751 (8.0454)\tPrec@1 62.000 (61.634)\tPrec@5 93.000 (95.439)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.3305 (7.9377)\tPrec@1 67.000 (62.333)\tPrec@5 97.000 (95.549)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 5.9009 (7.8906)\tPrec@1 73.000 (62.443)\tPrec@5 97.000 (95.574)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 7.2501 (7.8771)\tPrec@1 62.000 (62.549)\tPrec@5 96.000 (95.592)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.5810 (7.8361)\tPrec@1 66.000 (62.815)\tPrec@5 92.000 (95.691)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.7093 (7.9056)\tPrec@1 64.000 (62.527)\tPrec@5 96.000 (95.648)\n",
      "val Results: Prec@1 62.400 Prec@5 95.700 Loss 7.92743\n",
      "val Class Accuracy: [0.925,0.990,0.821,0.611,0.665,0.738,0.674,0.527,0.222,0.067]\n",
      "Best Prec@1: 66.830\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [127][0/97], lr: 0.01000\tTime 0.481 (0.481)\tData 0.248 (0.248)\tLoss 1.4268 (1.4268)\tPrec@1 90.625 (90.625)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [127][10/97], lr: 0.01000\tTime 0.326 (0.346)\tData 0.000 (0.037)\tLoss 2.1601 (1.9587)\tPrec@1 85.938 (88.068)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [127][20/97], lr: 0.01000\tTime 0.326 (0.340)\tData 0.000 (0.028)\tLoss 1.4380 (1.9437)\tPrec@1 92.969 (88.356)\tPrec@5 100.000 (99.702)\n",
      "Epoch: [127][30/97], lr: 0.01000\tTime 0.326 (0.337)\tData 0.000 (0.024)\tLoss 1.6652 (1.9386)\tPrec@1 87.500 (88.256)\tPrec@5 98.438 (99.572)\n",
      "Epoch: [127][40/97], lr: 0.01000\tTime 0.326 (0.336)\tData 0.000 (0.023)\tLoss 1.9342 (1.9734)\tPrec@1 89.844 (88.034)\tPrec@5 99.219 (99.466)\n",
      "Epoch: [127][50/97], lr: 0.01000\tTime 0.326 (0.335)\tData 0.000 (0.022)\tLoss 2.1942 (1.9817)\tPrec@1 86.719 (87.990)\tPrec@5 98.438 (99.357)\n",
      "Epoch: [127][60/97], lr: 0.01000\tTime 0.328 (0.334)\tData 0.000 (0.021)\tLoss 2.8010 (2.0400)\tPrec@1 85.156 (87.615)\tPrec@5 100.000 (99.385)\n",
      "Epoch: [127][70/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.020)\tLoss 1.0754 (2.0356)\tPrec@1 93.750 (87.632)\tPrec@5 100.000 (99.362)\n",
      "Epoch: [127][80/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 1.7645 (2.0178)\tPrec@1 89.844 (87.780)\tPrec@5 100.000 (99.412)\n",
      "Epoch: [127][90/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.020)\tLoss 2.3096 (2.0217)\tPrec@1 85.938 (87.732)\tPrec@5 99.219 (99.433)\n",
      "Epoch: [127][96/97], lr: 0.01000\tTime 0.318 (0.332)\tData 0.000 (0.020)\tLoss 1.7681 (2.0214)\tPrec@1 88.983 (87.683)\tPrec@5 100.000 (99.436)\n",
      "Test: [0/100]\tTime 0.241 (0.241)\tLoss 8.4447 (8.4447)\tPrec@1 60.000 (60.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 6.4112 (7.7360)\tPrec@1 68.000 (62.273)\tPrec@5 94.000 (96.091)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.1907 (7.6533)\tPrec@1 68.000 (62.857)\tPrec@5 97.000 (96.476)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 6.5071 (7.6797)\tPrec@1 66.000 (62.387)\tPrec@5 98.000 (96.548)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.8167 (7.7260)\tPrec@1 61.000 (62.098)\tPrec@5 96.000 (96.488)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.2948 (7.7351)\tPrec@1 63.000 (62.020)\tPrec@5 98.000 (96.529)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 5.7702 (7.6565)\tPrec@1 71.000 (62.164)\tPrec@5 98.000 (96.607)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 6.9057 (7.6472)\tPrec@1 67.000 (62.296)\tPrec@5 100.000 (96.676)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.5202 (7.6235)\tPrec@1 63.000 (62.309)\tPrec@5 96.000 (96.790)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.3849 (7.6854)\tPrec@1 59.000 (62.121)\tPrec@5 98.000 (96.714)\n",
      "val Results: Prec@1 62.090 Prec@5 96.700 Loss 7.69686\n",
      "val Class Accuracy: [0.945,0.993,0.611,0.836,0.815,0.430,0.465,0.663,0.349,0.102]\n",
      "Best Prec@1: 66.830\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [128][0/97], lr: 0.01000\tTime 0.424 (0.424)\tData 0.225 (0.225)\tLoss 2.2596 (2.2596)\tPrec@1 85.156 (85.156)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [128][10/97], lr: 0.01000\tTime 0.325 (0.343)\tData 0.000 (0.035)\tLoss 2.4349 (2.1808)\tPrec@1 84.375 (86.506)\tPrec@5 99.219 (99.148)\n",
      "Epoch: [128][20/97], lr: 0.01000\tTime 0.341 (0.336)\tData 0.000 (0.026)\tLoss 1.9734 (2.0284)\tPrec@1 89.844 (87.723)\tPrec@5 100.000 (99.293)\n",
      "Epoch: [128][30/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.024)\tLoss 2.3411 (2.0844)\tPrec@1 87.500 (87.248)\tPrec@5 98.438 (99.294)\n",
      "Epoch: [128][40/97], lr: 0.01000\tTime 0.331 (0.333)\tData 0.000 (0.022)\tLoss 2.3054 (2.1000)\tPrec@1 85.938 (87.100)\tPrec@5 100.000 (99.333)\n",
      "Epoch: [128][50/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.021)\tLoss 1.9294 (2.0920)\tPrec@1 87.500 (87.148)\tPrec@5 100.000 (99.326)\n",
      "Epoch: [128][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.8511 (2.0592)\tPrec@1 82.031 (87.372)\tPrec@5 99.219 (99.347)\n",
      "Epoch: [128][70/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 1.9998 (2.0786)\tPrec@1 85.938 (87.181)\tPrec@5 100.000 (99.373)\n",
      "Epoch: [128][80/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 2.2593 (2.0909)\tPrec@1 84.375 (87.133)\tPrec@5 99.219 (99.344)\n",
      "Epoch: [128][90/97], lr: 0.01000\tTime 0.327 (0.331)\tData 0.000 (0.019)\tLoss 2.1709 (2.0691)\tPrec@1 85.938 (87.251)\tPrec@5 98.438 (99.382)\n",
      "Epoch: [128][96/97], lr: 0.01000\tTime 0.316 (0.331)\tData 0.000 (0.020)\tLoss 2.0539 (2.0856)\tPrec@1 87.288 (87.208)\tPrec@5 99.153 (99.363)\n",
      "Test: [0/100]\tTime 0.253 (0.253)\tLoss 9.4381 (9.4381)\tPrec@1 52.000 (52.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.9572 (8.8584)\tPrec@1 62.000 (57.818)\tPrec@5 97.000 (97.182)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.8231 (8.7459)\tPrec@1 61.000 (58.476)\tPrec@5 97.000 (97.095)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.0803 (8.6958)\tPrec@1 62.000 (58.677)\tPrec@5 99.000 (97.097)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.5123 (8.5936)\tPrec@1 58.000 (59.415)\tPrec@5 97.000 (97.000)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.7770 (8.5141)\tPrec@1 63.000 (59.745)\tPrec@5 98.000 (97.216)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.4540 (8.5245)\tPrec@1 63.000 (59.557)\tPrec@5 99.000 (97.213)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.8581 (8.4970)\tPrec@1 66.000 (59.789)\tPrec@5 100.000 (97.352)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.8740 (8.4682)\tPrec@1 57.000 (59.802)\tPrec@5 96.000 (97.420)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.9774 (8.5528)\tPrec@1 57.000 (59.374)\tPrec@5 98.000 (97.407)\n",
      "val Results: Prec@1 59.440 Prec@5 97.340 Loss 8.54925\n",
      "val Class Accuracy: [0.969,0.977,0.708,0.894,0.698,0.275,0.286,0.593,0.274,0.270]\n",
      "Best Prec@1: 66.830\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [129][0/97], lr: 0.01000\tTime 0.407 (0.407)\tData 0.214 (0.214)\tLoss 2.0247 (2.0247)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [129][10/97], lr: 0.01000\tTime 0.327 (0.340)\tData 0.000 (0.034)\tLoss 2.1542 (2.0239)\tPrec@1 89.062 (87.358)\tPrec@5 99.219 (99.503)\n",
      "Epoch: [129][20/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.026)\tLoss 1.9524 (1.9567)\tPrec@1 90.625 (87.835)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [129][30/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.023)\tLoss 1.9182 (2.0294)\tPrec@1 87.500 (87.626)\tPrec@5 99.219 (99.244)\n",
      "Epoch: [129][40/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.022)\tLoss 2.2777 (2.0375)\tPrec@1 84.375 (87.405)\tPrec@5 99.219 (99.181)\n",
      "Epoch: [129][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 2.4532 (2.0517)\tPrec@1 85.938 (87.362)\tPrec@5 100.000 (99.188)\n",
      "Epoch: [129][60/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 2.2681 (2.0907)\tPrec@1 85.156 (87.052)\tPrec@5 100.000 (99.116)\n",
      "Epoch: [129][70/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 2.2252 (2.0927)\tPrec@1 85.156 (87.082)\tPrec@5 100.000 (99.153)\n",
      "Epoch: [129][80/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 1.8810 (2.0712)\tPrec@1 89.062 (87.240)\tPrec@5 99.219 (99.122)\n",
      "Epoch: [129][90/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.019)\tLoss 2.5500 (2.0771)\tPrec@1 83.594 (87.028)\tPrec@5 100.000 (99.141)\n",
      "Epoch: [129][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 1.3401 (2.0688)\tPrec@1 91.525 (87.071)\tPrec@5 100.000 (99.162)\n",
      "Test: [0/100]\tTime 0.266 (0.266)\tLoss 8.2071 (8.2071)\tPrec@1 65.000 (65.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 5.8060 (7.8067)\tPrec@1 74.000 (64.000)\tPrec@5 96.000 (97.273)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.5482 (7.8358)\tPrec@1 65.000 (63.619)\tPrec@5 98.000 (97.048)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.1128 (7.8522)\tPrec@1 66.000 (63.258)\tPrec@5 98.000 (97.194)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 7.4471 (7.8519)\tPrec@1 66.000 (63.610)\tPrec@5 94.000 (97.000)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.5234 (7.8044)\tPrec@1 65.000 (63.765)\tPrec@5 98.000 (97.157)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.2635 (7.7556)\tPrec@1 71.000 (63.803)\tPrec@5 97.000 (97.131)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 7.6238 (7.7397)\tPrec@1 65.000 (63.915)\tPrec@5 98.000 (97.254)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.6219 (7.6956)\tPrec@1 70.000 (64.173)\tPrec@5 97.000 (97.370)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.7920 (7.7779)\tPrec@1 64.000 (63.846)\tPrec@5 98.000 (97.330)\n",
      "val Results: Prec@1 63.730 Prec@5 97.300 Loss 7.80113\n",
      "val Class Accuracy: [0.955,0.993,0.791,0.701,0.738,0.484,0.674,0.651,0.219,0.167]\n",
      "Best Prec@1: 66.830\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [130][0/97], lr: 0.01000\tTime 0.443 (0.443)\tData 0.246 (0.246)\tLoss 2.4594 (2.4594)\tPrec@1 85.938 (85.938)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [130][10/97], lr: 0.01000\tTime 0.336 (0.347)\tData 0.000 (0.037)\tLoss 2.0887 (1.9452)\tPrec@1 85.156 (88.707)\tPrec@5 100.000 (99.716)\n",
      "Epoch: [130][20/97], lr: 0.01000\tTime 0.325 (0.339)\tData 0.000 (0.028)\tLoss 1.8840 (1.9261)\tPrec@1 88.281 (88.839)\tPrec@5 100.000 (99.740)\n",
      "Epoch: [130][30/97], lr: 0.01000\tTime 0.330 (0.336)\tData 0.000 (0.024)\tLoss 1.3735 (1.8928)\tPrec@1 92.188 (88.810)\tPrec@5 100.000 (99.672)\n",
      "Epoch: [130][40/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.023)\tLoss 1.4439 (1.8949)\tPrec@1 92.969 (88.643)\tPrec@5 100.000 (99.562)\n",
      "Epoch: [130][50/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.022)\tLoss 2.0727 (1.8989)\tPrec@1 89.062 (88.526)\tPrec@5 99.219 (99.540)\n",
      "Epoch: [130][60/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.021)\tLoss 2.0054 (1.9045)\tPrec@1 92.188 (88.550)\tPrec@5 100.000 (99.501)\n",
      "Epoch: [130][70/97], lr: 0.01000\tTime 0.323 (0.333)\tData 0.000 (0.020)\tLoss 3.3216 (1.9573)\tPrec@1 80.469 (88.237)\tPrec@5 97.656 (99.472)\n",
      "Epoch: [130][80/97], lr: 0.01000\tTime 0.330 (0.333)\tData 0.000 (0.020)\tLoss 1.3868 (1.9537)\tPrec@1 90.625 (88.339)\tPrec@5 100.000 (99.450)\n",
      "Epoch: [130][90/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 1.9296 (1.9676)\tPrec@1 89.062 (88.238)\tPrec@5 100.000 (99.390)\n",
      "Epoch: [130][96/97], lr: 0.01000\tTime 0.317 (0.332)\tData 0.000 (0.020)\tLoss 1.5689 (1.9726)\tPrec@1 88.983 (88.183)\tPrec@5 99.153 (99.363)\n",
      "Test: [0/100]\tTime 0.254 (0.254)\tLoss 7.7417 (7.7417)\tPrec@1 65.000 (65.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 6.3884 (8.1169)\tPrec@1 66.000 (60.818)\tPrec@5 98.000 (95.273)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.4432 (8.0507)\tPrec@1 62.000 (61.714)\tPrec@5 96.000 (95.381)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.3070 (7.9575)\tPrec@1 69.000 (62.258)\tPrec@5 97.000 (95.194)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.9924 (7.9788)\tPrec@1 64.000 (62.268)\tPrec@5 96.000 (95.268)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.0345 (7.9241)\tPrec@1 60.000 (62.490)\tPrec@5 95.000 (95.333)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.1298 (7.9003)\tPrec@1 68.000 (62.262)\tPrec@5 95.000 (95.443)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.8057 (7.8476)\tPrec@1 61.000 (62.606)\tPrec@5 96.000 (95.423)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.1841 (7.8025)\tPrec@1 67.000 (62.753)\tPrec@5 92.000 (95.531)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.4346 (7.8688)\tPrec@1 68.000 (62.505)\tPrec@5 98.000 (95.440)\n",
      "val Results: Prec@1 62.700 Prec@5 95.370 Loss 7.86337\n",
      "val Class Accuracy: [0.956,0.980,0.851,0.733,0.618,0.545,0.587,0.532,0.420,0.048]\n",
      "Best Prec@1: 66.830\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [131][0/97], lr: 0.01000\tTime 0.405 (0.405)\tData 0.184 (0.184)\tLoss 1.8045 (1.8045)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [131][10/97], lr: 0.01000\tTime 0.325 (0.340)\tData 0.000 (0.032)\tLoss 1.7266 (2.2255)\tPrec@1 89.844 (85.866)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [131][20/97], lr: 0.01000\tTime 0.335 (0.334)\tData 0.000 (0.025)\tLoss 2.3638 (2.1460)\tPrec@1 84.375 (86.793)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [131][30/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.022)\tLoss 2.0323 (2.1461)\tPrec@1 87.500 (86.593)\tPrec@5 98.438 (99.168)\n",
      "Epoch: [131][40/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 1.7847 (2.0977)\tPrec@1 87.500 (86.833)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [131][50/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.020)\tLoss 2.0811 (2.0699)\tPrec@1 89.844 (87.270)\tPrec@5 99.219 (99.249)\n",
      "Epoch: [131][60/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 2.3137 (2.0934)\tPrec@1 85.938 (87.116)\tPrec@5 98.438 (99.244)\n",
      "Epoch: [131][70/97], lr: 0.01000\tTime 0.324 (0.330)\tData 0.000 (0.020)\tLoss 1.7779 (2.0694)\tPrec@1 89.062 (87.379)\tPrec@5 100.000 (99.274)\n",
      "Epoch: [131][80/97], lr: 0.01000\tTime 0.322 (0.330)\tData 0.000 (0.019)\tLoss 2.7476 (2.0569)\tPrec@1 82.812 (87.471)\tPrec@5 98.438 (99.228)\n",
      "Epoch: [131][90/97], lr: 0.01000\tTime 0.322 (0.330)\tData 0.000 (0.019)\tLoss 2.6415 (2.0635)\tPrec@1 82.812 (87.397)\tPrec@5 98.438 (99.245)\n",
      "Epoch: [131][96/97], lr: 0.01000\tTime 0.316 (0.329)\tData 0.000 (0.020)\tLoss 2.0730 (2.0488)\tPrec@1 88.983 (87.490)\tPrec@5 99.153 (99.250)\n",
      "Test: [0/100]\tTime 0.265 (0.265)\tLoss 8.1716 (8.1716)\tPrec@1 58.000 (58.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.3686 (8.3866)\tPrec@1 71.000 (59.000)\tPrec@5 97.000 (95.182)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.6799 (8.2132)\tPrec@1 66.000 (60.000)\tPrec@5 97.000 (95.667)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.1200 (8.2490)\tPrec@1 58.000 (59.484)\tPrec@5 95.000 (95.484)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 8.8795 (8.2796)\tPrec@1 53.000 (59.561)\tPrec@5 95.000 (95.317)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.8480 (8.1454)\tPrec@1 62.000 (60.137)\tPrec@5 97.000 (95.490)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.4016 (8.0959)\tPrec@1 71.000 (60.459)\tPrec@5 98.000 (95.656)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.3669 (8.0412)\tPrec@1 58.000 (60.930)\tPrec@5 99.000 (95.775)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.0469 (7.9913)\tPrec@1 56.000 (61.160)\tPrec@5 91.000 (95.852)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.8443 (8.0636)\tPrec@1 58.000 (60.901)\tPrec@5 97.000 (95.857)\n",
      "val Results: Prec@1 60.940 Prec@5 95.840 Loss 8.06420\n",
      "val Class Accuracy: [0.928,0.959,0.718,0.824,0.571,0.658,0.566,0.397,0.410,0.063]\n",
      "Best Prec@1: 66.830\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [132][0/97], lr: 0.01000\tTime 0.395 (0.395)\tData 0.207 (0.207)\tLoss 1.9877 (1.9877)\tPrec@1 87.500 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [132][10/97], lr: 0.01000\tTime 0.331 (0.339)\tData 0.000 (0.034)\tLoss 1.6922 (2.0197)\tPrec@1 91.406 (87.500)\tPrec@5 99.219 (98.935)\n",
      "Epoch: [132][20/97], lr: 0.01000\tTime 0.324 (0.335)\tData 0.000 (0.026)\tLoss 1.9342 (1.9804)\tPrec@1 88.281 (87.760)\tPrec@5 98.438 (98.921)\n",
      "Epoch: [132][30/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.023)\tLoss 1.7923 (1.9340)\tPrec@1 89.062 (88.206)\tPrec@5 99.219 (99.118)\n",
      "Epoch: [132][40/97], lr: 0.01000\tTime 0.330 (0.332)\tData 0.000 (0.022)\tLoss 1.9144 (1.9445)\tPrec@1 89.844 (88.167)\tPrec@5 99.219 (99.181)\n",
      "Epoch: [132][50/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.021)\tLoss 2.5218 (1.9926)\tPrec@1 86.719 (87.883)\tPrec@5 98.438 (99.203)\n",
      "Epoch: [132][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 1.7056 (2.0374)\tPrec@1 89.062 (87.628)\tPrec@5 97.656 (99.168)\n",
      "Epoch: [132][70/97], lr: 0.01000\tTime 0.330 (0.331)\tData 0.000 (0.020)\tLoss 2.2801 (2.0609)\tPrec@1 88.281 (87.489)\tPrec@5 97.656 (99.142)\n",
      "Epoch: [132][80/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.020)\tLoss 2.2442 (2.0515)\tPrec@1 87.500 (87.645)\tPrec@5 100.000 (99.142)\n",
      "Epoch: [132][90/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.019)\tLoss 1.4142 (2.0497)\tPrec@1 89.844 (87.594)\tPrec@5 100.000 (99.167)\n",
      "Epoch: [132][96/97], lr: 0.01000\tTime 0.317 (0.330)\tData 0.000 (0.020)\tLoss 1.8503 (2.0432)\tPrec@1 87.288 (87.587)\tPrec@5 97.458 (99.178)\n",
      "Test: [0/100]\tTime 0.239 (0.239)\tLoss 7.4250 (7.4250)\tPrec@1 63.000 (63.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 4.6409 (6.6665)\tPrec@1 77.000 (67.545)\tPrec@5 99.000 (98.091)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.1661 (6.6330)\tPrec@1 74.000 (68.048)\tPrec@5 100.000 (97.762)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 5.9481 (6.6692)\tPrec@1 70.000 (68.065)\tPrec@5 98.000 (97.871)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 6.4403 (6.6903)\tPrec@1 68.000 (68.268)\tPrec@5 97.000 (97.805)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.3740 (6.6486)\tPrec@1 68.000 (68.294)\tPrec@5 99.000 (97.843)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 5.5736 (6.6083)\tPrec@1 71.000 (68.475)\tPrec@5 97.000 (97.787)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 6.4435 (6.6095)\tPrec@1 67.000 (68.239)\tPrec@5 100.000 (97.930)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.8216 (6.5644)\tPrec@1 68.000 (68.395)\tPrec@5 98.000 (97.914)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.4208 (6.5984)\tPrec@1 71.000 (68.363)\tPrec@5 100.000 (97.901)\n",
      "val Results: Prec@1 68.430 Prec@5 97.890 Loss 6.59168\n",
      "val Class Accuracy: [0.947,0.988,0.666,0.815,0.733,0.633,0.733,0.611,0.501,0.216]\n",
      "Best Prec@1: 68.430\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [133][0/97], lr: 0.01000\tTime 0.482 (0.482)\tData 0.259 (0.259)\tLoss 1.7580 (1.7580)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [133][10/97], lr: 0.01000\tTime 0.324 (0.344)\tData 0.000 (0.039)\tLoss 1.6996 (1.8510)\tPrec@1 90.625 (88.565)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [133][20/97], lr: 0.01000\tTime 0.325 (0.337)\tData 0.000 (0.029)\tLoss 1.6527 (1.8959)\tPrec@1 91.406 (88.467)\tPrec@5 99.219 (99.256)\n",
      "Epoch: [133][30/97], lr: 0.01000\tTime 0.327 (0.334)\tData 0.000 (0.025)\tLoss 2.4766 (1.9536)\tPrec@1 85.938 (88.155)\tPrec@5 100.000 (99.320)\n",
      "Epoch: [133][40/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.023)\tLoss 2.1869 (1.9810)\tPrec@1 86.719 (87.900)\tPrec@5 100.000 (99.371)\n",
      "Epoch: [133][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.022)\tLoss 2.0835 (1.9840)\tPrec@1 84.375 (87.745)\tPrec@5 99.219 (99.357)\n",
      "Epoch: [133][60/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.021)\tLoss 2.0988 (1.9913)\tPrec@1 86.719 (87.615)\tPrec@5 100.000 (99.372)\n",
      "Epoch: [133][70/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.021)\tLoss 2.3971 (1.9972)\tPrec@1 85.938 (87.632)\tPrec@5 98.438 (99.340)\n",
      "Epoch: [133][80/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 2.1192 (2.0254)\tPrec@1 86.719 (87.471)\tPrec@5 99.219 (99.315)\n",
      "Epoch: [133][90/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 2.6531 (2.0246)\tPrec@1 82.812 (87.491)\tPrec@5 100.000 (99.322)\n",
      "Epoch: [133][96/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.021)\tLoss 2.3103 (2.0379)\tPrec@1 84.746 (87.377)\tPrec@5 100.000 (99.339)\n",
      "Test: [0/100]\tTime 0.261 (0.261)\tLoss 8.0600 (8.0600)\tPrec@1 63.000 (63.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 5.9179 (7.8752)\tPrec@1 73.000 (62.909)\tPrec@5 98.000 (96.545)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.0259 (7.7595)\tPrec@1 69.000 (63.238)\tPrec@5 98.000 (96.714)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.8359 (7.8474)\tPrec@1 67.000 (62.774)\tPrec@5 96.000 (96.613)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.5546 (7.8770)\tPrec@1 64.000 (62.561)\tPrec@5 95.000 (96.537)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.9136 (7.7739)\tPrec@1 73.000 (63.196)\tPrec@5 96.000 (96.686)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.4688 (7.7393)\tPrec@1 68.000 (63.279)\tPrec@5 98.000 (96.656)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 7.6929 (7.7204)\tPrec@1 64.000 (63.380)\tPrec@5 96.000 (96.662)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.4658 (7.6915)\tPrec@1 63.000 (63.568)\tPrec@5 93.000 (96.679)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.5265 (7.7628)\tPrec@1 66.000 (63.198)\tPrec@5 97.000 (96.582)\n",
      "val Results: Prec@1 63.060 Prec@5 96.620 Loss 7.78974\n",
      "val Class Accuracy: [0.849,0.985,0.615,0.663,0.585,0.643,0.913,0.614,0.259,0.180]\n",
      "Best Prec@1: 68.430\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [134][0/97], lr: 0.01000\tTime 0.427 (0.427)\tData 0.208 (0.208)\tLoss 1.9887 (1.9887)\tPrec@1 87.500 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [134][10/97], lr: 0.01000\tTime 0.326 (0.343)\tData 0.000 (0.034)\tLoss 1.8923 (2.1029)\tPrec@1 89.062 (87.145)\tPrec@5 99.219 (99.574)\n",
      "Epoch: [134][20/97], lr: 0.01000\tTime 0.330 (0.335)\tData 0.000 (0.026)\tLoss 2.1368 (2.1544)\tPrec@1 85.156 (86.496)\tPrec@5 98.438 (99.107)\n",
      "Epoch: [134][30/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.023)\tLoss 2.1428 (2.0818)\tPrec@1 85.938 (86.920)\tPrec@5 99.219 (99.194)\n",
      "Epoch: [134][40/97], lr: 0.01000\tTime 0.329 (0.332)\tData 0.000 (0.022)\tLoss 1.7825 (2.0949)\tPrec@1 88.281 (86.966)\tPrec@5 98.438 (99.104)\n",
      "Epoch: [134][50/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.021)\tLoss 2.0986 (2.0703)\tPrec@1 86.719 (87.178)\tPrec@5 99.219 (99.157)\n",
      "Epoch: [134][60/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 1.6434 (2.0412)\tPrec@1 89.844 (87.398)\tPrec@5 99.219 (99.193)\n",
      "Epoch: [134][70/97], lr: 0.01000\tTime 0.327 (0.331)\tData 0.000 (0.020)\tLoss 2.6105 (2.0771)\tPrec@1 85.156 (87.225)\tPrec@5 97.656 (99.219)\n",
      "Epoch: [134][80/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.019)\tLoss 1.9127 (2.0869)\tPrec@1 90.625 (87.259)\tPrec@5 99.219 (99.199)\n",
      "Epoch: [134][90/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.019)\tLoss 1.9602 (2.0946)\tPrec@1 91.406 (87.225)\tPrec@5 99.219 (99.193)\n",
      "Epoch: [134][96/97], lr: 0.01000\tTime 0.317 (0.330)\tData 0.000 (0.020)\tLoss 1.6336 (2.0781)\tPrec@1 89.831 (87.345)\tPrec@5 100.000 (99.210)\n",
      "Test: [0/100]\tTime 0.268 (0.268)\tLoss 8.5592 (8.5592)\tPrec@1 59.000 (59.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 6.3408 (7.6474)\tPrec@1 67.000 (63.818)\tPrec@5 97.000 (95.000)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.2727 (7.5120)\tPrec@1 70.000 (64.238)\tPrec@5 99.000 (95.524)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.4213 (7.4330)\tPrec@1 70.000 (64.677)\tPrec@5 97.000 (95.645)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 7.6502 (7.4590)\tPrec@1 64.000 (64.585)\tPrec@5 96.000 (95.585)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.3902 (7.4089)\tPrec@1 65.000 (64.706)\tPrec@5 98.000 (95.863)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.6164 (7.3827)\tPrec@1 65.000 (64.820)\tPrec@5 95.000 (95.967)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 7.2077 (7.3378)\tPrec@1 68.000 (65.155)\tPrec@5 98.000 (95.915)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.7242 (7.2892)\tPrec@1 68.000 (65.420)\tPrec@5 97.000 (95.877)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.6582 (7.3687)\tPrec@1 63.000 (65.143)\tPrec@5 97.000 (95.846)\n",
      "val Results: Prec@1 65.110 Prec@5 95.890 Loss 7.37787\n",
      "val Class Accuracy: [0.899,0.981,0.809,0.667,0.548,0.751,0.670,0.709,0.190,0.287]\n",
      "Best Prec@1: 68.430\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [135][0/97], lr: 0.01000\tTime 0.466 (0.466)\tData 0.251 (0.251)\tLoss 1.8096 (1.8096)\tPrec@1 88.281 (88.281)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [135][10/97], lr: 0.01000\tTime 0.328 (0.345)\tData 0.000 (0.038)\tLoss 1.3835 (1.7084)\tPrec@1 90.625 (89.631)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [135][20/97], lr: 0.01000\tTime 0.330 (0.338)\tData 0.000 (0.028)\tLoss 2.0358 (1.8763)\tPrec@1 87.500 (88.095)\tPrec@5 99.219 (99.368)\n",
      "Epoch: [135][30/97], lr: 0.01000\tTime 0.327 (0.335)\tData 0.000 (0.025)\tLoss 1.5884 (1.9777)\tPrec@1 90.625 (87.727)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [135][40/97], lr: 0.01000\tTime 0.328 (0.334)\tData 0.000 (0.023)\tLoss 2.0198 (1.9939)\tPrec@1 87.500 (87.729)\tPrec@5 99.219 (99.314)\n",
      "Epoch: [135][50/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.022)\tLoss 2.1118 (1.9776)\tPrec@1 85.938 (87.776)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [135][60/97], lr: 0.01000\tTime 0.331 (0.332)\tData 0.000 (0.021)\tLoss 1.7749 (1.9868)\tPrec@1 89.844 (87.705)\tPrec@5 99.219 (99.232)\n",
      "Epoch: [135][70/97], lr: 0.01000\tTime 0.318 (0.332)\tData 0.000 (0.020)\tLoss 1.6209 (2.0098)\tPrec@1 89.844 (87.423)\tPrec@5 97.656 (99.219)\n",
      "Epoch: [135][80/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 2.6106 (2.0146)\tPrec@1 82.031 (87.452)\tPrec@5 99.219 (99.209)\n",
      "Epoch: [135][90/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 2.4029 (2.0174)\tPrec@1 84.375 (87.440)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [135][96/97], lr: 0.01000\tTime 0.319 (0.331)\tData 0.000 (0.020)\tLoss 2.1938 (2.0208)\tPrec@1 88.136 (87.442)\tPrec@5 99.153 (99.218)\n",
      "Test: [0/100]\tTime 0.256 (0.256)\tLoss 12.2638 (12.2638)\tPrec@1 43.000 (43.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 9.0848 (10.4983)\tPrec@1 57.000 (49.364)\tPrec@5 92.000 (92.000)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 8.3706 (10.4435)\tPrec@1 60.000 (50.000)\tPrec@5 94.000 (91.905)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 9.9026 (10.3912)\tPrec@1 49.000 (50.129)\tPrec@5 91.000 (91.839)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.7197 (10.3779)\tPrec@1 53.000 (50.244)\tPrec@5 85.000 (91.512)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.6817 (10.2569)\tPrec@1 58.000 (51.039)\tPrec@5 90.000 (91.412)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.9385 (10.2563)\tPrec@1 58.000 (50.967)\tPrec@5 92.000 (91.410)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 10.1187 (10.2594)\tPrec@1 55.000 (51.155)\tPrec@5 96.000 (91.479)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.7377 (10.1935)\tPrec@1 58.000 (51.420)\tPrec@5 90.000 (91.605)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.5207 (10.2780)\tPrec@1 52.000 (51.055)\tPrec@5 97.000 (91.429)\n",
      "val Results: Prec@1 51.060 Prec@5 91.380 Loss 10.30832\n",
      "val Class Accuracy: [0.939,0.992,0.746,0.284,0.652,0.572,0.346,0.395,0.106,0.074]\n",
      "Best Prec@1: 68.430\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [136][0/97], lr: 0.01000\tTime 0.482 (0.482)\tData 0.266 (0.266)\tLoss 2.4113 (2.4113)\tPrec@1 85.938 (85.938)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [136][10/97], lr: 0.01000\tTime 0.324 (0.346)\tData 0.000 (0.039)\tLoss 1.8525 (2.0207)\tPrec@1 89.062 (87.926)\tPrec@5 100.000 (99.148)\n",
      "Epoch: [136][20/97], lr: 0.01000\tTime 0.324 (0.338)\tData 0.000 (0.029)\tLoss 1.4847 (1.9437)\tPrec@1 89.844 (88.318)\tPrec@5 99.219 (99.107)\n",
      "Epoch: [136][30/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.025)\tLoss 2.1109 (1.9761)\tPrec@1 88.281 (87.954)\tPrec@5 100.000 (99.168)\n",
      "Epoch: [136][40/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.023)\tLoss 2.3400 (2.0005)\tPrec@1 89.062 (87.710)\tPrec@5 97.656 (99.123)\n",
      "Epoch: [136][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.022)\tLoss 2.1647 (1.9768)\tPrec@1 83.594 (87.960)\tPrec@5 100.000 (99.173)\n",
      "Epoch: [136][60/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 1.8481 (1.9495)\tPrec@1 89.844 (88.179)\tPrec@5 99.219 (99.206)\n",
      "Epoch: [136][70/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.021)\tLoss 2.2595 (1.9777)\tPrec@1 85.156 (87.995)\tPrec@5 100.000 (99.274)\n",
      "Epoch: [136][80/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.020)\tLoss 1.6085 (1.9765)\tPrec@1 90.625 (87.973)\tPrec@5 100.000 (99.267)\n",
      "Epoch: [136][90/97], lr: 0.01000\tTime 0.329 (0.331)\tData 0.000 (0.020)\tLoss 2.6658 (1.9907)\tPrec@1 82.812 (87.895)\tPrec@5 100.000 (99.330)\n",
      "Epoch: [136][96/97], lr: 0.01000\tTime 0.316 (0.330)\tData 0.000 (0.020)\tLoss 1.7494 (1.9777)\tPrec@1 88.136 (88.030)\tPrec@5 99.153 (99.315)\n",
      "Test: [0/100]\tTime 0.233 (0.233)\tLoss 8.3054 (8.3054)\tPrec@1 61.000 (61.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 6.6279 (7.4066)\tPrec@1 71.000 (64.909)\tPrec@5 95.000 (97.000)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.2215 (7.1215)\tPrec@1 67.000 (66.381)\tPrec@5 96.000 (96.619)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 6.3071 (7.0214)\tPrec@1 69.000 (66.710)\tPrec@5 97.000 (96.387)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 6.2226 (6.9878)\tPrec@1 73.000 (67.049)\tPrec@5 96.000 (96.415)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.8230 (6.9373)\tPrec@1 65.000 (67.157)\tPrec@5 97.000 (96.510)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.3033 (6.9329)\tPrec@1 72.000 (67.148)\tPrec@5 93.000 (96.557)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 6.1421 (6.9080)\tPrec@1 70.000 (67.296)\tPrec@5 97.000 (96.535)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.7323 (6.8878)\tPrec@1 70.000 (67.358)\tPrec@5 96.000 (96.617)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.6527 (6.9597)\tPrec@1 68.000 (67.000)\tPrec@5 99.000 (96.648)\n",
      "val Results: Prec@1 66.930 Prec@5 96.670 Loss 6.96818\n",
      "val Class Accuracy: [0.921,0.970,0.751,0.675,0.801,0.675,0.394,0.605,0.507,0.394]\n",
      "Best Prec@1: 68.430\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [137][0/97], lr: 0.01000\tTime 0.415 (0.415)\tData 0.214 (0.214)\tLoss 2.0258 (2.0258)\tPrec@1 89.844 (89.844)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [137][10/97], lr: 0.01000\tTime 0.327 (0.341)\tData 0.000 (0.034)\tLoss 1.7075 (1.8830)\tPrec@1 89.844 (88.778)\tPrec@5 99.219 (99.503)\n",
      "Epoch: [137][20/97], lr: 0.01000\tTime 0.328 (0.335)\tData 0.000 (0.026)\tLoss 2.4898 (1.9797)\tPrec@1 82.031 (87.909)\tPrec@5 99.219 (99.405)\n",
      "Epoch: [137][30/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.023)\tLoss 1.8111 (1.9703)\tPrec@1 89.062 (88.105)\tPrec@5 98.438 (99.194)\n",
      "Epoch: [137][40/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.022)\tLoss 2.4419 (1.9616)\tPrec@1 84.375 (88.148)\tPrec@5 97.656 (99.181)\n",
      "Epoch: [137][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 2.4748 (1.9865)\tPrec@1 85.156 (87.960)\tPrec@5 100.000 (99.249)\n",
      "Epoch: [137][60/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 2.1475 (2.0373)\tPrec@1 84.375 (87.615)\tPrec@5 97.656 (99.232)\n",
      "Epoch: [137][70/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 1.4671 (2.0437)\tPrec@1 92.188 (87.533)\tPrec@5 98.438 (99.241)\n",
      "Epoch: [137][80/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 2.5628 (2.0837)\tPrec@1 86.719 (87.346)\tPrec@5 98.438 (99.209)\n",
      "Epoch: [137][90/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.019)\tLoss 1.8477 (2.0637)\tPrec@1 90.625 (87.363)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [137][96/97], lr: 0.01000\tTime 0.318 (0.330)\tData 0.000 (0.020)\tLoss 1.9812 (2.0445)\tPrec@1 90.678 (87.498)\tPrec@5 100.000 (99.210)\n",
      "Test: [0/100]\tTime 0.233 (0.233)\tLoss 11.1082 (11.1082)\tPrec@1 51.000 (51.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 8.8203 (10.0947)\tPrec@1 61.000 (53.818)\tPrec@5 91.000 (94.818)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 8.4501 (10.0477)\tPrec@1 58.000 (53.714)\tPrec@5 94.000 (94.381)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.2811 (10.0699)\tPrec@1 58.000 (53.452)\tPrec@5 94.000 (94.129)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.6058 (10.0583)\tPrec@1 51.000 (53.854)\tPrec@5 95.000 (94.122)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.4894 (9.9243)\tPrec@1 60.000 (54.706)\tPrec@5 95.000 (94.314)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.6666 (9.8897)\tPrec@1 65.000 (54.705)\tPrec@5 95.000 (94.311)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.8026 (9.8887)\tPrec@1 57.000 (54.620)\tPrec@5 99.000 (94.254)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.7018 (9.8462)\tPrec@1 59.000 (54.741)\tPrec@5 94.000 (94.346)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.7315 (9.9392)\tPrec@1 49.000 (54.374)\tPrec@5 97.000 (94.275)\n",
      "val Results: Prec@1 54.460 Prec@5 94.310 Loss 9.93488\n",
      "val Class Accuracy: [0.974,0.979,0.458,0.888,0.630,0.500,0.144,0.388,0.290,0.195]\n",
      "Best Prec@1: 68.430\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [138][0/97], lr: 0.01000\tTime 0.443 (0.443)\tData 0.233 (0.233)\tLoss 2.2794 (2.2794)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [138][10/97], lr: 0.01000\tTime 0.329 (0.343)\tData 0.000 (0.036)\tLoss 1.5830 (1.9175)\tPrec@1 89.062 (88.139)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [138][20/97], lr: 0.01000\tTime 0.323 (0.335)\tData 0.000 (0.027)\tLoss 1.8375 (2.0252)\tPrec@1 91.406 (87.872)\tPrec@5 100.000 (99.256)\n",
      "Epoch: [138][30/97], lr: 0.01000\tTime 0.330 (0.334)\tData 0.000 (0.024)\tLoss 2.2066 (2.0253)\tPrec@1 85.156 (87.903)\tPrec@5 99.219 (99.370)\n",
      "Epoch: [138][40/97], lr: 0.01000\tTime 0.331 (0.333)\tData 0.000 (0.022)\tLoss 2.2656 (2.0083)\tPrec@1 86.719 (87.957)\tPrec@5 99.219 (99.371)\n",
      "Epoch: [138][50/97], lr: 0.01000\tTime 0.328 (0.333)\tData 0.000 (0.021)\tLoss 2.7296 (2.0057)\tPrec@1 78.906 (87.791)\tPrec@5 100.000 (99.387)\n",
      "Epoch: [138][60/97], lr: 0.01000\tTime 0.329 (0.333)\tData 0.000 (0.021)\tLoss 2.5556 (2.0265)\tPrec@1 85.156 (87.666)\tPrec@5 98.438 (99.385)\n",
      "Epoch: [138][70/97], lr: 0.01000\tTime 0.330 (0.332)\tData 0.000 (0.020)\tLoss 2.6566 (2.0363)\tPrec@1 82.031 (87.610)\tPrec@5 100.000 (99.362)\n",
      "Epoch: [138][80/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.4785 (2.0466)\tPrec@1 84.375 (87.548)\tPrec@5 99.219 (99.392)\n",
      "Epoch: [138][90/97], lr: 0.01000\tTime 0.322 (0.332)\tData 0.000 (0.019)\tLoss 2.1211 (2.0301)\tPrec@1 86.719 (87.706)\tPrec@5 98.438 (99.416)\n",
      "Epoch: [138][96/97], lr: 0.01000\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 1.8190 (2.0510)\tPrec@1 88.136 (87.522)\tPrec@5 100.000 (99.395)\n",
      "Test: [0/100]\tTime 0.226 (0.226)\tLoss 9.7480 (9.7480)\tPrec@1 52.000 (52.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 7.4586 (9.9435)\tPrec@1 62.000 (51.273)\tPrec@5 96.000 (94.091)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 9.7206 (10.0035)\tPrec@1 50.000 (50.619)\tPrec@5 96.000 (94.000)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.4048 (9.9876)\tPrec@1 51.000 (50.645)\tPrec@5 97.000 (93.742)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.5290 (10.0397)\tPrec@1 48.000 (50.659)\tPrec@5 95.000 (93.732)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 10.6865 (10.0009)\tPrec@1 46.000 (50.784)\tPrec@5 96.000 (93.725)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 8.8737 (10.0407)\tPrec@1 53.000 (50.393)\tPrec@5 97.000 (93.754)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.8935 (10.0439)\tPrec@1 51.000 (50.507)\tPrec@5 98.000 (93.775)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.9114 (10.0207)\tPrec@1 51.000 (50.519)\tPrec@5 91.000 (93.877)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.3109 (10.0727)\tPrec@1 53.000 (50.330)\tPrec@5 98.000 (93.780)\n",
      "val Results: Prec@1 50.370 Prec@5 93.770 Loss 10.07925\n",
      "val Class Accuracy: [0.874,0.854,0.927,0.284,0.518,0.156,0.740,0.366,0.315,0.003]\n",
      "Best Prec@1: 68.430\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [139][0/97], lr: 0.01000\tTime 0.493 (0.493)\tData 0.246 (0.246)\tLoss 2.9432 (2.9432)\tPrec@1 78.906 (78.906)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [139][10/97], lr: 0.01000\tTime 0.329 (0.348)\tData 0.000 (0.037)\tLoss 1.8626 (2.1445)\tPrec@1 88.281 (87.003)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [139][20/97], lr: 0.01000\tTime 0.326 (0.340)\tData 0.000 (0.028)\tLoss 1.8895 (2.0172)\tPrec@1 89.844 (88.132)\tPrec@5 99.219 (99.442)\n",
      "Epoch: [139][30/97], lr: 0.01000\tTime 0.330 (0.337)\tData 0.000 (0.024)\tLoss 2.8384 (2.0692)\tPrec@1 82.031 (87.601)\tPrec@5 99.219 (99.521)\n",
      "Epoch: [139][40/97], lr: 0.01000\tTime 0.324 (0.335)\tData 0.000 (0.023)\tLoss 1.7786 (2.0176)\tPrec@1 88.281 (87.862)\tPrec@5 99.219 (99.428)\n",
      "Epoch: [139][50/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.022)\tLoss 1.8100 (1.9807)\tPrec@1 88.281 (88.006)\tPrec@5 100.000 (99.464)\n",
      "Epoch: [139][60/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.021)\tLoss 3.1584 (2.0412)\tPrec@1 78.906 (87.590)\tPrec@5 98.438 (99.385)\n",
      "Epoch: [139][70/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 2.5204 (2.0509)\tPrec@1 84.375 (87.544)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [139][80/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 2.1193 (2.0624)\tPrec@1 87.500 (87.432)\tPrec@5 100.000 (99.441)\n",
      "Epoch: [139][90/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 2.3463 (2.0574)\tPrec@1 83.594 (87.457)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [139][96/97], lr: 0.01000\tTime 0.317 (0.332)\tData 0.000 (0.020)\tLoss 2.5375 (2.0522)\tPrec@1 86.441 (87.506)\tPrec@5 99.153 (99.452)\n",
      "Test: [0/100]\tTime 0.244 (0.244)\tLoss 8.9755 (8.9755)\tPrec@1 61.000 (61.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 6.2756 (8.3402)\tPrec@1 72.000 (62.000)\tPrec@5 96.000 (96.545)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.6821 (8.2551)\tPrec@1 66.000 (61.667)\tPrec@5 95.000 (96.000)\n",
      "Test: [30/100]\tTime 0.074 (0.079)\tLoss 6.5283 (8.2015)\tPrec@1 68.000 (61.968)\tPrec@5 96.000 (95.871)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.4224 (8.2273)\tPrec@1 68.000 (62.049)\tPrec@5 95.000 (95.537)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.5300 (8.1044)\tPrec@1 62.000 (62.627)\tPrec@5 97.000 (95.686)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7237 (8.0796)\tPrec@1 71.000 (62.459)\tPrec@5 95.000 (95.623)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.6366 (8.0629)\tPrec@1 58.000 (62.479)\tPrec@5 96.000 (95.704)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.3996 (8.0296)\tPrec@1 60.000 (62.593)\tPrec@5 96.000 (95.741)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.8302 (8.0952)\tPrec@1 63.000 (62.209)\tPrec@5 98.000 (95.780)\n",
      "val Results: Prec@1 61.930 Prec@5 95.810 Loss 8.14953\n",
      "val Class Accuracy: [0.942,0.995,0.558,0.714,0.715,0.594,0.716,0.509,0.396,0.054]\n",
      "Best Prec@1: 68.430\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [140][0/97], lr: 0.01000\tTime 0.424 (0.424)\tData 0.223 (0.223)\tLoss 1.4763 (1.4763)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [140][10/97], lr: 0.01000\tTime 0.329 (0.349)\tData 0.000 (0.035)\tLoss 1.6682 (2.0431)\tPrec@1 89.844 (87.287)\tPrec@5 100.000 (99.148)\n",
      "Epoch: [140][20/97], lr: 0.01000\tTime 0.327 (0.339)\tData 0.000 (0.027)\tLoss 1.8822 (1.9892)\tPrec@1 87.500 (88.021)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [140][30/97], lr: 0.01000\tTime 0.328 (0.336)\tData 0.000 (0.024)\tLoss 1.8463 (2.0298)\tPrec@1 89.062 (87.752)\tPrec@5 99.219 (99.320)\n",
      "Epoch: [140][40/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.022)\tLoss 2.7400 (2.0333)\tPrec@1 85.938 (87.710)\tPrec@5 98.438 (99.352)\n",
      "Epoch: [140][50/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.021)\tLoss 2.6406 (2.0096)\tPrec@1 84.375 (87.929)\tPrec@5 99.219 (99.387)\n",
      "Epoch: [140][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 1.3211 (1.9673)\tPrec@1 89.844 (88.012)\tPrec@5 100.000 (99.398)\n",
      "Epoch: [140][70/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 1.9295 (1.9945)\tPrec@1 85.156 (87.918)\tPrec@5 97.656 (99.395)\n",
      "Epoch: [140][80/97], lr: 0.01000\tTime 0.338 (0.332)\tData 0.000 (0.020)\tLoss 2.0512 (2.0008)\tPrec@1 88.281 (87.934)\tPrec@5 100.000 (99.354)\n",
      "Epoch: [140][90/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.019)\tLoss 1.9302 (1.9987)\tPrec@1 89.062 (88.015)\tPrec@5 99.219 (99.365)\n",
      "Epoch: [140][96/97], lr: 0.01000\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 2.6852 (2.0015)\tPrec@1 84.746 (87.957)\tPrec@5 99.153 (99.355)\n",
      "Test: [0/100]\tTime 0.235 (0.235)\tLoss 8.3519 (8.3519)\tPrec@1 60.000 (60.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 7.0118 (8.6823)\tPrec@1 67.000 (60.000)\tPrec@5 97.000 (94.727)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.8416 (8.5353)\tPrec@1 57.000 (60.048)\tPrec@5 98.000 (95.048)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.9795 (8.5249)\tPrec@1 65.000 (59.935)\tPrec@5 97.000 (94.710)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.3084 (8.6099)\tPrec@1 63.000 (59.195)\tPrec@5 98.000 (94.732)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.1934 (8.6279)\tPrec@1 59.000 (59.000)\tPrec@5 96.000 (94.882)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.3564 (8.5705)\tPrec@1 68.000 (59.098)\tPrec@5 96.000 (94.984)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.3778 (8.5512)\tPrec@1 63.000 (59.028)\tPrec@5 96.000 (94.930)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.0054 (8.5357)\tPrec@1 63.000 (58.864)\tPrec@5 92.000 (94.988)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.9756 (8.5778)\tPrec@1 61.000 (58.582)\tPrec@5 99.000 (94.835)\n",
      "val Results: Prec@1 58.360 Prec@5 94.730 Loss 8.63374\n",
      "val Class Accuracy: [0.713,0.943,0.812,0.499,0.926,0.362,0.617,0.374,0.541,0.049]\n",
      "Best Prec@1: 68.430\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [141][0/97], lr: 0.01000\tTime 0.444 (0.444)\tData 0.220 (0.220)\tLoss 2.0023 (2.0023)\tPrec@1 86.719 (86.719)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [141][10/97], lr: 0.01000\tTime 0.324 (0.344)\tData 0.000 (0.035)\tLoss 2.2012 (1.9205)\tPrec@1 88.281 (88.991)\tPrec@5 100.000 (99.006)\n",
      "Epoch: [141][20/97], lr: 0.01000\tTime 0.326 (0.336)\tData 0.000 (0.026)\tLoss 1.9473 (2.0222)\tPrec@1 88.281 (87.798)\tPrec@5 100.000 (99.107)\n",
      "Epoch: [141][30/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.024)\tLoss 3.1771 (2.0473)\tPrec@1 81.250 (87.777)\tPrec@5 98.438 (99.168)\n",
      "Epoch: [141][40/97], lr: 0.01000\tTime 0.329 (0.334)\tData 0.000 (0.022)\tLoss 2.3477 (2.0418)\tPrec@1 85.156 (87.652)\tPrec@5 100.000 (99.200)\n",
      "Epoch: [141][50/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.021)\tLoss 2.1639 (2.0774)\tPrec@1 86.719 (87.408)\tPrec@5 99.219 (99.234)\n",
      "Epoch: [141][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 1.8300 (2.0793)\tPrec@1 90.625 (87.551)\tPrec@5 100.000 (99.193)\n",
      "Epoch: [141][70/97], lr: 0.01000\tTime 0.356 (0.332)\tData 0.000 (0.020)\tLoss 1.8518 (2.0740)\tPrec@1 86.719 (87.544)\tPrec@5 100.000 (99.208)\n",
      "Epoch: [141][80/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 2.5865 (2.0706)\tPrec@1 83.594 (87.490)\tPrec@5 98.438 (99.248)\n",
      "Epoch: [141][90/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.019)\tLoss 2.2897 (2.0707)\tPrec@1 84.375 (87.534)\tPrec@5 99.219 (99.253)\n",
      "Epoch: [141][96/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 2.2569 (2.0815)\tPrec@1 86.441 (87.450)\tPrec@5 100.000 (99.250)\n",
      "Test: [0/100]\tTime 0.238 (0.238)\tLoss 9.1798 (9.1798)\tPrec@1 57.000 (57.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 7.8805 (8.9855)\tPrec@1 61.000 (57.545)\tPrec@5 92.000 (91.727)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.7078 (8.9884)\tPrec@1 60.000 (57.381)\tPrec@5 96.000 (93.143)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 8.3889 (9.0156)\tPrec@1 57.000 (56.935)\tPrec@5 94.000 (93.161)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.1498 (8.9607)\tPrec@1 58.000 (57.390)\tPrec@5 92.000 (93.195)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.7007 (8.9198)\tPrec@1 55.000 (57.510)\tPrec@5 97.000 (93.490)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.8678 (8.8603)\tPrec@1 67.000 (57.623)\tPrec@5 96.000 (93.607)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.1805 (8.8639)\tPrec@1 59.000 (57.549)\tPrec@5 94.000 (93.465)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.5702 (8.8167)\tPrec@1 60.000 (57.728)\tPrec@5 91.000 (93.531)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.4823 (8.8884)\tPrec@1 52.000 (57.495)\tPrec@5 96.000 (93.363)\n",
      "val Results: Prec@1 57.530 Prec@5 93.290 Loss 8.89804\n",
      "val Class Accuracy: [0.952,0.964,0.812,0.845,0.642,0.194,0.340,0.607,0.179,0.218]\n",
      "Best Prec@1: 68.430\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [142][0/97], lr: 0.01000\tTime 0.469 (0.469)\tData 0.232 (0.232)\tLoss 2.7048 (2.7048)\tPrec@1 81.250 (81.250)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [142][10/97], lr: 0.01000\tTime 0.324 (0.345)\tData 0.000 (0.036)\tLoss 1.3981 (1.9618)\tPrec@1 88.281 (87.500)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [142][20/97], lr: 0.01000\tTime 0.324 (0.338)\tData 0.000 (0.027)\tLoss 1.7218 (1.9943)\tPrec@1 89.062 (87.351)\tPrec@5 100.000 (99.368)\n",
      "Epoch: [142][30/97], lr: 0.01000\tTime 0.327 (0.335)\tData 0.000 (0.024)\tLoss 2.4411 (1.9739)\tPrec@1 83.594 (87.576)\tPrec@5 100.000 (99.269)\n",
      "Epoch: [142][40/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.022)\tLoss 1.9025 (1.9599)\tPrec@1 90.625 (87.671)\tPrec@5 97.656 (99.333)\n",
      "Epoch: [142][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 1.7869 (1.9381)\tPrec@1 88.281 (87.914)\tPrec@5 99.219 (99.311)\n",
      "Epoch: [142][60/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.021)\tLoss 1.5692 (1.9687)\tPrec@1 90.625 (87.743)\tPrec@5 98.438 (99.308)\n",
      "Epoch: [142][70/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 1.5601 (2.0083)\tPrec@1 91.406 (87.511)\tPrec@5 99.219 (99.285)\n",
      "Epoch: [142][80/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 2.2159 (2.0433)\tPrec@1 89.062 (87.375)\tPrec@5 100.000 (99.286)\n",
      "Epoch: [142][90/97], lr: 0.01000\tTime 0.327 (0.331)\tData 0.000 (0.019)\tLoss 2.3081 (2.0317)\tPrec@1 85.156 (87.423)\tPrec@5 99.219 (99.305)\n",
      "Epoch: [142][96/97], lr: 0.01000\tTime 0.319 (0.331)\tData 0.000 (0.020)\tLoss 1.7078 (2.0279)\tPrec@1 90.678 (87.474)\tPrec@5 100.000 (99.339)\n",
      "Test: [0/100]\tTime 0.237 (0.237)\tLoss 9.4177 (9.4177)\tPrec@1 55.000 (55.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 7.0861 (8.5302)\tPrec@1 65.000 (59.818)\tPrec@5 93.000 (92.727)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.5423 (8.3781)\tPrec@1 64.000 (60.762)\tPrec@5 97.000 (93.190)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.4006 (8.3312)\tPrec@1 64.000 (61.000)\tPrec@5 94.000 (93.484)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.8766 (8.3845)\tPrec@1 62.000 (60.683)\tPrec@5 95.000 (93.585)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.3528 (8.3326)\tPrec@1 60.000 (60.980)\tPrec@5 96.000 (93.804)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.6178 (8.2916)\tPrec@1 68.000 (60.967)\tPrec@5 96.000 (93.918)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.5601 (8.3166)\tPrec@1 62.000 (60.845)\tPrec@5 97.000 (93.845)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.2982 (8.2537)\tPrec@1 65.000 (61.123)\tPrec@5 91.000 (93.790)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.8914 (8.3191)\tPrec@1 63.000 (60.912)\tPrec@5 96.000 (93.703)\n",
      "val Results: Prec@1 60.760 Prec@5 93.540 Loss 8.37046\n",
      "val Class Accuracy: [0.891,0.961,0.845,0.634,0.682,0.349,0.704,0.792,0.166,0.052]\n",
      "Best Prec@1: 68.430\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [143][0/97], lr: 0.01000\tTime 0.388 (0.388)\tData 0.200 (0.200)\tLoss 2.3167 (2.3167)\tPrec@1 87.500 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [143][10/97], lr: 0.01000\tTime 0.329 (0.343)\tData 0.000 (0.033)\tLoss 1.9437 (2.0072)\tPrec@1 89.844 (88.210)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [143][20/97], lr: 0.01000\tTime 0.328 (0.335)\tData 0.000 (0.026)\tLoss 2.1806 (1.9567)\tPrec@1 85.938 (88.058)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [143][30/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.023)\tLoss 2.2498 (2.0043)\tPrec@1 86.719 (87.702)\tPrec@5 96.875 (99.345)\n",
      "Epoch: [143][40/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.022)\tLoss 2.1485 (1.9791)\tPrec@1 85.938 (87.995)\tPrec@5 99.219 (99.314)\n",
      "Epoch: [143][50/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.021)\tLoss 1.9314 (2.0067)\tPrec@1 89.062 (87.806)\tPrec@5 100.000 (99.357)\n",
      "Epoch: [143][60/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 1.6740 (2.0201)\tPrec@1 91.406 (87.718)\tPrec@5 100.000 (99.411)\n",
      "Epoch: [143][70/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 1.2663 (1.9892)\tPrec@1 93.750 (87.896)\tPrec@5 99.219 (99.417)\n",
      "Epoch: [143][80/97], lr: 0.01000\tTime 0.323 (0.330)\tData 0.000 (0.019)\tLoss 2.0893 (2.0011)\tPrec@1 89.844 (87.809)\tPrec@5 98.438 (99.402)\n",
      "Epoch: [143][90/97], lr: 0.01000\tTime 0.324 (0.330)\tData 0.000 (0.019)\tLoss 1.6076 (2.0062)\tPrec@1 89.062 (87.715)\tPrec@5 100.000 (99.416)\n",
      "Epoch: [143][96/97], lr: 0.01000\tTime 0.317 (0.330)\tData 0.000 (0.020)\tLoss 2.3085 (2.0150)\tPrec@1 85.593 (87.683)\tPrec@5 99.153 (99.444)\n",
      "Test: [0/100]\tTime 0.254 (0.254)\tLoss 9.1026 (9.1026)\tPrec@1 59.000 (59.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.6340 (8.8650)\tPrec@1 63.000 (58.545)\tPrec@5 97.000 (96.545)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 8.6829 (8.8476)\tPrec@1 56.000 (58.048)\tPrec@5 96.000 (96.667)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.8463 (8.8712)\tPrec@1 57.000 (57.613)\tPrec@5 96.000 (96.484)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.4150 (8.9035)\tPrec@1 56.000 (57.585)\tPrec@5 97.000 (96.341)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.9887 (8.7728)\tPrec@1 55.000 (58.098)\tPrec@5 96.000 (96.353)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.9610 (8.8093)\tPrec@1 68.000 (57.803)\tPrec@5 97.000 (96.393)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.0209 (8.7654)\tPrec@1 61.000 (58.042)\tPrec@5 98.000 (96.479)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.8001 (8.7692)\tPrec@1 55.000 (57.852)\tPrec@5 96.000 (96.556)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.4951 (8.8124)\tPrec@1 59.000 (57.659)\tPrec@5 99.000 (96.593)\n",
      "val Results: Prec@1 57.870 Prec@5 96.610 Loss 8.79310\n",
      "val Class Accuracy: [0.894,0.944,0.623,0.949,0.581,0.255,0.317,0.326,0.698,0.200]\n",
      "Best Prec@1: 68.430\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [144][0/97], lr: 0.01000\tTime 0.448 (0.448)\tData 0.227 (0.227)\tLoss 2.1061 (2.1061)\tPrec@1 86.719 (86.719)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [144][10/97], lr: 0.01000\tTime 0.327 (0.345)\tData 0.000 (0.035)\tLoss 2.5823 (2.2757)\tPrec@1 81.250 (86.080)\tPrec@5 99.219 (99.077)\n",
      "Epoch: [144][20/97], lr: 0.01000\tTime 0.324 (0.337)\tData 0.000 (0.027)\tLoss 1.6829 (2.1238)\tPrec@1 89.062 (86.979)\tPrec@5 98.438 (99.182)\n",
      "Epoch: [144][30/97], lr: 0.01000\tTime 0.325 (0.335)\tData 0.000 (0.024)\tLoss 2.9011 (2.1162)\tPrec@1 80.469 (87.097)\tPrec@5 99.219 (99.320)\n",
      "Epoch: [144][40/97], lr: 0.01000\tTime 0.329 (0.333)\tData 0.000 (0.022)\tLoss 2.0503 (2.0633)\tPrec@1 88.281 (87.538)\tPrec@5 97.656 (99.257)\n",
      "Epoch: [144][50/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 1.8451 (2.0517)\tPrec@1 86.719 (87.331)\tPrec@5 98.438 (99.295)\n",
      "Epoch: [144][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 1.4241 (2.0532)\tPrec@1 92.188 (87.410)\tPrec@5 100.000 (99.360)\n",
      "Epoch: [144][70/97], lr: 0.01000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 2.2035 (2.0547)\tPrec@1 88.281 (87.423)\tPrec@5 100.000 (99.373)\n",
      "Epoch: [144][80/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 1.9962 (2.0686)\tPrec@1 89.062 (87.336)\tPrec@5 100.000 (99.373)\n",
      "Epoch: [144][90/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.019)\tLoss 1.4979 (2.0452)\tPrec@1 89.844 (87.483)\tPrec@5 99.219 (99.365)\n",
      "Epoch: [144][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 1.7435 (2.0289)\tPrec@1 89.831 (87.530)\tPrec@5 99.153 (99.371)\n",
      "Test: [0/100]\tTime 0.256 (0.256)\tLoss 10.4770 (10.4770)\tPrec@1 53.000 (53.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.6812 (8.9638)\tPrec@1 68.000 (58.545)\tPrec@5 94.000 (94.273)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.9813 (8.9627)\tPrec@1 58.000 (58.190)\tPrec@5 97.000 (94.571)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.7880 (8.8101)\tPrec@1 62.000 (58.806)\tPrec@5 92.000 (94.290)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.1530 (8.7400)\tPrec@1 56.000 (59.293)\tPrec@5 94.000 (94.244)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.9915 (8.6797)\tPrec@1 62.000 (59.569)\tPrec@5 99.000 (94.255)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7430 (8.6506)\tPrec@1 65.000 (59.377)\tPrec@5 95.000 (94.443)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.0249 (8.6351)\tPrec@1 63.000 (59.690)\tPrec@5 97.000 (94.282)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.0169 (8.5901)\tPrec@1 62.000 (59.926)\tPrec@5 95.000 (94.395)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.8364 (8.6927)\tPrec@1 60.000 (59.363)\tPrec@5 97.000 (94.363)\n",
      "val Results: Prec@1 59.220 Prec@5 94.310 Loss 8.73334\n",
      "val Class Accuracy: [0.974,0.966,0.847,0.704,0.685,0.389,0.455,0.580,0.053,0.269]\n",
      "Best Prec@1: 68.430\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [145][0/97], lr: 0.01000\tTime 0.469 (0.469)\tData 0.247 (0.247)\tLoss 1.5148 (1.5148)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [145][10/97], lr: 0.01000\tTime 0.328 (0.344)\tData 0.000 (0.037)\tLoss 2.0069 (2.0014)\tPrec@1 86.719 (87.855)\tPrec@5 100.000 (99.006)\n",
      "Epoch: [145][20/97], lr: 0.01000\tTime 0.324 (0.336)\tData 0.000 (0.028)\tLoss 2.0565 (2.0305)\tPrec@1 90.625 (87.872)\tPrec@5 100.000 (99.070)\n",
      "Epoch: [145][30/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.024)\tLoss 2.4021 (2.0200)\tPrec@1 85.938 (87.777)\tPrec@5 99.219 (99.118)\n",
      "Epoch: [145][40/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.023)\tLoss 2.1933 (1.9811)\tPrec@1 85.938 (88.072)\tPrec@5 99.219 (99.162)\n",
      "Epoch: [145][50/97], lr: 0.01000\tTime 0.329 (0.332)\tData 0.000 (0.022)\tLoss 1.6226 (1.9853)\tPrec@1 89.062 (87.960)\tPrec@5 100.000 (99.188)\n",
      "Epoch: [145][60/97], lr: 0.01000\tTime 0.329 (0.331)\tData 0.000 (0.021)\tLoss 1.5138 (1.9957)\tPrec@1 89.844 (87.820)\tPrec@5 99.219 (99.206)\n",
      "Epoch: [145][70/97], lr: 0.01000\tTime 0.328 (0.331)\tData 0.000 (0.020)\tLoss 2.0348 (2.0174)\tPrec@1 91.406 (87.797)\tPrec@5 100.000 (99.175)\n",
      "Epoch: [145][80/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 2.2985 (2.0246)\tPrec@1 87.500 (87.770)\tPrec@5 98.438 (99.122)\n",
      "Epoch: [145][90/97], lr: 0.01000\tTime 0.328 (0.331)\tData 0.000 (0.020)\tLoss 1.9971 (2.0178)\tPrec@1 86.719 (87.749)\tPrec@5 100.000 (99.116)\n",
      "Epoch: [145][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 2.6233 (2.0144)\tPrec@1 84.746 (87.804)\tPrec@5 99.153 (99.154)\n",
      "Test: [0/100]\tTime 0.246 (0.246)\tLoss 8.8030 (8.8030)\tPrec@1 59.000 (59.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.0397 (8.2855)\tPrec@1 64.000 (60.909)\tPrec@5 92.000 (93.273)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.5435 (8.1771)\tPrec@1 62.000 (60.905)\tPrec@5 97.000 (93.619)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.2420 (8.1944)\tPrec@1 67.000 (61.161)\tPrec@5 93.000 (93.774)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.4701 (8.1205)\tPrec@1 64.000 (61.512)\tPrec@5 95.000 (93.854)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.3003 (8.0602)\tPrec@1 69.000 (61.863)\tPrec@5 95.000 (93.941)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7366 (8.0100)\tPrec@1 68.000 (62.066)\tPrec@5 95.000 (93.984)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.3490 (8.0253)\tPrec@1 68.000 (61.944)\tPrec@5 97.000 (93.958)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.6809 (7.9925)\tPrec@1 64.000 (62.099)\tPrec@5 95.000 (94.074)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.2666 (8.0878)\tPrec@1 61.000 (61.571)\tPrec@5 94.000 (93.901)\n",
      "val Results: Prec@1 61.510 Prec@5 93.890 Loss 8.12754\n",
      "val Class Accuracy: [0.936,0.985,0.785,0.747,0.845,0.414,0.353,0.660,0.162,0.264]\n",
      "Best Prec@1: 68.430\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [146][0/97], lr: 0.01000\tTime 0.444 (0.444)\tData 0.239 (0.239)\tLoss 0.9347 (0.9347)\tPrec@1 94.531 (94.531)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [146][10/97], lr: 0.01000\tTime 0.326 (0.344)\tData 0.000 (0.037)\tLoss 1.7545 (1.8926)\tPrec@1 89.062 (88.210)\tPrec@5 98.438 (99.290)\n",
      "Epoch: [146][20/97], lr: 0.01000\tTime 0.334 (0.336)\tData 0.000 (0.027)\tLoss 1.8065 (1.9537)\tPrec@1 91.406 (87.686)\tPrec@5 100.000 (99.293)\n",
      "Epoch: [146][30/97], lr: 0.01000\tTime 0.323 (0.333)\tData 0.000 (0.024)\tLoss 2.7275 (1.9659)\tPrec@1 82.812 (87.424)\tPrec@5 99.219 (99.345)\n",
      "Epoch: [146][40/97], lr: 0.01000\tTime 0.329 (0.333)\tData 0.000 (0.022)\tLoss 2.2923 (1.9668)\tPrec@1 84.375 (87.557)\tPrec@5 100.000 (99.352)\n",
      "Epoch: [146][50/97], lr: 0.01000\tTime 0.322 (0.331)\tData 0.000 (0.021)\tLoss 1.5998 (1.9545)\tPrec@1 89.844 (87.730)\tPrec@5 98.438 (99.372)\n",
      "Epoch: [146][60/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.021)\tLoss 2.0773 (1.9612)\tPrec@1 88.281 (87.692)\tPrec@5 99.219 (99.347)\n",
      "Epoch: [146][70/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 2.4403 (1.9924)\tPrec@1 85.938 (87.599)\tPrec@5 98.438 (99.252)\n",
      "Epoch: [146][80/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 2.0596 (2.0368)\tPrec@1 87.500 (87.452)\tPrec@5 99.219 (99.257)\n",
      "Epoch: [146][90/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 1.6601 (2.0059)\tPrec@1 89.844 (87.637)\tPrec@5 100.000 (99.287)\n",
      "Epoch: [146][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 2.7132 (2.0291)\tPrec@1 83.051 (87.498)\tPrec@5 99.153 (99.266)\n",
      "Test: [0/100]\tTime 0.280 (0.280)\tLoss 9.4946 (9.4946)\tPrec@1 56.000 (56.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 7.4192 (9.1074)\tPrec@1 65.000 (57.364)\tPrec@5 95.000 (93.182)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 7.7209 (8.9957)\tPrec@1 63.000 (58.190)\tPrec@5 94.000 (93.476)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 8.4517 (9.0420)\tPrec@1 58.000 (58.226)\tPrec@5 91.000 (93.161)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 9.7554 (9.0344)\tPrec@1 56.000 (58.415)\tPrec@5 90.000 (93.122)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 8.7251 (8.9826)\tPrec@1 62.000 (58.588)\tPrec@5 94.000 (93.137)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.7010 (8.9507)\tPrec@1 65.000 (58.590)\tPrec@5 94.000 (93.164)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.9113 (8.9197)\tPrec@1 58.000 (58.732)\tPrec@5 96.000 (93.155)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.9981 (8.8721)\tPrec@1 62.000 (58.802)\tPrec@5 89.000 (93.210)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.4321 (8.9171)\tPrec@1 57.000 (58.747)\tPrec@5 96.000 (93.176)\n",
      "val Results: Prec@1 58.790 Prec@5 93.130 Loss 8.94715\n",
      "val Class Accuracy: [0.973,0.992,0.815,0.644,0.802,0.406,0.714,0.302,0.214,0.017]\n",
      "Best Prec@1: 68.430\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [147][0/97], lr: 0.01000\tTime 0.431 (0.431)\tData 0.207 (0.207)\tLoss 1.4240 (1.4240)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [147][10/97], lr: 0.01000\tTime 0.328 (0.341)\tData 0.000 (0.034)\tLoss 1.5553 (1.9089)\tPrec@1 91.406 (89.134)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [147][20/97], lr: 0.01000\tTime 0.325 (0.336)\tData 0.000 (0.026)\tLoss 2.1341 (1.9755)\tPrec@1 89.844 (88.579)\tPrec@5 98.438 (99.182)\n",
      "Epoch: [147][30/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.023)\tLoss 1.7471 (1.9433)\tPrec@1 88.281 (88.432)\tPrec@5 100.000 (99.370)\n",
      "Epoch: [147][40/97], lr: 0.01000\tTime 0.329 (0.333)\tData 0.000 (0.022)\tLoss 1.8963 (1.9034)\tPrec@1 89.062 (88.605)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [147][50/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.021)\tLoss 1.7991 (1.9165)\tPrec@1 89.062 (88.542)\tPrec@5 99.219 (99.203)\n",
      "Epoch: [147][60/97], lr: 0.01000\tTime 0.329 (0.333)\tData 0.000 (0.020)\tLoss 1.6522 (1.9298)\tPrec@1 89.062 (88.397)\tPrec@5 98.438 (99.103)\n",
      "Epoch: [147][70/97], lr: 0.01000\tTime 0.328 (0.332)\tData 0.000 (0.020)\tLoss 2.0973 (1.9495)\tPrec@1 85.156 (88.182)\tPrec@5 99.219 (99.109)\n",
      "Epoch: [147][80/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 1.8755 (1.9775)\tPrec@1 88.281 (88.002)\tPrec@5 99.219 (99.122)\n",
      "Epoch: [147][90/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.019)\tLoss 2.2033 (1.9771)\tPrec@1 85.156 (88.015)\tPrec@5 99.219 (99.116)\n",
      "Epoch: [147][96/97], lr: 0.01000\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 1.5759 (1.9736)\tPrec@1 90.678 (88.046)\tPrec@5 100.000 (99.154)\n",
      "Test: [0/100]\tTime 0.232 (0.232)\tLoss 7.8313 (7.8313)\tPrec@1 62.000 (62.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 6.8898 (7.6028)\tPrec@1 67.000 (64.545)\tPrec@5 97.000 (96.727)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.8916 (7.6945)\tPrec@1 65.000 (64.048)\tPrec@5 95.000 (97.095)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 6.9296 (7.6486)\tPrec@1 68.000 (64.484)\tPrec@5 96.000 (97.097)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.1304 (7.6455)\tPrec@1 68.000 (64.512)\tPrec@5 98.000 (97.098)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.2363 (7.6181)\tPrec@1 66.000 (64.490)\tPrec@5 96.000 (97.039)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 5.2995 (7.5444)\tPrec@1 74.000 (64.607)\tPrec@5 98.000 (97.148)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.7451 (7.5415)\tPrec@1 63.000 (64.704)\tPrec@5 97.000 (97.254)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.9614 (7.5077)\tPrec@1 69.000 (64.778)\tPrec@5 96.000 (97.259)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.9315 (7.6010)\tPrec@1 63.000 (64.330)\tPrec@5 99.000 (97.231)\n",
      "val Results: Prec@1 64.180 Prec@5 97.290 Loss 7.62487\n",
      "val Class Accuracy: [0.978,0.927,0.746,0.526,0.777,0.591,0.762,0.670,0.254,0.187]\n",
      "Best Prec@1: 68.430\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [148][0/97], lr: 0.01000\tTime 0.466 (0.466)\tData 0.260 (0.260)\tLoss 1.5901 (1.5901)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [148][10/97], lr: 0.01000\tTime 0.335 (0.348)\tData 0.000 (0.038)\tLoss 2.7799 (1.8653)\tPrec@1 85.156 (88.707)\tPrec@5 98.438 (99.574)\n",
      "Epoch: [148][20/97], lr: 0.01000\tTime 0.326 (0.337)\tData 0.000 (0.028)\tLoss 2.0536 (1.9501)\tPrec@1 86.719 (88.281)\tPrec@5 98.438 (99.516)\n",
      "Epoch: [148][30/97], lr: 0.01000\tTime 0.327 (0.336)\tData 0.000 (0.024)\tLoss 2.3388 (1.9095)\tPrec@1 84.375 (88.558)\tPrec@5 99.219 (99.521)\n",
      "Epoch: [148][40/97], lr: 0.01000\tTime 0.329 (0.334)\tData 0.000 (0.023)\tLoss 1.9139 (1.9190)\tPrec@1 89.844 (88.529)\tPrec@5 98.438 (99.428)\n",
      "Epoch: [148][50/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.022)\tLoss 2.0609 (1.9156)\tPrec@1 85.938 (88.404)\tPrec@5 99.219 (99.341)\n",
      "Epoch: [148][60/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.021)\tLoss 2.6702 (1.9197)\tPrec@1 83.594 (88.358)\tPrec@5 98.438 (99.296)\n",
      "Epoch: [148][70/97], lr: 0.01000\tTime 0.328 (0.332)\tData 0.000 (0.020)\tLoss 2.4714 (1.9549)\tPrec@1 82.812 (88.094)\tPrec@5 98.438 (99.296)\n",
      "Epoch: [148][80/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 1.2998 (1.9964)\tPrec@1 92.188 (87.789)\tPrec@5 98.438 (99.257)\n",
      "Epoch: [148][90/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 1.6612 (2.0162)\tPrec@1 90.625 (87.689)\tPrec@5 100.000 (99.270)\n",
      "Epoch: [148][96/97], lr: 0.01000\tTime 0.319 (0.332)\tData 0.000 (0.020)\tLoss 1.6960 (2.0141)\tPrec@1 88.983 (87.716)\tPrec@5 100.000 (99.250)\n",
      "Test: [0/100]\tTime 0.253 (0.253)\tLoss 10.1272 (10.1272)\tPrec@1 56.000 (56.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.5195 (9.3837)\tPrec@1 63.000 (56.455)\tPrec@5 96.000 (94.364)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.5717 (9.4244)\tPrec@1 61.000 (55.857)\tPrec@5 95.000 (94.333)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 9.2169 (9.4789)\tPrec@1 54.000 (55.742)\tPrec@5 95.000 (94.452)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.5795 (9.4691)\tPrec@1 58.000 (55.854)\tPrec@5 93.000 (94.366)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.8452 (9.3475)\tPrec@1 59.000 (56.392)\tPrec@5 95.000 (94.412)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.1725 (9.3000)\tPrec@1 65.000 (56.377)\tPrec@5 94.000 (94.492)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.8140 (9.2834)\tPrec@1 57.000 (56.535)\tPrec@5 96.000 (94.549)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.7714 (9.2289)\tPrec@1 61.000 (56.840)\tPrec@5 91.000 (94.556)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.3246 (9.3279)\tPrec@1 58.000 (56.264)\tPrec@5 94.000 (94.527)\n",
      "val Results: Prec@1 56.200 Prec@5 94.530 Loss 9.34910\n",
      "val Class Accuracy: [0.935,0.993,0.873,0.812,0.522,0.416,0.337,0.420,0.082,0.230]\n",
      "Best Prec@1: 68.430\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [149][0/97], lr: 0.01000\tTime 0.464 (0.464)\tData 0.237 (0.237)\tLoss 2.3030 (2.3030)\tPrec@1 83.594 (83.594)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [149][10/97], lr: 0.01000\tTime 0.325 (0.346)\tData 0.000 (0.037)\tLoss 2.2698 (2.2508)\tPrec@1 86.719 (86.364)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [149][20/97], lr: 0.01000\tTime 0.325 (0.336)\tData 0.000 (0.027)\tLoss 1.3948 (2.1765)\tPrec@1 89.844 (86.682)\tPrec@5 100.000 (99.107)\n",
      "Epoch: [149][30/97], lr: 0.01000\tTime 0.328 (0.335)\tData 0.000 (0.024)\tLoss 1.5107 (2.1037)\tPrec@1 89.844 (87.248)\tPrec@5 99.219 (99.194)\n",
      "Epoch: [149][40/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.022)\tLoss 1.5125 (2.0804)\tPrec@1 92.969 (87.386)\tPrec@5 100.000 (99.162)\n",
      "Epoch: [149][50/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.021)\tLoss 2.1875 (2.0647)\tPrec@1 87.500 (87.408)\tPrec@5 99.219 (99.203)\n",
      "Epoch: [149][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 1.9328 (2.0237)\tPrec@1 89.062 (87.679)\tPrec@5 98.438 (99.180)\n",
      "Epoch: [149][70/97], lr: 0.01000\tTime 0.328 (0.332)\tData 0.000 (0.020)\tLoss 2.0182 (2.0388)\tPrec@1 86.719 (87.577)\tPrec@5 100.000 (99.208)\n",
      "Epoch: [149][80/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.0807 (2.0074)\tPrec@1 89.062 (87.857)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [149][90/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 1.9638 (2.0094)\tPrec@1 85.938 (87.904)\tPrec@5 99.219 (99.236)\n",
      "Epoch: [149][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 3.0272 (2.0238)\tPrec@1 83.051 (87.828)\tPrec@5 99.153 (99.258)\n",
      "Test: [0/100]\tTime 0.265 (0.265)\tLoss 6.8752 (6.8752)\tPrec@1 73.000 (73.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.6132 (7.3519)\tPrec@1 68.000 (66.182)\tPrec@5 96.000 (95.182)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.0488 (7.1204)\tPrec@1 76.000 (67.286)\tPrec@5 99.000 (95.619)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.3226 (7.2166)\tPrec@1 66.000 (66.581)\tPrec@5 99.000 (95.806)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 7.5686 (7.2184)\tPrec@1 66.000 (66.683)\tPrec@5 94.000 (95.805)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.9520 (7.1845)\tPrec@1 67.000 (66.706)\tPrec@5 96.000 (96.039)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.3270 (7.1651)\tPrec@1 68.000 (66.852)\tPrec@5 96.000 (95.984)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 7.6148 (7.1733)\tPrec@1 64.000 (66.732)\tPrec@5 97.000 (95.944)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.8797 (7.1371)\tPrec@1 69.000 (67.037)\tPrec@5 95.000 (95.975)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.2323 (7.2175)\tPrec@1 70.000 (66.626)\tPrec@5 97.000 (95.989)\n",
      "val Results: Prec@1 66.580 Prec@5 95.990 Loss 7.22644\n",
      "val Class Accuracy: [0.857,0.992,0.711,0.465,0.632,0.892,0.641,0.563,0.490,0.415]\n",
      "Best Prec@1: 68.430\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [150][0/97], lr: 0.01000\tTime 0.432 (0.432)\tData 0.207 (0.207)\tLoss 1.3858 (1.3858)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [150][10/97], lr: 0.01000\tTime 0.323 (0.341)\tData 0.000 (0.034)\tLoss 2.1393 (1.8857)\tPrec@1 88.281 (88.849)\tPrec@5 97.656 (99.290)\n",
      "Epoch: [150][20/97], lr: 0.01000\tTime 0.323 (0.334)\tData 0.000 (0.026)\tLoss 1.7385 (1.8637)\tPrec@1 91.406 (89.137)\tPrec@5 98.438 (99.330)\n",
      "Epoch: [150][30/97], lr: 0.01000\tTime 0.327 (0.334)\tData 0.000 (0.023)\tLoss 2.3428 (1.8699)\tPrec@1 85.156 (89.163)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [150][40/97], lr: 0.01000\tTime 0.329 (0.333)\tData 0.000 (0.022)\tLoss 1.3704 (1.8913)\tPrec@1 91.406 (88.853)\tPrec@5 100.000 (99.390)\n",
      "Epoch: [150][50/97], lr: 0.01000\tTime 0.329 (0.332)\tData 0.000 (0.021)\tLoss 2.0967 (1.9314)\tPrec@1 84.375 (88.450)\tPrec@5 100.000 (99.449)\n",
      "Epoch: [150][60/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 1.3745 (1.9076)\tPrec@1 92.188 (88.563)\tPrec@5 100.000 (99.436)\n",
      "Epoch: [150][70/97], lr: 0.01000\tTime 0.333 (0.332)\tData 0.000 (0.020)\tLoss 1.8254 (1.9375)\tPrec@1 89.844 (88.369)\tPrec@5 98.438 (99.384)\n",
      "Epoch: [150][80/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 1.9418 (1.9494)\tPrec@1 89.062 (88.281)\tPrec@5 100.000 (99.412)\n",
      "Epoch: [150][90/97], lr: 0.01000\tTime 0.332 (0.332)\tData 0.000 (0.019)\tLoss 1.5173 (1.9346)\tPrec@1 89.844 (88.316)\tPrec@5 99.219 (99.408)\n",
      "Epoch: [150][96/97], lr: 0.01000\tTime 0.316 (0.331)\tData 0.000 (0.020)\tLoss 2.1842 (1.9413)\tPrec@1 84.746 (88.296)\tPrec@5 100.000 (99.404)\n",
      "Test: [0/100]\tTime 0.249 (0.249)\tLoss 7.1708 (7.1708)\tPrec@1 67.000 (67.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 5.8158 (7.1100)\tPrec@1 74.000 (66.182)\tPrec@5 99.000 (97.273)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.6086 (7.0475)\tPrec@1 73.000 (66.095)\tPrec@5 97.000 (97.238)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.0303 (7.0871)\tPrec@1 67.000 (66.355)\tPrec@5 96.000 (96.806)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.0557 (7.1437)\tPrec@1 67.000 (66.122)\tPrec@5 96.000 (96.634)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.6869 (7.0640)\tPrec@1 68.000 (66.471)\tPrec@5 95.000 (96.706)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 5.6425 (7.0319)\tPrec@1 74.000 (66.541)\tPrec@5 98.000 (96.689)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 6.8234 (7.0244)\tPrec@1 70.000 (66.718)\tPrec@5 98.000 (96.690)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.4518 (7.0152)\tPrec@1 66.000 (66.753)\tPrec@5 94.000 (96.741)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.9482 (7.0674)\tPrec@1 68.000 (66.484)\tPrec@5 98.000 (96.747)\n",
      "val Results: Prec@1 66.250 Prec@5 96.680 Loss 7.11508\n",
      "val Class Accuracy: [0.947,0.963,0.839,0.684,0.709,0.620,0.708,0.464,0.461,0.230]\n",
      "Best Prec@1: 68.430\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [151][0/97], lr: 0.01000\tTime 0.474 (0.474)\tData 0.277 (0.277)\tLoss 1.7988 (1.7988)\tPrec@1 89.062 (89.062)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [151][10/97], lr: 0.01000\tTime 0.330 (0.348)\tData 0.000 (0.040)\tLoss 2.1941 (1.7259)\tPrec@1 86.719 (89.560)\tPrec@5 98.438 (99.148)\n",
      "Epoch: [151][20/97], lr: 0.01000\tTime 0.323 (0.339)\tData 0.000 (0.029)\tLoss 1.8339 (1.7950)\tPrec@1 91.406 (89.137)\tPrec@5 98.438 (99.256)\n",
      "Epoch: [151][30/97], lr: 0.01000\tTime 0.329 (0.336)\tData 0.000 (0.025)\tLoss 1.7882 (1.8377)\tPrec@1 89.062 (88.987)\tPrec@5 100.000 (99.370)\n",
      "Epoch: [151][40/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.023)\tLoss 1.7138 (1.8789)\tPrec@1 89.844 (88.720)\tPrec@5 100.000 (99.390)\n",
      "Epoch: [151][50/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.022)\tLoss 1.9401 (1.8699)\tPrec@1 88.281 (88.695)\tPrec@5 100.000 (99.433)\n",
      "Epoch: [151][60/97], lr: 0.01000\tTime 0.327 (0.333)\tData 0.000 (0.021)\tLoss 1.6531 (1.8707)\tPrec@1 91.406 (88.614)\tPrec@5 99.219 (99.462)\n",
      "Epoch: [151][70/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.021)\tLoss 1.7953 (1.8680)\tPrec@1 87.500 (88.644)\tPrec@5 99.219 (99.461)\n",
      "Epoch: [151][80/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.6460 (1.8951)\tPrec@1 84.375 (88.571)\tPrec@5 100.000 (99.508)\n",
      "Epoch: [151][90/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 2.4473 (1.9173)\tPrec@1 85.156 (88.556)\tPrec@5 99.219 (99.493)\n",
      "Epoch: [151][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.021)\tLoss 2.6436 (1.9480)\tPrec@1 81.356 (88.296)\tPrec@5 97.458 (99.484)\n",
      "Test: [0/100]\tTime 0.235 (0.235)\tLoss 9.4916 (9.4916)\tPrec@1 59.000 (59.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 7.3539 (9.1490)\tPrec@1 64.000 (57.273)\tPrec@5 94.000 (97.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 8.1762 (9.0405)\tPrec@1 57.000 (57.571)\tPrec@5 99.000 (97.571)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 8.3715 (9.0009)\tPrec@1 57.000 (57.581)\tPrec@5 98.000 (97.677)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.0193 (9.0767)\tPrec@1 47.000 (57.146)\tPrec@5 99.000 (97.366)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.2261 (9.0134)\tPrec@1 56.000 (57.255)\tPrec@5 97.000 (97.353)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 7.5110 (9.0607)\tPrec@1 63.000 (56.787)\tPrec@5 98.000 (97.557)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.1347 (9.0760)\tPrec@1 64.000 (56.732)\tPrec@5 99.000 (97.648)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.3983 (9.0108)\tPrec@1 67.000 (57.123)\tPrec@5 96.000 (97.667)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.3143 (9.0742)\tPrec@1 57.000 (56.791)\tPrec@5 99.000 (97.714)\n",
      "val Results: Prec@1 56.800 Prec@5 97.750 Loss 9.07906\n",
      "val Class Accuracy: [0.911,0.970,0.950,0.480,0.470,0.285,0.481,0.430,0.397,0.306]\n",
      "Best Prec@1: 68.430\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [152][0/97], lr: 0.01000\tTime 0.459 (0.459)\tData 0.252 (0.252)\tLoss 1.7529 (1.7529)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [152][10/97], lr: 0.01000\tTime 0.326 (0.345)\tData 0.000 (0.038)\tLoss 2.5495 (2.1146)\tPrec@1 84.375 (86.790)\tPrec@5 99.219 (99.148)\n",
      "Epoch: [152][20/97], lr: 0.01000\tTime 0.325 (0.337)\tData 0.000 (0.028)\tLoss 1.3733 (2.0600)\tPrec@1 92.969 (87.760)\tPrec@5 99.219 (99.293)\n",
      "Epoch: [152][30/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.025)\tLoss 2.6291 (2.0466)\tPrec@1 82.812 (87.752)\tPrec@5 100.000 (99.420)\n",
      "Epoch: [152][40/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.023)\tLoss 2.4427 (2.0755)\tPrec@1 84.375 (87.329)\tPrec@5 97.656 (99.371)\n",
      "Epoch: [152][50/97], lr: 0.01000\tTime 0.328 (0.332)\tData 0.000 (0.022)\tLoss 1.2525 (2.0793)\tPrec@1 94.531 (87.485)\tPrec@5 100.000 (99.311)\n",
      "Epoch: [152][60/97], lr: 0.01000\tTime 0.339 (0.332)\tData 0.000 (0.021)\tLoss 1.9822 (2.0442)\tPrec@1 87.500 (87.769)\tPrec@5 100.000 (99.347)\n",
      "Epoch: [152][70/97], lr: 0.01000\tTime 0.327 (0.331)\tData 0.000 (0.020)\tLoss 1.4563 (2.0382)\tPrec@1 90.625 (87.775)\tPrec@5 98.438 (99.285)\n",
      "Epoch: [152][80/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 1.4464 (2.0109)\tPrec@1 92.188 (88.002)\tPrec@5 99.219 (99.267)\n",
      "Epoch: [152][90/97], lr: 0.01000\tTime 0.322 (0.331)\tData 0.000 (0.020)\tLoss 2.4373 (1.9970)\tPrec@1 84.375 (88.084)\tPrec@5 99.219 (99.253)\n",
      "Epoch: [152][96/97], lr: 0.01000\tTime 0.319 (0.330)\tData 0.000 (0.020)\tLoss 2.6498 (2.0022)\tPrec@1 82.203 (88.014)\tPrec@5 99.153 (99.234)\n",
      "Test: [0/100]\tTime 0.255 (0.255)\tLoss 6.3033 (6.3033)\tPrec@1 76.000 (76.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 5.1435 (6.8675)\tPrec@1 76.000 (68.364)\tPrec@5 99.000 (97.364)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.4889 (6.5810)\tPrec@1 65.000 (69.095)\tPrec@5 99.000 (97.143)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.4759 (6.7002)\tPrec@1 69.000 (68.387)\tPrec@5 96.000 (96.806)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 6.8616 (6.7569)\tPrec@1 67.000 (68.098)\tPrec@5 97.000 (96.561)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.3791 (6.6696)\tPrec@1 74.000 (68.804)\tPrec@5 97.000 (96.765)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 5.8601 (6.6764)\tPrec@1 71.000 (68.705)\tPrec@5 96.000 (96.656)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 6.0457 (6.6408)\tPrec@1 73.000 (68.873)\tPrec@5 99.000 (96.789)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.8460 (6.6188)\tPrec@1 69.000 (69.099)\tPrec@5 97.000 (96.889)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.7545 (6.6598)\tPrec@1 69.000 (69.022)\tPrec@5 98.000 (96.868)\n",
      "val Results: Prec@1 69.010 Prec@5 96.870 Loss 6.68862\n",
      "val Class Accuracy: [0.893,0.970,0.716,0.771,0.912,0.508,0.629,0.236,0.659,0.607]\n",
      "Best Prec@1: 69.010\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [153][0/97], lr: 0.01000\tTime 0.457 (0.457)\tData 0.231 (0.231)\tLoss 1.8673 (1.8673)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [153][10/97], lr: 0.01000\tTime 0.329 (0.346)\tData 0.000 (0.036)\tLoss 1.7541 (1.8100)\tPrec@1 89.062 (89.205)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [153][20/97], lr: 0.01000\tTime 0.326 (0.338)\tData 0.000 (0.027)\tLoss 1.9375 (1.9017)\tPrec@1 87.500 (88.728)\tPrec@5 99.219 (99.405)\n",
      "Epoch: [153][30/97], lr: 0.01000\tTime 0.329 (0.337)\tData 0.000 (0.024)\tLoss 2.0816 (1.9570)\tPrec@1 87.500 (88.432)\tPrec@5 100.000 (99.471)\n",
      "Epoch: [153][40/97], lr: 0.01000\tTime 0.330 (0.335)\tData 0.000 (0.022)\tLoss 1.6896 (1.9679)\tPrec@1 89.844 (88.186)\tPrec@5 100.000 (99.505)\n",
      "Epoch: [153][50/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.021)\tLoss 2.0184 (1.9916)\tPrec@1 89.844 (88.205)\tPrec@5 99.219 (99.387)\n",
      "Epoch: [153][60/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.021)\tLoss 2.1996 (1.9802)\tPrec@1 84.375 (88.140)\tPrec@5 100.000 (99.411)\n",
      "Epoch: [153][70/97], lr: 0.01000\tTime 0.330 (0.333)\tData 0.000 (0.020)\tLoss 1.6672 (1.9773)\tPrec@1 89.062 (88.105)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [153][80/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.020)\tLoss 1.9686 (1.9819)\tPrec@1 87.500 (88.021)\tPrec@5 97.656 (99.402)\n",
      "Epoch: [153][90/97], lr: 0.01000\tTime 0.322 (0.332)\tData 0.000 (0.019)\tLoss 1.8628 (1.9670)\tPrec@1 89.062 (88.135)\tPrec@5 99.219 (99.373)\n",
      "Epoch: [153][96/97], lr: 0.01000\tTime 0.317 (0.332)\tData 0.000 (0.020)\tLoss 2.5084 (1.9681)\tPrec@1 86.441 (88.135)\tPrec@5 100.000 (99.371)\n",
      "Test: [0/100]\tTime 0.257 (0.257)\tLoss 7.4680 (7.4680)\tPrec@1 64.000 (64.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.6145 (7.9710)\tPrec@1 68.000 (60.636)\tPrec@5 95.000 (92.091)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.0387 (7.7097)\tPrec@1 66.000 (62.952)\tPrec@5 96.000 (92.810)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.9692 (7.7418)\tPrec@1 64.000 (62.935)\tPrec@5 92.000 (92.258)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.3975 (7.7473)\tPrec@1 67.000 (63.073)\tPrec@5 92.000 (92.317)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.7704 (7.6585)\tPrec@1 69.000 (63.725)\tPrec@5 97.000 (92.569)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.3910 (7.5810)\tPrec@1 69.000 (64.049)\tPrec@5 96.000 (92.852)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 6.9421 (7.5885)\tPrec@1 66.000 (64.155)\tPrec@5 94.000 (92.746)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.3327 (7.5548)\tPrec@1 66.000 (64.296)\tPrec@5 90.000 (92.617)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.2026 (7.5855)\tPrec@1 60.000 (64.154)\tPrec@5 95.000 (92.374)\n",
      "val Results: Prec@1 64.200 Prec@5 92.310 Loss 7.58893\n",
      "val Class Accuracy: [0.907,0.936,0.641,0.765,0.698,0.609,0.821,0.705,0.296,0.042]\n",
      "Best Prec@1: 69.010\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [154][0/97], lr: 0.01000\tTime 0.424 (0.424)\tData 0.219 (0.219)\tLoss 1.3894 (1.3894)\tPrec@1 92.969 (92.969)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [154][10/97], lr: 0.01000\tTime 0.322 (0.341)\tData 0.000 (0.035)\tLoss 2.0963 (1.9269)\tPrec@1 87.500 (88.423)\tPrec@5 98.438 (99.290)\n",
      "Epoch: [154][20/97], lr: 0.01000\tTime 0.323 (0.334)\tData 0.000 (0.027)\tLoss 1.3631 (1.9804)\tPrec@1 92.188 (87.946)\tPrec@5 98.438 (99.368)\n",
      "Epoch: [154][30/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.024)\tLoss 1.2256 (1.9650)\tPrec@1 92.188 (88.105)\tPrec@5 100.000 (99.294)\n",
      "Epoch: [154][40/97], lr: 0.01000\tTime 0.330 (0.333)\tData 0.000 (0.022)\tLoss 1.7213 (1.9708)\tPrec@1 89.062 (88.129)\tPrec@5 99.219 (99.371)\n",
      "Epoch: [154][50/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 2.2320 (1.9575)\tPrec@1 88.281 (88.174)\tPrec@5 98.438 (99.387)\n",
      "Epoch: [154][60/97], lr: 0.01000\tTime 0.328 (0.332)\tData 0.000 (0.021)\tLoss 2.0103 (1.9553)\tPrec@1 89.844 (88.128)\tPrec@5 98.438 (99.436)\n",
      "Epoch: [154][70/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 2.3712 (1.9632)\tPrec@1 82.031 (88.094)\tPrec@5 100.000 (99.384)\n",
      "Epoch: [154][80/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 1.4793 (1.9579)\tPrec@1 92.969 (88.117)\tPrec@5 99.219 (99.392)\n",
      "Epoch: [154][90/97], lr: 0.01000\tTime 0.329 (0.331)\tData 0.000 (0.019)\tLoss 2.1868 (1.9841)\tPrec@1 87.500 (87.981)\tPrec@5 99.219 (99.382)\n",
      "Epoch: [154][96/97], lr: 0.01000\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 2.2201 (1.9970)\tPrec@1 86.441 (87.877)\tPrec@5 100.000 (99.395)\n",
      "Test: [0/100]\tTime 0.250 (0.250)\tLoss 10.9520 (10.9520)\tPrec@1 49.000 (49.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 8.3562 (10.0854)\tPrec@1 59.000 (51.545)\tPrec@5 94.000 (94.091)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 8.3888 (10.0964)\tPrec@1 57.000 (51.619)\tPrec@5 99.000 (94.619)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 9.4390 (10.1137)\tPrec@1 52.000 (51.097)\tPrec@5 97.000 (94.677)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.9312 (10.1199)\tPrec@1 46.000 (51.122)\tPrec@5 92.000 (94.512)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.8344 (10.0554)\tPrec@1 52.000 (51.451)\tPrec@5 94.000 (94.490)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.3824 (10.0076)\tPrec@1 60.000 (51.344)\tPrec@5 96.000 (94.590)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.6831 (9.9837)\tPrec@1 52.000 (51.408)\tPrec@5 94.000 (94.606)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.2325 (9.9511)\tPrec@1 56.000 (51.519)\tPrec@5 95.000 (94.691)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.0562 (10.0343)\tPrec@1 50.000 (51.165)\tPrec@5 98.000 (94.538)\n",
      "val Results: Prec@1 51.040 Prec@5 94.530 Loss 10.04912\n",
      "val Class Accuracy: [0.965,0.970,0.788,0.734,0.467,0.273,0.269,0.538,0.085,0.015]\n",
      "Best Prec@1: 69.010\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [155][0/97], lr: 0.01000\tTime 0.474 (0.474)\tData 0.263 (0.263)\tLoss 1.7171 (1.7171)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [155][10/97], lr: 0.01000\tTime 0.325 (0.352)\tData 0.000 (0.039)\tLoss 1.2817 (2.0556)\tPrec@1 93.750 (88.352)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [155][20/97], lr: 0.01000\tTime 0.353 (0.342)\tData 0.000 (0.029)\tLoss 2.2305 (1.9796)\tPrec@1 87.500 (88.467)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [155][30/97], lr: 0.01000\tTime 0.334 (0.346)\tData 0.000 (0.025)\tLoss 1.6438 (1.9731)\tPrec@1 89.844 (88.357)\tPrec@5 100.000 (99.269)\n",
      "Epoch: [155][40/97], lr: 0.01000\tTime 0.336 (0.344)\tData 0.000 (0.023)\tLoss 1.7015 (1.9822)\tPrec@1 89.062 (88.319)\tPrec@5 99.219 (99.257)\n",
      "Epoch: [155][50/97], lr: 0.01000\tTime 0.364 (0.343)\tData 0.000 (0.022)\tLoss 1.8249 (1.9521)\tPrec@1 90.625 (88.588)\tPrec@5 100.000 (99.249)\n",
      "Epoch: [155][60/97], lr: 0.01000\tTime 0.331 (0.342)\tData 0.000 (0.021)\tLoss 1.8962 (1.9475)\tPrec@1 89.844 (88.435)\tPrec@5 99.219 (99.296)\n",
      "Epoch: [155][70/97], lr: 0.01000\tTime 0.329 (0.342)\tData 0.000 (0.021)\tLoss 1.6147 (1.9791)\tPrec@1 89.844 (88.193)\tPrec@5 99.219 (99.329)\n",
      "Epoch: [155][80/97], lr: 0.01000\tTime 0.352 (0.342)\tData 0.000 (0.020)\tLoss 1.7578 (2.0043)\tPrec@1 89.062 (88.011)\tPrec@5 100.000 (99.325)\n",
      "Epoch: [155][90/97], lr: 0.01000\tTime 0.328 (0.341)\tData 0.000 (0.020)\tLoss 2.4422 (2.0066)\tPrec@1 85.156 (87.938)\tPrec@5 99.219 (99.287)\n",
      "Epoch: [155][96/97], lr: 0.01000\tTime 0.329 (0.340)\tData 0.000 (0.020)\tLoss 2.4567 (2.0178)\tPrec@1 83.051 (87.812)\tPrec@5 100.000 (99.299)\n",
      "Test: [0/100]\tTime 0.259 (0.259)\tLoss 8.1039 (8.1039)\tPrec@1 61.000 (61.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.4728 (8.1147)\tPrec@1 69.000 (60.727)\tPrec@5 99.000 (97.182)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.9634 (8.0696)\tPrec@1 68.000 (61.048)\tPrec@5 97.000 (97.095)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.9909 (8.0750)\tPrec@1 59.000 (61.032)\tPrec@5 98.000 (96.839)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.3787 (8.0752)\tPrec@1 59.000 (61.317)\tPrec@5 96.000 (96.780)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 8.0941 (7.9923)\tPrec@1 61.000 (61.804)\tPrec@5 97.000 (96.922)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7743 (7.9378)\tPrec@1 67.000 (61.984)\tPrec@5 96.000 (97.033)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 7.7860 (7.9151)\tPrec@1 62.000 (62.141)\tPrec@5 98.000 (97.028)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.6707 (7.9007)\tPrec@1 56.000 (62.062)\tPrec@5 96.000 (97.025)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.2977 (7.9508)\tPrec@1 62.000 (61.890)\tPrec@5 99.000 (97.044)\n",
      "val Results: Prec@1 61.840 Prec@5 97.020 Loss 7.98415\n",
      "val Class Accuracy: [0.955,0.965,0.709,0.663,0.805,0.673,0.557,0.405,0.344,0.108]\n",
      "Best Prec@1: 69.010\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [156][0/97], lr: 0.01000\tTime 0.470 (0.470)\tData 0.243 (0.243)\tLoss 2.2152 (2.2152)\tPrec@1 85.938 (85.938)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [156][10/97], lr: 0.01000\tTime 0.322 (0.349)\tData 0.000 (0.037)\tLoss 2.1647 (2.0193)\tPrec@1 84.375 (87.642)\tPrec@5 99.219 (99.361)\n",
      "Epoch: [156][20/97], lr: 0.01000\tTime 0.325 (0.338)\tData 0.000 (0.028)\tLoss 2.1438 (2.0786)\tPrec@1 85.938 (87.016)\tPrec@5 98.438 (99.330)\n",
      "Epoch: [156][30/97], lr: 0.01000\tTime 0.326 (0.337)\tData 0.000 (0.024)\tLoss 1.4765 (2.0308)\tPrec@1 88.281 (87.172)\tPrec@5 98.438 (99.294)\n",
      "Epoch: [156][40/97], lr: 0.01000\tTime 0.327 (0.335)\tData 0.000 (0.023)\tLoss 2.2088 (2.0776)\tPrec@1 84.375 (86.833)\tPrec@5 98.438 (99.352)\n",
      "Epoch: [156][50/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.021)\tLoss 1.5059 (2.0654)\tPrec@1 90.625 (86.994)\tPrec@5 100.000 (99.341)\n",
      "Epoch: [156][60/97], lr: 0.01000\tTime 0.329 (0.333)\tData 0.000 (0.021)\tLoss 2.5535 (2.0579)\tPrec@1 83.594 (87.013)\tPrec@5 99.219 (99.360)\n",
      "Epoch: [156][70/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 1.3677 (2.0412)\tPrec@1 92.188 (87.159)\tPrec@5 97.656 (99.340)\n",
      "Epoch: [156][80/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.0487 (2.0638)\tPrec@1 90.625 (87.095)\tPrec@5 99.219 (99.277)\n",
      "Epoch: [156][90/97], lr: 0.01000\tTime 0.328 (0.332)\tData 0.000 (0.020)\tLoss 1.5297 (2.0301)\tPrec@1 89.062 (87.337)\tPrec@5 99.219 (99.287)\n",
      "Epoch: [156][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 1.7655 (2.0079)\tPrec@1 88.983 (87.490)\tPrec@5 100.000 (99.299)\n",
      "Test: [0/100]\tTime 0.257 (0.257)\tLoss 9.1479 (9.1479)\tPrec@1 57.000 (57.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.9770 (8.4718)\tPrec@1 67.000 (60.000)\tPrec@5 97.000 (96.273)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.1163 (8.2629)\tPrec@1 64.000 (60.714)\tPrec@5 96.000 (96.095)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.2499 (8.2885)\tPrec@1 68.000 (60.968)\tPrec@5 97.000 (95.871)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.5690 (8.3568)\tPrec@1 62.000 (60.512)\tPrec@5 94.000 (95.634)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.6342 (8.3357)\tPrec@1 64.000 (60.588)\tPrec@5 94.000 (95.804)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.2765 (8.3654)\tPrec@1 66.000 (60.213)\tPrec@5 95.000 (95.803)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.9926 (8.3803)\tPrec@1 62.000 (60.183)\tPrec@5 96.000 (95.775)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.6075 (8.3483)\tPrec@1 56.000 (60.259)\tPrec@5 94.000 (95.852)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.4786 (8.4217)\tPrec@1 62.000 (60.044)\tPrec@5 97.000 (95.791)\n",
      "val Results: Prec@1 60.130 Prec@5 95.680 Loss 8.42881\n",
      "val Class Accuracy: [0.964,0.974,0.780,0.653,0.659,0.495,0.412,0.625,0.438,0.013]\n",
      "Best Prec@1: 69.010\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [157][0/97], lr: 0.01000\tTime 0.424 (0.424)\tData 0.215 (0.215)\tLoss 2.2920 (2.2920)\tPrec@1 85.938 (85.938)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [157][10/97], lr: 0.01000\tTime 0.329 (0.342)\tData 0.000 (0.034)\tLoss 2.6958 (2.0429)\tPrec@1 87.500 (87.855)\tPrec@5 98.438 (99.361)\n",
      "Epoch: [157][20/97], lr: 0.01000\tTime 0.319 (0.335)\tData 0.000 (0.026)\tLoss 2.1049 (1.9766)\tPrec@1 85.156 (88.021)\tPrec@5 99.219 (99.516)\n",
      "Epoch: [157][30/97], lr: 0.01000\tTime 0.327 (0.334)\tData 0.000 (0.023)\tLoss 2.0590 (2.0100)\tPrec@1 88.281 (87.853)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [157][40/97], lr: 0.01000\tTime 0.326 (0.333)\tData 0.000 (0.022)\tLoss 1.5194 (2.0173)\tPrec@1 92.188 (87.729)\tPrec@5 100.000 (99.352)\n",
      "Epoch: [157][50/97], lr: 0.01000\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 1.4342 (2.0007)\tPrec@1 91.406 (87.791)\tPrec@5 98.438 (99.387)\n",
      "Epoch: [157][60/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.020)\tLoss 1.8532 (1.9826)\tPrec@1 89.062 (87.884)\tPrec@5 100.000 (99.462)\n",
      "Epoch: [157][70/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 2.0003 (1.9488)\tPrec@1 86.719 (88.072)\tPrec@5 99.219 (99.428)\n",
      "Epoch: [157][80/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 1.2798 (1.9390)\tPrec@1 93.750 (88.194)\tPrec@5 99.219 (99.402)\n",
      "Epoch: [157][90/97], lr: 0.01000\tTime 0.329 (0.331)\tData 0.000 (0.019)\tLoss 1.7707 (1.9535)\tPrec@1 89.844 (88.084)\tPrec@5 99.219 (99.416)\n",
      "Epoch: [157][96/97], lr: 0.01000\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 1.8487 (1.9506)\tPrec@1 88.983 (88.086)\tPrec@5 98.305 (99.363)\n",
      "Test: [0/100]\tTime 0.231 (0.231)\tLoss 8.4237 (8.4237)\tPrec@1 61.000 (61.000)\tPrec@5 90.000 (90.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 7.2511 (8.7079)\tPrec@1 65.000 (57.818)\tPrec@5 92.000 (89.727)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 7.7408 (8.4001)\tPrec@1 60.000 (59.476)\tPrec@5 97.000 (91.143)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.7782 (8.4434)\tPrec@1 63.000 (59.194)\tPrec@5 91.000 (91.097)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.1932 (8.5341)\tPrec@1 58.000 (58.976)\tPrec@5 90.000 (90.976)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.5451 (8.4552)\tPrec@1 62.000 (59.333)\tPrec@5 95.000 (91.039)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.8941 (8.3958)\tPrec@1 67.000 (59.607)\tPrec@5 95.000 (91.246)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.4913 (8.4131)\tPrec@1 60.000 (59.394)\tPrec@5 94.000 (91.085)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.0987 (8.3717)\tPrec@1 60.000 (59.716)\tPrec@5 89.000 (91.148)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.9316 (8.3865)\tPrec@1 62.000 (59.725)\tPrec@5 93.000 (91.055)\n",
      "val Results: Prec@1 59.630 Prec@5 91.020 Loss 8.40823\n",
      "val Class Accuracy: [0.689,0.906,0.762,0.847,0.668,0.466,0.731,0.288,0.441,0.165]\n",
      "Best Prec@1: 69.010\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [158][0/97], lr: 0.01000\tTime 0.437 (0.437)\tData 0.243 (0.243)\tLoss 1.8970 (1.8970)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [158][10/97], lr: 0.01000\tTime 0.326 (0.347)\tData 0.000 (0.037)\tLoss 1.6248 (1.9463)\tPrec@1 89.844 (88.210)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [158][20/97], lr: 0.01000\tTime 0.332 (0.339)\tData 0.000 (0.028)\tLoss 1.5369 (1.8805)\tPrec@1 92.188 (89.025)\tPrec@5 99.219 (99.405)\n",
      "Epoch: [158][30/97], lr: 0.01000\tTime 0.325 (0.337)\tData 0.000 (0.024)\tLoss 1.4500 (1.9100)\tPrec@1 92.969 (88.810)\tPrec@5 99.219 (99.294)\n",
      "Epoch: [158][40/97], lr: 0.01000\tTime 0.328 (0.335)\tData 0.000 (0.023)\tLoss 1.4368 (1.9338)\tPrec@1 90.625 (88.453)\tPrec@5 100.000 (99.295)\n",
      "Epoch: [158][50/97], lr: 0.01000\tTime 0.325 (0.335)\tData 0.000 (0.022)\tLoss 1.5801 (1.9351)\tPrec@1 87.500 (88.205)\tPrec@5 100.000 (99.311)\n",
      "Epoch: [158][60/97], lr: 0.01000\tTime 0.331 (0.334)\tData 0.000 (0.021)\tLoss 2.2600 (1.9545)\tPrec@1 85.938 (88.051)\tPrec@5 99.219 (99.308)\n",
      "Epoch: [158][70/97], lr: 0.01000\tTime 0.328 (0.334)\tData 0.000 (0.020)\tLoss 1.9886 (1.9539)\tPrec@1 89.062 (87.918)\tPrec@5 100.000 (99.307)\n",
      "Epoch: [158][80/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 1.8181 (1.9457)\tPrec@1 89.844 (87.963)\tPrec@5 99.219 (99.334)\n",
      "Epoch: [158][90/97], lr: 0.01000\tTime 0.323 (0.333)\tData 0.000 (0.020)\tLoss 1.9814 (1.9500)\tPrec@1 88.281 (87.912)\tPrec@5 99.219 (99.373)\n",
      "Epoch: [158][96/97], lr: 0.01000\tTime 0.318 (0.332)\tData 0.000 (0.020)\tLoss 1.8323 (1.9481)\tPrec@1 87.288 (87.909)\tPrec@5 100.000 (99.387)\n",
      "Test: [0/100]\tTime 0.276 (0.276)\tLoss 8.1225 (8.1225)\tPrec@1 65.000 (65.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 6.1803 (7.9596)\tPrec@1 70.000 (63.091)\tPrec@5 96.000 (97.091)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.3118 (7.8660)\tPrec@1 68.000 (63.381)\tPrec@5 99.000 (97.000)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.1672 (7.8482)\tPrec@1 62.000 (62.935)\tPrec@5 97.000 (97.032)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 7.4899 (7.8663)\tPrec@1 66.000 (63.098)\tPrec@5 97.000 (96.805)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.6424 (7.8114)\tPrec@1 64.000 (63.471)\tPrec@5 99.000 (96.961)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.3196 (7.7676)\tPrec@1 71.000 (63.492)\tPrec@5 97.000 (97.098)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.0325 (7.7787)\tPrec@1 64.000 (63.479)\tPrec@5 98.000 (97.056)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.7352 (7.7603)\tPrec@1 56.000 (63.358)\tPrec@5 94.000 (97.062)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.2178 (7.8084)\tPrec@1 62.000 (63.066)\tPrec@5 99.000 (97.022)\n",
      "val Results: Prec@1 62.790 Prec@5 97.030 Loss 7.84967\n",
      "val Class Accuracy: [0.954,0.973,0.694,0.858,0.709,0.529,0.701,0.520,0.251,0.090]\n",
      "Best Prec@1: 69.010\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [159][0/97], lr: 0.01000\tTime 0.416 (0.416)\tData 0.219 (0.219)\tLoss 1.5165 (1.5165)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [159][10/97], lr: 0.01000\tTime 0.326 (0.340)\tData 0.000 (0.034)\tLoss 1.7632 (1.7252)\tPrec@1 90.625 (89.418)\tPrec@5 97.656 (99.290)\n",
      "Epoch: [159][20/97], lr: 0.01000\tTime 0.329 (0.335)\tData 0.000 (0.026)\tLoss 1.9156 (1.7372)\tPrec@1 87.500 (89.621)\tPrec@5 100.000 (99.330)\n",
      "Epoch: [159][30/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.023)\tLoss 1.7599 (1.8331)\tPrec@1 90.625 (88.810)\tPrec@5 96.875 (99.143)\n",
      "Epoch: [159][40/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.022)\tLoss 2.4501 (1.9130)\tPrec@1 84.375 (88.243)\tPrec@5 100.000 (99.085)\n",
      "Epoch: [159][50/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.021)\tLoss 2.3930 (1.9526)\tPrec@1 82.031 (87.852)\tPrec@5 98.438 (98.989)\n",
      "Epoch: [159][60/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 1.9864 (1.9354)\tPrec@1 88.281 (87.987)\tPrec@5 100.000 (99.052)\n",
      "Epoch: [159][70/97], lr: 0.01000\tTime 0.329 (0.331)\tData 0.000 (0.020)\tLoss 1.6388 (1.9219)\tPrec@1 92.188 (88.138)\tPrec@5 100.000 (99.120)\n",
      "Epoch: [159][80/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 1.7380 (1.9060)\tPrec@1 91.406 (88.301)\tPrec@5 98.438 (99.132)\n",
      "Epoch: [159][90/97], lr: 0.01000\tTime 0.324 (0.330)\tData 0.000 (0.019)\tLoss 2.7393 (1.9122)\tPrec@1 82.812 (88.298)\tPrec@5 97.656 (99.150)\n",
      "Epoch: [159][96/97], lr: 0.01000\tTime 0.323 (0.330)\tData 0.000 (0.020)\tLoss 1.6960 (1.9186)\tPrec@1 91.525 (88.272)\tPrec@5 97.458 (99.138)\n",
      "Test: [0/100]\tTime 0.244 (0.244)\tLoss 7.7869 (7.7869)\tPrec@1 67.000 (67.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 5.5963 (7.1771)\tPrec@1 76.000 (66.364)\tPrec@5 97.000 (95.909)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.5244 (7.1037)\tPrec@1 66.000 (66.429)\tPrec@5 98.000 (96.095)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 6.6012 (7.1404)\tPrec@1 69.000 (66.452)\tPrec@5 95.000 (95.935)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.8052 (7.2047)\tPrec@1 64.000 (65.878)\tPrec@5 94.000 (95.805)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.0343 (7.2124)\tPrec@1 68.000 (65.706)\tPrec@5 96.000 (95.922)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 5.9462 (7.1706)\tPrec@1 71.000 (65.738)\tPrec@5 99.000 (95.984)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.3057 (7.1411)\tPrec@1 64.000 (65.859)\tPrec@5 96.000 (95.958)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.1575 (7.1348)\tPrec@1 64.000 (65.716)\tPrec@5 93.000 (96.025)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.3918 (7.2110)\tPrec@1 68.000 (65.363)\tPrec@5 97.000 (96.033)\n",
      "val Results: Prec@1 65.220 Prec@5 95.910 Loss 7.23483\n",
      "val Class Accuracy: [0.954,0.948,0.687,0.669,0.893,0.635,0.557,0.432,0.582,0.165]\n",
      "Best Prec@1: 69.010\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [160][0/97], lr: 0.00010\tTime 0.441 (0.441)\tData 0.221 (0.221)\tLoss 6.6840 (6.6840)\tPrec@1 85.938 (85.938)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [160][10/97], lr: 0.00010\tTime 0.326 (0.346)\tData 0.000 (0.035)\tLoss 9.9040 (6.3544)\tPrec@1 89.844 (87.784)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [160][20/97], lr: 0.00010\tTime 0.329 (0.338)\tData 0.000 (0.027)\tLoss 3.8996 (5.5821)\tPrec@1 90.625 (87.984)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [160][30/97], lr: 0.00010\tTime 0.328 (0.337)\tData 0.000 (0.024)\tLoss 5.6537 (5.4426)\tPrec@1 86.719 (88.080)\tPrec@5 100.000 (99.420)\n",
      "Epoch: [160][40/97], lr: 0.00010\tTime 0.328 (0.335)\tData 0.000 (0.022)\tLoss 4.9386 (5.2965)\tPrec@1 91.406 (88.129)\tPrec@5 100.000 (99.390)\n",
      "Epoch: [160][50/97], lr: 0.00010\tTime 0.325 (0.334)\tData 0.000 (0.021)\tLoss 2.6207 (5.1956)\tPrec@1 90.625 (87.960)\tPrec@5 100.000 (99.311)\n",
      "Epoch: [160][60/97], lr: 0.00010\tTime 0.334 (0.334)\tData 0.000 (0.020)\tLoss 5.8624 (5.1340)\tPrec@1 87.500 (88.064)\tPrec@5 100.000 (99.270)\n",
      "Epoch: [160][70/97], lr: 0.00010\tTime 0.329 (0.333)\tData 0.000 (0.020)\tLoss 3.7426 (4.8736)\tPrec@1 83.594 (88.182)\tPrec@5 98.438 (99.274)\n",
      "Epoch: [160][80/97], lr: 0.00010\tTime 0.326 (0.333)\tData 0.000 (0.020)\tLoss 3.9268 (4.7073)\tPrec@1 78.125 (87.973)\tPrec@5 100.000 (99.286)\n",
      "Epoch: [160][90/97], lr: 0.00010\tTime 0.323 (0.333)\tData 0.000 (0.019)\tLoss 1.9039 (4.5725)\tPrec@1 89.062 (87.783)\tPrec@5 99.219 (99.245)\n",
      "Epoch: [160][96/97], lr: 0.00010\tTime 0.317 (0.333)\tData 0.000 (0.020)\tLoss 2.8357 (4.5334)\tPrec@1 93.220 (87.812)\tPrec@5 100.000 (99.258)\n",
      "Test: [0/100]\tTime 0.256 (0.256)\tLoss 4.7718 (4.7718)\tPrec@1 79.000 (79.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 5.2249 (6.6051)\tPrec@1 79.000 (78.636)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.7687 (6.5393)\tPrec@1 80.000 (78.667)\tPrec@5 99.000 (98.143)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 5.3818 (6.5722)\tPrec@1 81.000 (78.290)\tPrec@5 97.000 (98.032)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 4.7370 (6.6194)\tPrec@1 84.000 (78.390)\tPrec@5 99.000 (98.049)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.7489 (6.6969)\tPrec@1 77.000 (78.706)\tPrec@5 98.000 (98.098)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.6303 (6.7016)\tPrec@1 80.000 (78.492)\tPrec@5 98.000 (98.098)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 4.9050 (6.7217)\tPrec@1 76.000 (78.761)\tPrec@5 98.000 (98.099)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.5425 (6.6760)\tPrec@1 78.000 (78.519)\tPrec@5 97.000 (98.185)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.5063 (6.7447)\tPrec@1 81.000 (78.209)\tPrec@5 99.000 (98.264)\n",
      "val Results: Prec@1 78.080 Prec@5 98.270 Loss 6.79638\n",
      "val Class Accuracy: [0.929,0.950,0.758,0.702,0.782,0.691,0.790,0.701,0.723,0.782]\n",
      "Best Prec@1: 78.080\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [161][0/97], lr: 0.00010\tTime 0.443 (0.443)\tData 0.226 (0.226)\tLoss 6.8796 (6.8796)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [161][10/97], lr: 0.00010\tTime 0.333 (0.346)\tData 0.000 (0.036)\tLoss 3.2723 (3.3994)\tPrec@1 86.719 (86.151)\tPrec@5 99.219 (99.077)\n",
      "Epoch: [161][20/97], lr: 0.00010\tTime 0.332 (0.337)\tData 0.000 (0.027)\tLoss 2.7866 (3.6335)\tPrec@1 86.719 (86.310)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [161][30/97], lr: 0.00010\tTime 0.325 (0.336)\tData 0.000 (0.024)\tLoss 3.3334 (3.8487)\tPrec@1 79.688 (85.938)\tPrec@5 99.219 (99.143)\n",
      "Epoch: [161][40/97], lr: 0.00010\tTime 0.326 (0.334)\tData 0.000 (0.022)\tLoss 2.3074 (3.7684)\tPrec@1 85.938 (85.614)\tPrec@5 98.438 (99.066)\n",
      "Epoch: [161][50/97], lr: 0.00010\tTime 0.326 (0.333)\tData 0.000 (0.021)\tLoss 2.2457 (3.6714)\tPrec@1 89.062 (85.907)\tPrec@5 100.000 (99.050)\n",
      "Epoch: [161][60/97], lr: 0.00010\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 2.9066 (3.6425)\tPrec@1 84.375 (85.681)\tPrec@5 99.219 (99.091)\n",
      "Epoch: [161][70/97], lr: 0.00010\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 2.4789 (3.6255)\tPrec@1 87.500 (85.607)\tPrec@5 99.219 (99.087)\n",
      "Epoch: [161][80/97], lr: 0.00010\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 3.3631 (3.6330)\tPrec@1 85.938 (85.600)\tPrec@5 99.219 (99.113)\n",
      "Epoch: [161][90/97], lr: 0.00010\tTime 0.327 (0.332)\tData 0.000 (0.019)\tLoss 2.5026 (3.6232)\tPrec@1 89.062 (85.809)\tPrec@5 99.219 (99.064)\n",
      "Epoch: [161][96/97], lr: 0.00010\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 1.6843 (3.5994)\tPrec@1 89.831 (85.934)\tPrec@5 100.000 (99.065)\n",
      "Test: [0/100]\tTime 0.266 (0.266)\tLoss 3.9135 (3.9135)\tPrec@1 81.000 (81.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 4.3964 (5.8685)\tPrec@1 82.000 (79.364)\tPrec@5 99.000 (98.909)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.2451 (5.7907)\tPrec@1 79.000 (79.524)\tPrec@5 100.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.3857 (5.8177)\tPrec@1 83.000 (79.677)\tPrec@5 99.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 3.8315 (5.8609)\tPrec@1 82.000 (79.341)\tPrec@5 99.000 (98.317)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.9415 (5.9007)\tPrec@1 78.000 (79.510)\tPrec@5 98.000 (98.412)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.1030 (5.8988)\tPrec@1 81.000 (79.410)\tPrec@5 99.000 (98.410)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 4.1009 (5.9027)\tPrec@1 78.000 (79.676)\tPrec@5 98.000 (98.408)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.5000 (5.8578)\tPrec@1 78.000 (79.444)\tPrec@5 98.000 (98.457)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.1848 (5.9227)\tPrec@1 82.000 (79.088)\tPrec@5 99.000 (98.527)\n",
      "val Results: Prec@1 78.980 Prec@5 98.540 Loss 5.96451\n",
      "val Class Accuracy: [0.906,0.952,0.733,0.692,0.766,0.719,0.824,0.742,0.757,0.807]\n",
      "Best Prec@1: 78.980\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [162][0/97], lr: 0.00010\tTime 0.447 (0.447)\tData 0.212 (0.212)\tLoss 1.8714 (1.8714)\tPrec@1 83.594 (83.594)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [162][10/97], lr: 0.00010\tTime 0.329 (0.346)\tData 0.000 (0.034)\tLoss 5.2723 (3.1632)\tPrec@1 88.281 (85.582)\tPrec@5 97.656 (98.935)\n",
      "Epoch: [162][20/97], lr: 0.00010\tTime 0.329 (0.339)\tData 0.000 (0.026)\tLoss 3.2224 (3.2604)\tPrec@1 79.688 (85.565)\tPrec@5 99.219 (99.070)\n",
      "Epoch: [162][30/97], lr: 0.00010\tTime 0.326 (0.336)\tData 0.000 (0.023)\tLoss 1.5984 (3.1931)\tPrec@1 89.062 (85.131)\tPrec@5 99.219 (99.118)\n",
      "Epoch: [162][40/97], lr: 0.00010\tTime 0.328 (0.335)\tData 0.000 (0.022)\tLoss 4.8553 (3.2984)\tPrec@1 83.594 (85.633)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [162][50/97], lr: 0.00010\tTime 0.329 (0.334)\tData 0.000 (0.021)\tLoss 1.7432 (3.3581)\tPrec@1 85.156 (85.585)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [162][60/97], lr: 0.00010\tTime 0.326 (0.333)\tData 0.000 (0.020)\tLoss 2.4275 (3.3776)\tPrec@1 84.375 (85.464)\tPrec@5 99.219 (99.168)\n",
      "Epoch: [162][70/97], lr: 0.00010\tTime 0.333 (0.333)\tData 0.000 (0.020)\tLoss 7.4287 (3.3723)\tPrec@1 79.688 (85.431)\tPrec@5 99.219 (99.175)\n",
      "Epoch: [162][80/97], lr: 0.00010\tTime 0.326 (0.333)\tData 0.000 (0.020)\tLoss 4.0036 (3.3870)\tPrec@1 80.469 (85.667)\tPrec@5 98.438 (99.180)\n",
      "Epoch: [162][90/97], lr: 0.00010\tTime 0.324 (0.332)\tData 0.000 (0.019)\tLoss 2.8785 (3.3314)\tPrec@1 86.719 (85.671)\tPrec@5 100.000 (99.133)\n",
      "Epoch: [162][96/97], lr: 0.00010\tTime 0.318 (0.332)\tData 0.000 (0.020)\tLoss 1.3959 (3.3564)\tPrec@1 86.441 (85.644)\tPrec@5 98.305 (99.154)\n",
      "Test: [0/100]\tTime 0.269 (0.269)\tLoss 3.2326 (3.2326)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 3.7794 (5.1909)\tPrec@1 81.000 (79.636)\tPrec@5 99.000 (98.909)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.0054 (5.1767)\tPrec@1 81.000 (79.714)\tPrec@5 100.000 (98.571)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 3.8695 (5.1985)\tPrec@1 84.000 (79.935)\tPrec@5 99.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 3.5459 (5.2479)\tPrec@1 82.000 (79.707)\tPrec@5 98.000 (98.390)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.1489 (5.2565)\tPrec@1 78.000 (79.765)\tPrec@5 99.000 (98.490)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7875 (5.2542)\tPrec@1 81.000 (79.721)\tPrec@5 99.000 (98.443)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.4339 (5.2490)\tPrec@1 78.000 (79.930)\tPrec@5 99.000 (98.465)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.5166 (5.2026)\tPrec@1 80.000 (79.827)\tPrec@5 97.000 (98.469)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.1973 (5.2631)\tPrec@1 85.000 (79.495)\tPrec@5 100.000 (98.549)\n",
      "val Results: Prec@1 79.410 Prec@5 98.550 Loss 5.28865\n",
      "val Class Accuracy: [0.901,0.937,0.744,0.683,0.749,0.722,0.845,0.769,0.744,0.847]\n",
      "Best Prec@1: 79.410\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [163][0/97], lr: 0.00010\tTime 0.469 (0.469)\tData 0.264 (0.264)\tLoss 7.3503 (7.3503)\tPrec@1 78.906 (78.906)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [163][10/97], lr: 0.00010\tTime 0.338 (0.347)\tData 0.000 (0.039)\tLoss 3.4303 (3.3557)\tPrec@1 82.812 (85.014)\tPrec@5 99.219 (99.148)\n",
      "Epoch: [163][20/97], lr: 0.00010\tTime 0.325 (0.338)\tData 0.000 (0.029)\tLoss 2.1583 (3.3357)\tPrec@1 89.844 (84.933)\tPrec@5 98.438 (99.033)\n",
      "Epoch: [163][30/97], lr: 0.00010\tTime 0.324 (0.335)\tData 0.000 (0.025)\tLoss 4.4772 (3.2792)\tPrec@1 79.688 (85.005)\tPrec@5 98.438 (98.967)\n",
      "Epoch: [163][40/97], lr: 0.00010\tTime 0.326 (0.334)\tData 0.000 (0.023)\tLoss 4.6848 (3.2774)\tPrec@1 79.688 (85.252)\tPrec@5 98.438 (98.990)\n",
      "Epoch: [163][50/97], lr: 0.00010\tTime 0.324 (0.333)\tData 0.000 (0.022)\tLoss 3.5917 (3.2378)\tPrec@1 85.938 (85.279)\tPrec@5 99.219 (99.035)\n",
      "Epoch: [163][60/97], lr: 0.00010\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 4.1742 (3.2988)\tPrec@1 85.938 (85.425)\tPrec@5 98.438 (98.950)\n",
      "Epoch: [163][70/97], lr: 0.00010\tTime 0.324 (0.332)\tData 0.000 (0.021)\tLoss 3.5562 (3.2917)\tPrec@1 83.594 (85.618)\tPrec@5 99.219 (98.999)\n",
      "Epoch: [163][80/97], lr: 0.00010\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 4.4659 (3.2996)\tPrec@1 90.625 (85.696)\tPrec@5 98.438 (99.064)\n",
      "Epoch: [163][90/97], lr: 0.00010\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 4.1145 (3.3249)\tPrec@1 81.250 (85.508)\tPrec@5 98.438 (99.099)\n",
      "Epoch: [163][96/97], lr: 0.00010\tTime 0.319 (0.331)\tData 0.000 (0.020)\tLoss 2.1058 (3.3783)\tPrec@1 84.746 (85.580)\tPrec@5 97.458 (99.081)\n",
      "Test: [0/100]\tTime 0.244 (0.244)\tLoss 3.4689 (3.4689)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 3.8479 (5.4025)\tPrec@1 83.000 (79.636)\tPrec@5 99.000 (99.182)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.2782 (5.3806)\tPrec@1 80.000 (79.429)\tPrec@5 100.000 (98.667)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.2779 (5.4376)\tPrec@1 83.000 (79.774)\tPrec@5 99.000 (98.484)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.5758 (5.5021)\tPrec@1 83.000 (79.927)\tPrec@5 99.000 (98.439)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 5.4809 (5.4939)\tPrec@1 78.000 (80.078)\tPrec@5 99.000 (98.471)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.2258 (5.4832)\tPrec@1 82.000 (80.066)\tPrec@5 98.000 (98.443)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.2616 (5.4753)\tPrec@1 78.000 (80.239)\tPrec@5 98.000 (98.451)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.8317 (5.4329)\tPrec@1 81.000 (80.136)\tPrec@5 97.000 (98.494)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.3042 (5.4906)\tPrec@1 82.000 (79.802)\tPrec@5 100.000 (98.571)\n",
      "val Results: Prec@1 79.600 Prec@5 98.600 Loss 5.53343\n",
      "val Class Accuracy: [0.908,0.936,0.762,0.676,0.768,0.711,0.849,0.762,0.777,0.811]\n",
      "Best Prec@1: 79.600\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [164][0/97], lr: 0.00010\tTime 0.427 (0.427)\tData 0.247 (0.247)\tLoss 2.8328 (2.8328)\tPrec@1 82.812 (82.812)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [164][10/97], lr: 0.00010\tTime 0.327 (0.343)\tData 0.000 (0.038)\tLoss 1.7802 (3.4737)\tPrec@1 87.500 (86.790)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [164][20/97], lr: 0.00010\tTime 0.332 (0.336)\tData 0.000 (0.028)\tLoss 2.5169 (3.0213)\tPrec@1 82.031 (86.272)\tPrec@5 96.875 (99.070)\n",
      "Epoch: [164][30/97], lr: 0.00010\tTime 0.325 (0.335)\tData 0.000 (0.025)\tLoss 1.9323 (2.8008)\tPrec@1 86.719 (86.442)\tPrec@5 98.438 (99.118)\n",
      "Epoch: [164][40/97], lr: 0.00010\tTime 0.330 (0.333)\tData 0.000 (0.023)\tLoss 1.9320 (2.7998)\tPrec@1 92.188 (86.319)\tPrec@5 99.219 (99.104)\n",
      "Epoch: [164][50/97], lr: 0.00010\tTime 0.332 (0.333)\tData 0.000 (0.022)\tLoss 4.1549 (2.8731)\tPrec@1 84.375 (86.152)\tPrec@5 100.000 (99.127)\n",
      "Epoch: [164][60/97], lr: 0.00010\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 2.3554 (3.0132)\tPrec@1 91.406 (86.488)\tPrec@5 99.219 (99.116)\n",
      "Epoch: [164][70/97], lr: 0.00010\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 1.8006 (2.9994)\tPrec@1 86.719 (86.752)\tPrec@5 97.656 (99.142)\n",
      "Epoch: [164][80/97], lr: 0.00010\tTime 0.328 (0.332)\tData 0.000 (0.020)\tLoss 4.1013 (3.0144)\tPrec@1 88.281 (86.998)\tPrec@5 99.219 (99.190)\n",
      "Epoch: [164][90/97], lr: 0.00010\tTime 0.328 (0.331)\tData 0.000 (0.020)\tLoss 2.4072 (3.0548)\tPrec@1 86.719 (86.959)\tPrec@5 100.000 (99.184)\n",
      "Epoch: [164][96/97], lr: 0.00010\tTime 0.320 (0.331)\tData 0.000 (0.020)\tLoss 2.5653 (3.0621)\tPrec@1 89.831 (86.901)\tPrec@5 99.153 (99.162)\n",
      "Test: [0/100]\tTime 0.249 (0.249)\tLoss 3.5757 (3.5757)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 3.9053 (5.5687)\tPrec@1 83.000 (80.273)\tPrec@5 99.000 (99.273)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.3688 (5.4978)\tPrec@1 80.000 (80.143)\tPrec@5 100.000 (98.714)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.4868 (5.5501)\tPrec@1 83.000 (80.258)\tPrec@5 99.000 (98.516)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.6404 (5.6245)\tPrec@1 84.000 (80.244)\tPrec@5 98.000 (98.488)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 5.5908 (5.6475)\tPrec@1 78.000 (80.392)\tPrec@5 99.000 (98.569)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.6480 (5.6337)\tPrec@1 84.000 (80.311)\tPrec@5 98.000 (98.508)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.2620 (5.6204)\tPrec@1 80.000 (80.507)\tPrec@5 99.000 (98.563)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.7369 (5.5823)\tPrec@1 81.000 (80.395)\tPrec@5 97.000 (98.605)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.8334 (5.6422)\tPrec@1 82.000 (80.055)\tPrec@5 100.000 (98.659)\n",
      "val Results: Prec@1 79.930 Prec@5 98.670 Loss 5.68246\n",
      "val Class Accuracy: [0.897,0.961,0.769,0.679,0.780,0.723,0.842,0.751,0.786,0.805]\n",
      "Best Prec@1: 79.930\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [165][0/97], lr: 0.00010\tTime 0.440 (0.440)\tData 0.234 (0.234)\tLoss 1.9439 (1.9439)\tPrec@1 85.156 (85.156)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [165][10/97], lr: 0.00010\tTime 0.331 (0.345)\tData 0.000 (0.036)\tLoss 4.3228 (3.2885)\tPrec@1 89.062 (86.435)\tPrec@5 98.438 (99.148)\n",
      "Epoch: [165][20/97], lr: 0.00010\tTime 0.320 (0.336)\tData 0.000 (0.027)\tLoss 2.5673 (3.1409)\tPrec@1 85.938 (85.938)\tPrec@5 100.000 (99.293)\n",
      "Epoch: [165][30/97], lr: 0.00010\tTime 0.326 (0.335)\tData 0.000 (0.024)\tLoss 4.6885 (3.1704)\tPrec@1 82.031 (85.736)\tPrec@5 99.219 (99.194)\n",
      "Epoch: [165][40/97], lr: 0.00010\tTime 0.332 (0.334)\tData 0.000 (0.022)\tLoss 1.8078 (3.2193)\tPrec@1 87.500 (85.442)\tPrec@5 99.219 (99.181)\n",
      "Epoch: [165][50/97], lr: 0.00010\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 3.7438 (3.2068)\tPrec@1 89.062 (85.509)\tPrec@5 99.219 (99.188)\n",
      "Epoch: [165][60/97], lr: 0.00010\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 1.6310 (3.1411)\tPrec@1 92.969 (85.643)\tPrec@5 100.000 (99.155)\n",
      "Epoch: [165][70/97], lr: 0.00010\tTime 0.330 (0.332)\tData 0.000 (0.020)\tLoss 3.3093 (3.1086)\tPrec@1 83.594 (85.695)\tPrec@5 99.219 (99.142)\n",
      "Epoch: [165][80/97], lr: 0.00010\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.1951 (3.0701)\tPrec@1 80.469 (85.687)\tPrec@5 100.000 (99.161)\n",
      "Epoch: [165][90/97], lr: 0.00010\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 1.9697 (3.0322)\tPrec@1 86.719 (85.877)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [165][96/97], lr: 0.00010\tTime 0.320 (0.331)\tData 0.000 (0.020)\tLoss 3.6604 (3.0115)\tPrec@1 85.593 (85.926)\tPrec@5 98.305 (99.234)\n",
      "Test: [0/100]\tTime 0.254 (0.254)\tLoss 3.7389 (3.7389)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 3.9594 (6.1742)\tPrec@1 85.000 (80.909)\tPrec@5 99.000 (99.364)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.5923 (6.1189)\tPrec@1 79.000 (80.476)\tPrec@5 100.000 (98.857)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.8115 (6.1815)\tPrec@1 84.000 (80.323)\tPrec@5 99.000 (98.645)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.9891 (6.2786)\tPrec@1 84.000 (80.171)\tPrec@5 99.000 (98.561)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.5480 (6.3237)\tPrec@1 79.000 (80.392)\tPrec@5 99.000 (98.647)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.5153 (6.3188)\tPrec@1 83.000 (80.180)\tPrec@5 98.000 (98.574)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.8774 (6.3238)\tPrec@1 75.000 (80.225)\tPrec@5 99.000 (98.606)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.8997 (6.2810)\tPrec@1 80.000 (80.086)\tPrec@5 98.000 (98.679)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.4490 (6.3303)\tPrec@1 81.000 (79.637)\tPrec@5 100.000 (98.714)\n",
      "val Results: Prec@1 79.370 Prec@5 98.710 Loss 6.38446\n",
      "val Class Accuracy: [0.911,0.966,0.770,0.697,0.773,0.690,0.865,0.744,0.789,0.732]\n",
      "Best Prec@1: 79.930\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [166][0/97], lr: 0.00010\tTime 0.445 (0.445)\tData 0.234 (0.234)\tLoss 2.4496 (2.4496)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [166][10/97], lr: 0.00010\tTime 0.327 (0.347)\tData 0.000 (0.036)\tLoss 7.1720 (3.1761)\tPrec@1 82.031 (86.577)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [166][20/97], lr: 0.00010\tTime 0.325 (0.339)\tData 0.000 (0.027)\tLoss 1.5714 (3.1042)\tPrec@1 89.844 (87.388)\tPrec@5 99.219 (99.330)\n",
      "Epoch: [166][30/97], lr: 0.00010\tTime 0.328 (0.336)\tData 0.000 (0.024)\tLoss 2.5142 (3.0761)\tPrec@1 86.719 (87.324)\tPrec@5 99.219 (99.143)\n",
      "Epoch: [166][40/97], lr: 0.00010\tTime 0.328 (0.334)\tData 0.000 (0.022)\tLoss 0.9483 (3.1043)\tPrec@1 95.312 (87.386)\tPrec@5 99.219 (99.200)\n",
      "Epoch: [166][50/97], lr: 0.00010\tTime 0.324 (0.333)\tData 0.000 (0.021)\tLoss 2.4874 (3.0386)\tPrec@1 85.156 (87.331)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [166][60/97], lr: 0.00010\tTime 0.327 (0.333)\tData 0.000 (0.021)\tLoss 2.3819 (2.9545)\tPrec@1 89.062 (87.564)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [166][70/97], lr: 0.00010\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 3.6219 (2.9796)\tPrec@1 82.812 (87.368)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [166][80/97], lr: 0.00010\tTime 0.333 (0.332)\tData 0.000 (0.020)\tLoss 2.5029 (2.9768)\tPrec@1 84.375 (87.288)\tPrec@5 99.219 (99.199)\n",
      "Epoch: [166][90/97], lr: 0.00010\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 2.3031 (3.0018)\tPrec@1 89.062 (87.363)\tPrec@5 100.000 (99.202)\n",
      "Epoch: [166][96/97], lr: 0.00010\tTime 0.319 (0.332)\tData 0.000 (0.020)\tLoss 3.5710 (3.0277)\tPrec@1 85.593 (87.264)\tPrec@5 97.458 (99.202)\n",
      "Test: [0/100]\tTime 0.270 (0.270)\tLoss 3.5499 (3.5499)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 3.4988 (5.4574)\tPrec@1 84.000 (80.818)\tPrec@5 99.000 (99.182)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.4534 (5.3333)\tPrec@1 80.000 (80.714)\tPrec@5 100.000 (98.619)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.2841 (5.4084)\tPrec@1 82.000 (80.548)\tPrec@5 99.000 (98.452)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 3.6493 (5.4655)\tPrec@1 83.000 (80.512)\tPrec@5 99.000 (98.390)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.5632 (5.4691)\tPrec@1 79.000 (80.745)\tPrec@5 99.000 (98.490)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.3778 (5.4636)\tPrec@1 83.000 (80.541)\tPrec@5 98.000 (98.443)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.3888 (5.4424)\tPrec@1 80.000 (80.746)\tPrec@5 98.000 (98.479)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.9122 (5.3989)\tPrec@1 83.000 (80.642)\tPrec@5 98.000 (98.568)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 3.9131 (5.4544)\tPrec@1 85.000 (80.341)\tPrec@5 100.000 (98.582)\n",
      "val Results: Prec@1 80.150 Prec@5 98.590 Loss 5.47751\n",
      "val Class Accuracy: [0.903,0.949,0.770,0.688,0.794,0.728,0.859,0.724,0.783,0.817]\n",
      "Best Prec@1: 80.150\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [167][0/97], lr: 0.00010\tTime 0.422 (0.422)\tData 0.194 (0.194)\tLoss 1.6286 (1.6286)\tPrec@1 89.062 (89.062)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [167][10/97], lr: 0.00010\tTime 0.325 (0.347)\tData 0.000 (0.033)\tLoss 6.3713 (3.2532)\tPrec@1 84.375 (86.080)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [167][20/97], lr: 0.00010\tTime 0.333 (0.338)\tData 0.000 (0.025)\tLoss 1.3773 (3.1187)\tPrec@1 88.281 (86.756)\tPrec@5 98.438 (99.479)\n",
      "Epoch: [167][30/97], lr: 0.00010\tTime 0.325 (0.336)\tData 0.000 (0.023)\tLoss 2.6991 (3.1066)\tPrec@1 85.156 (86.870)\tPrec@5 98.438 (99.521)\n",
      "Epoch: [167][40/97], lr: 0.00010\tTime 0.326 (0.334)\tData 0.000 (0.021)\tLoss 2.8382 (2.9549)\tPrec@1 85.938 (87.081)\tPrec@5 97.656 (99.371)\n",
      "Epoch: [167][50/97], lr: 0.00010\tTime 0.327 (0.333)\tData 0.000 (0.021)\tLoss 2.4008 (3.0771)\tPrec@1 85.156 (86.964)\tPrec@5 99.219 (99.295)\n",
      "Epoch: [167][60/97], lr: 0.00010\tTime 0.326 (0.333)\tData 0.000 (0.020)\tLoss 5.6983 (3.0791)\tPrec@1 82.031 (86.885)\tPrec@5 99.219 (99.270)\n",
      "Epoch: [167][70/97], lr: 0.00010\tTime 0.326 (0.333)\tData 0.000 (0.020)\tLoss 2.7459 (2.9602)\tPrec@1 90.625 (87.258)\tPrec@5 100.000 (99.263)\n",
      "Epoch: [167][80/97], lr: 0.00010\tTime 0.324 (0.332)\tData 0.000 (0.019)\tLoss 4.8888 (2.9335)\tPrec@1 84.375 (87.404)\tPrec@5 97.656 (99.257)\n",
      "Epoch: [167][90/97], lr: 0.00010\tTime 0.324 (0.332)\tData 0.000 (0.019)\tLoss 2.1371 (2.9116)\tPrec@1 91.406 (87.577)\tPrec@5 100.000 (99.287)\n",
      "Epoch: [167][96/97], lr: 0.00010\tTime 0.318 (0.332)\tData 0.000 (0.020)\tLoss 1.7562 (2.8855)\tPrec@1 86.441 (87.425)\tPrec@5 99.153 (99.266)\n",
      "Test: [0/100]\tTime 0.244 (0.244)\tLoss 3.4176 (3.4176)\tPrec@1 86.000 (86.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 3.2328 (5.5447)\tPrec@1 83.000 (80.455)\tPrec@5 99.000 (99.091)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.1361 (5.4337)\tPrec@1 79.000 (80.476)\tPrec@5 100.000 (98.476)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.0193 (5.5001)\tPrec@1 82.000 (80.484)\tPrec@5 99.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.4543 (5.5498)\tPrec@1 83.000 (80.390)\tPrec@5 98.000 (98.415)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 5.6425 (5.5368)\tPrec@1 79.000 (80.667)\tPrec@5 99.000 (98.490)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.4908 (5.5213)\tPrec@1 81.000 (80.557)\tPrec@5 99.000 (98.459)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.4255 (5.5178)\tPrec@1 80.000 (80.803)\tPrec@5 98.000 (98.507)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.2536 (5.4857)\tPrec@1 81.000 (80.704)\tPrec@5 98.000 (98.593)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 3.9804 (5.5365)\tPrec@1 87.000 (80.374)\tPrec@5 100.000 (98.615)\n",
      "val Results: Prec@1 80.270 Prec@5 98.630 Loss 5.56629\n",
      "val Class Accuracy: [0.907,0.958,0.773,0.683,0.765,0.734,0.856,0.777,0.793,0.781]\n",
      "Best Prec@1: 80.270\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [168][0/97], lr: 0.00010\tTime 0.442 (0.442)\tData 0.217 (0.217)\tLoss 2.5619 (2.5619)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [168][10/97], lr: 0.00010\tTime 0.328 (0.344)\tData 0.000 (0.034)\tLoss 4.2896 (3.0688)\tPrec@1 82.031 (87.855)\tPrec@5 99.219 (99.361)\n",
      "Epoch: [168][20/97], lr: 0.00010\tTime 0.323 (0.335)\tData 0.000 (0.026)\tLoss 2.8350 (2.9098)\tPrec@1 89.062 (87.388)\tPrec@5 97.656 (99.405)\n",
      "Epoch: [168][30/97], lr: 0.00010\tTime 0.325 (0.334)\tData 0.000 (0.023)\tLoss 2.1375 (2.8205)\tPrec@1 86.719 (87.500)\tPrec@5 98.438 (99.294)\n",
      "Epoch: [168][40/97], lr: 0.00010\tTime 0.326 (0.333)\tData 0.000 (0.022)\tLoss 3.9552 (2.6939)\tPrec@1 85.938 (87.919)\tPrec@5 99.219 (99.257)\n",
      "Epoch: [168][50/97], lr: 0.00010\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 4.9747 (2.7691)\tPrec@1 88.281 (87.592)\tPrec@5 99.219 (99.311)\n",
      "Epoch: [168][60/97], lr: 0.00010\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 4.1708 (2.8931)\tPrec@1 84.375 (87.436)\tPrec@5 98.438 (99.308)\n",
      "Epoch: [168][70/97], lr: 0.00010\tTime 0.353 (0.332)\tData 0.000 (0.020)\tLoss 2.2278 (2.9405)\tPrec@1 88.281 (87.335)\tPrec@5 97.656 (99.296)\n",
      "Epoch: [168][80/97], lr: 0.00010\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 1.1036 (2.8852)\tPrec@1 91.406 (87.404)\tPrec@5 100.000 (99.286)\n",
      "Epoch: [168][90/97], lr: 0.00010\tTime 0.322 (0.331)\tData 0.000 (0.019)\tLoss 1.5563 (2.8255)\tPrec@1 88.281 (87.491)\tPrec@5 99.219 (99.287)\n",
      "Epoch: [168][96/97], lr: 0.00010\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 2.7394 (2.8524)\tPrec@1 85.593 (87.490)\tPrec@5 98.305 (99.291)\n",
      "Test: [0/100]\tTime 0.243 (0.243)\tLoss 3.1643 (3.1643)\tPrec@1 87.000 (87.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 3.1260 (5.1785)\tPrec@1 83.000 (80.182)\tPrec@5 99.000 (99.000)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.1770 (5.1526)\tPrec@1 80.000 (80.667)\tPrec@5 100.000 (98.476)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.4046 (5.1930)\tPrec@1 79.000 (80.742)\tPrec@5 99.000 (98.419)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.8653 (5.2572)\tPrec@1 83.000 (80.756)\tPrec@5 98.000 (98.390)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 5.2098 (5.2260)\tPrec@1 79.000 (80.961)\tPrec@5 99.000 (98.431)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.3896 (5.2122)\tPrec@1 82.000 (80.852)\tPrec@5 100.000 (98.410)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.4956 (5.2088)\tPrec@1 80.000 (81.028)\tPrec@5 98.000 (98.451)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.4837 (5.1711)\tPrec@1 81.000 (80.889)\tPrec@5 97.000 (98.506)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 3.9419 (5.2336)\tPrec@1 84.000 (80.560)\tPrec@5 100.000 (98.549)\n",
      "val Results: Prec@1 80.430 Prec@5 98.540 Loss 5.25622\n",
      "val Class Accuracy: [0.918,0.947,0.765,0.687,0.780,0.736,0.860,0.767,0.749,0.834]\n",
      "Best Prec@1: 80.430\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [169][0/97], lr: 0.00010\tTime 0.561 (0.561)\tData 0.304 (0.304)\tLoss 4.5222 (4.5222)\tPrec@1 88.281 (88.281)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [169][10/97], lr: 0.00010\tTime 0.334 (0.371)\tData 0.000 (0.042)\tLoss 1.5119 (2.9495)\tPrec@1 87.500 (87.287)\tPrec@5 97.656 (98.935)\n",
      "Epoch: [169][20/97], lr: 0.00010\tTime 0.324 (0.350)\tData 0.000 (0.030)\tLoss 3.9958 (2.8709)\tPrec@1 85.156 (87.202)\tPrec@5 99.219 (99.070)\n",
      "Epoch: [169][30/97], lr: 0.00010\tTime 0.331 (0.345)\tData 0.000 (0.026)\tLoss 3.3545 (2.8524)\tPrec@1 85.156 (87.072)\tPrec@5 100.000 (99.244)\n",
      "Epoch: [169][40/97], lr: 0.00010\tTime 0.327 (0.341)\tData 0.000 (0.024)\tLoss 4.2487 (2.9170)\tPrec@1 85.938 (87.443)\tPrec@5 100.000 (99.257)\n",
      "Epoch: [169][50/97], lr: 0.00010\tTime 0.326 (0.339)\tData 0.000 (0.023)\tLoss 1.8545 (2.7628)\tPrec@1 90.625 (87.730)\tPrec@5 99.219 (99.203)\n",
      "Epoch: [169][60/97], lr: 0.00010\tTime 0.327 (0.338)\tData 0.000 (0.022)\tLoss 2.6101 (2.8190)\tPrec@1 88.281 (87.602)\tPrec@5 100.000 (99.206)\n",
      "Epoch: [169][70/97], lr: 0.00010\tTime 0.327 (0.336)\tData 0.000 (0.021)\tLoss 1.4000 (2.7844)\tPrec@1 91.406 (87.544)\tPrec@5 100.000 (99.197)\n",
      "Epoch: [169][80/97], lr: 0.00010\tTime 0.325 (0.336)\tData 0.000 (0.021)\tLoss 2.6486 (2.8546)\tPrec@1 90.625 (87.577)\tPrec@5 99.219 (99.190)\n",
      "Epoch: [169][90/97], lr: 0.00010\tTime 0.325 (0.335)\tData 0.000 (0.020)\tLoss 2.0855 (2.8192)\tPrec@1 84.375 (87.500)\tPrec@5 100.000 (99.210)\n",
      "Epoch: [169][96/97], lr: 0.00010\tTime 0.317 (0.335)\tData 0.000 (0.021)\tLoss 1.8107 (2.7796)\tPrec@1 88.983 (87.554)\tPrec@5 100.000 (99.234)\n",
      "Test: [0/100]\tTime 0.251 (0.251)\tLoss 3.0336 (3.0336)\tPrec@1 87.000 (87.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 3.3398 (5.3716)\tPrec@1 86.000 (80.909)\tPrec@5 99.000 (99.182)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 4.9109 (5.3130)\tPrec@1 81.000 (81.048)\tPrec@5 100.000 (98.571)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.2795 (5.3457)\tPrec@1 82.000 (81.097)\tPrec@5 99.000 (98.419)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.7953 (5.4450)\tPrec@1 83.000 (80.780)\tPrec@5 98.000 (98.488)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 5.4026 (5.4288)\tPrec@1 80.000 (80.941)\tPrec@5 99.000 (98.529)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.5865 (5.3943)\tPrec@1 85.000 (80.836)\tPrec@5 100.000 (98.508)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.2265 (5.3850)\tPrec@1 78.000 (81.000)\tPrec@5 99.000 (98.521)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.6668 (5.3459)\tPrec@1 81.000 (80.864)\tPrec@5 97.000 (98.580)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.2946 (5.3943)\tPrec@1 84.000 (80.571)\tPrec@5 100.000 (98.615)\n",
      "val Results: Prec@1 80.410 Prec@5 98.640 Loss 5.40906\n",
      "val Class Accuracy: [0.908,0.956,0.757,0.699,0.766,0.770,0.836,0.768,0.778,0.803]\n",
      "Best Prec@1: 80.430\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [170][0/97], lr: 0.00010\tTime 0.435 (0.435)\tData 0.244 (0.244)\tLoss 3.1836 (3.1836)\tPrec@1 85.938 (85.938)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [170][10/97], lr: 0.00010\tTime 0.326 (0.345)\tData 0.000 (0.037)\tLoss 1.4864 (2.7829)\tPrec@1 92.188 (87.855)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [170][20/97], lr: 0.00010\tTime 0.338 (0.337)\tData 0.000 (0.028)\tLoss 1.4455 (2.7684)\tPrec@1 86.719 (87.202)\tPrec@5 97.656 (99.256)\n",
      "Epoch: [170][30/97], lr: 0.00010\tTime 0.327 (0.336)\tData 0.000 (0.024)\tLoss 3.6277 (2.6867)\tPrec@1 90.625 (87.424)\tPrec@5 100.000 (99.244)\n",
      "Epoch: [170][40/97], lr: 0.00010\tTime 0.324 (0.334)\tData 0.000 (0.022)\tLoss 2.6457 (2.8262)\tPrec@1 80.469 (87.233)\tPrec@5 97.656 (99.181)\n",
      "Epoch: [170][50/97], lr: 0.00010\tTime 0.329 (0.334)\tData 0.000 (0.021)\tLoss 1.8262 (2.7909)\tPrec@1 89.844 (87.148)\tPrec@5 98.438 (99.157)\n",
      "Epoch: [170][60/97], lr: 0.00010\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 5.6688 (2.8513)\tPrec@1 86.719 (87.282)\tPrec@5 98.438 (99.142)\n",
      "Epoch: [170][70/97], lr: 0.00010\tTime 0.327 (0.333)\tData 0.000 (0.020)\tLoss 2.7929 (2.9330)\tPrec@1 90.625 (87.313)\tPrec@5 100.000 (99.153)\n",
      "Epoch: [170][80/97], lr: 0.00010\tTime 0.327 (0.333)\tData 0.000 (0.020)\tLoss 1.9089 (2.8962)\tPrec@1 88.281 (87.269)\tPrec@5 99.219 (99.190)\n",
      "Epoch: [170][90/97], lr: 0.00010\tTime 0.322 (0.333)\tData 0.000 (0.020)\tLoss 2.2948 (2.9066)\tPrec@1 88.281 (87.294)\tPrec@5 100.000 (99.245)\n",
      "Epoch: [170][96/97], lr: 0.00010\tTime 0.318 (0.332)\tData 0.000 (0.020)\tLoss 2.8833 (2.9484)\tPrec@1 89.831 (87.192)\tPrec@5 99.153 (99.234)\n",
      "Test: [0/100]\tTime 0.277 (0.277)\tLoss 2.6678 (2.6678)\tPrec@1 87.000 (87.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 3.1015 (4.7971)\tPrec@1 85.000 (81.000)\tPrec@5 99.000 (99.091)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 5.0581 (4.7742)\tPrec@1 81.000 (81.238)\tPrec@5 100.000 (98.619)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.0875 (4.8786)\tPrec@1 82.000 (81.226)\tPrec@5 99.000 (98.452)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 3.7877 (4.9717)\tPrec@1 83.000 (80.854)\tPrec@5 99.000 (98.488)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.3860 (4.9305)\tPrec@1 80.000 (81.118)\tPrec@5 99.000 (98.529)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 5.4808 (4.9214)\tPrec@1 86.000 (81.066)\tPrec@5 100.000 (98.475)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.1385 (4.9154)\tPrec@1 79.000 (81.211)\tPrec@5 98.000 (98.521)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.3635 (4.8641)\tPrec@1 82.000 (81.148)\tPrec@5 97.000 (98.580)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 3.2767 (4.9030)\tPrec@1 87.000 (80.879)\tPrec@5 100.000 (98.604)\n",
      "val Results: Prec@1 80.740 Prec@5 98.620 Loss 4.91846\n",
      "val Class Accuracy: [0.912,0.934,0.771,0.687,0.754,0.767,0.852,0.759,0.798,0.840]\n",
      "Best Prec@1: 80.740\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [171][0/97], lr: 0.00010\tTime 0.417 (0.417)\tData 0.212 (0.212)\tLoss 2.8464 (2.8464)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [171][10/97], lr: 0.00010\tTime 0.325 (0.342)\tData 0.000 (0.035)\tLoss 3.2361 (2.4844)\tPrec@1 80.469 (87.500)\tPrec@5 97.656 (99.219)\n",
      "Epoch: [171][20/97], lr: 0.00010\tTime 0.329 (0.337)\tData 0.000 (0.026)\tLoss 3.7246 (2.6060)\tPrec@1 85.938 (87.388)\tPrec@5 100.000 (99.107)\n",
      "Epoch: [171][30/97], lr: 0.00010\tTime 0.327 (0.336)\tData 0.000 (0.023)\tLoss 5.8127 (2.7703)\tPrec@1 82.031 (87.172)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [171][40/97], lr: 0.00010\tTime 0.322 (0.335)\tData 0.000 (0.022)\tLoss 4.8365 (2.9059)\tPrec@1 85.938 (87.024)\tPrec@5 99.219 (99.276)\n",
      "Epoch: [171][50/97], lr: 0.00010\tTime 0.324 (0.333)\tData 0.000 (0.021)\tLoss 1.1183 (2.8339)\tPrec@1 92.188 (87.025)\tPrec@5 100.000 (99.295)\n",
      "Epoch: [171][60/97], lr: 0.00010\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 3.5553 (2.8077)\tPrec@1 83.594 (87.039)\tPrec@5 99.219 (99.334)\n",
      "Epoch: [171][70/97], lr: 0.00010\tTime 0.330 (0.333)\tData 0.000 (0.020)\tLoss 2.2364 (2.7838)\tPrec@1 84.375 (87.115)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [171][80/97], lr: 0.00010\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.7343 (2.7931)\tPrec@1 86.719 (87.143)\tPrec@5 99.219 (99.306)\n",
      "Epoch: [171][90/97], lr: 0.00010\tTime 0.324 (0.332)\tData 0.000 (0.019)\tLoss 2.8984 (2.8633)\tPrec@1 89.844 (87.165)\tPrec@5 99.219 (99.227)\n",
      "Epoch: [171][96/97], lr: 0.00010\tTime 0.319 (0.331)\tData 0.000 (0.020)\tLoss 1.9367 (2.8638)\tPrec@1 89.831 (87.288)\tPrec@5 100.000 (99.226)\n",
      "Test: [0/100]\tTime 0.278 (0.278)\tLoss 3.0618 (3.0618)\tPrec@1 87.000 (87.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 3.7693 (5.8028)\tPrec@1 84.000 (81.000)\tPrec@5 98.000 (98.727)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 5.2754 (5.7249)\tPrec@1 82.000 (80.857)\tPrec@5 100.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 4.4348 (5.7610)\tPrec@1 81.000 (80.903)\tPrec@5 99.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 3.9180 (5.8335)\tPrec@1 82.000 (80.634)\tPrec@5 98.000 (98.341)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.8203 (5.8064)\tPrec@1 78.000 (80.765)\tPrec@5 99.000 (98.392)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.8256 (5.7845)\tPrec@1 81.000 (80.721)\tPrec@5 100.000 (98.393)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.7093 (5.7823)\tPrec@1 79.000 (80.915)\tPrec@5 98.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.2394 (5.7396)\tPrec@1 84.000 (80.827)\tPrec@5 97.000 (98.469)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.6903 (5.7739)\tPrec@1 85.000 (80.560)\tPrec@5 100.000 (98.495)\n",
      "val Results: Prec@1 80.440 Prec@5 98.520 Loss 5.79328\n",
      "val Class Accuracy: [0.914,0.968,0.764,0.696,0.759,0.758,0.860,0.777,0.774,0.774]\n",
      "Best Prec@1: 80.740\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [172][0/97], lr: 0.00010\tTime 0.436 (0.436)\tData 0.230 (0.230)\tLoss 4.0666 (4.0666)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [172][10/97], lr: 0.00010\tTime 0.327 (0.344)\tData 0.000 (0.036)\tLoss 3.3073 (3.6471)\tPrec@1 89.844 (88.139)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [172][20/97], lr: 0.00010\tTime 0.322 (0.336)\tData 0.000 (0.027)\tLoss 3.9508 (3.3774)\tPrec@1 89.844 (87.798)\tPrec@5 99.219 (99.182)\n",
      "Epoch: [172][30/97], lr: 0.00010\tTime 0.330 (0.334)\tData 0.000 (0.024)\tLoss 3.3167 (3.1377)\tPrec@1 92.188 (88.231)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [172][40/97], lr: 0.00010\tTime 0.329 (0.333)\tData 0.000 (0.022)\tLoss 3.2003 (3.1512)\tPrec@1 82.031 (88.072)\tPrec@5 99.219 (99.314)\n",
      "Epoch: [172][50/97], lr: 0.00010\tTime 0.324 (0.332)\tData 0.000 (0.021)\tLoss 2.7183 (3.0914)\tPrec@1 89.062 (88.036)\tPrec@5 100.000 (99.357)\n",
      "Epoch: [172][60/97], lr: 0.00010\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 2.1835 (3.0346)\tPrec@1 91.406 (88.089)\tPrec@5 97.656 (99.283)\n",
      "Epoch: [172][70/97], lr: 0.00010\tTime 0.329 (0.332)\tData 0.000 (0.020)\tLoss 1.8231 (3.0138)\tPrec@1 89.062 (88.061)\tPrec@5 100.000 (99.340)\n",
      "Epoch: [172][80/97], lr: 0.00010\tTime 0.331 (0.332)\tData 0.000 (0.020)\tLoss 1.6233 (2.9498)\tPrec@1 89.062 (88.030)\tPrec@5 100.000 (99.306)\n",
      "Epoch: [172][90/97], lr: 0.00010\tTime 0.324 (0.332)\tData 0.000 (0.019)\tLoss 0.9503 (2.8776)\tPrec@1 90.625 (88.092)\tPrec@5 100.000 (99.287)\n",
      "Epoch: [172][96/97], lr: 0.00010\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 4.0220 (2.8904)\tPrec@1 86.441 (88.038)\tPrec@5 100.000 (99.315)\n",
      "Test: [0/100]\tTime 0.256 (0.256)\tLoss 2.9170 (2.9170)\tPrec@1 88.000 (88.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 3.8744 (5.5665)\tPrec@1 82.000 (81.273)\tPrec@5 99.000 (98.909)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.1737 (5.4837)\tPrec@1 83.000 (81.571)\tPrec@5 100.000 (98.667)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.3024 (5.5089)\tPrec@1 82.000 (81.258)\tPrec@5 99.000 (98.548)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.6333 (5.5844)\tPrec@1 82.000 (80.927)\tPrec@5 99.000 (98.512)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 5.6989 (5.5755)\tPrec@1 79.000 (81.098)\tPrec@5 99.000 (98.490)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.4134 (5.5394)\tPrec@1 81.000 (81.066)\tPrec@5 99.000 (98.475)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.2257 (5.5201)\tPrec@1 80.000 (81.254)\tPrec@5 98.000 (98.451)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.8969 (5.4698)\tPrec@1 83.000 (81.198)\tPrec@5 97.000 (98.543)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.1463 (5.5019)\tPrec@1 86.000 (80.934)\tPrec@5 100.000 (98.549)\n",
      "val Results: Prec@1 80.820 Prec@5 98.580 Loss 5.52417\n",
      "val Class Accuracy: [0.907,0.964,0.769,0.706,0.801,0.760,0.845,0.754,0.788,0.788]\n",
      "Best Prec@1: 80.820\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [173][0/97], lr: 0.00010\tTime 0.543 (0.543)\tData 0.290 (0.290)\tLoss 2.5748 (2.5748)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [173][10/97], lr: 0.00010\tTime 0.339 (0.363)\tData 0.000 (0.041)\tLoss 2.0596 (3.2642)\tPrec@1 87.500 (87.358)\tPrec@5 100.000 (99.148)\n",
      "Epoch: [173][20/97], lr: 0.00010\tTime 0.326 (0.349)\tData 0.000 (0.030)\tLoss 4.5314 (3.0340)\tPrec@1 85.156 (87.835)\tPrec@5 97.656 (99.182)\n",
      "Epoch: [173][30/97], lr: 0.00010\tTime 0.324 (0.344)\tData 0.000 (0.026)\tLoss 5.9683 (3.0115)\tPrec@1 77.344 (87.399)\tPrec@5 97.656 (99.118)\n",
      "Epoch: [173][40/97], lr: 0.00010\tTime 0.325 (0.340)\tData 0.000 (0.024)\tLoss 1.8450 (2.9402)\tPrec@1 89.062 (87.119)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [173][50/97], lr: 0.00010\tTime 0.325 (0.338)\tData 0.000 (0.022)\tLoss 3.1163 (2.8563)\tPrec@1 87.500 (87.439)\tPrec@5 98.438 (99.188)\n",
      "Epoch: [173][60/97], lr: 0.00010\tTime 0.327 (0.337)\tData 0.000 (0.022)\tLoss 2.7161 (2.8945)\tPrec@1 91.406 (87.705)\tPrec@5 100.000 (99.168)\n",
      "Epoch: [173][70/97], lr: 0.00010\tTime 0.324 (0.336)\tData 0.000 (0.021)\tLoss 2.0946 (2.8738)\tPrec@1 90.625 (87.709)\tPrec@5 99.219 (99.153)\n",
      "Epoch: [173][80/97], lr: 0.00010\tTime 0.326 (0.335)\tData 0.000 (0.020)\tLoss 2.4232 (2.9176)\tPrec@1 84.375 (87.654)\tPrec@5 99.219 (99.113)\n",
      "Epoch: [173][90/97], lr: 0.00010\tTime 0.326 (0.335)\tData 0.000 (0.020)\tLoss 1.0881 (2.7983)\tPrec@1 93.750 (87.740)\tPrec@5 99.219 (99.124)\n",
      "Epoch: [173][96/97], lr: 0.00010\tTime 0.317 (0.335)\tData 0.000 (0.021)\tLoss 2.6947 (2.8048)\tPrec@1 84.746 (87.716)\tPrec@5 98.305 (99.121)\n",
      "Test: [0/100]\tTime 0.240 (0.240)\tLoss 2.8798 (2.8798)\tPrec@1 87.000 (87.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 3.4605 (5.5088)\tPrec@1 84.000 (81.273)\tPrec@5 99.000 (98.727)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.3438 (5.4552)\tPrec@1 82.000 (81.429)\tPrec@5 100.000 (98.476)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.2975 (5.4878)\tPrec@1 82.000 (81.161)\tPrec@5 99.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.7253 (5.5689)\tPrec@1 83.000 (80.976)\tPrec@5 99.000 (98.366)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 5.6926 (5.5581)\tPrec@1 80.000 (81.196)\tPrec@5 99.000 (98.471)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.4566 (5.5280)\tPrec@1 82.000 (81.082)\tPrec@5 99.000 (98.459)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.3215 (5.5119)\tPrec@1 79.000 (81.239)\tPrec@5 98.000 (98.479)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.7739 (5.4589)\tPrec@1 84.000 (81.160)\tPrec@5 97.000 (98.543)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.0580 (5.4956)\tPrec@1 86.000 (80.879)\tPrec@5 100.000 (98.593)\n",
      "val Results: Prec@1 80.730 Prec@5 98.600 Loss 5.51830\n",
      "val Class Accuracy: [0.914,0.960,0.773,0.707,0.787,0.750,0.870,0.729,0.783,0.800]\n",
      "Best Prec@1: 80.820\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [174][0/97], lr: 0.00010\tTime 0.436 (0.436)\tData 0.194 (0.194)\tLoss 1.4628 (1.4628)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [174][10/97], lr: 0.00010\tTime 0.326 (0.342)\tData 0.000 (0.032)\tLoss 1.4172 (2.8371)\tPrec@1 88.281 (88.068)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [174][20/97], lr: 0.00010\tTime 0.335 (0.337)\tData 0.000 (0.025)\tLoss 2.4490 (2.8922)\tPrec@1 89.844 (87.314)\tPrec@5 98.438 (99.256)\n",
      "Epoch: [174][30/97], lr: 0.00010\tTime 0.326 (0.335)\tData 0.000 (0.022)\tLoss 3.0694 (2.7994)\tPrec@1 88.281 (87.777)\tPrec@5 99.219 (99.068)\n",
      "Epoch: [174][40/97], lr: 0.00010\tTime 0.329 (0.334)\tData 0.000 (0.021)\tLoss 2.3837 (2.7694)\tPrec@1 89.062 (87.691)\tPrec@5 100.000 (99.162)\n",
      "Epoch: [174][50/97], lr: 0.00010\tTime 0.323 (0.333)\tData 0.000 (0.020)\tLoss 2.4997 (2.7996)\tPrec@1 83.594 (87.638)\tPrec@5 99.219 (99.173)\n",
      "Epoch: [174][60/97], lr: 0.00010\tTime 0.326 (0.333)\tData 0.000 (0.020)\tLoss 1.4545 (2.7602)\tPrec@1 92.188 (87.718)\tPrec@5 100.000 (99.244)\n",
      "Epoch: [174][70/97], lr: 0.00010\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 5.4485 (2.7085)\tPrec@1 85.156 (87.896)\tPrec@5 99.219 (99.263)\n",
      "Epoch: [174][80/97], lr: 0.00010\tTime 0.325 (0.332)\tData 0.000 (0.019)\tLoss 2.3369 (2.6566)\tPrec@1 89.844 (88.040)\tPrec@5 99.219 (99.306)\n",
      "Epoch: [174][90/97], lr: 0.00010\tTime 0.324 (0.332)\tData 0.000 (0.019)\tLoss 2.0208 (2.6331)\tPrec@1 87.500 (88.118)\tPrec@5 99.219 (99.322)\n",
      "Epoch: [174][96/97], lr: 0.00010\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 2.1249 (2.6514)\tPrec@1 92.373 (88.135)\tPrec@5 99.153 (99.339)\n",
      "Test: [0/100]\tTime 0.287 (0.287)\tLoss 3.2562 (3.2562)\tPrec@1 86.000 (86.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 3.3026 (5.7184)\tPrec@1 84.000 (81.364)\tPrec@5 99.000 (98.727)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 5.3017 (5.6568)\tPrec@1 81.000 (81.238)\tPrec@5 99.000 (98.429)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 4.3574 (5.6702)\tPrec@1 81.000 (80.839)\tPrec@5 100.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 3.7398 (5.7516)\tPrec@1 84.000 (80.854)\tPrec@5 98.000 (98.317)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.0960 (5.7364)\tPrec@1 78.000 (80.980)\tPrec@5 99.000 (98.392)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.5625 (5.6859)\tPrec@1 84.000 (81.049)\tPrec@5 99.000 (98.393)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.2901 (5.6755)\tPrec@1 79.000 (81.225)\tPrec@5 98.000 (98.423)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.0326 (5.6387)\tPrec@1 82.000 (81.222)\tPrec@5 97.000 (98.494)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.1786 (5.6755)\tPrec@1 87.000 (80.923)\tPrec@5 100.000 (98.549)\n",
      "val Results: Prec@1 80.740 Prec@5 98.560 Loss 5.70395\n",
      "val Class Accuracy: [0.909,0.965,0.800,0.709,0.799,0.716,0.844,0.755,0.787,0.790]\n",
      "Best Prec@1: 80.820\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [175][0/97], lr: 0.00010\tTime 0.448 (0.448)\tData 0.200 (0.200)\tLoss 2.5614 (2.5614)\tPrec@1 90.625 (90.625)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [175][10/97], lr: 0.00010\tTime 0.327 (0.350)\tData 0.000 (0.033)\tLoss 2.8171 (2.6267)\tPrec@1 89.062 (88.636)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [175][20/97], lr: 0.00010\tTime 0.323 (0.340)\tData 0.000 (0.026)\tLoss 3.3513 (2.8675)\tPrec@1 83.594 (87.388)\tPrec@5 99.219 (99.182)\n",
      "Epoch: [175][30/97], lr: 0.00010\tTime 0.324 (0.337)\tData 0.000 (0.023)\tLoss 2.3780 (2.7800)\tPrec@1 89.844 (87.752)\tPrec@5 99.219 (99.294)\n",
      "Epoch: [175][40/97], lr: 0.00010\tTime 0.329 (0.335)\tData 0.000 (0.022)\tLoss 2.1280 (2.6852)\tPrec@1 92.188 (87.919)\tPrec@5 100.000 (99.352)\n",
      "Epoch: [175][50/97], lr: 0.00010\tTime 0.324 (0.334)\tData 0.000 (0.021)\tLoss 3.0018 (2.7319)\tPrec@1 85.156 (87.868)\tPrec@5 100.000 (99.387)\n",
      "Epoch: [175][60/97], lr: 0.00010\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 4.9504 (2.7903)\tPrec@1 86.719 (87.756)\tPrec@5 100.000 (99.372)\n",
      "Epoch: [175][70/97], lr: 0.00010\tTime 0.331 (0.332)\tData 0.000 (0.020)\tLoss 3.8233 (2.8367)\tPrec@1 84.375 (87.775)\tPrec@5 99.219 (99.373)\n",
      "Epoch: [175][80/97], lr: 0.00010\tTime 0.326 (0.332)\tData 0.000 (0.019)\tLoss 2.7796 (2.7710)\tPrec@1 85.938 (87.780)\tPrec@5 98.438 (99.315)\n",
      "Epoch: [175][90/97], lr: 0.00010\tTime 0.324 (0.332)\tData 0.000 (0.019)\tLoss 3.6220 (2.7477)\tPrec@1 84.375 (87.843)\tPrec@5 96.875 (99.296)\n",
      "Epoch: [175][96/97], lr: 0.00010\tTime 0.316 (0.332)\tData 0.000 (0.020)\tLoss 2.7244 (2.7139)\tPrec@1 86.441 (87.780)\tPrec@5 100.000 (99.299)\n",
      "Test: [0/100]\tTime 0.241 (0.241)\tLoss 3.2134 (3.2134)\tPrec@1 87.000 (87.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 3.1061 (5.3390)\tPrec@1 84.000 (80.818)\tPrec@5 99.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.3075 (5.3287)\tPrec@1 82.000 (80.952)\tPrec@5 100.000 (98.429)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.9682 (5.3397)\tPrec@1 80.000 (80.645)\tPrec@5 99.000 (98.419)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.6914 (5.3955)\tPrec@1 81.000 (80.610)\tPrec@5 98.000 (98.415)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 5.4307 (5.3759)\tPrec@1 81.000 (80.941)\tPrec@5 99.000 (98.471)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.6321 (5.3457)\tPrec@1 84.000 (80.918)\tPrec@5 100.000 (98.443)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.2169 (5.3133)\tPrec@1 79.000 (81.169)\tPrec@5 98.000 (98.465)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.4558 (5.2707)\tPrec@1 82.000 (81.062)\tPrec@5 97.000 (98.519)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 3.8052 (5.3193)\tPrec@1 84.000 (80.802)\tPrec@5 100.000 (98.549)\n",
      "val Results: Prec@1 80.650 Prec@5 98.570 Loss 5.33550\n",
      "val Class Accuracy: [0.914,0.957,0.776,0.704,0.778,0.730,0.859,0.773,0.745,0.829]\n",
      "Best Prec@1: 80.820\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [176][0/97], lr: 0.00010\tTime 0.386 (0.386)\tData 0.185 (0.185)\tLoss 1.2048 (1.2048)\tPrec@1 91.406 (91.406)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [176][10/97], lr: 0.00010\tTime 0.325 (0.338)\tData 0.000 (0.032)\tLoss 1.7294 (2.5308)\tPrec@1 87.500 (87.429)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [176][20/97], lr: 0.00010\tTime 0.330 (0.333)\tData 0.000 (0.025)\tLoss 2.1474 (2.5251)\tPrec@1 86.719 (87.686)\tPrec@5 99.219 (99.368)\n",
      "Epoch: [176][30/97], lr: 0.00010\tTime 0.330 (0.332)\tData 0.000 (0.022)\tLoss 2.3050 (2.6837)\tPrec@1 89.844 (88.130)\tPrec@5 100.000 (99.420)\n",
      "Epoch: [176][40/97], lr: 0.00010\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 1.1074 (2.5970)\tPrec@1 96.094 (88.300)\tPrec@5 100.000 (99.390)\n",
      "Epoch: [176][50/97], lr: 0.00010\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 3.1837 (2.4996)\tPrec@1 88.281 (88.419)\tPrec@5 98.438 (99.372)\n",
      "Epoch: [176][60/97], lr: 0.00010\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 1.5030 (2.4644)\tPrec@1 91.406 (88.409)\tPrec@5 99.219 (99.296)\n",
      "Epoch: [176][70/97], lr: 0.00010\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 2.1748 (2.4648)\tPrec@1 92.188 (88.479)\tPrec@5 100.000 (99.307)\n",
      "Epoch: [176][80/97], lr: 0.00010\tTime 0.323 (0.331)\tData 0.000 (0.019)\tLoss 2.2500 (2.4953)\tPrec@1 89.062 (88.416)\tPrec@5 98.438 (99.296)\n",
      "Epoch: [176][90/97], lr: 0.00010\tTime 0.323 (0.331)\tData 0.000 (0.019)\tLoss 1.3954 (2.5155)\tPrec@1 92.969 (88.376)\tPrec@5 100.000 (99.330)\n",
      "Epoch: [176][96/97], lr: 0.00010\tTime 0.317 (0.330)\tData 0.000 (0.020)\tLoss 2.6661 (2.5029)\tPrec@1 86.441 (88.401)\tPrec@5 100.000 (99.363)\n",
      "Test: [0/100]\tTime 0.272 (0.272)\tLoss 3.3566 (3.3566)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 3.0503 (5.8654)\tPrec@1 85.000 (81.636)\tPrec@5 99.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.4950 (5.8222)\tPrec@1 80.000 (81.571)\tPrec@5 100.000 (98.524)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.7023 (5.8463)\tPrec@1 80.000 (81.194)\tPrec@5 99.000 (98.419)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 3.7323 (5.9181)\tPrec@1 82.000 (81.049)\tPrec@5 99.000 (98.415)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.5267 (5.9459)\tPrec@1 81.000 (81.176)\tPrec@5 99.000 (98.471)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.0338 (5.9406)\tPrec@1 81.000 (81.066)\tPrec@5 100.000 (98.443)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.7506 (5.9388)\tPrec@1 78.000 (81.225)\tPrec@5 98.000 (98.423)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.3959 (5.9007)\tPrec@1 83.000 (81.148)\tPrec@5 96.000 (98.481)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.5304 (5.9422)\tPrec@1 83.000 (80.813)\tPrec@5 100.000 (98.516)\n",
      "val Results: Prec@1 80.670 Prec@5 98.550 Loss 5.96708\n",
      "val Class Accuracy: [0.917,0.969,0.785,0.717,0.787,0.738,0.872,0.745,0.772,0.765]\n",
      "Best Prec@1: 80.820\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [177][0/97], lr: 0.00010\tTime 0.439 (0.439)\tData 0.229 (0.229)\tLoss 5.1774 (5.1774)\tPrec@1 85.938 (85.938)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [177][10/97], lr: 0.00010\tTime 0.328 (0.344)\tData 0.000 (0.036)\tLoss 4.4577 (3.3431)\tPrec@1 88.281 (88.139)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [177][20/97], lr: 0.00010\tTime 0.328 (0.335)\tData 0.000 (0.027)\tLoss 1.2718 (2.9673)\tPrec@1 91.406 (88.281)\tPrec@5 99.219 (99.293)\n",
      "Epoch: [177][30/97], lr: 0.00010\tTime 0.328 (0.335)\tData 0.000 (0.024)\tLoss 2.8967 (2.7333)\tPrec@1 82.812 (88.004)\tPrec@5 99.219 (99.345)\n",
      "Epoch: [177][40/97], lr: 0.00010\tTime 0.327 (0.334)\tData 0.000 (0.022)\tLoss 1.8483 (2.7135)\tPrec@1 89.844 (87.976)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [177][50/97], lr: 0.00010\tTime 0.330 (0.333)\tData 0.000 (0.021)\tLoss 2.9425 (2.7129)\tPrec@1 88.281 (88.143)\tPrec@5 100.000 (99.234)\n",
      "Epoch: [177][60/97], lr: 0.00010\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 2.7382 (2.6621)\tPrec@1 88.281 (88.115)\tPrec@5 97.656 (99.283)\n",
      "Epoch: [177][70/97], lr: 0.00010\tTime 0.333 (0.332)\tData 0.000 (0.020)\tLoss 2.1847 (2.6551)\tPrec@1 90.625 (87.995)\tPrec@5 100.000 (99.285)\n",
      "Epoch: [177][80/97], lr: 0.00010\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 0.9648 (2.5755)\tPrec@1 90.625 (87.963)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [177][90/97], lr: 0.00010\tTime 0.323 (0.331)\tData 0.000 (0.019)\tLoss 2.1990 (2.6318)\tPrec@1 87.500 (87.964)\tPrec@5 99.219 (99.202)\n",
      "Epoch: [177][96/97], lr: 0.00010\tTime 0.315 (0.331)\tData 0.000 (0.020)\tLoss 2.7062 (2.6310)\tPrec@1 88.983 (88.014)\tPrec@5 100.000 (99.218)\n",
      "Test: [0/100]\tTime 0.237 (0.237)\tLoss 3.1981 (3.1981)\tPrec@1 87.000 (87.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 2.8195 (4.8930)\tPrec@1 85.000 (81.818)\tPrec@5 99.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.0702 (4.8739)\tPrec@1 83.000 (82.190)\tPrec@5 100.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.1713 (4.9714)\tPrec@1 80.000 (81.774)\tPrec@5 100.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.3860 (5.0371)\tPrec@1 83.000 (81.585)\tPrec@5 99.000 (98.390)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 5.6838 (5.0482)\tPrec@1 79.000 (81.569)\tPrec@5 99.000 (98.392)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.4316 (5.0460)\tPrec@1 84.000 (81.574)\tPrec@5 99.000 (98.361)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.0352 (5.0382)\tPrec@1 78.000 (81.662)\tPrec@5 98.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.3048 (5.0015)\tPrec@1 82.000 (81.593)\tPrec@5 97.000 (98.444)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 3.1948 (5.0378)\tPrec@1 86.000 (81.352)\tPrec@5 100.000 (98.484)\n",
      "val Results: Prec@1 81.230 Prec@5 98.470 Loss 5.05392\n",
      "val Class Accuracy: [0.903,0.947,0.782,0.712,0.792,0.759,0.858,0.742,0.798,0.830]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [178][0/97], lr: 0.00010\tTime 0.450 (0.450)\tData 0.227 (0.227)\tLoss 1.8336 (1.8336)\tPrec@1 85.938 (85.938)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [178][10/97], lr: 0.00010\tTime 0.334 (0.345)\tData 0.000 (0.035)\tLoss 2.4022 (2.6611)\tPrec@1 87.500 (87.429)\tPrec@5 99.219 (98.935)\n",
      "Epoch: [178][20/97], lr: 0.00010\tTime 0.323 (0.337)\tData 0.000 (0.026)\tLoss 2.0417 (2.5647)\tPrec@1 84.375 (87.500)\tPrec@5 100.000 (99.256)\n",
      "Epoch: [178][30/97], lr: 0.00010\tTime 0.325 (0.336)\tData 0.000 (0.023)\tLoss 4.5997 (2.6451)\tPrec@1 84.375 (87.374)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [178][40/97], lr: 0.00010\tTime 0.326 (0.334)\tData 0.000 (0.022)\tLoss 4.5754 (2.5918)\tPrec@1 84.375 (87.748)\tPrec@5 99.219 (99.200)\n",
      "Epoch: [178][50/97], lr: 0.00010\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 3.6565 (2.6546)\tPrec@1 86.719 (87.791)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [178][60/97], lr: 0.00010\tTime 0.326 (0.333)\tData 0.000 (0.020)\tLoss 1.6858 (2.7011)\tPrec@1 91.406 (87.590)\tPrec@5 100.000 (99.244)\n",
      "Epoch: [178][70/97], lr: 0.00010\tTime 0.323 (0.332)\tData 0.000 (0.020)\tLoss 3.9225 (2.6813)\tPrec@1 87.500 (87.709)\tPrec@5 99.219 (99.307)\n",
      "Epoch: [178][80/97], lr: 0.00010\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 2.8820 (2.6825)\tPrec@1 88.281 (87.770)\tPrec@5 100.000 (99.315)\n",
      "Epoch: [178][90/97], lr: 0.00010\tTime 0.330 (0.332)\tData 0.000 (0.019)\tLoss 1.6039 (2.6256)\tPrec@1 85.938 (87.886)\tPrec@5 100.000 (99.305)\n",
      "Epoch: [178][96/97], lr: 0.00010\tTime 0.320 (0.332)\tData 0.000 (0.020)\tLoss 1.7062 (2.6497)\tPrec@1 93.220 (87.957)\tPrec@5 100.000 (99.307)\n",
      "Test: [0/100]\tTime 0.250 (0.250)\tLoss 3.6240 (3.6240)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 3.3114 (5.9162)\tPrec@1 84.000 (80.636)\tPrec@5 98.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.4278 (5.8377)\tPrec@1 82.000 (80.762)\tPrec@5 99.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.7373 (5.8641)\tPrec@1 81.000 (80.548)\tPrec@5 100.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.8856 (5.9618)\tPrec@1 83.000 (80.341)\tPrec@5 98.000 (98.317)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.1564 (5.9772)\tPrec@1 82.000 (80.588)\tPrec@5 99.000 (98.353)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.8868 (5.9166)\tPrec@1 85.000 (80.656)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.5263 (5.9076)\tPrec@1 78.000 (80.915)\tPrec@5 98.000 (98.338)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.2342 (5.8781)\tPrec@1 81.000 (80.728)\tPrec@5 96.000 (98.395)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.5330 (5.9333)\tPrec@1 85.000 (80.473)\tPrec@5 100.000 (98.418)\n",
      "val Results: Prec@1 80.370 Prec@5 98.420 Loss 5.96104\n",
      "val Class Accuracy: [0.911,0.967,0.791,0.711,0.770,0.744,0.866,0.753,0.762,0.762]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [179][0/97], lr: 0.00010\tTime 0.433 (0.433)\tData 0.210 (0.210)\tLoss 2.9989 (2.9989)\tPrec@1 88.281 (88.281)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [179][10/97], lr: 0.00010\tTime 0.327 (0.344)\tData 0.000 (0.034)\tLoss 3.9617 (3.0255)\tPrec@1 89.844 (88.423)\tPrec@5 99.219 (99.148)\n",
      "Epoch: [179][20/97], lr: 0.00010\tTime 0.328 (0.337)\tData 0.000 (0.026)\tLoss 1.4724 (2.6282)\tPrec@1 86.719 (88.690)\tPrec@5 99.219 (99.368)\n",
      "Epoch: [179][30/97], lr: 0.00010\tTime 0.326 (0.335)\tData 0.000 (0.023)\tLoss 3.1636 (2.5729)\tPrec@1 89.844 (88.533)\tPrec@5 96.875 (99.244)\n",
      "Epoch: [179][40/97], lr: 0.00010\tTime 0.325 (0.334)\tData 0.000 (0.022)\tLoss 1.8838 (2.6598)\tPrec@1 89.844 (88.167)\tPrec@5 99.219 (99.295)\n",
      "Epoch: [179][50/97], lr: 0.00010\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 3.0843 (2.6237)\tPrec@1 82.031 (88.174)\tPrec@5 96.094 (99.280)\n",
      "Epoch: [179][60/97], lr: 0.00010\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 0.9910 (2.5568)\tPrec@1 89.062 (88.217)\tPrec@5 99.219 (99.334)\n",
      "Epoch: [179][70/97], lr: 0.00010\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 5.2884 (2.6250)\tPrec@1 85.156 (88.270)\tPrec@5 98.438 (99.329)\n",
      "Epoch: [179][80/97], lr: 0.00010\tTime 0.324 (0.331)\tData 0.000 (0.019)\tLoss 4.1135 (2.6181)\tPrec@1 84.375 (88.117)\tPrec@5 98.438 (99.315)\n",
      "Epoch: [179][90/97], lr: 0.00010\tTime 0.323 (0.331)\tData 0.000 (0.019)\tLoss 1.0951 (2.5990)\tPrec@1 91.406 (88.058)\tPrec@5 99.219 (99.262)\n",
      "Epoch: [179][96/97], lr: 0.00010\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 2.6776 (2.6473)\tPrec@1 87.288 (87.877)\tPrec@5 98.305 (99.275)\n",
      "Test: [0/100]\tTime 0.249 (0.249)\tLoss 3.4614 (3.4614)\tPrec@1 88.000 (88.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 3.3112 (5.8872)\tPrec@1 84.000 (80.727)\tPrec@5 98.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.3325 (5.8355)\tPrec@1 80.000 (80.905)\tPrec@5 99.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.3726 (5.8563)\tPrec@1 81.000 (80.903)\tPrec@5 100.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.6080 (5.9492)\tPrec@1 80.000 (80.756)\tPrec@5 97.000 (98.317)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.2164 (5.9472)\tPrec@1 81.000 (80.961)\tPrec@5 99.000 (98.314)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.6624 (5.8838)\tPrec@1 83.000 (81.000)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.2361 (5.8736)\tPrec@1 81.000 (81.296)\tPrec@5 98.000 (98.338)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.1127 (5.8415)\tPrec@1 82.000 (81.247)\tPrec@5 97.000 (98.407)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.2935 (5.8856)\tPrec@1 86.000 (81.022)\tPrec@5 100.000 (98.429)\n",
      "val Results: Prec@1 80.870 Prec@5 98.450 Loss 5.91529\n",
      "val Class Accuracy: [0.911,0.974,0.785,0.695,0.792,0.761,0.857,0.782,0.779,0.751]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [180][0/97], lr: 0.00000\tTime 0.430 (0.430)\tData 0.248 (0.248)\tLoss 2.2632 (2.2632)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [180][10/97], lr: 0.00000\tTime 0.328 (0.343)\tData 0.000 (0.038)\tLoss 2.8022 (2.9499)\tPrec@1 87.500 (88.707)\tPrec@5 100.000 (98.935)\n",
      "Epoch: [180][20/97], lr: 0.00000\tTime 0.330 (0.335)\tData 0.000 (0.028)\tLoss 3.3819 (2.6449)\tPrec@1 92.969 (88.876)\tPrec@5 100.000 (99.070)\n",
      "Epoch: [180][30/97], lr: 0.00000\tTime 0.325 (0.334)\tData 0.000 (0.024)\tLoss 2.1814 (2.6393)\tPrec@1 82.812 (88.357)\tPrec@5 98.438 (99.042)\n",
      "Epoch: [180][40/97], lr: 0.00000\tTime 0.323 (0.334)\tData 0.000 (0.023)\tLoss 2.6540 (2.5496)\tPrec@1 85.938 (88.415)\tPrec@5 97.656 (99.143)\n",
      "Epoch: [180][50/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.022)\tLoss 3.8743 (2.5868)\tPrec@1 86.719 (88.557)\tPrec@5 98.438 (99.066)\n",
      "Epoch: [180][60/97], lr: 0.00000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 2.9010 (2.5623)\tPrec@1 89.844 (88.729)\tPrec@5 99.219 (99.027)\n",
      "Epoch: [180][70/97], lr: 0.00000\tTime 0.337 (0.332)\tData 0.000 (0.020)\tLoss 3.3224 (2.5683)\tPrec@1 84.375 (88.699)\tPrec@5 100.000 (99.131)\n",
      "Epoch: [180][80/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 2.3718 (2.5223)\tPrec@1 88.281 (88.908)\tPrec@5 99.219 (99.161)\n",
      "Epoch: [180][90/97], lr: 0.00000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 2.9822 (2.4813)\tPrec@1 86.719 (89.054)\tPrec@5 98.438 (99.210)\n",
      "Epoch: [180][96/97], lr: 0.00000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 1.8654 (2.5001)\tPrec@1 86.441 (88.965)\tPrec@5 100.000 (99.226)\n",
      "Test: [0/100]\tTime 0.257 (0.257)\tLoss 3.4737 (3.4737)\tPrec@1 88.000 (88.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 3.3305 (5.8559)\tPrec@1 84.000 (81.000)\tPrec@5 98.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.3803 (5.8259)\tPrec@1 82.000 (80.952)\tPrec@5 100.000 (98.429)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.5287 (5.8360)\tPrec@1 82.000 (80.968)\tPrec@5 99.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.7412 (5.9229)\tPrec@1 80.000 (80.780)\tPrec@5 97.000 (98.317)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.1333 (5.9217)\tPrec@1 82.000 (81.020)\tPrec@5 99.000 (98.333)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7494 (5.8570)\tPrec@1 82.000 (81.016)\tPrec@5 100.000 (98.361)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.3564 (5.8437)\tPrec@1 81.000 (81.352)\tPrec@5 98.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.9090 (5.8087)\tPrec@1 82.000 (81.272)\tPrec@5 97.000 (98.444)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.5012 (5.8594)\tPrec@1 85.000 (80.967)\tPrec@5 100.000 (98.473)\n",
      "val Results: Prec@1 80.780 Prec@5 98.470 Loss 5.88852\n",
      "val Class Accuracy: [0.916,0.975,0.779,0.711,0.777,0.747,0.858,0.778,0.768,0.769]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [181][0/97], lr: 0.00000\tTime 0.446 (0.446)\tData 0.224 (0.224)\tLoss 1.6440 (1.6440)\tPrec@1 91.406 (91.406)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [181][10/97], lr: 0.00000\tTime 0.325 (0.342)\tData 0.000 (0.035)\tLoss 1.9369 (2.6447)\tPrec@1 85.938 (87.429)\tPrec@5 98.438 (99.148)\n",
      "Epoch: [181][20/97], lr: 0.00000\tTime 0.323 (0.335)\tData 0.000 (0.027)\tLoss 2.8610 (2.6436)\tPrec@1 92.188 (87.686)\tPrec@5 99.219 (99.033)\n",
      "Epoch: [181][30/97], lr: 0.00000\tTime 0.328 (0.334)\tData 0.000 (0.024)\tLoss 2.0080 (2.6239)\tPrec@1 91.406 (87.802)\tPrec@5 100.000 (99.143)\n",
      "Epoch: [181][40/97], lr: 0.00000\tTime 0.329 (0.333)\tData 0.000 (0.022)\tLoss 3.5812 (2.5492)\tPrec@1 90.625 (88.072)\tPrec@5 100.000 (99.143)\n",
      "Epoch: [181][50/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 1.6540 (2.5054)\tPrec@1 91.406 (88.343)\tPrec@5 98.438 (99.142)\n",
      "Epoch: [181][60/97], lr: 0.00000\tTime 0.329 (0.332)\tData 0.000 (0.021)\tLoss 4.0835 (2.5809)\tPrec@1 89.844 (88.256)\tPrec@5 99.219 (99.168)\n",
      "Epoch: [181][70/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 2.0251 (2.6355)\tPrec@1 92.969 (88.391)\tPrec@5 100.000 (99.230)\n",
      "Epoch: [181][80/97], lr: 0.00000\tTime 0.331 (0.332)\tData 0.000 (0.020)\tLoss 2.0385 (2.6339)\tPrec@1 88.281 (88.551)\tPrec@5 97.656 (99.219)\n",
      "Epoch: [181][90/97], lr: 0.00000\tTime 0.332 (0.332)\tData 0.000 (0.019)\tLoss 1.1139 (2.6001)\tPrec@1 92.969 (88.556)\tPrec@5 100.000 (99.210)\n",
      "Epoch: [181][96/97], lr: 0.00000\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 1.9979 (2.5725)\tPrec@1 87.288 (88.522)\tPrec@5 100.000 (99.210)\n",
      "Test: [0/100]\tTime 0.262 (0.262)\tLoss 3.4909 (3.4909)\tPrec@1 86.000 (86.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 3.3226 (5.7975)\tPrec@1 84.000 (81.000)\tPrec@5 98.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.3438 (5.7423)\tPrec@1 81.000 (81.238)\tPrec@5 99.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.4895 (5.7671)\tPrec@1 82.000 (81.129)\tPrec@5 100.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 3.5888 (5.8468)\tPrec@1 80.000 (80.951)\tPrec@5 97.000 (98.341)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.0010 (5.8344)\tPrec@1 81.000 (81.098)\tPrec@5 99.000 (98.353)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.6994 (5.7811)\tPrec@1 82.000 (81.098)\tPrec@5 100.000 (98.344)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.2620 (5.7622)\tPrec@1 81.000 (81.437)\tPrec@5 98.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.0740 (5.7320)\tPrec@1 82.000 (81.358)\tPrec@5 97.000 (98.457)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.2566 (5.7743)\tPrec@1 86.000 (81.066)\tPrec@5 100.000 (98.451)\n",
      "val Results: Prec@1 80.890 Prec@5 98.440 Loss 5.80184\n",
      "val Class Accuracy: [0.913,0.970,0.781,0.716,0.795,0.747,0.854,0.770,0.774,0.769]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [182][0/97], lr: 0.00000\tTime 0.380 (0.380)\tData 0.207 (0.207)\tLoss 2.0867 (2.0867)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [182][10/97], lr: 0.00000\tTime 0.326 (0.344)\tData 0.000 (0.034)\tLoss 2.7842 (2.5730)\tPrec@1 92.969 (88.210)\tPrec@5 99.219 (99.716)\n",
      "Epoch: [182][20/97], lr: 0.00000\tTime 0.333 (0.336)\tData 0.000 (0.026)\tLoss 2.8660 (2.8646)\tPrec@1 89.844 (88.132)\tPrec@5 100.000 (99.256)\n",
      "Epoch: [182][30/97], lr: 0.00000\tTime 0.331 (0.335)\tData 0.000 (0.023)\tLoss 1.3940 (2.8140)\tPrec@1 91.406 (88.432)\tPrec@5 100.000 (99.345)\n",
      "Epoch: [182][40/97], lr: 0.00000\tTime 0.326 (0.334)\tData 0.000 (0.022)\tLoss 2.2768 (2.7593)\tPrec@1 88.281 (88.662)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [182][50/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 3.7628 (2.6633)\tPrec@1 86.719 (88.618)\tPrec@5 100.000 (99.464)\n",
      "Epoch: [182][60/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 1.4072 (2.6876)\tPrec@1 88.281 (88.614)\tPrec@5 100.000 (99.449)\n",
      "Epoch: [182][70/97], lr: 0.00000\tTime 0.328 (0.332)\tData 0.000 (0.020)\tLoss 1.1853 (2.6114)\tPrec@1 94.531 (88.732)\tPrec@5 100.000 (99.439)\n",
      "Epoch: [182][80/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 2.4945 (2.5497)\tPrec@1 89.062 (88.754)\tPrec@5 98.438 (99.402)\n",
      "Epoch: [182][90/97], lr: 0.00000\tTime 0.324 (0.331)\tData 0.000 (0.019)\tLoss 2.0675 (2.5564)\tPrec@1 91.406 (88.685)\tPrec@5 98.438 (99.339)\n",
      "Epoch: [182][96/97], lr: 0.00000\tTime 0.311 (0.331)\tData 0.000 (0.020)\tLoss 3.1480 (2.5457)\tPrec@1 88.136 (88.699)\tPrec@5 100.000 (99.347)\n",
      "Test: [0/100]\tTime 0.258 (0.258)\tLoss 3.4640 (3.4640)\tPrec@1 86.000 (86.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 3.2554 (5.7851)\tPrec@1 84.000 (80.909)\tPrec@5 98.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.2644 (5.7561)\tPrec@1 82.000 (80.952)\tPrec@5 100.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.4297 (5.7638)\tPrec@1 82.000 (80.903)\tPrec@5 100.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 3.6712 (5.8545)\tPrec@1 81.000 (80.805)\tPrec@5 98.000 (98.366)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.1067 (5.8520)\tPrec@1 81.000 (80.980)\tPrec@5 99.000 (98.373)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.6633 (5.7923)\tPrec@1 83.000 (80.967)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.2144 (5.7796)\tPrec@1 80.000 (81.254)\tPrec@5 98.000 (98.366)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.9360 (5.7464)\tPrec@1 82.000 (81.198)\tPrec@5 97.000 (98.444)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.2791 (5.7947)\tPrec@1 85.000 (80.901)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 80.740 Prec@5 98.480 Loss 5.82497\n",
      "val Class Accuracy: [0.910,0.973,0.798,0.702,0.774,0.754,0.851,0.777,0.772,0.763]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [183][0/97], lr: 0.00000\tTime 0.465 (0.465)\tData 0.253 (0.253)\tLoss 1.6019 (1.6019)\tPrec@1 88.281 (88.281)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [183][10/97], lr: 0.00000\tTime 0.328 (0.348)\tData 0.000 (0.038)\tLoss 2.7966 (2.5857)\tPrec@1 89.844 (88.991)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [183][20/97], lr: 0.00000\tTime 0.324 (0.338)\tData 0.000 (0.028)\tLoss 2.0876 (2.5325)\tPrec@1 90.625 (89.583)\tPrec@5 100.000 (99.182)\n",
      "Epoch: [183][30/97], lr: 0.00000\tTime 0.329 (0.336)\tData 0.000 (0.025)\tLoss 3.4594 (2.7288)\tPrec@1 84.375 (88.533)\tPrec@5 99.219 (99.194)\n",
      "Epoch: [183][40/97], lr: 0.00000\tTime 0.327 (0.334)\tData 0.000 (0.023)\tLoss 5.1921 (2.7346)\tPrec@1 88.281 (88.739)\tPrec@5 99.219 (99.314)\n",
      "Epoch: [183][50/97], lr: 0.00000\tTime 0.322 (0.333)\tData 0.000 (0.022)\tLoss 2.0726 (2.6412)\tPrec@1 90.625 (88.741)\tPrec@5 100.000 (99.357)\n",
      "Epoch: [183][60/97], lr: 0.00000\tTime 0.331 (0.334)\tData 0.000 (0.021)\tLoss 5.4815 (2.6481)\tPrec@1 88.281 (88.640)\tPrec@5 98.438 (99.321)\n",
      "Epoch: [183][70/97], lr: 0.00000\tTime 0.331 (0.335)\tData 0.000 (0.021)\tLoss 1.4713 (2.5897)\tPrec@1 89.844 (88.886)\tPrec@5 99.219 (99.296)\n",
      "Epoch: [183][80/97], lr: 0.00000\tTime 0.329 (0.335)\tData 0.000 (0.020)\tLoss 4.6718 (2.6041)\tPrec@1 85.156 (88.850)\tPrec@5 97.656 (99.286)\n",
      "Epoch: [183][90/97], lr: 0.00000\tTime 0.326 (0.335)\tData 0.000 (0.020)\tLoss 3.4680 (2.5699)\tPrec@1 92.188 (88.959)\tPrec@5 100.000 (99.356)\n",
      "Epoch: [183][96/97], lr: 0.00000\tTime 0.316 (0.334)\tData 0.000 (0.020)\tLoss 3.0956 (2.5671)\tPrec@1 80.508 (88.836)\tPrec@5 99.153 (99.387)\n",
      "Test: [0/100]\tTime 0.271 (0.271)\tLoss 3.5297 (3.5297)\tPrec@1 88.000 (88.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 3.2267 (5.8301)\tPrec@1 84.000 (80.727)\tPrec@5 98.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.3744 (5.7895)\tPrec@1 81.000 (80.714)\tPrec@5 100.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.4875 (5.7963)\tPrec@1 81.000 (80.806)\tPrec@5 100.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 3.6914 (5.8810)\tPrec@1 80.000 (80.634)\tPrec@5 97.000 (98.293)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.0139 (5.8730)\tPrec@1 82.000 (80.882)\tPrec@5 99.000 (98.314)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7110 (5.8148)\tPrec@1 82.000 (80.902)\tPrec@5 100.000 (98.344)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.3693 (5.8047)\tPrec@1 81.000 (81.211)\tPrec@5 98.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.0552 (5.7745)\tPrec@1 82.000 (81.123)\tPrec@5 97.000 (98.457)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.3989 (5.8240)\tPrec@1 85.000 (80.835)\tPrec@5 100.000 (98.484)\n",
      "val Results: Prec@1 80.660 Prec@5 98.490 Loss 5.85214\n",
      "val Class Accuracy: [0.917,0.973,0.780,0.705,0.781,0.747,0.860,0.774,0.762,0.767]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [184][0/97], lr: 0.00000\tTime 0.445 (0.445)\tData 0.241 (0.241)\tLoss 2.7007 (2.7007)\tPrec@1 85.156 (85.156)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [184][10/97], lr: 0.00000\tTime 0.331 (0.342)\tData 0.000 (0.037)\tLoss 2.3696 (2.3491)\tPrec@1 85.938 (88.281)\tPrec@5 98.438 (99.006)\n",
      "Epoch: [184][20/97], lr: 0.00000\tTime 0.323 (0.334)\tData 0.000 (0.027)\tLoss 1.3150 (2.2717)\tPrec@1 91.406 (88.690)\tPrec@5 100.000 (99.144)\n",
      "Epoch: [184][30/97], lr: 0.00000\tTime 0.326 (0.334)\tData 0.000 (0.024)\tLoss 4.1359 (2.3597)\tPrec@1 82.031 (88.483)\tPrec@5 97.656 (99.143)\n",
      "Epoch: [184][40/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.022)\tLoss 1.1190 (2.3114)\tPrec@1 92.188 (88.929)\tPrec@5 98.438 (99.104)\n",
      "Epoch: [184][50/97], lr: 0.00000\tTime 0.327 (0.333)\tData 0.000 (0.021)\tLoss 3.3818 (2.2682)\tPrec@1 89.062 (89.001)\tPrec@5 100.000 (99.157)\n",
      "Epoch: [184][60/97], lr: 0.00000\tTime 0.327 (0.332)\tData 0.000 (0.021)\tLoss 0.7310 (2.1732)\tPrec@1 92.969 (89.075)\tPrec@5 100.000 (99.180)\n",
      "Epoch: [184][70/97], lr: 0.00000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 2.9674 (2.2648)\tPrec@1 84.375 (88.798)\tPrec@5 98.438 (99.230)\n",
      "Epoch: [184][80/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 1.0072 (2.3200)\tPrec@1 89.062 (88.677)\tPrec@5 99.219 (99.180)\n",
      "Epoch: [184][90/97], lr: 0.00000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 1.1801 (2.3279)\tPrec@1 92.188 (88.676)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [184][96/97], lr: 0.00000\tTime 0.316 (0.331)\tData 0.000 (0.020)\tLoss 2.3582 (2.3199)\tPrec@1 88.983 (88.578)\tPrec@5 98.305 (99.210)\n",
      "Test: [0/100]\tTime 0.237 (0.237)\tLoss 3.5155 (3.5155)\tPrec@1 86.000 (86.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 3.3326 (5.8851)\tPrec@1 84.000 (80.636)\tPrec@5 98.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.3517 (5.8282)\tPrec@1 81.000 (80.714)\tPrec@5 99.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.5410 (5.8511)\tPrec@1 81.000 (80.710)\tPrec@5 100.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.7284 (5.9373)\tPrec@1 81.000 (80.512)\tPrec@5 97.000 (98.341)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.0755 (5.9392)\tPrec@1 81.000 (80.686)\tPrec@5 99.000 (98.353)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7671 (5.8790)\tPrec@1 82.000 (80.770)\tPrec@5 100.000 (98.361)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.4396 (5.8651)\tPrec@1 81.000 (81.056)\tPrec@5 98.000 (98.394)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.0519 (5.8328)\tPrec@1 81.000 (81.025)\tPrec@5 97.000 (98.457)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.4502 (5.8812)\tPrec@1 85.000 (80.758)\tPrec@5 100.000 (98.495)\n",
      "val Results: Prec@1 80.590 Prec@5 98.490 Loss 5.90736\n",
      "val Class Accuracy: [0.911,0.973,0.769,0.700,0.789,0.763,0.865,0.760,0.766,0.763]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [185][0/97], lr: 0.00000\tTime 0.393 (0.393)\tData 0.206 (0.206)\tLoss 5.2785 (5.2785)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [185][10/97], lr: 0.00000\tTime 0.326 (0.339)\tData 0.000 (0.033)\tLoss 1.4062 (2.4578)\tPrec@1 92.969 (88.068)\tPrec@5 100.000 (99.148)\n",
      "Epoch: [185][20/97], lr: 0.00000\tTime 0.338 (0.334)\tData 0.000 (0.026)\tLoss 1.5668 (2.5544)\tPrec@1 91.406 (87.946)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [185][30/97], lr: 0.00000\tTime 0.329 (0.334)\tData 0.000 (0.023)\tLoss 1.4330 (2.5169)\tPrec@1 89.062 (88.029)\tPrec@5 99.219 (99.244)\n",
      "Epoch: [185][40/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.022)\tLoss 3.6023 (2.6137)\tPrec@1 86.719 (87.957)\tPrec@5 98.438 (99.162)\n",
      "Epoch: [185][50/97], lr: 0.00000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 3.1212 (2.5689)\tPrec@1 87.500 (87.929)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [185][60/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 2.3631 (2.4693)\tPrec@1 89.062 (88.089)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [185][70/97], lr: 0.00000\tTime 0.328 (0.332)\tData 0.000 (0.020)\tLoss 3.3808 (2.5296)\tPrec@1 86.719 (88.193)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [185][80/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.019)\tLoss 2.5364 (2.5664)\tPrec@1 85.938 (88.185)\tPrec@5 97.656 (99.209)\n",
      "Epoch: [185][90/97], lr: 0.00000\tTime 0.327 (0.332)\tData 0.000 (0.019)\tLoss 2.9932 (2.5816)\tPrec@1 85.938 (88.118)\tPrec@5 100.000 (99.202)\n",
      "Epoch: [185][96/97], lr: 0.00000\tTime 0.318 (0.332)\tData 0.000 (0.020)\tLoss 1.6084 (2.5283)\tPrec@1 90.678 (88.215)\tPrec@5 98.305 (99.146)\n",
      "Test: [0/100]\tTime 0.279 (0.279)\tLoss 3.4813 (3.4813)\tPrec@1 87.000 (87.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 3.2127 (5.8110)\tPrec@1 84.000 (81.000)\tPrec@5 98.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 5.3002 (5.7672)\tPrec@1 81.000 (80.952)\tPrec@5 100.000 (98.429)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 4.4491 (5.7845)\tPrec@1 82.000 (80.968)\tPrec@5 100.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 3.6461 (5.8732)\tPrec@1 80.000 (80.780)\tPrec@5 97.000 (98.341)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.0645 (5.8669)\tPrec@1 81.000 (80.961)\tPrec@5 99.000 (98.353)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7108 (5.8105)\tPrec@1 82.000 (80.951)\tPrec@5 100.000 (98.344)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.2961 (5.7990)\tPrec@1 80.000 (81.338)\tPrec@5 98.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.0443 (5.7660)\tPrec@1 82.000 (81.272)\tPrec@5 97.000 (98.444)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.2795 (5.8125)\tPrec@1 86.000 (81.000)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 80.820 Prec@5 98.470 Loss 5.84186\n",
      "val Class Accuracy: [0.913,0.972,0.781,0.705,0.783,0.754,0.864,0.775,0.774,0.761]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [186][0/97], lr: 0.00000\tTime 0.441 (0.441)\tData 0.235 (0.235)\tLoss 3.3363 (3.3363)\tPrec@1 82.812 (82.812)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [186][10/97], lr: 0.00000\tTime 0.328 (0.344)\tData 0.000 (0.037)\tLoss 2.8308 (2.3558)\tPrec@1 89.844 (88.281)\tPrec@5 98.438 (99.290)\n",
      "Epoch: [186][20/97], lr: 0.00000\tTime 0.325 (0.336)\tData 0.000 (0.027)\tLoss 2.8529 (2.2982)\tPrec@1 89.844 (89.174)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [186][30/97], lr: 0.00000\tTime 0.326 (0.335)\tData 0.000 (0.024)\tLoss 1.6654 (2.3885)\tPrec@1 88.281 (88.659)\tPrec@5 98.438 (99.370)\n",
      "Epoch: [186][40/97], lr: 0.00000\tTime 0.329 (0.334)\tData 0.000 (0.022)\tLoss 3.2367 (2.4468)\tPrec@1 87.500 (88.510)\tPrec@5 100.000 (99.466)\n",
      "Epoch: [186][50/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 3.5858 (2.4393)\tPrec@1 86.719 (88.450)\tPrec@5 100.000 (99.418)\n",
      "Epoch: [186][60/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 3.0986 (2.4551)\tPrec@1 88.281 (88.768)\tPrec@5 98.438 (99.372)\n",
      "Epoch: [186][70/97], lr: 0.00000\tTime 0.329 (0.332)\tData 0.000 (0.020)\tLoss 1.6775 (2.4556)\tPrec@1 90.625 (88.666)\tPrec@5 98.438 (99.351)\n",
      "Epoch: [186][80/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 3.0174 (2.4632)\tPrec@1 81.250 (88.571)\tPrec@5 98.438 (99.373)\n",
      "Epoch: [186][90/97], lr: 0.00000\tTime 0.323 (0.332)\tData 0.000 (0.020)\tLoss 1.7224 (2.4734)\tPrec@1 86.719 (88.607)\tPrec@5 100.000 (99.356)\n",
      "Epoch: [186][96/97], lr: 0.00000\tTime 0.320 (0.332)\tData 0.000 (0.020)\tLoss 1.6301 (2.4633)\tPrec@1 87.288 (88.707)\tPrec@5 100.000 (99.355)\n",
      "Test: [0/100]\tTime 0.233 (0.233)\tLoss 3.6288 (3.6288)\tPrec@1 86.000 (86.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 3.4651 (6.0592)\tPrec@1 84.000 (80.545)\tPrec@5 98.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.3910 (5.9951)\tPrec@1 82.000 (80.762)\tPrec@5 99.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.5565 (6.0038)\tPrec@1 82.000 (80.710)\tPrec@5 100.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.8905 (6.1009)\tPrec@1 80.000 (80.512)\tPrec@5 97.000 (98.293)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.3391 (6.1096)\tPrec@1 81.000 (80.667)\tPrec@5 99.000 (98.333)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 6.8131 (6.0465)\tPrec@1 85.000 (80.770)\tPrec@5 100.000 (98.344)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.5345 (6.0375)\tPrec@1 81.000 (81.113)\tPrec@5 99.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.2565 (6.0064)\tPrec@1 82.000 (81.025)\tPrec@5 97.000 (98.457)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.6164 (6.0548)\tPrec@1 85.000 (80.769)\tPrec@5 100.000 (98.484)\n",
      "val Results: Prec@1 80.620 Prec@5 98.480 Loss 6.08363\n",
      "val Class Accuracy: [0.917,0.976,0.797,0.705,0.779,0.751,0.852,0.773,0.766,0.746]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [187][0/97], lr: 0.00000\tTime 0.443 (0.443)\tData 0.201 (0.201)\tLoss 2.4218 (2.4218)\tPrec@1 88.281 (88.281)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [187][10/97], lr: 0.00000\tTime 0.325 (0.344)\tData 0.000 (0.033)\tLoss 1.9068 (2.6021)\tPrec@1 90.625 (88.423)\tPrec@5 99.219 (99.148)\n",
      "Epoch: [187][20/97], lr: 0.00000\tTime 0.324 (0.336)\tData 0.000 (0.026)\tLoss 1.8990 (2.8118)\tPrec@1 88.281 (87.686)\tPrec@5 99.219 (99.293)\n",
      "Epoch: [187][30/97], lr: 0.00000\tTime 0.327 (0.335)\tData 0.000 (0.023)\tLoss 5.1243 (2.8151)\tPrec@1 85.938 (88.080)\tPrec@5 98.438 (99.320)\n",
      "Epoch: [187][40/97], lr: 0.00000\tTime 0.324 (0.333)\tData 0.000 (0.022)\tLoss 1.4500 (2.7515)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (99.352)\n",
      "Epoch: [187][50/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 1.9673 (2.6861)\tPrec@1 88.281 (88.266)\tPrec@5 99.219 (99.341)\n",
      "Epoch: [187][60/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 2.6116 (2.6448)\tPrec@1 87.500 (88.422)\tPrec@5 99.219 (99.283)\n",
      "Epoch: [187][70/97], lr: 0.00000\tTime 0.323 (0.332)\tData 0.000 (0.020)\tLoss 1.7406 (2.6773)\tPrec@1 89.844 (88.413)\tPrec@5 99.219 (99.263)\n",
      "Epoch: [187][80/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.019)\tLoss 2.8312 (2.6464)\tPrec@1 89.844 (88.561)\tPrec@5 99.219 (99.267)\n",
      "Epoch: [187][90/97], lr: 0.00000\tTime 0.331 (0.332)\tData 0.000 (0.019)\tLoss 2.0483 (2.6567)\tPrec@1 87.500 (88.453)\tPrec@5 99.219 (99.279)\n",
      "Epoch: [187][96/97], lr: 0.00000\tTime 0.317 (0.332)\tData 0.000 (0.020)\tLoss 1.3244 (2.6460)\tPrec@1 91.525 (88.522)\tPrec@5 100.000 (99.275)\n",
      "Test: [0/100]\tTime 0.247 (0.247)\tLoss 3.5130 (3.5130)\tPrec@1 88.000 (88.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 3.4426 (5.9451)\tPrec@1 84.000 (81.000)\tPrec@5 98.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.3228 (5.8792)\tPrec@1 82.000 (80.905)\tPrec@5 99.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.4903 (5.8951)\tPrec@1 81.000 (80.839)\tPrec@5 100.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.7438 (5.9886)\tPrec@1 80.000 (80.659)\tPrec@5 97.000 (98.317)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.1575 (5.9926)\tPrec@1 81.000 (80.784)\tPrec@5 99.000 (98.333)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.6994 (5.9302)\tPrec@1 82.000 (80.787)\tPrec@5 100.000 (98.344)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.4356 (5.9193)\tPrec@1 81.000 (81.141)\tPrec@5 98.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.1007 (5.8875)\tPrec@1 81.000 (81.099)\tPrec@5 97.000 (98.444)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.5177 (5.9362)\tPrec@1 85.000 (80.846)\tPrec@5 100.000 (98.473)\n",
      "val Results: Prec@1 80.670 Prec@5 98.480 Loss 5.96440\n",
      "val Class Accuracy: [0.911,0.973,0.780,0.702,0.782,0.763,0.861,0.773,0.769,0.753]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [188][0/97], lr: 0.00000\tTime 0.468 (0.468)\tData 0.256 (0.256)\tLoss 6.7471 (6.7471)\tPrec@1 82.812 (82.812)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [188][10/97], lr: 0.00000\tTime 0.328 (0.352)\tData 0.000 (0.038)\tLoss 3.4444 (3.1495)\tPrec@1 90.625 (88.920)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [188][20/97], lr: 0.00000\tTime 0.331 (0.340)\tData 0.000 (0.028)\tLoss 3.1963 (2.8700)\tPrec@1 81.250 (88.579)\tPrec@5 98.438 (99.293)\n",
      "Epoch: [188][30/97], lr: 0.00000\tTime 0.324 (0.336)\tData 0.000 (0.025)\tLoss 5.7307 (2.8102)\tPrec@1 82.812 (88.357)\tPrec@5 100.000 (99.446)\n",
      "Epoch: [188][40/97], lr: 0.00000\tTime 0.323 (0.335)\tData 0.000 (0.023)\tLoss 1.3821 (2.8086)\tPrec@1 91.406 (88.681)\tPrec@5 100.000 (99.466)\n",
      "Epoch: [188][50/97], lr: 0.00000\tTime 0.324 (0.333)\tData 0.000 (0.022)\tLoss 0.8644 (2.7104)\tPrec@1 89.844 (88.618)\tPrec@5 99.219 (99.479)\n",
      "Epoch: [188][60/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 2.2730 (2.6387)\tPrec@1 88.281 (88.729)\tPrec@5 100.000 (99.424)\n",
      "Epoch: [188][70/97], lr: 0.00000\tTime 0.326 (0.333)\tData 0.000 (0.021)\tLoss 2.0869 (2.5371)\tPrec@1 91.406 (89.051)\tPrec@5 99.219 (99.362)\n",
      "Epoch: [188][80/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 2.2265 (2.5334)\tPrec@1 89.844 (88.918)\tPrec@5 98.438 (99.315)\n",
      "Epoch: [188][90/97], lr: 0.00000\tTime 0.323 (0.332)\tData 0.000 (0.020)\tLoss 1.8720 (2.5207)\tPrec@1 94.531 (88.908)\tPrec@5 99.219 (99.330)\n",
      "Epoch: [188][96/97], lr: 0.00000\tTime 0.316 (0.332)\tData 0.000 (0.020)\tLoss 1.7133 (2.5003)\tPrec@1 90.678 (88.933)\tPrec@5 99.153 (99.331)\n",
      "Test: [0/100]\tTime 0.296 (0.296)\tLoss 3.4269 (3.4269)\tPrec@1 88.000 (88.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 3.2875 (5.7556)\tPrec@1 84.000 (81.182)\tPrec@5 98.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 5.3270 (5.7298)\tPrec@1 82.000 (81.238)\tPrec@5 100.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 4.4608 (5.7432)\tPrec@1 82.000 (81.097)\tPrec@5 100.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 3.6382 (5.8264)\tPrec@1 80.000 (80.878)\tPrec@5 97.000 (98.317)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.9997 (5.8124)\tPrec@1 82.000 (81.059)\tPrec@5 99.000 (98.333)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 6.6754 (5.7593)\tPrec@1 82.000 (81.016)\tPrec@5 100.000 (98.361)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.3168 (5.7494)\tPrec@1 80.000 (81.296)\tPrec@5 98.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.0445 (5.7182)\tPrec@1 82.000 (81.148)\tPrec@5 97.000 (98.457)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.2917 (5.7682)\tPrec@1 85.000 (80.846)\tPrec@5 100.000 (98.473)\n",
      "val Results: Prec@1 80.690 Prec@5 98.480 Loss 5.79646\n",
      "val Class Accuracy: [0.918,0.968,0.782,0.718,0.776,0.733,0.857,0.774,0.771,0.772]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [189][0/97], lr: 0.00000\tTime 0.411 (0.411)\tData 0.197 (0.197)\tLoss 2.1115 (2.1115)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [189][10/97], lr: 0.00000\tTime 0.325 (0.341)\tData 0.000 (0.033)\tLoss 2.8402 (2.6310)\tPrec@1 88.281 (89.276)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [189][20/97], lr: 0.00000\tTime 0.332 (0.335)\tData 0.000 (0.026)\tLoss 1.3694 (2.5399)\tPrec@1 89.062 (88.876)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [189][30/97], lr: 0.00000\tTime 0.322 (0.334)\tData 0.000 (0.023)\tLoss 2.0669 (2.6345)\tPrec@1 91.406 (88.634)\tPrec@5 99.219 (99.294)\n",
      "Epoch: [189][40/97], lr: 0.00000\tTime 0.324 (0.333)\tData 0.000 (0.021)\tLoss 1.2783 (2.5063)\tPrec@1 90.625 (88.720)\tPrec@5 98.438 (99.200)\n",
      "Epoch: [189][50/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.021)\tLoss 2.5121 (2.5191)\tPrec@1 86.719 (88.756)\tPrec@5 100.000 (99.203)\n",
      "Epoch: [189][60/97], lr: 0.00000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 1.4753 (2.4714)\tPrec@1 88.281 (88.870)\tPrec@5 98.438 (99.180)\n",
      "Epoch: [189][70/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 4.6650 (2.5411)\tPrec@1 82.812 (88.721)\tPrec@5 99.219 (99.186)\n",
      "Epoch: [189][80/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.019)\tLoss 1.6054 (2.5724)\tPrec@1 89.062 (88.600)\tPrec@5 100.000 (99.180)\n",
      "Epoch: [189][90/97], lr: 0.00000\tTime 0.323 (0.331)\tData 0.000 (0.019)\tLoss 2.5288 (2.5751)\tPrec@1 89.062 (88.642)\tPrec@5 98.438 (99.193)\n",
      "Epoch: [189][96/97], lr: 0.00000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 3.4294 (2.5853)\tPrec@1 88.136 (88.522)\tPrec@5 100.000 (99.194)\n",
      "Test: [0/100]\tTime 0.263 (0.263)\tLoss 3.4724 (3.4724)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 3.1605 (5.7615)\tPrec@1 84.000 (81.000)\tPrec@5 98.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.3487 (5.7250)\tPrec@1 81.000 (81.190)\tPrec@5 99.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.4232 (5.7473)\tPrec@1 82.000 (81.097)\tPrec@5 100.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 3.5532 (5.8275)\tPrec@1 80.000 (81.000)\tPrec@5 97.000 (98.341)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.9892 (5.8173)\tPrec@1 81.000 (81.137)\tPrec@5 99.000 (98.353)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.6534 (5.7612)\tPrec@1 84.000 (81.180)\tPrec@5 100.000 (98.344)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 3.2738 (5.7489)\tPrec@1 81.000 (81.465)\tPrec@5 98.000 (98.394)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.0062 (5.7190)\tPrec@1 83.000 (81.370)\tPrec@5 97.000 (98.469)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.2685 (5.7628)\tPrec@1 86.000 (81.077)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 80.900 Prec@5 98.470 Loss 5.79375\n",
      "val Class Accuracy: [0.913,0.973,0.786,0.702,0.808,0.736,0.860,0.772,0.775,0.765]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [190][0/97], lr: 0.00000\tTime 0.434 (0.434)\tData 0.231 (0.231)\tLoss 6.6170 (6.6170)\tPrec@1 84.375 (84.375)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [190][10/97], lr: 0.00000\tTime 0.330 (0.344)\tData 0.000 (0.036)\tLoss 1.6688 (2.8157)\tPrec@1 89.062 (87.642)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [190][20/97], lr: 0.00000\tTime 0.324 (0.336)\tData 0.000 (0.027)\tLoss 3.0474 (2.7039)\tPrec@1 85.938 (87.686)\tPrec@5 98.438 (99.256)\n",
      "Epoch: [190][30/97], lr: 0.00000\tTime 0.325 (0.334)\tData 0.000 (0.024)\tLoss 1.9114 (2.8068)\tPrec@1 86.719 (87.576)\tPrec@5 99.219 (99.320)\n",
      "Epoch: [190][40/97], lr: 0.00000\tTime 0.328 (0.333)\tData 0.000 (0.022)\tLoss 2.2376 (2.6678)\tPrec@1 89.062 (88.014)\tPrec@5 99.219 (99.238)\n",
      "Epoch: [190][50/97], lr: 0.00000\tTime 0.327 (0.332)\tData 0.000 (0.021)\tLoss 4.4807 (2.5699)\tPrec@1 92.969 (88.465)\tPrec@5 99.219 (99.280)\n",
      "Epoch: [190][60/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 1.4740 (2.5469)\tPrec@1 90.625 (88.563)\tPrec@5 99.219 (99.232)\n",
      "Epoch: [190][70/97], lr: 0.00000\tTime 0.328 (0.332)\tData 0.000 (0.020)\tLoss 2.3100 (2.6253)\tPrec@1 90.625 (88.655)\tPrec@5 100.000 (99.274)\n",
      "Epoch: [190][80/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 1.8990 (2.6021)\tPrec@1 87.500 (88.638)\tPrec@5 96.875 (99.257)\n",
      "Epoch: [190][90/97], lr: 0.00000\tTime 0.327 (0.331)\tData 0.000 (0.020)\tLoss 4.6818 (2.5763)\tPrec@1 80.469 (88.633)\tPrec@5 99.219 (99.253)\n",
      "Epoch: [190][96/97], lr: 0.00000\tTime 0.320 (0.331)\tData 0.000 (0.020)\tLoss 2.4297 (2.5772)\tPrec@1 85.593 (88.651)\tPrec@5 100.000 (99.275)\n",
      "Test: [0/100]\tTime 0.255 (0.255)\tLoss 3.4533 (3.4533)\tPrec@1 87.000 (87.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 3.2469 (5.7648)\tPrec@1 84.000 (80.909)\tPrec@5 98.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.3309 (5.7264)\tPrec@1 81.000 (81.000)\tPrec@5 99.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.4844 (5.7388)\tPrec@1 81.000 (80.935)\tPrec@5 100.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.6357 (5.8177)\tPrec@1 80.000 (80.780)\tPrec@5 97.000 (98.317)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.9900 (5.8152)\tPrec@1 80.000 (80.941)\tPrec@5 99.000 (98.333)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7473 (5.7555)\tPrec@1 84.000 (80.967)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.2776 (5.7392)\tPrec@1 82.000 (81.338)\tPrec@5 98.000 (98.366)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.9101 (5.7078)\tPrec@1 82.000 (81.259)\tPrec@5 97.000 (98.432)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.3363 (5.7572)\tPrec@1 85.000 (80.945)\tPrec@5 100.000 (98.440)\n",
      "val Results: Prec@1 80.780 Prec@5 98.450 Loss 5.78478\n",
      "val Class Accuracy: [0.912,0.974,0.789,0.700,0.783,0.749,0.859,0.774,0.764,0.774]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [191][0/97], lr: 0.00000\tTime 0.433 (0.433)\tData 0.230 (0.230)\tLoss 1.8641 (1.8641)\tPrec@1 86.719 (86.719)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [191][10/97], lr: 0.00000\tTime 0.330 (0.354)\tData 0.000 (0.036)\tLoss 1.3432 (2.5586)\tPrec@1 92.188 (87.642)\tPrec@5 98.438 (99.290)\n",
      "Epoch: [191][20/97], lr: 0.00000\tTime 0.340 (0.343)\tData 0.000 (0.027)\tLoss 2.0367 (2.3426)\tPrec@1 85.156 (88.244)\tPrec@5 99.219 (99.330)\n",
      "Epoch: [191][30/97], lr: 0.00000\tTime 0.324 (0.339)\tData 0.000 (0.024)\tLoss 3.3689 (2.4472)\tPrec@1 86.719 (87.727)\tPrec@5 100.000 (99.244)\n",
      "Epoch: [191][40/97], lr: 0.00000\tTime 0.325 (0.336)\tData 0.000 (0.022)\tLoss 2.0471 (2.5055)\tPrec@1 87.500 (87.919)\tPrec@5 99.219 (99.162)\n",
      "Epoch: [191][50/97], lr: 0.00000\tTime 0.320 (0.335)\tData 0.000 (0.021)\tLoss 2.0942 (2.4263)\tPrec@1 89.844 (87.837)\tPrec@5 100.000 (99.112)\n",
      "Epoch: [191][60/97], lr: 0.00000\tTime 0.324 (0.334)\tData 0.000 (0.021)\tLoss 1.4498 (2.4317)\tPrec@1 89.062 (88.115)\tPrec@5 100.000 (99.232)\n",
      "Epoch: [191][70/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 2.7166 (2.4378)\tPrec@1 89.062 (88.303)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [191][80/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 1.7920 (2.4794)\tPrec@1 88.281 (88.320)\tPrec@5 100.000 (99.286)\n",
      "Epoch: [191][90/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 1.7412 (2.4793)\tPrec@1 90.625 (88.341)\tPrec@5 98.438 (99.305)\n",
      "Epoch: [191][96/97], lr: 0.00000\tTime 0.317 (0.332)\tData 0.000 (0.020)\tLoss 2.8990 (2.4294)\tPrec@1 90.678 (88.465)\tPrec@5 100.000 (99.307)\n",
      "Test: [0/100]\tTime 0.269 (0.269)\tLoss 3.5935 (3.5935)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 3.4008 (6.0135)\tPrec@1 84.000 (80.455)\tPrec@5 98.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.4601 (5.9570)\tPrec@1 81.000 (80.952)\tPrec@5 99.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.5350 (5.9628)\tPrec@1 81.000 (80.903)\tPrec@5 100.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 3.7745 (6.0475)\tPrec@1 81.000 (80.780)\tPrec@5 97.000 (98.293)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.2101 (6.0481)\tPrec@1 81.000 (80.902)\tPrec@5 99.000 (98.333)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.8115 (5.9827)\tPrec@1 82.000 (80.934)\tPrec@5 100.000 (98.344)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.4820 (5.9705)\tPrec@1 82.000 (81.239)\tPrec@5 98.000 (98.394)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.1912 (5.9397)\tPrec@1 82.000 (81.111)\tPrec@5 97.000 (98.469)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.6065 (5.9879)\tPrec@1 86.000 (80.835)\tPrec@5 100.000 (98.495)\n",
      "val Results: Prec@1 80.670 Prec@5 98.500 Loss 6.01570\n",
      "val Class Accuracy: [0.915,0.976,0.792,0.711,0.794,0.734,0.860,0.772,0.760,0.753]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [192][0/97], lr: 0.00000\tTime 0.506 (0.506)\tData 0.296 (0.296)\tLoss 1.7931 (1.7931)\tPrec@1 85.938 (85.938)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [192][10/97], lr: 0.00000\tTime 0.331 (0.349)\tData 0.000 (0.042)\tLoss 2.1512 (1.9891)\tPrec@1 89.062 (89.205)\tPrec@5 99.219 (98.935)\n",
      "Epoch: [192][20/97], lr: 0.00000\tTime 0.325 (0.340)\tData 0.000 (0.030)\tLoss 4.3035 (2.2810)\tPrec@1 87.500 (89.249)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [192][30/97], lr: 0.00000\tTime 0.324 (0.337)\tData 0.000 (0.026)\tLoss 1.5092 (2.3859)\tPrec@1 92.969 (89.289)\tPrec@5 100.000 (99.194)\n",
      "Epoch: [192][40/97], lr: 0.00000\tTime 0.331 (0.335)\tData 0.000 (0.024)\tLoss 1.4321 (2.4922)\tPrec@1 92.188 (88.929)\tPrec@5 99.219 (99.276)\n",
      "Epoch: [192][50/97], lr: 0.00000\tTime 0.326 (0.334)\tData 0.000 (0.022)\tLoss 3.4750 (2.4543)\tPrec@1 84.375 (88.771)\tPrec@5 99.219 (99.311)\n",
      "Epoch: [192][60/97], lr: 0.00000\tTime 0.327 (0.334)\tData 0.000 (0.022)\tLoss 4.2601 (2.5482)\tPrec@1 85.156 (88.525)\tPrec@5 97.656 (99.257)\n",
      "Epoch: [192][70/97], lr: 0.00000\tTime 0.328 (0.334)\tData 0.000 (0.021)\tLoss 1.9517 (2.5312)\tPrec@1 86.719 (88.523)\tPrec@5 100.000 (99.252)\n",
      "Epoch: [192][80/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 1.9968 (2.4420)\tPrec@1 85.156 (88.706)\tPrec@5 98.438 (99.267)\n",
      "Epoch: [192][90/97], lr: 0.00000\tTime 0.323 (0.333)\tData 0.000 (0.020)\tLoss 1.9451 (2.4218)\tPrec@1 87.500 (88.642)\tPrec@5 99.219 (99.279)\n",
      "Epoch: [192][96/97], lr: 0.00000\tTime 0.319 (0.333)\tData 0.000 (0.021)\tLoss 4.0338 (2.4745)\tPrec@1 91.525 (88.610)\tPrec@5 99.153 (99.275)\n",
      "Test: [0/100]\tTime 0.238 (0.238)\tLoss 3.3727 (3.3727)\tPrec@1 86.000 (86.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 3.2816 (5.7337)\tPrec@1 84.000 (81.091)\tPrec@5 98.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.3109 (5.7030)\tPrec@1 82.000 (81.333)\tPrec@5 99.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.4475 (5.7217)\tPrec@1 82.000 (81.161)\tPrec@5 100.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.6108 (5.8029)\tPrec@1 80.000 (81.000)\tPrec@5 97.000 (98.366)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.1044 (5.8026)\tPrec@1 81.000 (81.157)\tPrec@5 99.000 (98.353)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7284 (5.7425)\tPrec@1 82.000 (81.180)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.2170 (5.7248)\tPrec@1 80.000 (81.465)\tPrec@5 98.000 (98.366)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.8642 (5.6891)\tPrec@1 81.000 (81.432)\tPrec@5 97.000 (98.432)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.2227 (5.7337)\tPrec@1 85.000 (81.121)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 80.920 Prec@5 98.490 Loss 5.76064\n",
      "val Class Accuracy: [0.911,0.972,0.778,0.716,0.789,0.759,0.854,0.763,0.778,0.772]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [193][0/97], lr: 0.00000\tTime 0.511 (0.511)\tData 0.300 (0.300)\tLoss 1.5479 (1.5479)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [193][10/97], lr: 0.00000\tTime 0.325 (0.347)\tData 0.000 (0.042)\tLoss 2.3514 (2.0160)\tPrec@1 89.062 (89.844)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [193][20/97], lr: 0.00000\tTime 0.325 (0.340)\tData 0.000 (0.030)\tLoss 2.6934 (2.3407)\tPrec@1 87.500 (89.174)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [193][30/97], lr: 0.00000\tTime 0.327 (0.337)\tData 0.000 (0.026)\tLoss 2.9625 (2.3487)\tPrec@1 88.281 (88.861)\tPrec@5 99.219 (99.320)\n",
      "Epoch: [193][40/97], lr: 0.00000\tTime 0.324 (0.335)\tData 0.000 (0.024)\tLoss 4.3124 (2.5412)\tPrec@1 90.625 (88.758)\tPrec@5 100.000 (99.295)\n",
      "Epoch: [193][50/97], lr: 0.00000\tTime 0.328 (0.335)\tData 0.000 (0.023)\tLoss 3.2961 (2.5999)\tPrec@1 89.062 (88.695)\tPrec@5 99.219 (99.295)\n",
      "Epoch: [193][60/97], lr: 0.00000\tTime 0.328 (0.334)\tData 0.000 (0.022)\tLoss 4.3185 (2.6310)\tPrec@1 82.812 (88.589)\tPrec@5 97.656 (99.296)\n",
      "Epoch: [193][70/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 2.9755 (2.6491)\tPrec@1 86.719 (88.633)\tPrec@5 100.000 (99.263)\n",
      "Epoch: [193][80/97], lr: 0.00000\tTime 0.329 (0.333)\tData 0.000 (0.021)\tLoss 3.7207 (2.6633)\tPrec@1 88.281 (88.378)\tPrec@5 100.000 (99.286)\n",
      "Epoch: [193][90/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 2.9415 (2.6717)\tPrec@1 86.719 (88.359)\tPrec@5 97.656 (99.279)\n",
      "Epoch: [193][96/97], lr: 0.00000\tTime 0.318 (0.333)\tData 0.000 (0.021)\tLoss 1.1597 (2.6657)\tPrec@1 87.288 (88.336)\tPrec@5 100.000 (99.283)\n",
      "Test: [0/100]\tTime 0.257 (0.257)\tLoss 3.4963 (3.4963)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 3.2669 (5.8155)\tPrec@1 84.000 (80.545)\tPrec@5 98.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.3368 (5.7689)\tPrec@1 81.000 (81.048)\tPrec@5 99.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.4556 (5.7877)\tPrec@1 83.000 (81.000)\tPrec@5 100.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.6339 (5.8683)\tPrec@1 80.000 (80.927)\tPrec@5 97.000 (98.390)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.0415 (5.8569)\tPrec@1 82.000 (81.059)\tPrec@5 99.000 (98.412)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.6703 (5.8039)\tPrec@1 82.000 (81.115)\tPrec@5 100.000 (98.410)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.3589 (5.7953)\tPrec@1 81.000 (81.437)\tPrec@5 98.000 (98.451)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.1228 (5.7654)\tPrec@1 82.000 (81.309)\tPrec@5 97.000 (98.519)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.2874 (5.8110)\tPrec@1 86.000 (81.033)\tPrec@5 100.000 (98.516)\n",
      "val Results: Prec@1 80.860 Prec@5 98.520 Loss 5.84106\n",
      "val Class Accuracy: [0.918,0.968,0.786,0.712,0.801,0.744,0.852,0.768,0.770,0.767]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [194][0/97], lr: 0.00000\tTime 0.461 (0.461)\tData 0.247 (0.247)\tLoss 2.8474 (2.8474)\tPrec@1 88.281 (88.281)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [194][10/97], lr: 0.00000\tTime 0.330 (0.348)\tData 0.000 (0.038)\tLoss 2.5395 (2.5704)\tPrec@1 89.844 (88.352)\tPrec@5 98.438 (98.722)\n",
      "Epoch: [194][20/97], lr: 0.00000\tTime 0.328 (0.338)\tData 0.000 (0.028)\tLoss 3.5085 (2.4473)\tPrec@1 89.844 (88.690)\tPrec@5 99.219 (99.033)\n",
      "Epoch: [194][30/97], lr: 0.00000\tTime 0.327 (0.336)\tData 0.000 (0.024)\tLoss 4.3833 (2.5530)\tPrec@1 86.719 (88.810)\tPrec@5 100.000 (99.244)\n",
      "Epoch: [194][40/97], lr: 0.00000\tTime 0.326 (0.334)\tData 0.000 (0.023)\tLoss 2.6879 (2.5204)\tPrec@1 91.406 (89.139)\tPrec@5 100.000 (99.257)\n",
      "Epoch: [194][50/97], lr: 0.00000\tTime 0.325 (0.334)\tData 0.000 (0.022)\tLoss 1.2563 (2.5047)\tPrec@1 88.281 (89.154)\tPrec@5 99.219 (99.326)\n",
      "Epoch: [194][60/97], lr: 0.00000\tTime 0.326 (0.333)\tData 0.000 (0.021)\tLoss 1.6538 (2.4975)\tPrec@1 82.812 (88.717)\tPrec@5 98.438 (99.308)\n",
      "Epoch: [194][70/97], lr: 0.00000\tTime 0.327 (0.333)\tData 0.000 (0.020)\tLoss 2.0673 (2.4713)\tPrec@1 86.719 (88.798)\tPrec@5 100.000 (99.329)\n",
      "Epoch: [194][80/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 5.6094 (2.5252)\tPrec@1 83.594 (88.532)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [194][90/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 1.6801 (2.4791)\tPrec@1 85.156 (88.496)\tPrec@5 98.438 (99.296)\n",
      "Epoch: [194][96/97], lr: 0.00000\tTime 0.323 (0.332)\tData 0.000 (0.020)\tLoss 2.4538 (2.4698)\tPrec@1 83.898 (88.449)\tPrec@5 99.153 (99.291)\n",
      "Test: [0/100]\tTime 0.257 (0.257)\tLoss 3.4113 (3.4113)\tPrec@1 87.000 (87.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 3.1864 (5.7297)\tPrec@1 84.000 (81.182)\tPrec@5 98.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.3277 (5.6910)\tPrec@1 81.000 (81.190)\tPrec@5 100.000 (98.524)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.4702 (5.7045)\tPrec@1 82.000 (81.129)\tPrec@5 100.000 (98.452)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.5802 (5.7835)\tPrec@1 80.000 (80.976)\tPrec@5 97.000 (98.390)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.9021 (5.7808)\tPrec@1 80.000 (81.078)\tPrec@5 99.000 (98.412)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7274 (5.7238)\tPrec@1 82.000 (81.049)\tPrec@5 100.000 (98.393)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.3034 (5.7082)\tPrec@1 81.000 (81.408)\tPrec@5 98.000 (98.423)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.8245 (5.6761)\tPrec@1 82.000 (81.309)\tPrec@5 97.000 (98.469)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.2293 (5.7249)\tPrec@1 86.000 (81.011)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 80.860 Prec@5 98.470 Loss 5.75291\n",
      "val Class Accuracy: [0.912,0.970,0.782,0.700,0.785,0.756,0.870,0.773,0.764,0.774]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [195][0/97], lr: 0.00000\tTime 0.458 (0.458)\tData 0.236 (0.236)\tLoss 2.6657 (2.6657)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [195][10/97], lr: 0.00000\tTime 0.328 (0.347)\tData 0.000 (0.037)\tLoss 3.0582 (2.3173)\tPrec@1 78.906 (88.281)\tPrec@5 99.219 (99.574)\n",
      "Epoch: [195][20/97], lr: 0.00000\tTime 0.323 (0.337)\tData 0.000 (0.027)\tLoss 2.8070 (2.3901)\tPrec@1 84.375 (88.504)\tPrec@5 100.000 (99.554)\n",
      "Epoch: [195][30/97], lr: 0.00000\tTime 0.325 (0.336)\tData 0.000 (0.024)\tLoss 1.9236 (2.2250)\tPrec@1 88.281 (89.315)\tPrec@5 99.219 (99.496)\n",
      "Epoch: [195][40/97], lr: 0.00000\tTime 0.328 (0.334)\tData 0.000 (0.022)\tLoss 1.1430 (2.2863)\tPrec@1 92.188 (89.253)\tPrec@5 100.000 (99.352)\n",
      "Epoch: [195][50/97], lr: 0.00000\tTime 0.324 (0.333)\tData 0.000 (0.021)\tLoss 3.4526 (2.3562)\tPrec@1 86.719 (88.971)\tPrec@5 99.219 (99.265)\n",
      "Epoch: [195][60/97], lr: 0.00000\tTime 0.324 (0.333)\tData 0.000 (0.021)\tLoss 2.9315 (2.3681)\tPrec@1 90.625 (89.101)\tPrec@5 100.000 (99.321)\n",
      "Epoch: [195][70/97], lr: 0.00000\tTime 0.345 (0.333)\tData 0.000 (0.020)\tLoss 3.2327 (2.3998)\tPrec@1 85.938 (88.974)\tPrec@5 96.094 (99.263)\n",
      "Epoch: [195][80/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 2.6654 (2.3491)\tPrec@1 87.500 (89.005)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [195][90/97], lr: 0.00000\tTime 0.323 (0.332)\tData 0.000 (0.020)\tLoss 3.5891 (2.3426)\tPrec@1 85.156 (89.037)\tPrec@5 97.656 (99.296)\n",
      "Epoch: [195][96/97], lr: 0.00000\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 2.0198 (2.4012)\tPrec@1 85.593 (88.884)\tPrec@5 100.000 (99.291)\n",
      "Test: [0/100]\tTime 0.246 (0.246)\tLoss 3.4606 (3.4606)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 3.2680 (5.8886)\tPrec@1 84.000 (80.727)\tPrec@5 98.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.3180 (5.8483)\tPrec@1 82.000 (80.810)\tPrec@5 100.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.4366 (5.8590)\tPrec@1 82.000 (80.871)\tPrec@5 100.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.7339 (5.9511)\tPrec@1 80.000 (80.707)\tPrec@5 97.000 (98.366)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.2123 (5.9542)\tPrec@1 81.000 (80.843)\tPrec@5 99.000 (98.353)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7671 (5.8946)\tPrec@1 82.000 (80.820)\tPrec@5 100.000 (98.344)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.3855 (5.8818)\tPrec@1 80.000 (81.099)\tPrec@5 98.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.0209 (5.8463)\tPrec@1 82.000 (81.086)\tPrec@5 97.000 (98.444)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.4034 (5.8949)\tPrec@1 85.000 (80.846)\tPrec@5 100.000 (98.473)\n",
      "val Results: Prec@1 80.680 Prec@5 98.490 Loss 5.92358\n",
      "val Class Accuracy: [0.912,0.974,0.783,0.707,0.783,0.762,0.854,0.766,0.772,0.755]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [196][0/97], lr: 0.00000\tTime 0.519 (0.519)\tData 0.272 (0.272)\tLoss 3.1230 (3.1230)\tPrec@1 85.156 (85.156)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [196][10/97], lr: 0.00000\tTime 0.325 (0.352)\tData 0.000 (0.039)\tLoss 2.0814 (2.1374)\tPrec@1 90.625 (89.418)\tPrec@5 98.438 (99.077)\n",
      "Epoch: [196][20/97], lr: 0.00000\tTime 0.323 (0.341)\tData 0.000 (0.029)\tLoss 3.3607 (2.2870)\tPrec@1 87.500 (88.579)\tPrec@5 99.219 (99.144)\n",
      "Epoch: [196][30/97], lr: 0.00000\tTime 0.334 (0.339)\tData 0.000 (0.026)\tLoss 4.3290 (2.4714)\tPrec@1 85.156 (88.458)\tPrec@5 100.000 (99.168)\n",
      "Epoch: [196][40/97], lr: 0.00000\tTime 0.325 (0.336)\tData 0.000 (0.024)\tLoss 1.1419 (2.3723)\tPrec@1 90.625 (88.929)\tPrec@5 100.000 (99.333)\n",
      "Epoch: [196][50/97], lr: 0.00000\tTime 0.331 (0.336)\tData 0.000 (0.022)\tLoss 4.2142 (2.4694)\tPrec@1 82.812 (88.771)\tPrec@5 100.000 (99.341)\n",
      "Epoch: [196][60/97], lr: 0.00000\tTime 0.323 (0.335)\tData 0.000 (0.022)\tLoss 2.3139 (2.4661)\tPrec@1 87.500 (88.589)\tPrec@5 100.000 (99.334)\n",
      "Epoch: [196][70/97], lr: 0.00000\tTime 0.325 (0.335)\tData 0.000 (0.021)\tLoss 2.5262 (2.4839)\tPrec@1 85.156 (88.446)\tPrec@5 98.438 (99.307)\n",
      "Epoch: [196][80/97], lr: 0.00000\tTime 0.326 (0.334)\tData 0.000 (0.021)\tLoss 2.2281 (2.5444)\tPrec@1 90.625 (88.426)\tPrec@5 99.219 (99.238)\n",
      "Epoch: [196][90/97], lr: 0.00000\tTime 0.328 (0.334)\tData 0.000 (0.020)\tLoss 3.1354 (2.5074)\tPrec@1 90.625 (88.565)\tPrec@5 99.219 (99.236)\n",
      "Epoch: [196][96/97], lr: 0.00000\tTime 0.323 (0.333)\tData 0.000 (0.021)\tLoss 2.3206 (2.4908)\tPrec@1 89.831 (88.618)\tPrec@5 100.000 (99.250)\n",
      "Test: [0/100]\tTime 0.247 (0.247)\tLoss 3.4973 (3.4973)\tPrec@1 87.000 (87.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 3.3763 (5.9340)\tPrec@1 84.000 (80.455)\tPrec@5 98.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.3932 (5.8912)\tPrec@1 82.000 (80.714)\tPrec@5 100.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.5093 (5.8978)\tPrec@1 81.000 (80.710)\tPrec@5 100.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.7551 (5.9824)\tPrec@1 81.000 (80.610)\tPrec@5 97.000 (98.293)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.1778 (5.9801)\tPrec@1 81.000 (80.765)\tPrec@5 99.000 (98.314)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7621 (5.9210)\tPrec@1 84.000 (80.787)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.4208 (5.9113)\tPrec@1 81.000 (81.070)\tPrec@5 98.000 (98.338)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.1077 (5.8804)\tPrec@1 83.000 (81.012)\tPrec@5 97.000 (98.420)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.5421 (5.9329)\tPrec@1 85.000 (80.725)\tPrec@5 100.000 (98.440)\n",
      "val Results: Prec@1 80.590 Prec@5 98.450 Loss 5.96244\n",
      "val Class Accuracy: [0.916,0.974,0.798,0.707,0.775,0.735,0.863,0.773,0.762,0.756]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [197][0/97], lr: 0.00000\tTime 0.393 (0.393)\tData 0.199 (0.199)\tLoss 4.0018 (4.0018)\tPrec@1 92.969 (92.969)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [197][10/97], lr: 0.00000\tTime 0.327 (0.341)\tData 0.000 (0.032)\tLoss 3.1987 (2.7309)\tPrec@1 86.719 (88.778)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [197][20/97], lr: 0.00000\tTime 0.331 (0.336)\tData 0.000 (0.025)\tLoss 2.7251 (2.5264)\tPrec@1 88.281 (88.393)\tPrec@5 100.000 (99.256)\n",
      "Epoch: [197][30/97], lr: 0.00000\tTime 0.324 (0.334)\tData 0.000 (0.022)\tLoss 2.8696 (2.4788)\tPrec@1 86.719 (88.810)\tPrec@5 100.000 (99.294)\n",
      "Epoch: [197][40/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 4.8394 (2.5225)\tPrec@1 87.500 (88.910)\tPrec@5 99.219 (99.314)\n",
      "Epoch: [197][50/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 4.1305 (2.4985)\tPrec@1 85.156 (88.634)\tPrec@5 99.219 (99.265)\n",
      "Epoch: [197][60/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 3.6741 (2.4987)\tPrec@1 88.281 (88.704)\tPrec@5 99.219 (99.283)\n",
      "Epoch: [197][70/97], lr: 0.00000\tTime 0.327 (0.331)\tData 0.000 (0.020)\tLoss 3.9881 (2.5284)\tPrec@1 85.938 (88.710)\tPrec@5 99.219 (99.307)\n",
      "Epoch: [197][80/97], lr: 0.00000\tTime 0.325 (0.331)\tData 0.000 (0.019)\tLoss 4.6498 (2.5065)\tPrec@1 88.281 (88.542)\tPrec@5 100.000 (99.267)\n",
      "Epoch: [197][90/97], lr: 0.00000\tTime 0.326 (0.331)\tData 0.000 (0.019)\tLoss 2.4620 (2.5844)\tPrec@1 88.281 (88.444)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [197][96/97], lr: 0.00000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 4.9528 (2.6094)\tPrec@1 88.136 (88.433)\tPrec@5 99.153 (99.307)\n",
      "Test: [0/100]\tTime 0.293 (0.293)\tLoss 3.4926 (3.4926)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 3.1729 (5.7653)\tPrec@1 84.000 (80.909)\tPrec@5 98.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 5.3664 (5.7278)\tPrec@1 81.000 (81.143)\tPrec@5 99.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 4.5197 (5.7339)\tPrec@1 81.000 (81.097)\tPrec@5 100.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 3.6836 (5.8154)\tPrec@1 80.000 (81.000)\tPrec@5 97.000 (98.341)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.9488 (5.8092)\tPrec@1 81.000 (81.137)\tPrec@5 99.000 (98.392)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 6.7588 (5.7512)\tPrec@1 83.000 (81.131)\tPrec@5 100.000 (98.377)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.3549 (5.7350)\tPrec@1 82.000 (81.465)\tPrec@5 98.000 (98.408)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 5.8846 (5.7015)\tPrec@1 82.000 (81.370)\tPrec@5 97.000 (98.469)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.3318 (5.7522)\tPrec@1 86.000 (81.088)\tPrec@5 100.000 (98.473)\n",
      "val Results: Prec@1 80.920 Prec@5 98.480 Loss 5.77967\n",
      "val Class Accuracy: [0.915,0.972,0.786,0.701,0.791,0.756,0.861,0.765,0.764,0.781]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [198][0/97], lr: 0.00000\tTime 0.450 (0.450)\tData 0.229 (0.229)\tLoss 2.2607 (2.2607)\tPrec@1 88.281 (88.281)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [198][10/97], lr: 0.00000\tTime 0.329 (0.343)\tData 0.000 (0.036)\tLoss 3.9419 (2.2837)\tPrec@1 82.031 (87.429)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [198][20/97], lr: 0.00000\tTime 0.324 (0.335)\tData 0.000 (0.027)\tLoss 2.7314 (2.2184)\tPrec@1 89.062 (88.542)\tPrec@5 99.219 (99.107)\n",
      "Epoch: [198][30/97], lr: 0.00000\tTime 0.326 (0.334)\tData 0.000 (0.024)\tLoss 3.2350 (2.3432)\tPrec@1 93.750 (88.836)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [198][40/97], lr: 0.00000\tTime 0.333 (0.333)\tData 0.000 (0.022)\tLoss 1.7936 (2.4350)\tPrec@1 89.062 (88.567)\tPrec@5 99.219 (99.200)\n",
      "Epoch: [198][50/97], lr: 0.00000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 1.4271 (2.4648)\tPrec@1 89.062 (88.496)\tPrec@5 98.438 (99.234)\n",
      "Epoch: [198][60/97], lr: 0.00000\tTime 0.327 (0.333)\tData 0.000 (0.021)\tLoss 2.2561 (2.4940)\tPrec@1 90.625 (88.461)\tPrec@5 100.000 (99.168)\n",
      "Epoch: [198][70/97], lr: 0.00000\tTime 0.332 (0.332)\tData 0.000 (0.020)\tLoss 3.1085 (2.5074)\tPrec@1 89.844 (88.622)\tPrec@5 100.000 (99.186)\n",
      "Epoch: [198][80/97], lr: 0.00000\tTime 0.323 (0.332)\tData 0.000 (0.020)\tLoss 5.0150 (2.5359)\tPrec@1 89.844 (88.677)\tPrec@5 98.438 (99.142)\n",
      "Epoch: [198][90/97], lr: 0.00000\tTime 0.323 (0.332)\tData 0.000 (0.020)\tLoss 1.9196 (2.5321)\tPrec@1 89.844 (88.711)\tPrec@5 99.219 (99.184)\n",
      "Epoch: [198][96/97], lr: 0.00000\tTime 0.315 (0.331)\tData 0.000 (0.020)\tLoss 2.0527 (2.5823)\tPrec@1 95.763 (88.755)\tPrec@5 100.000 (99.210)\n",
      "Test: [0/100]\tTime 0.229 (0.229)\tLoss 3.4422 (3.4422)\tPrec@1 86.000 (86.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 3.1307 (5.7173)\tPrec@1 84.000 (80.727)\tPrec@5 98.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.3847 (5.6802)\tPrec@1 81.000 (81.143)\tPrec@5 99.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.4686 (5.7056)\tPrec@1 82.000 (81.000)\tPrec@5 100.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.5407 (5.7799)\tPrec@1 81.000 (80.854)\tPrec@5 97.000 (98.341)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 5.8111 (5.7626)\tPrec@1 80.000 (80.961)\tPrec@5 99.000 (98.373)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.6827 (5.7137)\tPrec@1 82.000 (81.016)\tPrec@5 100.000 (98.361)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.3594 (5.7018)\tPrec@1 81.000 (81.282)\tPrec@5 98.000 (98.394)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.0401 (5.6736)\tPrec@1 82.000 (81.198)\tPrec@5 97.000 (98.469)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.2383 (5.7211)\tPrec@1 86.000 (80.890)\tPrec@5 100.000 (98.505)\n",
      "val Results: Prec@1 80.760 Prec@5 98.500 Loss 5.74748\n",
      "val Class Accuracy: [0.912,0.967,0.771,0.711,0.794,0.738,0.872,0.766,0.768,0.777]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [199][0/97], lr: 0.00000\tTime 0.431 (0.431)\tData 0.225 (0.225)\tLoss 3.8230 (3.8230)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [199][10/97], lr: 0.00000\tTime 0.326 (0.343)\tData 0.000 (0.036)\tLoss 2.9320 (2.3536)\tPrec@1 89.062 (89.560)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [199][20/97], lr: 0.00000\tTime 0.326 (0.336)\tData 0.000 (0.027)\tLoss 2.9048 (2.5674)\tPrec@1 89.062 (88.504)\tPrec@5 99.219 (99.479)\n",
      "Epoch: [199][30/97], lr: 0.00000\tTime 0.325 (0.334)\tData 0.000 (0.024)\tLoss 2.1520 (2.5437)\tPrec@1 85.938 (88.760)\tPrec@5 99.219 (99.496)\n",
      "Epoch: [199][40/97], lr: 0.00000\tTime 0.327 (0.333)\tData 0.000 (0.022)\tLoss 2.5468 (2.6887)\tPrec@1 90.625 (88.815)\tPrec@5 100.000 (99.524)\n",
      "Epoch: [199][50/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 3.2175 (2.5956)\tPrec@1 91.406 (88.971)\tPrec@5 100.000 (99.525)\n",
      "Epoch: [199][60/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 1.2798 (2.5343)\tPrec@1 90.625 (89.050)\tPrec@5 100.000 (99.462)\n",
      "Epoch: [199][70/97], lr: 0.00000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 3.9767 (2.6092)\tPrec@1 89.062 (88.963)\tPrec@5 96.094 (99.406)\n",
      "Epoch: [199][80/97], lr: 0.00000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 1.7554 (2.5769)\tPrec@1 89.062 (88.937)\tPrec@5 100.000 (99.392)\n",
      "Epoch: [199][90/97], lr: 0.00000\tTime 0.329 (0.331)\tData 0.000 (0.019)\tLoss 1.6454 (2.5368)\tPrec@1 89.844 (89.028)\tPrec@5 100.000 (99.373)\n",
      "Epoch: [199][96/97], lr: 0.00000\tTime 0.317 (0.331)\tData 0.000 (0.020)\tLoss 2.0528 (2.4785)\tPrec@1 80.508 (89.054)\tPrec@5 95.763 (99.363)\n",
      "Test: [0/100]\tTime 0.224 (0.224)\tLoss 3.4136 (3.4136)\tPrec@1 86.000 (86.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 3.2691 (5.7143)\tPrec@1 84.000 (81.364)\tPrec@5 98.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 5.3292 (5.6798)\tPrec@1 81.000 (81.476)\tPrec@5 99.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.4195 (5.7044)\tPrec@1 81.000 (81.194)\tPrec@5 100.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.5359 (5.7835)\tPrec@1 80.000 (81.024)\tPrec@5 97.000 (98.390)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.0083 (5.7736)\tPrec@1 81.000 (81.137)\tPrec@5 99.000 (98.373)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 6.6308 (5.7198)\tPrec@1 84.000 (81.197)\tPrec@5 100.000 (98.361)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.2648 (5.7064)\tPrec@1 81.000 (81.535)\tPrec@5 98.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.9709 (5.6759)\tPrec@1 82.000 (81.432)\tPrec@5 97.000 (98.457)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.2302 (5.7170)\tPrec@1 86.000 (81.143)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 80.970 Prec@5 98.470 Loss 5.74478\n",
      "val Class Accuracy: [0.912,0.970,0.785,0.711,0.804,0.751,0.848,0.768,0.775,0.773]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [200][0/97], lr: 0.00000\tTime 0.482 (0.482)\tData 0.275 (0.275)\tLoss 6.1675 (6.1675)\tPrec@1 85.938 (85.938)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [200][10/97], lr: 0.00000\tTime 0.351 (0.350)\tData 0.000 (0.040)\tLoss 1.5302 (2.6363)\tPrec@1 91.406 (89.986)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [200][20/97], lr: 0.00000\tTime 0.335 (0.340)\tData 0.000 (0.029)\tLoss 2.2936 (2.6032)\tPrec@1 90.625 (89.249)\tPrec@5 100.000 (99.591)\n",
      "Epoch: [200][30/97], lr: 0.00000\tTime 0.325 (0.337)\tData 0.000 (0.025)\tLoss 2.6094 (2.4575)\tPrec@1 92.188 (89.315)\tPrec@5 99.219 (99.496)\n",
      "Epoch: [200][40/97], lr: 0.00000\tTime 0.324 (0.335)\tData 0.000 (0.023)\tLoss 3.3231 (2.4774)\tPrec@1 91.406 (89.005)\tPrec@5 97.656 (99.371)\n",
      "Epoch: [200][50/97], lr: 0.00000\tTime 0.328 (0.334)\tData 0.000 (0.022)\tLoss 3.6603 (2.5548)\tPrec@1 85.938 (88.680)\tPrec@5 100.000 (99.418)\n",
      "Epoch: [200][60/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 1.3196 (2.5779)\tPrec@1 87.500 (88.550)\tPrec@5 99.219 (99.436)\n",
      "Epoch: [200][70/97], lr: 0.00000\tTime 0.324 (0.333)\tData 0.000 (0.021)\tLoss 2.3393 (2.5979)\tPrec@1 89.062 (88.666)\tPrec@5 100.000 (99.461)\n",
      "Epoch: [200][80/97], lr: 0.00000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 3.6762 (2.6285)\tPrec@1 86.719 (88.686)\tPrec@5 100.000 (99.460)\n",
      "Epoch: [200][90/97], lr: 0.00000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.8739 (2.6127)\tPrec@1 87.500 (88.642)\tPrec@5 99.219 (99.468)\n",
      "Epoch: [200][96/97], lr: 0.00000\tTime 0.316 (0.332)\tData 0.000 (0.021)\tLoss 2.2219 (2.5844)\tPrec@1 88.983 (88.699)\tPrec@5 100.000 (99.468)\n",
      "Test: [0/100]\tTime 0.276 (0.276)\tLoss 3.6389 (3.6389)\tPrec@1 88.000 (88.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 3.5410 (6.0923)\tPrec@1 84.000 (80.636)\tPrec@5 98.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 5.4750 (6.0119)\tPrec@1 82.000 (80.905)\tPrec@5 99.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.5986 (6.0137)\tPrec@1 81.000 (80.839)\tPrec@5 100.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 3.8283 (6.1000)\tPrec@1 80.000 (80.610)\tPrec@5 97.000 (98.366)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.1474 (6.0981)\tPrec@1 81.000 (80.784)\tPrec@5 99.000 (98.392)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.8393 (6.0335)\tPrec@1 82.000 (80.754)\tPrec@5 100.000 (98.410)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.6168 (6.0247)\tPrec@1 82.000 (81.127)\tPrec@5 98.000 (98.423)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.2993 (5.9972)\tPrec@1 82.000 (81.012)\tPrec@5 97.000 (98.481)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.7530 (6.0481)\tPrec@1 85.000 (80.747)\tPrec@5 100.000 (98.484)\n",
      "val Results: Prec@1 80.570 Prec@5 98.490 Loss 6.07540\n",
      "val Class Accuracy: [0.917,0.975,0.782,0.713,0.787,0.740,0.857,0.781,0.754,0.751]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [201][0/97], lr: 0.00000\tTime 0.433 (0.433)\tData 0.237 (0.237)\tLoss 2.0588 (2.0588)\tPrec@1 85.938 (85.938)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [201][10/97], lr: 0.00000\tTime 0.327 (0.343)\tData 0.000 (0.037)\tLoss 1.9812 (2.1622)\tPrec@1 90.625 (88.849)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [201][20/97], lr: 0.00000\tTime 0.323 (0.335)\tData 0.000 (0.027)\tLoss 3.4473 (2.2386)\tPrec@1 87.500 (88.951)\tPrec@5 97.656 (99.144)\n",
      "Epoch: [201][30/97], lr: 0.00000\tTime 0.325 (0.335)\tData 0.000 (0.024)\tLoss 0.9692 (2.3434)\tPrec@1 90.625 (89.012)\tPrec@5 99.219 (99.194)\n",
      "Epoch: [201][40/97], lr: 0.00000\tTime 0.335 (0.334)\tData 0.000 (0.022)\tLoss 2.7061 (2.4079)\tPrec@1 92.188 (89.024)\tPrec@5 99.219 (99.181)\n",
      "Epoch: [201][50/97], lr: 0.00000\tTime 0.324 (0.333)\tData 0.000 (0.021)\tLoss 0.5793 (2.4672)\tPrec@1 91.406 (88.955)\tPrec@5 100.000 (99.234)\n",
      "Epoch: [201][60/97], lr: 0.00000\tTime 0.332 (0.332)\tData 0.000 (0.021)\tLoss 2.8230 (2.5227)\tPrec@1 89.844 (88.806)\tPrec@5 98.438 (99.257)\n",
      "Epoch: [201][70/97], lr: 0.00000\tTime 0.330 (0.332)\tData 0.000 (0.020)\tLoss 2.3548 (2.4936)\tPrec@1 86.719 (88.732)\tPrec@5 100.000 (99.307)\n",
      "Epoch: [201][80/97], lr: 0.00000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 2.9770 (2.4895)\tPrec@1 85.156 (88.725)\tPrec@5 100.000 (99.306)\n",
      "Epoch: [201][90/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 3.2564 (2.4819)\tPrec@1 85.156 (88.685)\tPrec@5 100.000 (99.339)\n",
      "Epoch: [201][96/97], lr: 0.00000\tTime 0.317 (0.332)\tData 0.000 (0.020)\tLoss 3.7194 (2.5225)\tPrec@1 88.136 (88.562)\tPrec@5 100.000 (99.339)\n",
      "Test: [0/100]\tTime 0.257 (0.257)\tLoss 3.4748 (3.4748)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 3.2645 (5.7758)\tPrec@1 84.000 (81.273)\tPrec@5 98.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.3625 (5.7303)\tPrec@1 81.000 (81.286)\tPrec@5 99.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.4489 (5.7476)\tPrec@1 80.000 (81.032)\tPrec@5 100.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.6124 (5.8287)\tPrec@1 80.000 (80.976)\tPrec@5 97.000 (98.341)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.9982 (5.8220)\tPrec@1 82.000 (81.078)\tPrec@5 99.000 (98.333)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.6708 (5.7665)\tPrec@1 84.000 (81.148)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.3529 (5.7551)\tPrec@1 81.000 (81.437)\tPrec@5 98.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.0143 (5.7247)\tPrec@1 82.000 (81.333)\tPrec@5 97.000 (98.444)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.3075 (5.7714)\tPrec@1 86.000 (81.077)\tPrec@5 100.000 (98.451)\n",
      "val Results: Prec@1 80.910 Prec@5 98.460 Loss 5.79705\n",
      "val Class Accuracy: [0.914,0.971,0.780,0.704,0.805,0.757,0.853,0.765,0.771,0.771]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [202][0/97], lr: 0.00000\tTime 0.444 (0.444)\tData 0.238 (0.238)\tLoss 3.8026 (3.8026)\tPrec@1 87.500 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [202][10/97], lr: 0.00000\tTime 0.334 (0.345)\tData 0.000 (0.035)\tLoss 1.8523 (2.5409)\tPrec@1 91.406 (88.849)\tPrec@5 100.000 (99.006)\n",
      "Epoch: [202][20/97], lr: 0.00000\tTime 0.330 (0.337)\tData 0.000 (0.026)\tLoss 6.0160 (2.4240)\tPrec@1 86.719 (89.583)\tPrec@5 99.219 (99.182)\n",
      "Epoch: [202][30/97], lr: 0.00000\tTime 0.325 (0.335)\tData 0.000 (0.023)\tLoss 1.7695 (2.4671)\tPrec@1 86.719 (88.810)\tPrec@5 98.438 (99.017)\n",
      "Epoch: [202][40/97], lr: 0.00000\tTime 0.326 (0.334)\tData 0.000 (0.022)\tLoss 2.8649 (2.4895)\tPrec@1 87.500 (88.872)\tPrec@5 99.219 (99.123)\n",
      "Epoch: [202][50/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 1.9235 (2.5820)\tPrec@1 85.938 (88.603)\tPrec@5 98.438 (99.173)\n",
      "Epoch: [202][60/97], lr: 0.00000\tTime 0.328 (0.332)\tData 0.000 (0.020)\tLoss 1.8231 (2.5568)\tPrec@1 90.625 (88.819)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [202][70/97], lr: 0.00000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 0.9779 (2.5757)\tPrec@1 91.406 (88.754)\tPrec@5 99.219 (99.230)\n",
      "Epoch: [202][80/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 1.9078 (2.5851)\tPrec@1 91.406 (88.821)\tPrec@5 99.219 (99.209)\n",
      "Epoch: [202][90/97], lr: 0.00000\tTime 0.325 (0.331)\tData 0.000 (0.019)\tLoss 0.9840 (2.5214)\tPrec@1 94.531 (88.899)\tPrec@5 100.000 (99.262)\n",
      "Epoch: [202][96/97], lr: 0.00000\tTime 0.319 (0.331)\tData 0.000 (0.020)\tLoss 3.3380 (2.5346)\tPrec@1 83.051 (88.715)\tPrec@5 99.153 (99.266)\n",
      "Test: [0/100]\tTime 0.264 (0.264)\tLoss 3.5491 (3.5491)\tPrec@1 87.000 (87.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 3.3491 (5.9677)\tPrec@1 84.000 (80.818)\tPrec@5 98.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.3950 (5.8956)\tPrec@1 81.000 (80.952)\tPrec@5 99.000 (98.429)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.5619 (5.9012)\tPrec@1 80.000 (80.839)\tPrec@5 100.000 (98.419)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 3.7512 (5.9849)\tPrec@1 79.000 (80.610)\tPrec@5 97.000 (98.366)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.0651 (5.9890)\tPrec@1 80.000 (80.765)\tPrec@5 99.000 (98.392)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.8401 (5.9276)\tPrec@1 84.000 (80.803)\tPrec@5 100.000 (98.377)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.4756 (5.9135)\tPrec@1 82.000 (81.155)\tPrec@5 98.000 (98.408)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.0664 (5.8823)\tPrec@1 82.000 (81.074)\tPrec@5 97.000 (98.481)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.5806 (5.9315)\tPrec@1 85.000 (80.824)\tPrec@5 100.000 (98.505)\n",
      "val Results: Prec@1 80.660 Prec@5 98.510 Loss 5.95872\n",
      "val Class Accuracy: [0.913,0.975,0.786,0.691,0.787,0.762,0.864,0.770,0.757,0.761]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [203][0/97], lr: 0.00000\tTime 0.456 (0.456)\tData 0.233 (0.233)\tLoss 4.1836 (4.1836)\tPrec@1 89.062 (89.062)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [203][10/97], lr: 0.00000\tTime 0.326 (0.347)\tData 0.000 (0.036)\tLoss 2.1489 (2.2282)\tPrec@1 93.750 (90.270)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [203][20/97], lr: 0.00000\tTime 0.329 (0.337)\tData 0.000 (0.027)\tLoss 1.0942 (2.1614)\tPrec@1 90.625 (89.658)\tPrec@5 100.000 (99.330)\n",
      "Epoch: [203][30/97], lr: 0.00000\tTime 0.324 (0.334)\tData 0.000 (0.024)\tLoss 1.1982 (2.1395)\tPrec@1 90.625 (89.441)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [203][40/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.022)\tLoss 1.6229 (2.1630)\tPrec@1 88.281 (89.482)\tPrec@5 99.219 (99.352)\n",
      "Epoch: [203][50/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.021)\tLoss 4.1385 (2.4290)\tPrec@1 80.469 (89.001)\tPrec@5 97.656 (99.265)\n",
      "Epoch: [203][60/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.021)\tLoss 1.5517 (2.4331)\tPrec@1 89.062 (88.973)\tPrec@5 99.219 (99.360)\n",
      "Epoch: [203][70/97], lr: 0.00000\tTime 0.327 (0.332)\tData 0.000 (0.020)\tLoss 2.1852 (2.4800)\tPrec@1 87.500 (88.996)\tPrec@5 100.000 (99.384)\n",
      "Epoch: [203][80/97], lr: 0.00000\tTime 0.323 (0.331)\tData 0.000 (0.020)\tLoss 0.9849 (2.4964)\tPrec@1 92.188 (89.014)\tPrec@5 99.219 (99.383)\n",
      "Epoch: [203][90/97], lr: 0.00000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 2.9445 (2.5207)\tPrec@1 86.719 (89.045)\tPrec@5 99.219 (99.373)\n",
      "Epoch: [203][96/97], lr: 0.00000\tTime 0.319 (0.331)\tData 0.000 (0.020)\tLoss 2.2809 (2.5366)\tPrec@1 85.593 (88.884)\tPrec@5 99.153 (99.363)\n",
      "Test: [0/100]\tTime 0.258 (0.258)\tLoss 3.3700 (3.3700)\tPrec@1 88.000 (88.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 3.2848 (5.7502)\tPrec@1 84.000 (81.364)\tPrec@5 98.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.2906 (5.7186)\tPrec@1 82.000 (81.286)\tPrec@5 100.000 (98.429)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.4221 (5.7232)\tPrec@1 82.000 (81.194)\tPrec@5 100.000 (98.419)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.6150 (5.8128)\tPrec@1 80.000 (80.927)\tPrec@5 97.000 (98.415)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.0610 (5.8076)\tPrec@1 82.000 (81.118)\tPrec@5 99.000 (98.412)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7263 (5.7485)\tPrec@1 82.000 (81.066)\tPrec@5 100.000 (98.426)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.2403 (5.7336)\tPrec@1 80.000 (81.408)\tPrec@5 98.000 (98.437)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.8670 (5.6982)\tPrec@1 82.000 (81.321)\tPrec@5 97.000 (98.506)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.2787 (5.7465)\tPrec@1 85.000 (81.011)\tPrec@5 100.000 (98.505)\n",
      "val Results: Prec@1 80.860 Prec@5 98.510 Loss 5.77479\n",
      "val Class Accuracy: [0.915,0.974,0.781,0.709,0.774,0.758,0.850,0.780,0.772,0.773]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [204][0/97], lr: 0.00000\tTime 0.422 (0.422)\tData 0.232 (0.232)\tLoss 1.9163 (1.9163)\tPrec@1 87.500 (87.500)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [204][10/97], lr: 0.00000\tTime 0.334 (0.346)\tData 0.000 (0.036)\tLoss 4.1060 (2.5309)\tPrec@1 88.281 (88.565)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [204][20/97], lr: 0.00000\tTime 0.323 (0.337)\tData 0.000 (0.027)\tLoss 2.7426 (2.4412)\tPrec@1 89.062 (88.542)\tPrec@5 100.000 (99.330)\n",
      "Epoch: [204][30/97], lr: 0.00000\tTime 0.323 (0.335)\tData 0.000 (0.024)\tLoss 1.2929 (2.5130)\tPrec@1 88.281 (88.710)\tPrec@5 100.000 (99.269)\n",
      "Epoch: [204][40/97], lr: 0.00000\tTime 0.326 (0.333)\tData 0.000 (0.022)\tLoss 2.0281 (2.5791)\tPrec@1 88.281 (88.453)\tPrec@5 100.000 (99.314)\n",
      "Epoch: [204][50/97], lr: 0.00000\tTime 0.324 (0.333)\tData 0.000 (0.021)\tLoss 4.5235 (2.4815)\tPrec@1 89.844 (88.526)\tPrec@5 100.000 (99.341)\n",
      "Epoch: [204][60/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.021)\tLoss 1.2144 (2.4836)\tPrec@1 93.750 (88.525)\tPrec@5 100.000 (99.347)\n",
      "Epoch: [204][70/97], lr: 0.00000\tTime 0.332 (0.331)\tData 0.000 (0.020)\tLoss 2.8428 (2.4740)\tPrec@1 89.844 (88.655)\tPrec@5 99.219 (99.329)\n",
      "Epoch: [204][80/97], lr: 0.00000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 2.4441 (2.5481)\tPrec@1 91.406 (88.542)\tPrec@5 99.219 (99.286)\n",
      "Epoch: [204][90/97], lr: 0.00000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 1.7298 (2.5539)\tPrec@1 90.625 (88.616)\tPrec@5 100.000 (99.313)\n",
      "Epoch: [204][96/97], lr: 0.00000\tTime 0.315 (0.330)\tData 0.000 (0.020)\tLoss 1.1426 (2.5400)\tPrec@1 89.831 (88.659)\tPrec@5 100.000 (99.315)\n",
      "Test: [0/100]\tTime 0.241 (0.241)\tLoss 3.4142 (3.4142)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 3.2354 (5.8409)\tPrec@1 84.000 (80.545)\tPrec@5 98.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.3256 (5.8025)\tPrec@1 81.000 (81.048)\tPrec@5 99.000 (98.429)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.3539 (5.8294)\tPrec@1 83.000 (80.968)\tPrec@5 100.000 (98.419)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.6211 (5.9174)\tPrec@1 81.000 (80.854)\tPrec@5 97.000 (98.341)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.1807 (5.9116)\tPrec@1 80.000 (81.000)\tPrec@5 99.000 (98.333)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.6642 (5.8643)\tPrec@1 85.000 (81.115)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.2761 (5.8539)\tPrec@1 81.000 (81.352)\tPrec@5 98.000 (98.352)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.3192 (5.8258)\tPrec@1 82.000 (81.235)\tPrec@5 97.000 (98.432)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.2751 (5.8659)\tPrec@1 85.000 (80.912)\tPrec@5 100.000 (98.473)\n",
      "val Results: Prec@1 80.760 Prec@5 98.470 Loss 5.89599\n",
      "val Class Accuracy: [0.904,0.973,0.810,0.705,0.786,0.730,0.868,0.773,0.778,0.749]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [205][0/97], lr: 0.00000\tTime 0.490 (0.490)\tData 0.278 (0.278)\tLoss 3.7991 (3.7991)\tPrec@1 89.844 (89.844)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [205][10/97], lr: 0.00000\tTime 0.332 (0.351)\tData 0.000 (0.040)\tLoss 2.8678 (2.8169)\tPrec@1 91.406 (87.926)\tPrec@5 97.656 (98.793)\n",
      "Epoch: [205][20/97], lr: 0.00000\tTime 0.326 (0.340)\tData 0.000 (0.029)\tLoss 1.0538 (2.5228)\tPrec@1 92.188 (88.988)\tPrec@5 99.219 (99.144)\n",
      "Epoch: [205][30/97], lr: 0.00000\tTime 0.328 (0.337)\tData 0.000 (0.025)\tLoss 2.8463 (2.4645)\tPrec@1 85.156 (88.760)\tPrec@5 98.438 (98.891)\n",
      "Epoch: [205][40/97], lr: 0.00000\tTime 0.326 (0.336)\tData 0.000 (0.023)\tLoss 1.6413 (2.4441)\tPrec@1 90.625 (88.872)\tPrec@5 98.438 (98.971)\n",
      "Epoch: [205][50/97], lr: 0.00000\tTime 0.325 (0.335)\tData 0.000 (0.022)\tLoss 2.5167 (2.4930)\tPrec@1 88.281 (88.542)\tPrec@5 98.438 (98.989)\n",
      "Epoch: [205][60/97], lr: 0.00000\tTime 0.317 (0.334)\tData 0.000 (0.021)\tLoss 1.9433 (2.5293)\tPrec@1 89.062 (88.371)\tPrec@5 100.000 (99.052)\n",
      "Epoch: [205][70/97], lr: 0.00000\tTime 0.324 (0.333)\tData 0.000 (0.021)\tLoss 1.6846 (2.5204)\tPrec@1 91.406 (88.358)\tPrec@5 100.000 (99.109)\n",
      "Epoch: [205][80/97], lr: 0.00000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.0250 (2.4994)\tPrec@1 92.188 (88.436)\tPrec@5 99.219 (99.142)\n",
      "Epoch: [205][90/97], lr: 0.00000\tTime 0.328 (0.332)\tData 0.000 (0.020)\tLoss 3.0818 (2.5128)\tPrec@1 89.844 (88.504)\tPrec@5 100.000 (99.193)\n",
      "Epoch: [205][96/97], lr: 0.00000\tTime 0.318 (0.332)\tData 0.000 (0.021)\tLoss 4.2493 (2.5089)\tPrec@1 86.441 (88.554)\tPrec@5 98.305 (99.210)\n",
      "Test: [0/100]\tTime 0.240 (0.240)\tLoss 3.4693 (3.4693)\tPrec@1 87.000 (87.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 3.3341 (5.8969)\tPrec@1 84.000 (80.727)\tPrec@5 98.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.3883 (5.8473)\tPrec@1 82.000 (81.000)\tPrec@5 100.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.4834 (5.8562)\tPrec@1 81.000 (80.903)\tPrec@5 100.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.7372 (5.9468)\tPrec@1 81.000 (80.780)\tPrec@5 97.000 (98.317)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.1032 (5.9476)\tPrec@1 82.000 (80.922)\tPrec@5 99.000 (98.353)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7782 (5.8867)\tPrec@1 83.000 (80.951)\tPrec@5 100.000 (98.344)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.4022 (5.8755)\tPrec@1 80.000 (81.296)\tPrec@5 98.000 (98.366)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.0652 (5.8438)\tPrec@1 82.000 (81.198)\tPrec@5 97.000 (98.444)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.4913 (5.8929)\tPrec@1 85.000 (80.890)\tPrec@5 100.000 (98.473)\n",
      "val Results: Prec@1 80.770 Prec@5 98.480 Loss 5.92020\n",
      "val Class Accuracy: [0.914,0.973,0.791,0.708,0.779,0.752,0.862,0.777,0.763,0.758]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [206][0/97], lr: 0.00000\tTime 0.428 (0.428)\tData 0.230 (0.230)\tLoss 1.8679 (1.8679)\tPrec@1 90.625 (90.625)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [206][10/97], lr: 0.00000\tTime 0.323 (0.342)\tData 0.000 (0.036)\tLoss 2.0100 (2.0528)\tPrec@1 90.625 (89.418)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [206][20/97], lr: 0.00000\tTime 0.327 (0.334)\tData 0.000 (0.027)\tLoss 3.7998 (2.3148)\tPrec@1 85.938 (88.170)\tPrec@5 100.000 (99.591)\n",
      "Epoch: [206][30/97], lr: 0.00000\tTime 0.326 (0.333)\tData 0.000 (0.024)\tLoss 2.1805 (2.2779)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (99.471)\n",
      "Epoch: [206][40/97], lr: 0.00000\tTime 0.325 (0.332)\tData 0.000 (0.022)\tLoss 3.3904 (2.2831)\tPrec@1 90.625 (88.491)\tPrec@5 100.000 (99.447)\n",
      "Epoch: [206][50/97], lr: 0.00000\tTime 0.323 (0.332)\tData 0.000 (0.021)\tLoss 1.2586 (2.3108)\tPrec@1 86.719 (88.603)\tPrec@5 97.656 (99.403)\n",
      "Epoch: [206][60/97], lr: 0.00000\tTime 0.328 (0.332)\tData 0.000 (0.021)\tLoss 2.8235 (2.4051)\tPrec@1 88.281 (88.627)\tPrec@5 100.000 (99.411)\n",
      "Epoch: [206][70/97], lr: 0.00000\tTime 0.328 (0.332)\tData 0.000 (0.020)\tLoss 3.1680 (2.4766)\tPrec@1 85.938 (88.611)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [206][80/97], lr: 0.00000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 3.4691 (2.5664)\tPrec@1 84.375 (88.493)\tPrec@5 99.219 (99.412)\n",
      "Epoch: [206][90/97], lr: 0.00000\tTime 0.323 (0.331)\tData 0.000 (0.020)\tLoss 2.8308 (2.5975)\tPrec@1 88.281 (88.453)\tPrec@5 99.219 (99.399)\n",
      "Epoch: [206][96/97], lr: 0.00000\tTime 0.318 (0.330)\tData 0.000 (0.020)\tLoss 1.8679 (2.5652)\tPrec@1 88.983 (88.586)\tPrec@5 100.000 (99.420)\n",
      "Test: [0/100]\tTime 0.258 (0.258)\tLoss 3.5000 (3.5000)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 3.3817 (5.8984)\tPrec@1 84.000 (80.727)\tPrec@5 98.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.3648 (5.8410)\tPrec@1 81.000 (80.905)\tPrec@5 99.000 (98.476)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.5127 (5.8544)\tPrec@1 81.000 (80.935)\tPrec@5 100.000 (98.419)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.7099 (5.9394)\tPrec@1 81.000 (80.805)\tPrec@5 97.000 (98.390)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.0864 (5.9369)\tPrec@1 82.000 (80.941)\tPrec@5 99.000 (98.412)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7499 (5.8828)\tPrec@1 82.000 (80.951)\tPrec@5 100.000 (98.410)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.4818 (5.8724)\tPrec@1 80.000 (81.296)\tPrec@5 98.000 (98.423)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.0959 (5.8405)\tPrec@1 81.000 (81.198)\tPrec@5 97.000 (98.481)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.4596 (5.8867)\tPrec@1 86.000 (80.945)\tPrec@5 100.000 (98.505)\n",
      "val Results: Prec@1 80.760 Prec@5 98.500 Loss 5.91335\n",
      "val Class Accuracy: [0.914,0.973,0.776,0.711,0.789,0.760,0.860,0.763,0.767,0.763]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [207][0/97], lr: 0.00000\tTime 0.445 (0.445)\tData 0.245 (0.245)\tLoss 1.7897 (1.7897)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [207][10/97], lr: 0.00000\tTime 0.329 (0.343)\tData 0.000 (0.037)\tLoss 2.7653 (2.2506)\tPrec@1 88.281 (89.276)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [207][20/97], lr: 0.00000\tTime 0.324 (0.335)\tData 0.000 (0.028)\tLoss 2.6910 (2.4204)\tPrec@1 85.156 (88.653)\tPrec@5 99.219 (99.293)\n",
      "Epoch: [207][30/97], lr: 0.00000\tTime 0.327 (0.335)\tData 0.000 (0.024)\tLoss 2.3625 (2.5011)\tPrec@1 84.375 (88.911)\tPrec@5 98.438 (99.320)\n",
      "Epoch: [207][40/97], lr: 0.00000\tTime 0.329 (0.334)\tData 0.000 (0.023)\tLoss 1.0031 (2.4450)\tPrec@1 91.406 (88.605)\tPrec@5 100.000 (99.314)\n",
      "Epoch: [207][50/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.022)\tLoss 3.5927 (2.4616)\tPrec@1 87.500 (88.634)\tPrec@5 97.656 (99.295)\n",
      "Epoch: [207][60/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.021)\tLoss 1.3490 (2.4021)\tPrec@1 90.625 (88.640)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [207][70/97], lr: 0.00000\tTime 0.328 (0.333)\tData 0.000 (0.020)\tLoss 3.7160 (2.3978)\tPrec@1 89.062 (88.743)\tPrec@5 100.000 (99.329)\n",
      "Epoch: [207][80/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 2.4544 (2.3903)\tPrec@1 83.594 (88.821)\tPrec@5 97.656 (99.277)\n",
      "Epoch: [207][90/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 2.8684 (2.4086)\tPrec@1 87.500 (88.822)\tPrec@5 99.219 (99.279)\n",
      "Epoch: [207][96/97], lr: 0.00000\tTime 0.317 (0.332)\tData 0.000 (0.020)\tLoss 2.1489 (2.4002)\tPrec@1 88.136 (88.812)\tPrec@5 99.153 (99.299)\n",
      "Test: [0/100]\tTime 0.254 (0.254)\tLoss 3.4256 (3.4256)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 3.2215 (5.7487)\tPrec@1 84.000 (80.909)\tPrec@5 98.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.3432 (5.7073)\tPrec@1 81.000 (81.143)\tPrec@5 99.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.4680 (5.7232)\tPrec@1 81.000 (81.097)\tPrec@5 100.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.6366 (5.8044)\tPrec@1 79.000 (80.854)\tPrec@5 97.000 (98.317)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.9381 (5.7943)\tPrec@1 81.000 (80.961)\tPrec@5 99.000 (98.353)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7166 (5.7426)\tPrec@1 82.000 (80.984)\tPrec@5 100.000 (98.377)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.3444 (5.7300)\tPrec@1 81.000 (81.324)\tPrec@5 98.000 (98.408)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.9884 (5.6995)\tPrec@1 82.000 (81.222)\tPrec@5 97.000 (98.481)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.2436 (5.7452)\tPrec@1 86.000 (80.956)\tPrec@5 100.000 (98.516)\n",
      "val Results: Prec@1 80.840 Prec@5 98.510 Loss 5.77057\n",
      "val Class Accuracy: [0.913,0.971,0.785,0.703,0.786,0.755,0.861,0.767,0.767,0.776]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [208][0/97], lr: 0.00000\tTime 0.440 (0.440)\tData 0.229 (0.229)\tLoss 0.9639 (0.9639)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [208][10/97], lr: 0.00000\tTime 0.323 (0.340)\tData 0.000 (0.035)\tLoss 2.7254 (2.2476)\tPrec@1 87.500 (89.773)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [208][20/97], lr: 0.00000\tTime 0.323 (0.334)\tData 0.000 (0.027)\tLoss 3.5249 (2.5191)\tPrec@1 86.719 (88.244)\tPrec@5 100.000 (99.182)\n",
      "Epoch: [208][30/97], lr: 0.00000\tTime 0.326 (0.334)\tData 0.000 (0.024)\tLoss 1.8172 (2.4683)\tPrec@1 89.844 (88.382)\tPrec@5 100.000 (99.345)\n",
      "Epoch: [208][40/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.022)\tLoss 2.6855 (2.5502)\tPrec@1 90.625 (88.377)\tPrec@5 99.219 (99.371)\n",
      "Epoch: [208][50/97], lr: 0.00000\tTime 0.331 (0.332)\tData 0.000 (0.021)\tLoss 1.6319 (2.5241)\tPrec@1 88.281 (88.358)\tPrec@5 100.000 (99.372)\n",
      "Epoch: [208][60/97], lr: 0.00000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 2.2237 (2.5659)\tPrec@1 85.938 (88.307)\tPrec@5 99.219 (99.321)\n",
      "Epoch: [208][70/97], lr: 0.00000\tTime 0.323 (0.331)\tData 0.000 (0.020)\tLoss 4.2158 (2.5192)\tPrec@1 85.156 (88.457)\tPrec@5 98.438 (99.340)\n",
      "Epoch: [208][80/97], lr: 0.00000\tTime 0.331 (0.331)\tData 0.000 (0.020)\tLoss 2.2996 (2.4710)\tPrec@1 89.844 (88.686)\tPrec@5 98.438 (99.306)\n",
      "Epoch: [208][90/97], lr: 0.00000\tTime 0.327 (0.331)\tData 0.000 (0.019)\tLoss 1.3419 (2.4489)\tPrec@1 92.188 (88.736)\tPrec@5 99.219 (99.356)\n",
      "Epoch: [208][96/97], lr: 0.00000\tTime 0.317 (0.330)\tData 0.000 (0.020)\tLoss 2.0820 (2.4735)\tPrec@1 85.593 (88.764)\tPrec@5 99.153 (99.371)\n",
      "Test: [0/100]\tTime 0.247 (0.247)\tLoss 3.4181 (3.4181)\tPrec@1 88.000 (88.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 3.3618 (5.8570)\tPrec@1 84.000 (81.182)\tPrec@5 98.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.3486 (5.8119)\tPrec@1 82.000 (81.143)\tPrec@5 100.000 (98.476)\n",
      "Test: [30/100]\tTime 0.074 (0.079)\tLoss 4.4959 (5.8178)\tPrec@1 80.000 (80.968)\tPrec@5 100.000 (98.419)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.7374 (5.9057)\tPrec@1 82.000 (80.829)\tPrec@5 97.000 (98.341)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.1303 (5.8992)\tPrec@1 82.000 (81.039)\tPrec@5 99.000 (98.353)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7647 (5.8468)\tPrec@1 81.000 (81.000)\tPrec@5 100.000 (98.361)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.3823 (5.8316)\tPrec@1 79.000 (81.310)\tPrec@5 98.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.0359 (5.7969)\tPrec@1 82.000 (81.198)\tPrec@5 97.000 (98.457)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.3994 (5.8453)\tPrec@1 85.000 (80.912)\tPrec@5 100.000 (98.484)\n",
      "val Results: Prec@1 80.750 Prec@5 98.480 Loss 5.87128\n",
      "val Class Accuracy: [0.916,0.972,0.785,0.722,0.768,0.751,0.856,0.767,0.770,0.768]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [209][0/97], lr: 0.00000\tTime 0.439 (0.439)\tData 0.216 (0.216)\tLoss 2.0105 (2.0105)\tPrec@1 85.938 (85.938)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [209][10/97], lr: 0.00000\tTime 0.325 (0.340)\tData 0.000 (0.035)\tLoss 1.0893 (2.3203)\tPrec@1 92.188 (89.560)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [209][20/97], lr: 0.00000\tTime 0.327 (0.333)\tData 0.000 (0.026)\tLoss 2.5043 (2.4150)\tPrec@1 90.625 (89.137)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [209][30/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.023)\tLoss 2.1373 (2.4742)\tPrec@1 88.281 (88.634)\tPrec@5 99.219 (99.546)\n",
      "Epoch: [209][40/97], lr: 0.00000\tTime 0.325 (0.332)\tData 0.000 (0.022)\tLoss 2.2954 (2.3582)\tPrec@1 90.625 (89.082)\tPrec@5 99.219 (99.447)\n",
      "Epoch: [209][50/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.021)\tLoss 4.2031 (2.4258)\tPrec@1 85.156 (89.017)\tPrec@5 98.438 (99.403)\n",
      "Epoch: [209][60/97], lr: 0.00000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 2.9898 (2.5104)\tPrec@1 85.938 (89.062)\tPrec@5 100.000 (99.411)\n",
      "Epoch: [209][70/97], lr: 0.00000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 3.3200 (2.5310)\tPrec@1 88.281 (88.908)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [209][80/97], lr: 0.00000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 3.4141 (2.5560)\tPrec@1 85.938 (88.735)\tPrec@5 99.219 (99.383)\n",
      "Epoch: [209][90/97], lr: 0.00000\tTime 0.324 (0.331)\tData 0.000 (0.019)\tLoss 2.2139 (2.5005)\tPrec@1 89.844 (88.839)\tPrec@5 98.438 (99.348)\n",
      "Epoch: [209][96/97], lr: 0.00000\tTime 0.319 (0.331)\tData 0.000 (0.020)\tLoss 4.6030 (2.4997)\tPrec@1 84.746 (88.747)\tPrec@5 99.153 (99.331)\n",
      "Test: [0/100]\tTime 0.258 (0.258)\tLoss 3.3027 (3.3027)\tPrec@1 88.000 (88.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 3.1686 (5.7143)\tPrec@1 84.000 (81.273)\tPrec@5 98.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.3384 (5.6860)\tPrec@1 81.000 (81.429)\tPrec@5 100.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.3850 (5.6997)\tPrec@1 82.000 (81.258)\tPrec@5 100.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.5500 (5.7750)\tPrec@1 80.000 (81.000)\tPrec@5 97.000 (98.268)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.9887 (5.7656)\tPrec@1 82.000 (81.176)\tPrec@5 99.000 (98.294)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7210 (5.7073)\tPrec@1 82.000 (81.115)\tPrec@5 100.000 (98.295)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.2440 (5.6913)\tPrec@1 81.000 (81.493)\tPrec@5 98.000 (98.352)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.8388 (5.6597)\tPrec@1 82.000 (81.383)\tPrec@5 97.000 (98.432)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.2726 (5.7063)\tPrec@1 85.000 (81.055)\tPrec@5 100.000 (98.440)\n",
      "val Results: Prec@1 80.890 Prec@5 98.440 Loss 5.73389\n",
      "val Class Accuracy: [0.914,0.973,0.785,0.708,0.778,0.740,0.865,0.783,0.770,0.773]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [210][0/97], lr: 0.00000\tTime 0.422 (0.422)\tData 0.192 (0.192)\tLoss 1.1453 (1.1453)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [210][10/97], lr: 0.00000\tTime 0.329 (0.341)\tData 0.000 (0.033)\tLoss 1.0802 (2.2029)\tPrec@1 94.531 (88.778)\tPrec@5 98.438 (99.148)\n",
      "Epoch: [210][20/97], lr: 0.00000\tTime 0.327 (0.335)\tData 0.000 (0.025)\tLoss 1.1868 (2.1546)\tPrec@1 90.625 (89.025)\tPrec@5 99.219 (99.368)\n",
      "Epoch: [210][30/97], lr: 0.00000\tTime 0.320 (0.334)\tData 0.000 (0.023)\tLoss 1.6430 (2.1613)\tPrec@1 86.719 (88.936)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [210][40/97], lr: 0.00000\tTime 0.328 (0.332)\tData 0.000 (0.021)\tLoss 2.9474 (2.3819)\tPrec@1 85.938 (88.300)\tPrec@5 100.000 (99.466)\n",
      "Epoch: [210][50/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 2.0591 (2.4576)\tPrec@1 89.062 (88.343)\tPrec@5 99.219 (99.418)\n",
      "Epoch: [210][60/97], lr: 0.00000\tTime 0.329 (0.331)\tData 0.000 (0.020)\tLoss 3.4796 (2.5328)\tPrec@1 82.812 (88.281)\tPrec@5 96.875 (99.334)\n",
      "Epoch: [210][70/97], lr: 0.00000\tTime 0.354 (0.331)\tData 0.000 (0.020)\tLoss 2.3130 (2.5548)\tPrec@1 89.844 (88.358)\tPrec@5 100.000 (99.340)\n",
      "Epoch: [210][80/97], lr: 0.00000\tTime 0.329 (0.331)\tData 0.000 (0.019)\tLoss 3.2642 (2.5369)\tPrec@1 85.156 (88.426)\tPrec@5 99.219 (99.344)\n",
      "Epoch: [210][90/97], lr: 0.00000\tTime 0.322 (0.331)\tData 0.000 (0.019)\tLoss 0.9152 (2.5112)\tPrec@1 91.406 (88.333)\tPrec@5 99.219 (99.322)\n",
      "Epoch: [210][96/97], lr: 0.00000\tTime 0.316 (0.330)\tData 0.000 (0.020)\tLoss 1.7851 (2.5161)\tPrec@1 87.288 (88.320)\tPrec@5 99.153 (99.283)\n",
      "Test: [0/100]\tTime 0.223 (0.223)\tLoss 3.4407 (3.4407)\tPrec@1 86.000 (86.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 3.3268 (5.8921)\tPrec@1 84.000 (80.818)\tPrec@5 98.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 5.3050 (5.8312)\tPrec@1 82.000 (81.000)\tPrec@5 99.000 (98.476)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.4110 (5.8464)\tPrec@1 81.000 (81.000)\tPrec@5 100.000 (98.419)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.6631 (5.9366)\tPrec@1 80.000 (80.829)\tPrec@5 97.000 (98.415)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.1151 (5.9375)\tPrec@1 81.000 (80.922)\tPrec@5 99.000 (98.412)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 6.7041 (5.8746)\tPrec@1 82.000 (80.951)\tPrec@5 100.000 (98.393)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.4187 (5.8623)\tPrec@1 81.000 (81.254)\tPrec@5 98.000 (98.437)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.0686 (5.8302)\tPrec@1 81.000 (81.222)\tPrec@5 97.000 (98.494)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.4161 (5.8777)\tPrec@1 85.000 (80.956)\tPrec@5 100.000 (98.495)\n",
      "val Results: Prec@1 80.800 Prec@5 98.500 Loss 5.90612\n",
      "val Class Accuracy: [0.907,0.974,0.779,0.701,0.794,0.764,0.859,0.772,0.773,0.757]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [211][0/97], lr: 0.00000\tTime 0.421 (0.421)\tData 0.235 (0.235)\tLoss 1.3507 (1.3507)\tPrec@1 91.406 (91.406)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [211][10/97], lr: 0.00000\tTime 0.324 (0.340)\tData 0.000 (0.037)\tLoss 1.6792 (2.4696)\tPrec@1 88.281 (88.281)\tPrec@5 99.219 (99.574)\n",
      "Epoch: [211][20/97], lr: 0.00000\tTime 0.326 (0.336)\tData 0.000 (0.027)\tLoss 1.4884 (2.4852)\tPrec@1 92.969 (88.393)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [211][30/97], lr: 0.00000\tTime 0.328 (0.334)\tData 0.000 (0.024)\tLoss 1.7881 (2.4169)\tPrec@1 88.281 (88.407)\tPrec@5 99.219 (99.446)\n",
      "Epoch: [211][40/97], lr: 0.00000\tTime 0.324 (0.333)\tData 0.000 (0.023)\tLoss 3.6795 (2.3423)\tPrec@1 89.062 (88.681)\tPrec@5 99.219 (99.333)\n",
      "Epoch: [211][50/97], lr: 0.00000\tTime 0.327 (0.333)\tData 0.000 (0.021)\tLoss 1.5549 (2.3659)\tPrec@1 90.625 (88.894)\tPrec@5 98.438 (99.341)\n",
      "Epoch: [211][60/97], lr: 0.00000\tTime 0.328 (0.332)\tData 0.000 (0.021)\tLoss 0.4734 (2.4235)\tPrec@1 96.094 (88.858)\tPrec@5 100.000 (99.283)\n",
      "Epoch: [211][70/97], lr: 0.00000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 4.5253 (2.5643)\tPrec@1 85.156 (88.666)\tPrec@5 100.000 (99.274)\n",
      "Epoch: [211][80/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 2.5649 (2.5569)\tPrec@1 86.719 (88.638)\tPrec@5 100.000 (99.257)\n",
      "Epoch: [211][90/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.020)\tLoss 1.0919 (2.5211)\tPrec@1 93.750 (88.685)\tPrec@5 99.219 (99.245)\n",
      "Epoch: [211][96/97], lr: 0.00000\tTime 0.320 (0.331)\tData 0.000 (0.020)\tLoss 1.0845 (2.5120)\tPrec@1 90.678 (88.699)\tPrec@5 100.000 (99.266)\n",
      "Test: [0/100]\tTime 0.238 (0.238)\tLoss 3.4707 (3.4707)\tPrec@1 88.000 (88.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 3.3309 (5.9397)\tPrec@1 84.000 (80.727)\tPrec@5 98.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.4065 (5.8915)\tPrec@1 82.000 (81.000)\tPrec@5 99.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.5143 (5.9001)\tPrec@1 80.000 (80.903)\tPrec@5 99.000 (98.258)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.7779 (5.9897)\tPrec@1 80.000 (80.659)\tPrec@5 97.000 (98.268)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.1799 (5.9991)\tPrec@1 82.000 (80.804)\tPrec@5 99.000 (98.314)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.8525 (5.9294)\tPrec@1 82.000 (80.803)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.4792 (5.9147)\tPrec@1 82.000 (81.183)\tPrec@5 98.000 (98.352)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.9715 (5.8791)\tPrec@1 82.000 (81.111)\tPrec@5 97.000 (98.420)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.5536 (5.9275)\tPrec@1 86.000 (80.879)\tPrec@5 100.000 (98.418)\n",
      "val Results: Prec@1 80.700 Prec@5 98.430 Loss 5.95398\n",
      "val Class Accuracy: [0.917,0.977,0.776,0.695,0.788,0.764,0.858,0.773,0.763,0.759]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [212][0/97], lr: 0.00000\tTime 0.400 (0.400)\tData 0.205 (0.205)\tLoss 3.0100 (3.0100)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [212][10/97], lr: 0.00000\tTime 0.324 (0.340)\tData 0.000 (0.033)\tLoss 1.2595 (2.7848)\tPrec@1 91.406 (87.429)\tPrec@5 98.438 (99.077)\n",
      "Epoch: [212][20/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.025)\tLoss 1.9307 (2.6873)\tPrec@1 90.625 (87.798)\tPrec@5 100.000 (98.958)\n",
      "Epoch: [212][30/97], lr: 0.00000\tTime 0.325 (0.332)\tData 0.000 (0.023)\tLoss 1.9659 (2.7047)\tPrec@1 91.406 (87.752)\tPrec@5 100.000 (99.042)\n",
      "Epoch: [212][40/97], lr: 0.00000\tTime 0.324 (0.331)\tData 0.000 (0.021)\tLoss 3.2749 (2.7403)\tPrec@1 84.375 (87.786)\tPrec@5 100.000 (99.200)\n",
      "Epoch: [212][50/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.021)\tLoss 2.1484 (2.8268)\tPrec@1 92.188 (88.082)\tPrec@5 100.000 (99.311)\n",
      "Epoch: [212][60/97], lr: 0.00000\tTime 0.325 (0.330)\tData 0.000 (0.020)\tLoss 1.6268 (2.7114)\tPrec@1 88.281 (88.217)\tPrec@5 99.219 (99.334)\n",
      "Epoch: [212][70/97], lr: 0.00000\tTime 0.327 (0.330)\tData 0.000 (0.020)\tLoss 2.2422 (2.6381)\tPrec@1 85.938 (88.336)\tPrec@5 99.219 (99.285)\n",
      "Epoch: [212][80/97], lr: 0.00000\tTime 0.328 (0.330)\tData 0.000 (0.019)\tLoss 2.5283 (2.5466)\tPrec@1 85.938 (88.358)\tPrec@5 99.219 (99.267)\n",
      "Epoch: [212][90/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.019)\tLoss 2.3351 (2.5136)\tPrec@1 89.844 (88.590)\tPrec@5 100.000 (99.287)\n",
      "Epoch: [212][96/97], lr: 0.00000\tTime 0.317 (0.329)\tData 0.000 (0.020)\tLoss 3.0186 (2.4974)\tPrec@1 85.593 (88.554)\tPrec@5 100.000 (99.299)\n",
      "Test: [0/100]\tTime 0.253 (0.253)\tLoss 3.4570 (3.4570)\tPrec@1 86.000 (86.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 3.3814 (5.8621)\tPrec@1 84.000 (80.545)\tPrec@5 98.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.3784 (5.8075)\tPrec@1 81.000 (80.810)\tPrec@5 99.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.5031 (5.8183)\tPrec@1 81.000 (80.806)\tPrec@5 100.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.6623 (5.9065)\tPrec@1 80.000 (80.634)\tPrec@5 97.000 (98.293)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.0246 (5.9034)\tPrec@1 82.000 (80.843)\tPrec@5 99.000 (98.333)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7391 (5.8445)\tPrec@1 82.000 (80.836)\tPrec@5 100.000 (98.361)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.4088 (5.8350)\tPrec@1 81.000 (81.183)\tPrec@5 98.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.9966 (5.8024)\tPrec@1 82.000 (81.136)\tPrec@5 97.000 (98.444)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.4549 (5.8506)\tPrec@1 86.000 (80.912)\tPrec@5 100.000 (98.440)\n",
      "val Results: Prec@1 80.740 Prec@5 98.450 Loss 5.87768\n",
      "val Class Accuracy: [0.916,0.973,0.775,0.707,0.789,0.761,0.854,0.773,0.760,0.766]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [213][0/97], lr: 0.00000\tTime 0.399 (0.399)\tData 0.204 (0.204)\tLoss 1.4960 (1.4960)\tPrec@1 92.969 (92.969)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [213][10/97], lr: 0.00000\tTime 0.331 (0.342)\tData 0.000 (0.033)\tLoss 1.5660 (2.2492)\tPrec@1 88.281 (88.494)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [213][20/97], lr: 0.00000\tTime 0.323 (0.334)\tData 0.000 (0.025)\tLoss 1.9814 (2.3611)\tPrec@1 89.062 (88.207)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [213][30/97], lr: 0.00000\tTime 0.321 (0.333)\tData 0.000 (0.023)\tLoss 3.3023 (2.4414)\tPrec@1 92.969 (88.357)\tPrec@5 97.656 (99.446)\n",
      "Epoch: [213][40/97], lr: 0.00000\tTime 0.327 (0.331)\tData 0.000 (0.021)\tLoss 2.1841 (2.4364)\tPrec@1 90.625 (88.624)\tPrec@5 97.656 (99.371)\n",
      "Epoch: [213][50/97], lr: 0.00000\tTime 0.325 (0.331)\tData 0.000 (0.021)\tLoss 2.4349 (2.4722)\tPrec@1 91.406 (88.863)\tPrec@5 98.438 (99.326)\n",
      "Epoch: [213][60/97], lr: 0.00000\tTime 0.343 (0.332)\tData 0.000 (0.020)\tLoss 1.6555 (2.4549)\tPrec@1 84.375 (88.691)\tPrec@5 99.219 (99.372)\n",
      "Epoch: [213][70/97], lr: 0.00000\tTime 0.331 (0.333)\tData 0.000 (0.020)\tLoss 5.0369 (2.4685)\tPrec@1 89.062 (88.732)\tPrec@5 100.000 (99.373)\n",
      "Epoch: [213][80/97], lr: 0.00000\tTime 0.324 (0.333)\tData 0.000 (0.019)\tLoss 2.3672 (2.4658)\tPrec@1 91.406 (88.850)\tPrec@5 100.000 (99.402)\n",
      "Epoch: [213][90/97], lr: 0.00000\tTime 0.322 (0.332)\tData 0.000 (0.019)\tLoss 1.9533 (2.4625)\tPrec@1 94.531 (88.762)\tPrec@5 100.000 (99.390)\n",
      "Epoch: [213][96/97], lr: 0.00000\tTime 0.317 (0.332)\tData 0.000 (0.020)\tLoss 2.9786 (2.4907)\tPrec@1 93.220 (88.764)\tPrec@5 100.000 (99.387)\n",
      "Test: [0/100]\tTime 0.241 (0.241)\tLoss 3.3924 (3.3924)\tPrec@1 87.000 (87.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 3.2469 (5.8176)\tPrec@1 84.000 (81.000)\tPrec@5 98.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.3205 (5.7721)\tPrec@1 82.000 (81.095)\tPrec@5 100.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.3976 (5.7876)\tPrec@1 82.000 (81.032)\tPrec@5 100.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 3.6208 (5.8752)\tPrec@1 81.000 (80.902)\tPrec@5 97.000 (98.293)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.0722 (5.8714)\tPrec@1 81.000 (81.000)\tPrec@5 99.000 (98.333)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.6863 (5.8140)\tPrec@1 84.000 (81.033)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.2939 (5.8014)\tPrec@1 81.000 (81.380)\tPrec@5 98.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.0058 (5.7702)\tPrec@1 82.000 (81.309)\tPrec@5 97.000 (98.457)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.3075 (5.8157)\tPrec@1 85.000 (80.978)\tPrec@5 100.000 (98.473)\n",
      "val Results: Prec@1 80.840 Prec@5 98.480 Loss 5.84372\n",
      "val Class Accuracy: [0.912,0.973,0.798,0.703,0.783,0.744,0.863,0.778,0.770,0.760]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [214][0/97], lr: 0.00000\tTime 0.455 (0.455)\tData 0.250 (0.250)\tLoss 1.4426 (1.4426)\tPrec@1 95.312 (95.312)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [214][10/97], lr: 0.00000\tTime 0.323 (0.345)\tData 0.000 (0.038)\tLoss 2.5861 (2.6138)\tPrec@1 85.156 (87.855)\tPrec@5 97.656 (99.148)\n",
      "Epoch: [214][20/97], lr: 0.00000\tTime 0.324 (0.338)\tData 0.000 (0.028)\tLoss 2.7977 (2.5771)\tPrec@1 84.375 (88.430)\tPrec@5 98.438 (99.182)\n",
      "Epoch: [214][30/97], lr: 0.00000\tTime 0.325 (0.335)\tData 0.000 (0.025)\tLoss 3.4930 (2.7823)\tPrec@1 84.375 (88.357)\tPrec@5 99.219 (99.345)\n",
      "Epoch: [214][40/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.023)\tLoss 1.4581 (2.7241)\tPrec@1 90.625 (88.605)\tPrec@5 100.000 (99.447)\n",
      "Epoch: [214][50/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.022)\tLoss 2.0537 (2.6364)\tPrec@1 92.188 (88.511)\tPrec@5 99.219 (99.311)\n",
      "Epoch: [214][60/97], lr: 0.00000\tTime 0.327 (0.332)\tData 0.000 (0.021)\tLoss 2.6467 (2.6576)\tPrec@1 90.625 (88.563)\tPrec@5 99.219 (99.321)\n",
      "Epoch: [214][70/97], lr: 0.00000\tTime 0.323 (0.332)\tData 0.000 (0.021)\tLoss 1.4505 (2.7745)\tPrec@1 89.844 (88.292)\tPrec@5 100.000 (99.318)\n",
      "Epoch: [214][80/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 1.9879 (2.7244)\tPrec@1 89.062 (88.387)\tPrec@5 96.875 (99.277)\n",
      "Epoch: [214][90/97], lr: 0.00000\tTime 0.330 (0.331)\tData 0.000 (0.020)\tLoss 1.5587 (2.6858)\tPrec@1 88.281 (88.444)\tPrec@5 99.219 (99.202)\n",
      "Epoch: [214][96/97], lr: 0.00000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 0.6488 (2.6565)\tPrec@1 90.678 (88.554)\tPrec@5 99.153 (99.202)\n",
      "Test: [0/100]\tTime 0.269 (0.269)\tLoss 3.5228 (3.5228)\tPrec@1 86.000 (86.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 3.3797 (5.8093)\tPrec@1 84.000 (80.909)\tPrec@5 98.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.4438 (5.7546)\tPrec@1 81.000 (81.381)\tPrec@5 99.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.5452 (5.7608)\tPrec@1 81.000 (81.194)\tPrec@5 100.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 3.6694 (5.8387)\tPrec@1 80.000 (81.000)\tPrec@5 97.000 (98.366)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.9143 (5.8304)\tPrec@1 82.000 (81.118)\tPrec@5 99.000 (98.392)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7525 (5.7695)\tPrec@1 82.000 (81.115)\tPrec@5 100.000 (98.410)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.4089 (5.7591)\tPrec@1 82.000 (81.465)\tPrec@5 98.000 (98.451)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.9182 (5.7281)\tPrec@1 82.000 (81.358)\tPrec@5 97.000 (98.519)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.3865 (5.7790)\tPrec@1 86.000 (81.077)\tPrec@5 100.000 (98.516)\n",
      "val Results: Prec@1 80.950 Prec@5 98.520 Loss 5.80645\n",
      "val Class Accuracy: [0.920,0.970,0.785,0.709,0.801,0.750,0.852,0.773,0.754,0.781]\n",
      "Best Prec@1: 81.230\n",
      "\n",
      "cifar_train_lorot-E.py:472: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [215][0/97], lr: 0.00000\tTime 0.448 (0.448)\tData 0.209 (0.209)\tLoss 2.7937 (2.7937)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [215][10/97], lr: 0.00000\tTime 0.326 (0.361)\tData 0.000 (0.034)\tLoss 1.7421 (2.6116)\tPrec@1 89.062 (88.778)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [215][20/97], lr: 0.00000\tTime 0.322 (0.346)\tData 0.000 (0.026)\tLoss 2.2586 (2.3964)\tPrec@1 92.188 (88.728)\tPrec@5 98.438 (99.368)\n",
      "Epoch: [215][30/97], lr: 0.00000\tTime 0.325 (0.342)\tData 0.000 (0.023)\tLoss 2.7286 (2.3720)\tPrec@1 89.062 (88.785)\tPrec@5 100.000 (99.294)\n",
      "Epoch: [215][40/97], lr: 0.00000\tTime 0.338 (0.342)\tData 0.000 (0.022)\tLoss 1.8180 (2.5616)\tPrec@1 88.281 (88.700)\tPrec@5 100.000 (99.371)\n",
      "Epoch: [215][50/97], lr: 0.00000\tTime 0.340 (0.341)\tData 0.000 (0.021)\tLoss 3.7302 (2.5116)\tPrec@1 85.938 (88.833)\tPrec@5 100.000 (99.311)\n",
      "Epoch: [215][60/97], lr: 0.00000\tTime 0.386 (0.343)\tData 0.001 (0.020)\tLoss 2.6939 (2.4887)\tPrec@1 86.719 (88.832)\tPrec@5 100.000 (99.372)\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"cifar_train_lorot-E.py\", line 672, in <module>\n",
      "    main()\n",
      "  File \"cifar_train_lorot-E.py\", line 181, in main\n",
      "    main_worker(args.gpu, ngpus_per_node, args)\n",
      "  File \"cifar_train_lorot-E.py\", line 361, in main_worker\n",
      "    train(\n",
      "  File \"cifar_train_lorot-E.py\", line 452, in train\n",
      "    fliplabel = fliplabel.cuda()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python cifar_train_lorot-E.py --gpu 0 --imb_type exp --exp_str rotflipscldam --imb_factor 0.01 --loss_type LDAM --train_rule DRW -m \"rot fliplr sc\" --epochs 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar_train_lorot-E.py:175: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.\n",
      "  warnings.warn(\n",
      "Use GPU: 0 for training\n",
      "=> creating model 'resnet32'\n",
      "num_trans : 16\n",
      "num_flipped : 4\n",
      "num shuffled channel : 24\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[5000, 2997, 1796, 1077, 645, 387, 232, 139, 83, 50]\n",
      "/media/aristo/Data A/documents/kuliah/Project/Localizable-Rotation/Imbalanced/losses.py:49: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/TensorCompare.cpp:402.)\n",
      "  output = torch.where(index, x_m, x)\n",
      "Epoch: [0][96/97], lr: 0.00200\tTime 0.920 (0.383)\tData 0.000 (0.018)\tLoss 5.3696 (6.4340)\tPrec@1 55.932 (53.813)\tPrec@5 97.458 (91.133)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 12.3132 (12.8693)\tPrec@1 31.000 (23.989)\tPrec@5 83.000 (75.780)\n",
      "val Results: Prec@1 23.960 Prec@5 75.600 Loss 12.89148\n",
      "val Class Accuracy: [0.762,0.930,0.700,0.002,0.000,0.002,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 23.960\n",
      "\n",
      "Epoch: [1][96/97], lr: 0.00400\tTime 0.318 (0.347)\tData 0.000 (0.022)\tLoss 5.3090 (5.2836)\tPrec@1 60.169 (61.011)\tPrec@5 95.763 (93.310)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 12.3664 (12.9475)\tPrec@1 29.000 (22.923)\tPrec@5 82.000 (73.835)\n",
      "val Results: Prec@1 22.910 Prec@5 73.680 Loss 12.96547\n",
      "val Class Accuracy: [0.535,0.936,0.770,0.050,0.000,0.000,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 23.960\n",
      "\n",
      "Epoch: [2][96/97], lr: 0.00600\tTime 0.337 (0.350)\tData 0.000 (0.020)\tLoss 6.1261 (5.1010)\tPrec@1 52.542 (62.583)\tPrec@5 92.373 (93.890)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 12.0119 (12.1632)\tPrec@1 31.000 (26.956)\tPrec@5 87.000 (76.495)\n",
      "val Results: Prec@1 26.890 Prec@5 76.400 Loss 12.18169\n",
      "val Class Accuracy: [0.704,0.936,0.619,0.311,0.000,0.099,0.000,0.020,0.000,0.000]\n",
      "Best Prec@1: 26.890\n",
      "\n",
      "Epoch: [3][96/97], lr: 0.00800\tTime 0.326 (0.357)\tData 0.000 (0.021)\tLoss 5.4470 (4.8529)\tPrec@1 61.864 (64.977)\tPrec@5 92.373 (94.478))\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 14.6264 (15.3707)\tPrec@1 19.000 (16.022)\tPrec@5 77.000 (71.330)\n",
      "val Results: Prec@1 16.050 Prec@5 71.230 Loss 15.38704\n",
      "val Class Accuracy: [0.993,0.227,0.327,0.054,0.003,0.001,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 26.890\n",
      "\n",
      "Epoch: [4][96/97], lr: 0.01000\tTime 0.354 (0.371)\tData 0.000 (0.022)\tLoss 4.6382 (4.6779)\tPrec@1 61.864 (66.573)\tPrec@5 96.610 (94.857)\n",
      "Test: [90/100]\tTime 0.072 (0.076)\tLoss 11.4341 (11.7722)\tPrec@1 35.000 (29.527)\tPrec@5 80.000 (78.055)\n",
      "val Results: Prec@1 29.480 Prec@5 77.890 Loss 11.80121\n",
      "val Class Accuracy: [0.804,0.867,0.597,0.609,0.005,0.065,0.001,0.000,0.000,0.000]\n",
      "Best Prec@1: 29.480\n",
      "\n",
      "Epoch: [5][96/97], lr: 0.01000\tTime 0.305 (0.321)\tData 0.000 (0.020)\tLoss 4.0153 (4.4587)\tPrec@1 75.424 (68.161)\tPrec@5 94.915 (95.172)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 14.7793 (15.1083)\tPrec@1 26.000 (20.824)\tPrec@5 70.000 (75.055)\n",
      "val Results: Prec@1 20.860 Prec@5 75.020 Loss 15.14036\n",
      "val Class Accuracy: [0.682,0.993,0.183,0.181,0.010,0.000,0.017,0.020,0.000,0.000]\n",
      "Best Prec@1: 29.480\n",
      "\n",
      "Epoch: [6][96/97], lr: 0.01000\tTime 0.305 (0.323)\tData 0.000 (0.021)\tLoss 3.7886 (4.3258)\tPrec@1 75.424 (69.071)\tPrec@5 97.458 (95.655)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 12.0253 (12.6464)\tPrec@1 32.000 (26.813)\tPrec@5 95.000 (86.407)\n",
      "val Results: Prec@1 26.830 Prec@5 86.270 Loss 12.66456\n",
      "val Class Accuracy: [0.940,0.940,0.676,0.119,0.001,0.000,0.007,0.000,0.000,0.000]\n",
      "Best Prec@1: 29.480\n",
      "\n",
      "Epoch: [7][96/97], lr: 0.01000\tTime 0.304 (0.317)\tData 0.000 (0.020)\tLoss 4.0956 (4.1953)\tPrec@1 69.492 (70.667)\tPrec@5 97.458 (96.349)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 11.9068 (12.6339)\tPrec@1 31.000 (26.385)\tPrec@5 88.000 (84.868)\n",
      "val Results: Prec@1 26.340 Prec@5 85.040 Loss 12.65940\n",
      "val Class Accuracy: [0.964,0.965,0.494,0.080,0.092,0.005,0.007,0.027,0.000,0.000]\n",
      "Best Prec@1: 29.480\n",
      "\n",
      "Epoch: [8][96/97], lr: 0.01000\tTime 0.305 (0.320)\tData 0.000 (0.020)\tLoss 4.8719 (4.1353)\tPrec@1 67.797 (71.038)\tPrec@5 95.763 (96.074)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 11.6478 (11.5663)\tPrec@1 34.000 (32.209)\tPrec@5 92.000 (90.352)\n",
      "val Results: Prec@1 32.210 Prec@5 90.240 Loss 11.58795\n",
      "val Class Accuracy: [0.870,0.982,0.522,0.596,0.236,0.002,0.010,0.003,0.000,0.000]\n",
      "Best Prec@1: 32.210\n",
      "\n",
      "Epoch: [9][96/97], lr: 0.01000\tTime 0.307 (0.318)\tData 0.000 (0.020)\tLoss 3.4021 (3.9661)\tPrec@1 77.119 (72.924)\tPrec@5 96.610 (96.300)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 11.7665 (12.0553)\tPrec@1 32.000 (29.242)\tPrec@5 90.000 (85.220)\n",
      "val Results: Prec@1 29.160 Prec@5 85.300 Loss 12.08615\n",
      "val Class Accuracy: [0.966,0.948,0.321,0.334,0.343,0.003,0.000,0.001,0.000,0.000]\n",
      "Best Prec@1: 32.210\n",
      "\n",
      "Epoch: [10][96/97], lr: 0.01000\tTime 0.309 (0.318)\tData 0.000 (0.020)\tLoss 4.4647 (3.9101)\tPrec@1 70.339 (73.174)\tPrec@5 96.610 (96.776)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.1282 (11.0431)\tPrec@1 36.000 (35.879)\tPrec@5 92.000 (91.407)\n",
      "val Results: Prec@1 35.710 Prec@5 91.370 Loss 11.08161\n",
      "val Class Accuracy: [0.910,0.978,0.385,0.783,0.412,0.001,0.081,0.021,0.000,0.000]\n",
      "Best Prec@1: 35.710\n",
      "\n",
      "Epoch: [11][96/97], lr: 0.01000\tTime 0.306 (0.319)\tData 0.000 (0.020)\tLoss 3.3092 (3.8093)\tPrec@1 80.508 (73.996)\tPrec@5 97.458 (97.034)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.5608 (12.0414)\tPrec@1 37.000 (32.791)\tPrec@5 88.000 (84.198)\n",
      "val Results: Prec@1 32.640 Prec@5 84.030 Loss 12.07083\n",
      "val Class Accuracy: [0.955,0.845,0.646,0.674,0.104,0.000,0.040,0.000,0.000,0.000]\n",
      "Best Prec@1: 35.710\n",
      "\n",
      "Epoch: [12][96/97], lr: 0.01000\tTime 0.306 (0.319)\tData 0.000 (0.020)\tLoss 4.0695 (3.7722)\tPrec@1 69.492 (74.617)\tPrec@5 98.305 (97.114)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 11.5431 (11.5448)\tPrec@1 31.000 (33.264)\tPrec@5 94.000 (90.220)\n",
      "val Results: Prec@1 33.120 Prec@5 90.310 Loss 11.56550\n",
      "val Class Accuracy: [0.871,0.990,0.432,0.728,0.235,0.014,0.002,0.031,0.009,0.000]\n",
      "Best Prec@1: 35.710\n",
      "\n",
      "Epoch: [13][96/97], lr: 0.01000\tTime 0.306 (0.318)\tData 0.000 (0.020)\tLoss 2.9893 (3.6491)\tPrec@1 81.356 (75.818)\tPrec@5 98.305 (97.687)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.7465 (11.1805)\tPrec@1 41.000 (37.571)\tPrec@5 96.000 (92.132)\n",
      "val Results: Prec@1 37.430 Prec@5 92.150 Loss 11.21007\n",
      "val Class Accuracy: [0.916,0.980,0.746,0.412,0.404,0.032,0.210,0.043,0.000,0.000]\n",
      "Best Prec@1: 37.430\n",
      "\n",
      "Epoch: [14][96/97], lr: 0.01000\tTime 0.308 (0.319)\tData 0.000 (0.020)\tLoss 3.3204 (3.6267)\tPrec@1 77.119 (76.100)\tPrec@5 98.305 (97.356)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.3656 (11.5418)\tPrec@1 36.000 (35.934)\tPrec@5 94.000 (90.879)\n",
      "val Results: Prec@1 35.790 Prec@5 90.920 Loss 11.57099\n",
      "val Class Accuracy: [0.794,0.983,0.718,0.769,0.235,0.000,0.064,0.015,0.001,0.000]\n",
      "Best Prec@1: 37.430\n",
      "\n",
      "Epoch: [15][96/97], lr: 0.01000\tTime 0.308 (0.318)\tData 0.000 (0.020)\tLoss 3.4974 (3.5766)\tPrec@1 76.271 (76.189)\tPrec@5 97.458 (97.679)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.9477 (11.3475)\tPrec@1 36.000 (36.505)\tPrec@5 91.000 (86.582)\n",
      "val Results: Prec@1 36.520 Prec@5 86.560 Loss 11.37713\n",
      "val Class Accuracy: [0.895,0.983,0.635,0.700,0.255,0.055,0.002,0.120,0.005,0.002]\n",
      "Best Prec@1: 37.430\n",
      "\n",
      "Epoch: [16][96/97], lr: 0.01000\tTime 0.309 (0.320)\tData 0.000 (0.020)\tLoss 3.3677 (3.4825)\tPrec@1 76.271 (77.414)\tPrec@5 98.305 (97.751)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.6788 (12.0635)\tPrec@1 36.000 (32.934)\tPrec@5 93.000 (90.681)\n",
      "val Results: Prec@1 32.910 Prec@5 90.700 Loss 12.09720\n",
      "val Class Accuracy: [0.962,0.979,0.613,0.399,0.238,0.038,0.059,0.003,0.000,0.000]\n",
      "Best Prec@1: 37.430\n",
      "\n",
      "Epoch: [17][96/97], lr: 0.01000\tTime 0.309 (0.318)\tData 0.000 (0.020)\tLoss 3.3204 (3.4733)\tPrec@1 77.966 (77.035)\tPrec@5 99.153 (97.880)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 10.8863 (11.0516)\tPrec@1 39.000 (39.319)\tPrec@5 93.000 (91.692)\n",
      "val Results: Prec@1 39.220 Prec@5 91.800 Loss 11.07711\n",
      "val Class Accuracy: [0.924,0.974,0.559,0.779,0.598,0.043,0.042,0.002,0.001,0.000]\n",
      "Best Prec@1: 39.220\n",
      "\n",
      "Epoch: [18][96/97], lr: 0.01000\tTime 0.308 (0.320)\tData 0.000 (0.020)\tLoss 3.0433 (3.4305)\tPrec@1 79.661 (77.648)\tPrec@5 99.153 (98.074)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.7347 (10.9834)\tPrec@1 42.000 (39.429)\tPrec@5 97.000 (91.659)\n",
      "val Results: Prec@1 39.410 Prec@5 91.670 Loss 10.99367\n",
      "val Class Accuracy: [0.963,0.957,0.633,0.603,0.406,0.183,0.041,0.154,0.001,0.000]\n",
      "Best Prec@1: 39.410\n",
      "\n",
      "Epoch: [19][96/97], lr: 0.01000\tTime 0.305 (0.320)\tData 0.000 (0.020)\tLoss 2.1292 (3.3563)\tPrec@1 85.593 (78.132)\tPrec@5 97.458 (97.928))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.4765 (11.7463)\tPrec@1 44.000 (38.132)\tPrec@5 94.000 (88.352)\n",
      "val Results: Prec@1 38.110 Prec@5 88.220 Loss 11.77740\n",
      "val Class Accuracy: [0.954,0.988,0.667,0.447,0.478,0.202,0.015,0.060,0.000,0.000]\n",
      "Best Prec@1: 39.410\n",
      "\n",
      "Epoch: [20][96/97], lr: 0.01000\tTime 0.309 (0.320)\tData 0.000 (0.020)\tLoss 3.7245 (3.3362)\tPrec@1 76.271 (78.099)\tPrec@5 100.000 (98.114)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.6650 (10.6974)\tPrec@1 46.000 (42.659)\tPrec@5 93.000 (92.802)\n",
      "val Results: Prec@1 42.520 Prec@5 92.780 Loss 10.73719\n",
      "val Class Accuracy: [0.739,0.950,0.762,0.723,0.687,0.127,0.159,0.104,0.001,0.000]\n",
      "Best Prec@1: 42.520\n",
      "\n",
      "Epoch: [21][96/97], lr: 0.01000\tTime 0.309 (0.319)\tData 0.000 (0.020)\tLoss 3.3773 (3.2536)\tPrec@1 81.356 (79.131)\tPrec@5 98.305 (98.219)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 13.2485 (13.7616)\tPrec@1 33.000 (29.011)\tPrec@5 90.000 (88.341)\n",
      "val Results: Prec@1 28.990 Prec@5 88.270 Loss 13.79035\n",
      "val Class Accuracy: [0.624,0.869,0.959,0.042,0.259,0.011,0.135,0.000,0.000,0.000]\n",
      "Best Prec@1: 42.520\n",
      "\n",
      "Epoch: [22][96/97], lr: 0.01000\tTime 0.305 (0.319)\tData 0.000 (0.020)\tLoss 3.0600 (3.2177)\tPrec@1 82.203 (79.099)\tPrec@5 98.305 (98.098))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.5464 (10.7353)\tPrec@1 43.000 (42.934)\tPrec@5 98.000 (94.165)\n",
      "val Results: Prec@1 42.770 Prec@5 94.160 Loss 10.75721\n",
      "val Class Accuracy: [0.943,0.958,0.705,0.667,0.432,0.008,0.355,0.207,0.001,0.001]\n",
      "Best Prec@1: 42.770\n",
      "\n",
      "Epoch: [23][96/97], lr: 0.01000\tTime 0.309 (0.321)\tData 0.000 (0.020)\tLoss 3.7180 (3.1710)\tPrec@1 74.576 (79.897)\tPrec@5 99.153 (98.307))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.1748 (10.0733)\tPrec@1 45.000 (46.736)\tPrec@5 97.000 (92.473)\n",
      "val Results: Prec@1 46.690 Prec@5 92.420 Loss 10.09615\n",
      "val Class Accuracy: [0.956,0.981,0.506,0.473,0.604,0.627,0.113,0.398,0.007,0.004]\n",
      "Best Prec@1: 46.690\n",
      "\n",
      "Epoch: [24][96/97], lr: 0.01000\tTime 0.307 (0.319)\tData 0.000 (0.020)\tLoss 3.5347 (3.1650)\tPrec@1 78.814 (79.566)\tPrec@5 98.305 (98.227))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.1439 (11.5758)\tPrec@1 43.000 (38.670)\tPrec@5 94.000 (91.066)\n",
      "val Results: Prec@1 38.500 Prec@5 91.180 Loss 11.61317\n",
      "val Class Accuracy: [0.923,0.986,0.803,0.460,0.222,0.033,0.398,0.025,0.000,0.000]\n",
      "Best Prec@1: 46.690\n",
      "\n",
      "Epoch: [25][96/97], lr: 0.01000\tTime 0.308 (0.321)\tData 0.000 (0.020)\tLoss 2.4035 (3.0727)\tPrec@1 82.203 (80.372)\tPrec@5 99.153 (98.323)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.1635 (10.9961)\tPrec@1 41.000 (42.451)\tPrec@5 92.000 (91.681)\n",
      "val Results: Prec@1 42.470 Prec@5 91.660 Loss 11.02080\n",
      "val Class Accuracy: [0.959,0.964,0.703,0.463,0.302,0.518,0.268,0.069,0.001,0.000]\n",
      "Best Prec@1: 46.690\n",
      "\n",
      "Epoch: [26][96/97], lr: 0.01000\tTime 0.304 (0.320)\tData 0.000 (0.021)\tLoss 3.2322 (3.0834)\tPrec@1 78.814 (79.881)\tPrec@5 98.305 (98.396))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.7640 (11.2774)\tPrec@1 43.000 (41.198)\tPrec@5 88.000 (91.352)\n",
      "val Results: Prec@1 41.230 Prec@5 91.420 Loss 11.30581\n",
      "val Class Accuracy: [0.898,0.974,0.836,0.555,0.390,0.384,0.049,0.036,0.001,0.000]\n",
      "Best Prec@1: 46.690\n",
      "\n",
      "Epoch: [27][96/97], lr: 0.01000\tTime 0.307 (0.319)\tData 0.000 (0.020)\tLoss 3.3726 (3.0896)\tPrec@1 80.508 (80.219)\tPrec@5 99.153 (98.468))\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 10.4191 (10.2951)\tPrec@1 40.000 (45.286)\tPrec@5 94.000 (93.407)\n",
      "val Results: Prec@1 45.390 Prec@5 93.400 Loss 10.30005\n",
      "val Class Accuracy: [0.958,0.982,0.648,0.764,0.514,0.319,0.306,0.033,0.012,0.003]\n",
      "Best Prec@1: 46.690\n",
      "\n",
      "Epoch: [28][96/97], lr: 0.01000\tTime 0.309 (0.319)\tData 0.000 (0.020)\tLoss 2.3999 (3.0293)\tPrec@1 85.593 (80.759)\tPrec@5 99.153 (98.315)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.2892 (10.4507)\tPrec@1 45.000 (45.143)\tPrec@5 97.000 (94.099)\n",
      "val Results: Prec@1 45.110 Prec@5 94.170 Loss 10.46705\n",
      "val Class Accuracy: [0.915,0.984,0.811,0.577,0.402,0.289,0.291,0.239,0.002,0.001]\n",
      "Best Prec@1: 46.690\n",
      "\n",
      "Epoch: [29][96/97], lr: 0.01000\tTime 0.307 (0.320)\tData 0.000 (0.020)\tLoss 2.9088 (2.9544)\tPrec@1 81.356 (81.348)\tPrec@5 98.305 (98.468))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.3616 (10.5099)\tPrec@1 45.000 (46.725)\tPrec@5 86.000 (88.989)\n",
      "val Results: Prec@1 46.620 Prec@5 88.930 Loss 10.54230\n",
      "val Class Accuracy: [0.868,0.924,0.719,0.386,0.728,0.315,0.679,0.041,0.002,0.000]\n",
      "Best Prec@1: 46.690\n",
      "\n",
      "Epoch: [30][96/97], lr: 0.01000\tTime 0.307 (0.319)\tData 0.000 (0.020)\tLoss 3.4819 (2.9027)\tPrec@1 77.119 (81.686)\tPrec@5 98.305 (98.751))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.7742 (11.1393)\tPrec@1 40.000 (42.308)\tPrec@5 90.000 (93.143)\n",
      "val Results: Prec@1 42.310 Prec@5 93.260 Loss 11.14392\n",
      "val Class Accuracy: [0.876,0.969,0.665,0.743,0.103,0.507,0.317,0.049,0.000,0.002]\n",
      "Best Prec@1: 46.690\n",
      "\n",
      "Epoch: [31][96/97], lr: 0.01000\tTime 0.310 (0.319)\tData 0.000 (0.020)\tLoss 3.1855 (2.8989)\tPrec@1 78.814 (81.549)\tPrec@5 98.305 (98.565))\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 11.9726 (11.9401)\tPrec@1 40.000 (40.560)\tPrec@5 92.000 (89.846)\n",
      "val Results: Prec@1 40.530 Prec@5 89.880 Loss 11.97286\n",
      "val Class Accuracy: [0.774,0.978,0.920,0.558,0.336,0.003,0.456,0.009,0.019,0.000]\n",
      "Best Prec@1: 46.690\n",
      "\n",
      "Epoch: [32][96/97], lr: 0.01000\tTime 0.311 (0.320)\tData 0.000 (0.020)\tLoss 3.2945 (2.9128)\tPrec@1 80.508 (81.525)\tPrec@5 97.458 (98.606))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.3803 (10.4780)\tPrec@1 49.000 (45.659)\tPrec@5 96.000 (93.747)\n",
      "val Results: Prec@1 45.590 Prec@5 93.750 Loss 10.49767\n",
      "val Class Accuracy: [0.950,0.994,0.688,0.331,0.657,0.314,0.336,0.279,0.007,0.003]\n",
      "Best Prec@1: 46.690\n",
      "\n",
      "Epoch: [33][96/97], lr: 0.01000\tTime 0.309 (0.320)\tData 0.000 (0.020)\tLoss 3.3033 (2.9040)\tPrec@1 80.508 (81.396)\tPrec@5 96.610 (98.388))\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 11.2027 (11.4165)\tPrec@1 42.000 (41.077)\tPrec@5 97.000 (93.527)\n",
      "val Results: Prec@1 41.090 Prec@5 93.550 Loss 11.43823\n",
      "val Class Accuracy: [0.926,0.992,0.852,0.595,0.385,0.224,0.065,0.070,0.000,0.000]\n",
      "Best Prec@1: 46.690\n",
      "\n",
      "Epoch: [34][96/97], lr: 0.01000\tTime 0.305 (0.320)\tData 0.000 (0.021)\tLoss 2.4654 (2.7975)\tPrec@1 84.746 (82.259)\tPrec@5 98.305 (98.815))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.0077 (10.6100)\tPrec@1 53.000 (47.220)\tPrec@5 92.000 (90.121)\n",
      "val Results: Prec@1 47.090 Prec@5 90.070 Loss 10.63852\n",
      "val Class Accuracy: [0.952,0.904,0.808,0.479,0.355,0.348,0.622,0.239,0.002,0.000]\n",
      "Best Prec@1: 47.090\n",
      "\n",
      "Epoch: [35][96/97], lr: 0.01000\tTime 0.309 (0.321)\tData 0.000 (0.020)\tLoss 3.3604 (2.8411)\tPrec@1 77.119 (82.267)\tPrec@5 99.153 (98.622)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 9.7260 (10.9508)\tPrec@1 48.000 (42.802)\tPrec@5 92.000 (88.681))\n",
      "val Results: Prec@1 42.840 Prec@5 88.660 Loss 10.97451\n",
      "val Class Accuracy: [0.868,0.955,0.911,0.429,0.386,0.377,0.336,0.019,0.001,0.002]\n",
      "Best Prec@1: 47.090\n",
      "\n",
      "Epoch: [36][96/97], lr: 0.01000\tTime 0.307 (0.318)\tData 0.000 (0.020)\tLoss 3.1926 (2.8195)\tPrec@1 78.814 (82.259)\tPrec@5 97.458 (98.767)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.3145 (10.0108)\tPrec@1 50.000 (48.978)\tPrec@5 96.000 (92.527)\n",
      "val Results: Prec@1 48.810 Prec@5 92.550 Loss 10.02751\n",
      "val Class Accuracy: [0.854,0.943,0.584,0.763,0.471,0.235,0.851,0.143,0.034,0.003]\n",
      "Best Prec@1: 48.810\n",
      "\n",
      "Epoch: [37][96/97], lr: 0.01000\tTime 0.309 (0.321)\tData 0.000 (0.020)\tLoss 3.2519 (2.7909)\tPrec@1 77.966 (82.468)\tPrec@5 100.000 (98.815)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 10.0549 (10.6102)\tPrec@1 51.000 (46.681)\tPrec@5 98.000 (93.802)\n",
      "val Results: Prec@1 46.670 Prec@5 93.650 Loss 10.63373\n",
      "val Class Accuracy: [0.870,0.986,0.909,0.427,0.398,0.204,0.505,0.366,0.001,0.001]\n",
      "Best Prec@1: 48.810\n",
      "\n",
      "Epoch: [38][96/97], lr: 0.01000\tTime 0.307 (0.320)\tData 0.000 (0.020)\tLoss 2.7075 (2.7232)\tPrec@1 83.051 (82.992)\tPrec@5 98.305 (98.807))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.1295 (10.7199)\tPrec@1 48.000 (45.516)\tPrec@5 94.000 (91.945)\n",
      "val Results: Prec@1 45.440 Prec@5 91.990 Loss 10.74993\n",
      "val Class Accuracy: [0.907,0.979,0.849,0.425,0.542,0.603,0.083,0.130,0.025,0.001]\n",
      "Best Prec@1: 48.810\n",
      "\n",
      "Epoch: [39][96/97], lr: 0.01000\tTime 0.310 (0.320)\tData 0.000 (0.020)\tLoss 2.3863 (2.7447)\tPrec@1 87.288 (82.549)\tPrec@5 100.000 (98.597)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.2082 (9.3745)\tPrec@1 53.000 (51.692)\tPrec@5 97.000 (95.165))\n",
      "val Results: Prec@1 51.750 Prec@5 95.100 Loss 9.39066\n",
      "val Class Accuracy: [0.953,0.982,0.782,0.603,0.407,0.396,0.608,0.430,0.007,0.007]\n",
      "Best Prec@1: 51.750\n",
      "\n",
      "Epoch: [40][96/97], lr: 0.01000\tTime 0.310 (0.319)\tData 0.000 (0.020)\tLoss 2.4416 (2.7294)\tPrec@1 82.203 (82.629)\tPrec@5 98.305 (98.743))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.5029 (10.6672)\tPrec@1 44.000 (44.989)\tPrec@5 98.000 (94.286)\n",
      "val Results: Prec@1 44.960 Prec@5 94.260 Loss 10.68885\n",
      "val Class Accuracy: [0.963,0.950,0.841,0.514,0.582,0.197,0.161,0.270,0.018,0.000]\n",
      "Best Prec@1: 51.750\n",
      "\n",
      "Epoch: [41][96/97], lr: 0.01000\tTime 0.306 (0.320)\tData 0.000 (0.021)\tLoss 2.5973 (2.6635)\tPrec@1 83.898 (83.371)\tPrec@5 97.458 (98.863))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.7682 (9.0247)\tPrec@1 55.000 (53.429)\tPrec@5 98.000 (96.264))\n",
      "val Results: Prec@1 53.300 Prec@5 96.330 Loss 9.04286\n",
      "val Class Accuracy: [0.942,0.990,0.701,0.658,0.611,0.450,0.528,0.406,0.010,0.034]\n",
      "Best Prec@1: 53.300\n",
      "\n",
      "Epoch: [42][96/97], lr: 0.01000\tTime 0.306 (0.321)\tData 0.000 (0.020)\tLoss 2.1098 (2.6342)\tPrec@1 84.746 (83.516)\tPrec@5 99.153 (98.815))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.0119 (10.7312)\tPrec@1 50.000 (45.011)\tPrec@5 98.000 (94.011)\n",
      "val Results: Prec@1 45.010 Prec@5 93.910 Loss 10.76166\n",
      "val Class Accuracy: [0.900,0.939,0.903,0.534,0.500,0.229,0.284,0.205,0.006,0.001]\n",
      "Best Prec@1: 53.300\n",
      "\n",
      "Epoch: [43][96/97], lr: 0.01000\tTime 0.307 (0.319)\tData 0.000 (0.021)\tLoss 2.1966 (2.6335)\tPrec@1 86.441 (83.734)\tPrec@5 100.000 (98.976)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.6890 (9.3042)\tPrec@1 55.000 (51.615)\tPrec@5 96.000 (95.945))\n",
      "val Results: Prec@1 51.640 Prec@5 96.010 Loss 9.31704\n",
      "val Class Accuracy: [0.857,0.987,0.775,0.736,0.420,0.353,0.559,0.373,0.090,0.014]\n",
      "Best Prec@1: 53.300\n",
      "\n",
      "Epoch: [44][96/97], lr: 0.01000\tTime 0.310 (0.321)\tData 0.000 (0.020)\tLoss 2.4120 (2.6865)\tPrec@1 83.898 (83.097)\tPrec@5 99.153 (98.702))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.7517 (10.2263)\tPrec@1 49.000 (47.516)\tPrec@5 92.000 (92.319))\n",
      "val Results: Prec@1 47.490 Prec@5 92.270 Loss 10.24153\n",
      "val Class Accuracy: [0.924,0.939,0.759,0.810,0.604,0.475,0.031,0.166,0.032,0.009]\n",
      "Best Prec@1: 53.300\n",
      "\n",
      "Epoch: [45][96/97], lr: 0.01000\tTime 0.306 (0.320)\tData 0.000 (0.020)\tLoss 2.6980 (2.6225)\tPrec@1 83.051 (83.532)\tPrec@5 95.763 (98.888))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.3722 (9.1128)\tPrec@1 61.000 (53.868)\tPrec@5 95.000 (94.835))\n",
      "val Results: Prec@1 53.790 Prec@5 94.860 Loss 9.13536\n",
      "val Class Accuracy: [0.917,0.981,0.794,0.574,0.668,0.179,0.645,0.576,0.003,0.042]\n",
      "Best Prec@1: 53.790\n",
      "\n",
      "Epoch: [46][96/97], lr: 0.01000\tTime 0.306 (0.320)\tData 0.000 (0.020)\tLoss 3.0987 (2.5647)\tPrec@1 81.356 (83.992)\tPrec@5 100.000 (98.888)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.5207 (9.8236)\tPrec@1 50.000 (50.879)\tPrec@5 93.000 (91.912))\n",
      "val Results: Prec@1 50.940 Prec@5 91.890 Loss 9.83117\n",
      "val Class Accuracy: [0.778,0.946,0.679,0.514,0.602,0.850,0.345,0.355,0.021,0.004]\n",
      "Best Prec@1: 53.790\n",
      "\n",
      "Epoch: [47][96/97], lr: 0.01000\tTime 0.306 (0.319)\tData 0.000 (0.020)\tLoss 2.8017 (2.5253)\tPrec@1 80.508 (84.169)\tPrec@5 98.305 (98.904)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 9.6536 (9.0986)\tPrec@1 46.000 (52.736)\tPrec@5 99.000 (96.220))\n",
      "val Results: Prec@1 52.640 Prec@5 96.250 Loss 9.12023\n",
      "val Class Accuracy: [0.969,0.972,0.583,0.714,0.715,0.424,0.385,0.438,0.050,0.014]\n",
      "Best Prec@1: 53.790\n",
      "\n",
      "Epoch: [48][96/97], lr: 0.01000\tTime 0.309 (0.321)\tData 0.000 (0.020)\tLoss 2.5922 (2.5846)\tPrec@1 85.593 (84.096)\tPrec@5 100.000 (98.743)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.3818 (10.3976)\tPrec@1 49.000 (48.495)\tPrec@5 93.000 (92.363)\n",
      "val Results: Prec@1 48.470 Prec@5 92.270 Loss 10.42412\n",
      "val Class Accuracy: [0.901,0.966,0.888,0.481,0.549,0.225,0.627,0.189,0.021,0.000]\n",
      "Best Prec@1: 53.790\n",
      "\n",
      "Epoch: [49][96/97], lr: 0.01000\tTime 0.310 (0.319)\tData 0.000 (0.020)\tLoss 3.5395 (2.5291)\tPrec@1 76.271 (84.258)\tPrec@5 98.305 (98.952))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.6094 (9.5602)\tPrec@1 50.000 (51.429)\tPrec@5 94.000 (94.154))\n",
      "val Results: Prec@1 51.530 Prec@5 94.230 Loss 9.55928\n",
      "val Class Accuracy: [0.953,0.934,0.747,0.770,0.582,0.339,0.307,0.505,0.005,0.011]\n",
      "Best Prec@1: 53.790\n",
      "\n",
      "Epoch: [50][96/97], lr: 0.01000\tTime 0.311 (0.323)\tData 0.000 (0.020)\tLoss 3.4646 (2.5917)\tPrec@1 78.814 (83.879)\tPrec@5 98.305 (98.976))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.1535 (8.8623)\tPrec@1 51.000 (54.692)\tPrec@5 98.000 (96.209)\n",
      "val Results: Prec@1 54.470 Prec@5 96.250 Loss 8.88750\n",
      "val Class Accuracy: [0.979,0.978,0.717,0.623,0.698,0.456,0.569,0.360,0.057,0.010]\n",
      "Best Prec@1: 54.470\n",
      "\n",
      "Epoch: [51][96/97], lr: 0.01000\tTime 0.307 (0.321)\tData 0.000 (0.020)\tLoss 2.4393 (2.4802)\tPrec@1 86.441 (84.653)\tPrec@5 98.305 (98.960))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.7684 (10.1873)\tPrec@1 53.000 (48.934)\tPrec@5 95.000 (93.978))\n",
      "val Results: Prec@1 49.020 Prec@5 94.000 Loss 10.19312\n",
      "val Class Accuracy: [0.878,0.982,0.731,0.749,0.546,0.594,0.283,0.076,0.063,0.000]\n",
      "Best Prec@1: 54.470\n",
      "\n",
      "Epoch: [52][96/97], lr: 0.01000\tTime 0.308 (0.320)\tData 0.000 (0.020)\tLoss 2.7581 (2.5846)\tPrec@1 81.356 (83.927)\tPrec@5 98.305 (98.638))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.5049 (8.9074)\tPrec@1 62.000 (54.670)\tPrec@5 97.000 (95.934)\n",
      "val Results: Prec@1 54.610 Prec@5 96.000 Loss 8.92367\n",
      "val Class Accuracy: [0.944,0.974,0.793,0.565,0.352,0.568,0.672,0.556,0.034,0.003]\n",
      "Best Prec@1: 54.610\n",
      "\n",
      "Epoch: [53][96/97], lr: 0.01000\tTime 0.308 (0.320)\tData 0.000 (0.021)\tLoss 2.0966 (2.4887)\tPrec@1 86.441 (84.749)\tPrec@5 99.153 (98.960))\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 9.6109 (9.9978)\tPrec@1 56.000 (51.747)\tPrec@5 91.000 (91.000)))\n",
      "val Results: Prec@1 51.770 Prec@5 90.890 Loss 10.01027\n",
      "val Class Accuracy: [0.931,0.963,0.839,0.298,0.613,0.606,0.618,0.304,0.003,0.002]\n",
      "Best Prec@1: 54.610\n",
      "\n",
      "Epoch: [54][96/97], lr: 0.01000\tTime 0.308 (0.321)\tData 0.000 (0.020)\tLoss 3.2095 (2.4498)\tPrec@1 77.966 (84.814)\tPrec@5 96.610 (98.912))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.6285 (9.5350)\tPrec@1 51.000 (52.231)\tPrec@5 96.000 (93.978))\n",
      "val Results: Prec@1 52.230 Prec@5 93.970 Loss 9.55788\n",
      "val Class Accuracy: [0.970,0.964,0.664,0.585,0.732,0.578,0.137,0.541,0.040,0.012]\n",
      "Best Prec@1: 54.610\n",
      "\n",
      "Epoch: [55][96/97], lr: 0.01000\tTime 0.306 (0.319)\tData 0.000 (0.020)\tLoss 2.8776 (2.4677)\tPrec@1 81.356 (84.661)\tPrec@5 100.000 (99.089)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 8.2828 (7.8494)\tPrec@1 56.000 (59.462)\tPrec@5 99.000 (96.033)\n",
      "val Results: Prec@1 59.510 Prec@5 96.070 Loss 7.85659\n",
      "val Class Accuracy: [0.952,0.955,0.506,0.457,0.733,0.510,0.761,0.685,0.292,0.100]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [56][96/97], lr: 0.01000\tTime 0.310 (0.320)\tData 0.000 (0.020)\tLoss 2.6770 (2.4920)\tPrec@1 82.203 (84.548)\tPrec@5 99.153 (98.888))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.4058 (8.5514)\tPrec@1 57.000 (57.077)\tPrec@5 97.000 (95.945))\n",
      "val Results: Prec@1 57.140 Prec@5 96.010 Loss 8.56747\n",
      "val Class Accuracy: [0.949,0.991,0.717,0.427,0.647,0.785,0.575,0.562,0.058,0.003]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [57][96/97], lr: 0.01000\tTime 0.308 (0.319)\tData 0.000 (0.020)\tLoss 2.0754 (2.3846)\tPrec@1 87.288 (85.354)\tPrec@5 100.000 (98.992)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.4859 (8.7346)\tPrec@1 59.000 (56.604)\tPrec@5 98.000 (96.516)\n",
      "val Results: Prec@1 56.550 Prec@5 96.600 Loss 8.74635\n",
      "val Class Accuracy: [0.904,0.985,0.823,0.650,0.554,0.415,0.564,0.559,0.195,0.006]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [58][96/97], lr: 0.01000\tTime 0.311 (0.321)\tData 0.000 (0.020)\tLoss 2.4949 (2.4494)\tPrec@1 82.203 (84.838)\tPrec@5 99.153 (98.960))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.2356 (9.9412)\tPrec@1 46.000 (48.912)\tPrec@5 98.000 (95.418))\n",
      "val Results: Prec@1 48.820 Prec@5 95.420 Loss 9.96115\n",
      "val Class Accuracy: [0.981,0.965,0.509,0.506,0.388,0.642,0.522,0.349,0.015,0.005]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [59][96/97], lr: 0.01000\tTime 0.308 (0.321)\tData 0.000 (0.020)\tLoss 2.3723 (2.3860)\tPrec@1 87.288 (85.112)\tPrec@5 99.153 (99.089))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.2816 (9.8718)\tPrec@1 53.000 (49.462)\tPrec@5 98.000 (95.604))\n",
      "val Results: Prec@1 49.600 Prec@5 95.660 Loss 9.87830\n",
      "val Class Accuracy: [0.950,0.977,0.841,0.573,0.445,0.232,0.378,0.480,0.079,0.005]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [60][96/97], lr: 0.01000\tTime 0.307 (0.321)\tData 0.000 (0.020)\tLoss 2.2237 (2.4679)\tPrec@1 85.593 (84.757)\tPrec@5 99.153 (99.065))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.2531 (8.4234)\tPrec@1 61.000 (57.099)\tPrec@5 99.000 (96.615)\n",
      "val Results: Prec@1 56.990 Prec@5 96.650 Loss 8.44200\n",
      "val Class Accuracy: [0.959,0.979,0.691,0.750,0.618,0.337,0.669,0.496,0.174,0.026]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [61][96/97], lr: 0.01000\tTime 0.307 (0.319)\tData 0.000 (0.020)\tLoss 2.6050 (2.3746)\tPrec@1 82.203 (85.434)\tPrec@5 98.305 (98.928))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.2236 (9.2533)\tPrec@1 55.000 (54.110)\tPrec@5 97.000 (95.912))\n",
      "val Results: Prec@1 53.970 Prec@5 96.010 Loss 9.28349\n",
      "val Class Accuracy: [0.953,0.989,0.781,0.470,0.717,0.507,0.381,0.516,0.078,0.005]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [62][96/97], lr: 0.01000\tTime 0.311 (0.323)\tData 0.000 (0.021)\tLoss 2.1457 (2.3621)\tPrec@1 84.746 (85.563)\tPrec@5 97.458 (99.081))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.0016 (8.7189)\tPrec@1 56.000 (57.242)\tPrec@5 93.000 (94.077)\n",
      "val Results: Prec@1 57.080 Prec@5 93.970 Loss 8.76212\n",
      "val Class Accuracy: [0.837,0.937,0.819,0.570,0.870,0.489,0.499,0.448,0.233,0.006]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [63][96/97], lr: 0.01000\tTime 0.320 (0.324)\tData 0.000 (0.020)\tLoss 3.3314 (2.2964)\tPrec@1 79.661 (85.926)\tPrec@5 100.000 (99.041)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 8.4847 (9.0547)\tPrec@1 63.000 (55.560)\tPrec@5 98.000 (95.385))\n",
      "val Results: Prec@1 55.520 Prec@5 95.240 Loss 9.08332\n",
      "val Class Accuracy: [0.985,0.935,0.649,0.624,0.792,0.485,0.701,0.209,0.166,0.006]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [64][96/97], lr: 0.01000\tTime 0.308 (0.334)\tData 0.000 (0.025)\tLoss 2.9773 (2.3678)\tPrec@1 81.356 (85.459)\tPrec@5 99.153 (99.154))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.3002 (10.5535)\tPrec@1 48.000 (47.055)\tPrec@5 95.000 (92.022)\n",
      "val Results: Prec@1 46.930 Prec@5 91.950 Loss 10.57261\n",
      "val Class Accuracy: [0.903,0.987,0.849,0.617,0.323,0.644,0.227,0.045,0.096,0.002]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [65][96/97], lr: 0.01000\tTime 0.310 (0.322)\tData 0.000 (0.021)\tLoss 2.3196 (2.3575)\tPrec@1 84.746 (85.539)\tPrec@5 98.305 (98.984))\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 8.8161 (8.7305)\tPrec@1 55.000 (55.143)\tPrec@5 95.000 (95.626)\n",
      "val Results: Prec@1 55.010 Prec@5 95.560 Loss 8.76581\n",
      "val Class Accuracy: [0.962,0.973,0.651,0.590,0.853,0.584,0.139,0.294,0.353,0.102]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [66][96/97], lr: 0.01000\tTime 0.305 (0.322)\tData 0.000 (0.022)\tLoss 2.4658 (2.3393)\tPrec@1 81.356 (85.668)\tPrec@5 100.000 (99.113)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.3781 (8.7123)\tPrec@1 59.000 (57.297)\tPrec@5 91.000 (90.209)\n",
      "val Results: Prec@1 57.270 Prec@5 90.190 Loss 8.71925\n",
      "val Class Accuracy: [0.753,0.874,0.642,0.640,0.839,0.581,0.828,0.111,0.418,0.041]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [67][96/97], lr: 0.01000\tTime 0.308 (0.320)\tData 0.000 (0.020)\tLoss 3.1153 (2.3550)\tPrec@1 83.051 (85.531)\tPrec@5 98.305 (99.009))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.3325 (10.1240)\tPrec@1 48.000 (50.000)\tPrec@5 98.000 (93.648)\n",
      "val Results: Prec@1 49.990 Prec@5 93.670 Loss 10.13550\n",
      "val Class Accuracy: [0.985,0.946,0.602,0.652,0.516,0.403,0.449,0.384,0.061,0.001]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [68][96/97], lr: 0.01000\tTime 0.305 (0.318)\tData 0.000 (0.020)\tLoss 2.5996 (2.3168)\tPrec@1 84.746 (85.459)\tPrec@5 98.305 (99.121))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.3166 (8.5531)\tPrec@1 61.000 (56.714)\tPrec@5 99.000 (96.363)\n",
      "val Results: Prec@1 56.800 Prec@5 96.350 Loss 8.55758\n",
      "val Class Accuracy: [0.963,0.981,0.544,0.695,0.532,0.708,0.654,0.364,0.213,0.026]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [69][96/97], lr: 0.01000\tTime 0.312 (0.322)\tData 0.000 (0.021)\tLoss 2.3485 (2.3475)\tPrec@1 83.898 (85.467)\tPrec@5 99.153 (99.146))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.3090 (10.3101)\tPrec@1 50.000 (50.297)\tPrec@5 90.000 (89.714)\n",
      "val Results: Prec@1 50.250 Prec@5 89.720 Loss 10.33905\n",
      "val Class Accuracy: [0.930,0.990,0.821,0.669,0.435,0.502,0.572,0.102,0.003,0.001]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [70][96/97], lr: 0.01000\tTime 0.307 (0.319)\tData 0.000 (0.021)\tLoss 1.5545 (2.3308)\tPrec@1 89.831 (85.813)\tPrec@5 100.000 (99.073)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.9661 (9.9932)\tPrec@1 48.000 (49.132)\tPrec@5 95.000 (93.890)))\n",
      "val Results: Prec@1 49.060 Prec@5 93.960 Loss 10.00115\n",
      "val Class Accuracy: [0.923,0.991,0.786,0.604,0.208,0.379,0.522,0.357,0.132,0.004]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [71][96/97], lr: 0.01000\tTime 0.312 (0.324)\tData 0.000 (0.020)\tLoss 2.8639 (2.3011)\tPrec@1 80.508 (85.749)\tPrec@5 98.305 (99.226))\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 8.9107 (8.6548)\tPrec@1 50.000 (56.264)\tPrec@5 99.000 (95.429)\n",
      "val Results: Prec@1 56.250 Prec@5 95.400 Loss 8.66384\n",
      "val Class Accuracy: [0.965,0.990,0.542,0.539,0.597,0.408,0.688,0.535,0.357,0.004]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [72][96/97], lr: 0.01000\tTime 0.310 (0.319)\tData 0.000 (0.020)\tLoss 2.3442 (2.2558)\tPrec@1 85.593 (86.112)\tPrec@5 98.305 (99.049))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.1996 (8.3843)\tPrec@1 60.000 (58.824)\tPrec@5 97.000 (95.725)\n",
      "val Results: Prec@1 58.870 Prec@5 95.790 Loss 8.40061\n",
      "val Class Accuracy: [0.960,0.986,0.636,0.734,0.715,0.477,0.743,0.555,0.067,0.014]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [73][96/97], lr: 0.01000\tTime 0.308 (0.320)\tData 0.000 (0.020)\tLoss 2.4153 (2.2521)\tPrec@1 83.898 (86.087)\tPrec@5 97.458 (99.081))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.7116 (9.1529)\tPrec@1 51.000 (55.571)\tPrec@5 96.000 (96.242))\n",
      "val Results: Prec@1 55.520 Prec@5 96.260 Loss 9.14665\n",
      "val Class Accuracy: [0.918,0.973,0.548,0.833,0.367,0.705,0.295,0.548,0.305,0.060]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [74][96/97], lr: 0.01000\tTime 0.307 (0.319)\tData 0.000 (0.020)\tLoss 2.2915 (2.2857)\tPrec@1 88.136 (86.087)\tPrec@5 98.305 (99.065))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.1733 (8.3503)\tPrec@1 57.000 (59.253)\tPrec@5 95.000 (96.308)\n",
      "val Results: Prec@1 59.110 Prec@5 96.380 Loss 8.38366\n",
      "val Class Accuracy: [0.955,0.989,0.479,0.558,0.676,0.710,0.636,0.611,0.264,0.033]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [75][96/97], lr: 0.01000\tTime 0.310 (0.321)\tData 0.000 (0.020)\tLoss 2.7417 (2.2246)\tPrec@1 81.356 (86.539)\tPrec@5 98.305 (99.154))\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 9.0133 (8.6834)\tPrec@1 58.000 (58.066)\tPrec@5 98.000 (95.187))\n",
      "val Results: Prec@1 57.970 Prec@5 95.130 Loss 8.71822\n",
      "val Class Accuracy: [0.963,0.982,0.683,0.429,0.851,0.646,0.348,0.677,0.215,0.003]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [76][96/97], lr: 0.01000\tTime 0.309 (0.320)\tData 0.000 (0.020)\tLoss 2.6442 (2.2354)\tPrec@1 85.593 (86.297)\tPrec@5 99.153 (99.065))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.0871 (9.4120)\tPrec@1 60.000 (54.615)\tPrec@5 94.000 (92.505))\n",
      "val Results: Prec@1 54.650 Prec@5 92.560 Loss 9.39993\n",
      "val Class Accuracy: [0.955,0.966,0.691,0.468,0.563,0.875,0.277,0.483,0.183,0.004]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [77][96/97], lr: 0.01000\tTime 0.312 (0.319)\tData 0.000 (0.020)\tLoss 2.1744 (2.2633)\tPrec@1 88.983 (86.313)\tPrec@5 98.305 (99.081))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.9683 (10.2793)\tPrec@1 52.000 (49.143)\tPrec@5 95.000 (91.835))\n",
      "val Results: Prec@1 48.890 Prec@5 91.680 Loss 10.32575\n",
      "val Class Accuracy: [0.946,0.943,0.887,0.666,0.594,0.423,0.230,0.036,0.162,0.002]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [78][96/97], lr: 0.01000\tTime 0.309 (0.321)\tData 0.000 (0.020)\tLoss 1.6129 (2.2532)\tPrec@1 90.678 (86.216)\tPrec@5 100.000 (99.065)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.8262 (9.9011)\tPrec@1 51.000 (51.868)\tPrec@5 99.000 (96.374)))\n",
      "val Results: Prec@1 52.000 Prec@5 96.410 Loss 9.90518\n",
      "val Class Accuracy: [0.859,0.989,0.858,0.841,0.527,0.365,0.251,0.242,0.265,0.003]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [79][96/97], lr: 0.01000\tTime 0.313 (0.321)\tData 0.000 (0.020)\tLoss 1.7958 (2.2346)\tPrec@1 91.525 (86.515)\tPrec@5 99.153 (99.234))\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 9.9055 (10.0197)\tPrec@1 49.000 (50.000)\tPrec@5 93.000 (93.319))\n",
      "val Results: Prec@1 49.950 Prec@5 93.400 Loss 10.03260\n",
      "val Class Accuracy: [0.972,0.973,0.745,0.612,0.275,0.458,0.169,0.571,0.214,0.006]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [80][96/97], lr: 0.01000\tTime 0.309 (0.321)\tData 0.000 (0.020)\tLoss 2.8434 (2.2799)\tPrec@1 82.203 (85.934)\tPrec@5 100.000 (99.065)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.9111 (8.6295)\tPrec@1 57.000 (57.593)\tPrec@5 93.000 (93.769))\n",
      "val Results: Prec@1 57.730 Prec@5 93.750 Loss 8.62780\n",
      "val Class Accuracy: [0.906,0.950,0.688,0.728,0.642,0.777,0.573,0.299,0.177,0.033]\n",
      "Best Prec@1: 59.510\n",
      "\n",
      "Epoch: [81][96/97], lr: 0.01000\tTime 0.306 (0.319)\tData 0.000 (0.020)\tLoss 2.4045 (2.1934)\tPrec@1 87.288 (86.547)\tPrec@5 100.000 (99.178)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 7.7082 (7.8586)\tPrec@1 60.000 (60.978)\tPrec@5 97.000 (95.637)\n",
      "val Results: Prec@1 61.030 Prec@5 95.670 Loss 7.87217\n",
      "val Class Accuracy: [0.875,0.942,0.801,0.702,0.579,0.669,0.664,0.290,0.463,0.118]\n",
      "Best Prec@1: 61.030\n",
      "\n",
      "Epoch: [82][96/97], lr: 0.01000\tTime 0.307 (0.322)\tData 0.000 (0.020)\tLoss 2.0586 (2.1552)\tPrec@1 86.441 (86.619)\tPrec@5 99.153 (99.138))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.0807 (9.5171)\tPrec@1 58.000 (52.593)\tPrec@5 97.000 (94.769))\n",
      "val Results: Prec@1 52.700 Prec@5 94.740 Loss 9.51770\n",
      "val Class Accuracy: [0.874,0.977,0.868,0.499,0.267,0.327,0.778,0.351,0.288,0.041]\n",
      "Best Prec@1: 61.030\n",
      "\n",
      "Epoch: [83][96/97], lr: 0.01000\tTime 0.309 (0.320)\tData 0.000 (0.020)\tLoss 2.0990 (2.2365)\tPrec@1 87.288 (86.402)\tPrec@5 99.153 (99.275))\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 9.8262 (9.3164)\tPrec@1 54.000 (55.626)\tPrec@5 96.000 (95.571)\n",
      "val Results: Prec@1 55.640 Prec@5 95.630 Loss 9.32958\n",
      "val Class Accuracy: [0.900,0.998,0.629,0.621,0.606,0.607,0.443,0.446,0.311,0.003]\n",
      "Best Prec@1: 61.030\n",
      "\n",
      "Epoch: [84][96/97], lr: 0.01000\tTime 0.310 (0.319)\tData 0.000 (0.020)\tLoss 2.4229 (2.1865)\tPrec@1 84.746 (86.644)\tPrec@5 98.305 (99.250))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.2686 (8.3074)\tPrec@1 58.000 (58.176)\tPrec@5 98.000 (95.945)\n",
      "val Results: Prec@1 58.130 Prec@5 96.000 Loss 8.31109\n",
      "val Class Accuracy: [0.967,0.989,0.720,0.731,0.587,0.495,0.341,0.626,0.261,0.096]\n",
      "Best Prec@1: 61.030\n",
      "\n",
      "Epoch: [85][96/97], lr: 0.01000\tTime 0.312 (0.321)\tData 0.000 (0.020)\tLoss 1.8500 (2.1995)\tPrec@1 87.288 (86.450)\tPrec@5 100.000 (99.170)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 9.2691 (8.9287)\tPrec@1 56.000 (56.813)\tPrec@5 97.000 (94.967))\n",
      "val Results: Prec@1 56.730 Prec@5 94.890 Loss 8.94918\n",
      "val Class Accuracy: [0.951,0.969,0.843,0.573,0.605,0.605,0.489,0.449,0.157,0.032]\n",
      "Best Prec@1: 61.030\n",
      "\n",
      "Epoch: [86][96/97], lr: 0.01000\tTime 0.311 (0.320)\tData 0.000 (0.020)\tLoss 2.4363 (2.2071)\tPrec@1 85.593 (86.442)\tPrec@5 99.153 (99.339))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.5693 (8.7744)\tPrec@1 60.000 (56.033)\tPrec@5 96.000 (95.209)\n",
      "val Results: Prec@1 56.100 Prec@5 95.190 Loss 8.77053\n",
      "val Class Accuracy: [0.880,0.965,0.898,0.537,0.611,0.529,0.386,0.518,0.264,0.022]\n",
      "Best Prec@1: 61.030\n",
      "\n",
      "Epoch: [87][96/97], lr: 0.01000\tTime 0.310 (0.319)\tData 0.000 (0.020)\tLoss 1.9285 (2.1841)\tPrec@1 88.136 (86.789)\tPrec@5 99.153 (99.170))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.7036 (7.7870)\tPrec@1 52.000 (61.626)\tPrec@5 99.000 (96.956))\n",
      "val Results: Prec@1 61.770 Prec@5 97.000 Loss 7.77630\n",
      "val Class Accuracy: [0.863,0.976,0.566,0.906,0.660,0.478,0.562,0.492,0.587,0.087]\n",
      "Best Prec@1: 61.770\n",
      "\n",
      "Epoch: [88][96/97], lr: 0.01000\tTime 0.310 (0.321)\tData 0.000 (0.020)\tLoss 2.5924 (2.1718)\tPrec@1 86.441 (86.716)\tPrec@5 99.153 (99.250))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.1107 (10.4037)\tPrec@1 47.000 (49.923)\tPrec@5 96.000 (92.835)\n",
      "val Results: Prec@1 49.890 Prec@5 92.810 Loss 10.41688\n",
      "val Class Accuracy: [0.882,0.990,0.671,0.522,0.397,0.188,0.897,0.124,0.314,0.004]\n",
      "Best Prec@1: 61.770\n",
      "\n",
      "Epoch: [89][96/97], lr: 0.01000\tTime 0.307 (0.319)\tData 0.000 (0.020)\tLoss 2.5609 (2.2072)\tPrec@1 83.898 (86.458)\tPrec@5 99.153 (99.258))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.6451 (8.0438)\tPrec@1 63.000 (60.154)\tPrec@5 98.000 (95.495)\n",
      "val Results: Prec@1 60.230 Prec@5 95.460 Loss 8.04146\n",
      "val Class Accuracy: [0.956,0.990,0.720,0.574,0.489,0.654,0.613,0.497,0.435,0.095]\n",
      "Best Prec@1: 61.770\n",
      "\n",
      "Epoch: [90][96/97], lr: 0.01000\tTime 0.307 (0.320)\tData 0.000 (0.020)\tLoss 2.0824 (2.0418)\tPrec@1 88.136 (87.506)\tPrec@5 100.000 (99.202)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.0576 (8.1130)\tPrec@1 61.000 (59.758)\tPrec@5 96.000 (95.143)\n",
      "val Results: Prec@1 59.890 Prec@5 95.230 Loss 8.09816\n",
      "val Class Accuracy: [0.954,0.988,0.767,0.679,0.470,0.625,0.419,0.679,0.299,0.109]\n",
      "Best Prec@1: 61.770\n",
      "\n",
      "Epoch: [91][96/97], lr: 0.01000\tTime 0.307 (0.321)\tData 0.000 (0.020)\tLoss 2.0870 (2.1086)\tPrec@1 88.136 (87.127)\tPrec@5 99.153 (99.178))\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 7.8462 (7.2595)\tPrec@1 62.000 (64.989)\tPrec@5 99.000 (96.813))\n",
      "val Results: Prec@1 64.720 Prec@5 96.790 Loss 7.28760\n",
      "val Class Accuracy: [0.975,0.952,0.599,0.684,0.736,0.661,0.821,0.410,0.436,0.198]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [92][96/97], lr: 0.01000\tTime 0.305 (0.319)\tData 0.000 (0.020)\tLoss 2.1207 (2.1070)\tPrec@1 84.746 (87.127)\tPrec@5 98.305 (99.194))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.3554 (7.6443)\tPrec@1 66.000 (62.769)\tPrec@5 96.000 (96.187))\n",
      "val Results: Prec@1 62.730 Prec@5 96.110 Loss 7.65360\n",
      "val Class Accuracy: [0.923,0.979,0.822,0.599,0.647,0.571,0.636,0.676,0.398,0.022]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [93][96/97], lr: 0.01000\tTime 0.308 (0.321)\tData 0.000 (0.020)\tLoss 1.6995 (2.1460)\tPrec@1 91.525 (86.869)\tPrec@5 99.153 (99.202))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.9509 (9.6132)\tPrec@1 52.000 (54.033)\tPrec@5 91.000 (90.242))\n",
      "val Results: Prec@1 53.990 Prec@5 90.150 Loss 9.63477\n",
      "val Class Accuracy: [0.944,0.994,0.786,0.734,0.611,0.267,0.661,0.038,0.337,0.027]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [94][96/97], lr: 0.01000\tTime 0.306 (0.322)\tData 0.000 (0.021)\tLoss 1.4684 (2.1340)\tPrec@1 91.525 (86.910)\tPrec@5 100.000 (99.210)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.0615 (8.4817)\tPrec@1 53.000 (58.451)\tPrec@5 97.000 (95.396)\n",
      "val Results: Prec@1 58.450 Prec@5 95.480 Loss 8.48680\n",
      "val Class Accuracy: [0.928,0.994,0.779,0.672,0.494,0.417,0.599,0.532,0.427,0.003]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [95][96/97], lr: 0.01000\tTime 0.309 (0.320)\tData 0.000 (0.020)\tLoss 2.3802 (2.0985)\tPrec@1 83.898 (87.377)\tPrec@5 99.153 (99.162))\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 8.2207 (8.7551)\tPrec@1 61.000 (57.582)\tPrec@5 98.000 (96.418))\n",
      "val Results: Prec@1 57.650 Prec@5 96.480 Loss 8.75646\n",
      "val Class Accuracy: [0.887,0.997,0.538,0.357,0.766,0.833,0.590,0.541,0.237,0.019]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [96][96/97], lr: 0.01000\tTime 0.306 (0.320)\tData 0.000 (0.020)\tLoss 1.8234 (2.1608)\tPrec@1 88.136 (86.885)\tPrec@5 99.153 (99.081))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.4521 (8.8251)\tPrec@1 61.000 (56.242)\tPrec@5 94.000 (94.989)\n",
      "val Results: Prec@1 56.150 Prec@5 95.000 Loss 8.84274\n",
      "val Class Accuracy: [0.954,0.954,0.848,0.480,0.505,0.585,0.456,0.596,0.218,0.019]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [97][96/97], lr: 0.01000\tTime 0.309 (0.320)\tData 0.000 (0.020)\tLoss 2.6954 (2.0547)\tPrec@1 82.203 (87.466)\tPrec@5 98.305 (99.307))\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 7.8251 (7.9564)\tPrec@1 63.000 (60.527)\tPrec@5 98.000 (95.374)\n",
      "val Results: Prec@1 60.500 Prec@5 95.390 Loss 7.96581\n",
      "val Class Accuracy: [0.912,0.963,0.797,0.779,0.701,0.351,0.735,0.252,0.527,0.033]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [98][96/97], lr: 0.01000\tTime 0.313 (0.321)\tData 0.000 (0.020)\tLoss 1.7340 (2.1752)\tPrec@1 87.288 (86.676)\tPrec@5 98.305 (99.129))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.3805 (8.7647)\tPrec@1 61.000 (57.780)\tPrec@5 94.000 (92.451)\n",
      "val Results: Prec@1 57.770 Prec@5 92.430 Loss 8.77865\n",
      "val Class Accuracy: [0.942,0.983,0.729,0.808,0.697,0.429,0.450,0.692,0.041,0.006]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [99][96/97], lr: 0.01000\tTime 0.306 (0.320)\tData 0.000 (0.020)\tLoss 2.1646 (2.1116)\tPrec@1 83.898 (86.966)\tPrec@5 99.153 (99.226))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.8195 (8.3842)\tPrec@1 54.000 (58.714)\tPrec@5 96.000 (95.330)\n",
      "val Results: Prec@1 58.740 Prec@5 95.320 Loss 8.39042\n",
      "val Class Accuracy: [0.951,0.990,0.708,0.785,0.718,0.614,0.545,0.382,0.120,0.061]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [100][96/97], lr: 0.01000\tTime 0.308 (0.322)\tData 0.000 (0.020)\tLoss 1.3841 (2.0857)\tPrec@1 91.525 (87.151)\tPrec@5 99.153 (99.202))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.0048 (9.4597)\tPrec@1 56.000 (53.868)\tPrec@5 95.000 (92.835))\n",
      "val Results: Prec@1 53.910 Prec@5 92.820 Loss 9.47910\n",
      "val Class Accuracy: [0.974,0.984,0.851,0.672,0.571,0.425,0.540,0.301,0.057,0.016]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [101][96/97], lr: 0.01000\tTime 0.308 (0.320)\tData 0.000 (0.020)\tLoss 2.0857 (2.1063)\tPrec@1 85.593 (86.974)\tPrec@5 100.000 (99.283)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.2131 (8.8954)\tPrec@1 60.000 (56.824)\tPrec@5 92.000 (94.132))\n",
      "val Results: Prec@1 56.760 Prec@5 94.100 Loss 8.92434\n",
      "val Class Accuracy: [0.922,0.992,0.796,0.736,0.723,0.661,0.163,0.474,0.181,0.028]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [102][96/97], lr: 0.01000\tTime 0.310 (0.323)\tData 0.000 (0.020)\tLoss 2.9845 (2.0046)\tPrec@1 85.593 (87.538)\tPrec@5 100.000 (99.266)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.9628 (8.0949)\tPrec@1 65.000 (59.967)\tPrec@5 97.000 (93.418)\n",
      "val Results: Prec@1 59.840 Prec@5 93.510 Loss 8.11819\n",
      "val Class Accuracy: [0.837,0.946,0.849,0.737,0.744,0.378,0.539,0.602,0.229,0.123]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [103][96/97], lr: 0.01000\tTime 0.308 (0.320)\tData 0.000 (0.020)\tLoss 2.0147 (2.1283)\tPrec@1 88.136 (86.756)\tPrec@5 99.153 (99.186))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.9936 (7.9721)\tPrec@1 61.000 (61.681)\tPrec@5 97.000 (95.835))\n",
      "val Results: Prec@1 61.560 Prec@5 95.880 Loss 7.99650\n",
      "val Class Accuracy: [0.876,0.972,0.774,0.728,0.623,0.477,0.847,0.497,0.347,0.015]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [104][96/97], lr: 0.01000\tTime 0.307 (0.321)\tData 0.000 (0.020)\tLoss 2.0087 (2.0404)\tPrec@1 86.441 (87.659)\tPrec@5 99.153 (99.258))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.8612 (7.7011)\tPrec@1 61.000 (61.945)\tPrec@5 98.000 (97.374))\n",
      "val Results: Prec@1 61.810 Prec@5 97.380 Loss 7.73196\n",
      "val Class Accuracy: [0.962,0.981,0.520,0.674,0.849,0.688,0.637,0.296,0.453,0.121]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [105][96/97], lr: 0.01000\tTime 0.307 (0.318)\tData 0.000 (0.020)\tLoss 1.9780 (2.0564)\tPrec@1 87.288 (87.579)\tPrec@5 98.305 (99.250))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.7579 (8.0293)\tPrec@1 65.000 (61.747)\tPrec@5 93.000 (92.407)\n",
      "val Results: Prec@1 61.690 Prec@5 92.420 Loss 8.03761\n",
      "val Class Accuracy: [0.912,0.931,0.712,0.780,0.702,0.561,0.657,0.722,0.149,0.043]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [106][96/97], lr: 0.01000\tTime 0.306 (0.321)\tData 0.000 (0.020)\tLoss 1.8848 (2.0532)\tPrec@1 89.831 (87.571)\tPrec@5 98.305 (99.323))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.7291 (9.0142)\tPrec@1 58.000 (57.286)\tPrec@5 96.000 (93.868))\n",
      "val Results: Prec@1 57.280 Prec@5 93.840 Loss 9.03252\n",
      "val Class Accuracy: [0.902,0.989,0.702,0.611,0.695,0.816,0.504,0.393,0.096,0.020]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [107][96/97], lr: 0.01000\tTime 0.308 (0.321)\tData 0.000 (0.020)\tLoss 1.2306 (2.0984)\tPrec@1 94.068 (87.087)\tPrec@5 99.153 (99.412))\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 8.4551 (8.7119)\tPrec@1 62.000 (58.066)\tPrec@5 96.000 (93.451)\n",
      "val Results: Prec@1 58.120 Prec@5 93.440 Loss 8.71907\n",
      "val Class Accuracy: [0.828,0.980,0.697,0.856,0.528,0.539,0.843,0.291,0.222,0.028]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [108][96/97], lr: 0.01000\tTime 0.309 (0.322)\tData 0.000 (0.020)\tLoss 1.6986 (2.0807)\tPrec@1 88.983 (87.288)\tPrec@5 100.000 (99.307)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.9647 (8.0510)\tPrec@1 60.000 (59.681)\tPrec@5 98.000 (95.934)\n",
      "val Results: Prec@1 59.580 Prec@5 95.950 Loss 8.06716\n",
      "val Class Accuracy: [0.966,0.979,0.894,0.600,0.468,0.391,0.668,0.589,0.317,0.086]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [109][96/97], lr: 0.01000\tTime 0.309 (0.321)\tData 0.000 (0.020)\tLoss 3.3384 (2.0622)\tPrec@1 80.508 (87.490)\tPrec@5 100.000 (99.420)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 9.0948 (9.4637)\tPrec@1 58.000 (54.275)\tPrec@5 96.000 (94.593))\n",
      "val Results: Prec@1 54.290 Prec@5 94.540 Loss 9.48615\n",
      "val Class Accuracy: [0.922,0.988,0.919,0.648,0.464,0.398,0.589,0.323,0.172,0.006]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [110][96/97], lr: 0.01000\tTime 0.305 (0.320)\tData 0.000 (0.021)\tLoss 2.6711 (2.0445)\tPrec@1 83.051 (87.514)\tPrec@5 99.153 (99.291))\n",
      "Test: [90/100]\tTime 0.074 (0.075)\tLoss 7.1562 (7.3769)\tPrec@1 66.000 (64.121)\tPrec@5 98.000 (95.769)\n",
      "val Results: Prec@1 64.080 Prec@5 95.760 Loss 7.39339\n",
      "val Class Accuracy: [0.899,0.986,0.799,0.798,0.773,0.558,0.729,0.482,0.291,0.093]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [111][96/97], lr: 0.01000\tTime 0.309 (0.324)\tData 0.000 (0.021)\tLoss 2.1230 (2.0421)\tPrec@1 87.288 (87.506)\tPrec@5 98.305 (99.250))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.1710 (8.2002)\tPrec@1 57.000 (60.516)\tPrec@5 99.000 (96.462)\n",
      "val Results: Prec@1 60.360 Prec@5 96.350 Loss 8.23776\n",
      "val Class Accuracy: [0.932,0.991,0.727,0.810,0.848,0.446,0.453,0.300,0.468,0.061]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [112][96/97], lr: 0.01000\tTime 0.321 (0.321)\tData 0.000 (0.020)\tLoss 2.6230 (2.0807)\tPrec@1 81.356 (87.256)\tPrec@5 100.000 (99.210)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 8.5691 (8.4937)\tPrec@1 53.000 (56.330)\tPrec@5 98.000 (94.846)\n",
      "val Results: Prec@1 56.350 Prec@5 94.930 Loss 8.50960\n",
      "val Class Accuracy: [0.976,0.988,0.392,0.269,0.748,0.611,0.637,0.447,0.532,0.035]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [113][96/97], lr: 0.01000\tTime 0.309 (0.321)\tData 0.000 (0.020)\tLoss 2.1865 (2.0361)\tPrec@1 88.136 (87.748)\tPrec@5 99.153 (99.258))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.7400 (7.9378)\tPrec@1 63.000 (61.758)\tPrec@5 98.000 (95.901)\n",
      "val Results: Prec@1 61.990 Prec@5 95.860 Loss 7.92236\n",
      "val Class Accuracy: [0.908,0.994,0.694,0.815,0.751,0.596,0.568,0.441,0.384,0.048]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [114][96/97], lr: 0.01000\tTime 0.308 (0.319)\tData 0.000 (0.020)\tLoss 1.7229 (2.0217)\tPrec@1 91.525 (87.683)\tPrec@5 100.000 (99.283)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.7765 (9.1885)\tPrec@1 53.000 (56.044)\tPrec@5 95.000 (93.220)\n",
      "val Results: Prec@1 56.070 Prec@5 93.080 Loss 9.19781\n",
      "val Class Accuracy: [0.985,0.971,0.678,0.745,0.723,0.336,0.661,0.216,0.284,0.008]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [115][96/97], lr: 0.01000\tTime 0.309 (0.321)\tData 0.000 (0.020)\tLoss 1.5211 (2.0048)\tPrec@1 91.525 (87.812)\tPrec@5 99.153 (99.371))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.7773 (9.2200)\tPrec@1 57.000 (55.571)\tPrec@5 96.000 (94.505))\n",
      "val Results: Prec@1 55.380 Prec@5 94.480 Loss 9.25244\n",
      "val Class Accuracy: [0.926,0.928,0.900,0.500,0.721,0.592,0.201,0.306,0.351,0.113]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [116][96/97], lr: 0.01000\tTime 0.304 (0.334)\tData 0.000 (0.020)\tLoss 2.1131 (2.0842)\tPrec@1 87.288 (87.280)\tPrec@5 100.000 (99.266)\n",
      "Test: [90/100]\tTime 0.074 (0.075)\tLoss 7.8847 (8.6192)\tPrec@1 62.000 (56.890)\tPrec@5 96.000 (90.714))\n",
      "val Results: Prec@1 57.060 Prec@5 90.660 Loss 8.62420\n",
      "val Class Accuracy: [0.923,0.957,0.885,0.723,0.343,0.531,0.715,0.189,0.372,0.068]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [117][96/97], lr: 0.01000\tTime 0.310 (0.323)\tData 0.000 (0.020)\tLoss 1.9238 (2.0319)\tPrec@1 88.983 (87.812)\tPrec@5 98.305 (99.347))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.1912 (9.5231)\tPrec@1 51.000 (54.099)\tPrec@5 92.000 (91.538)\n",
      "val Results: Prec@1 53.940 Prec@5 91.590 Loss 9.55472\n",
      "val Class Accuracy: [0.936,0.959,0.762,0.392,0.646,0.292,0.839,0.401,0.142,0.025]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [118][96/97], lr: 0.01000\tTime 0.308 (0.321)\tData 0.000 (0.020)\tLoss 1.9265 (2.0287)\tPrec@1 90.678 (87.571)\tPrec@5 100.000 (99.154)\n",
      "Test: [90/100]\tTime 0.074 (0.075)\tLoss 8.0366 (7.4072)\tPrec@1 61.000 (63.945)\tPrec@5 95.000 (97.011)\n",
      "val Results: Prec@1 63.950 Prec@5 97.030 Loss 7.42652\n",
      "val Class Accuracy: [0.941,0.948,0.612,0.474,0.685,0.869,0.747,0.586,0.239,0.294]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [119][96/97], lr: 0.01000\tTime 0.312 (0.323)\tData 0.000 (0.020)\tLoss 1.7808 (2.0387)\tPrec@1 88.983 (87.748)\tPrec@5 100.000 (99.234)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.3882 (9.0353)\tPrec@1 56.000 (56.033)\tPrec@5 98.000 (93.659))\n",
      "val Results: Prec@1 56.030 Prec@5 93.590 Loss 9.05745\n",
      "val Class Accuracy: [0.976,0.994,0.675,0.645,0.488,0.321,0.789,0.423,0.286,0.006]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "Epoch: [120][96/97], lr: 0.01000\tTime 0.304 (0.341)\tData 0.000 (0.023)\tLoss 1.7277 (2.0417)\tPrec@1 88.983 (87.595)\tPrec@5 100.000 (99.315)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 8.0744 (7.4113)\tPrec@1 61.000 (64.802)\tPrec@5 95.000 (95.132))\n",
      "val Results: Prec@1 64.750 Prec@5 94.990 Loss 7.43800\n",
      "val Class Accuracy: [0.937,0.972,0.725,0.577,0.782,0.591,0.905,0.581,0.189,0.216]\n",
      "Best Prec@1: 64.750\n",
      "\n",
      "Epoch: [121][96/97], lr: 0.01000\tTime 0.308 (0.321)\tData 0.000 (0.020)\tLoss 2.1308 (2.0248)\tPrec@1 83.898 (87.611)\tPrec@5 100.000 (99.307)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.1600 (6.9683)\tPrec@1 67.000 (67.527)\tPrec@5 98.000 (96.824)\n",
      "val Results: Prec@1 67.590 Prec@5 96.840 Loss 6.96338\n",
      "val Class Accuracy: [0.934,0.975,0.683,0.795,0.771,0.694,0.693,0.303,0.483,0.428]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [122][96/97], lr: 0.01000\tTime 0.307 (0.318)\tData 0.000 (0.020)\tLoss 2.3188 (1.9907)\tPrec@1 85.593 (88.038)\tPrec@5 100.000 (99.275)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.0771 (8.6847)\tPrec@1 59.000 (58.121)\tPrec@5 94.000 (93.352)\n",
      "val Results: Prec@1 58.050 Prec@5 93.220 Loss 8.69775\n",
      "val Class Accuracy: [0.863,0.946,0.904,0.659,0.758,0.465,0.631,0.335,0.159,0.085]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [123][96/97], lr: 0.01000\tTime 0.308 (0.322)\tData 0.000 (0.021)\tLoss 2.3121 (1.9970)\tPrec@1 88.983 (87.708)\tPrec@5 97.458 (99.283))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.3116 (9.9351)\tPrec@1 59.000 (52.286)\tPrec@5 96.000 (94.242))\n",
      "val Results: Prec@1 52.330 Prec@5 94.310 Loss 9.94523\n",
      "val Class Accuracy: [0.868,0.995,0.896,0.771,0.538,0.299,0.340,0.168,0.345,0.013]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [124][96/97], lr: 0.01000\tTime 0.306 (0.318)\tData 0.000 (0.020)\tLoss 2.0978 (1.9705)\tPrec@1 83.898 (87.998)\tPrec@5 100.000 (99.250)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.0528 (7.1271)\tPrec@1 62.000 (64.451)\tPrec@5 98.000 (97.374))\n",
      "val Results: Prec@1 64.300 Prec@5 97.390 Loss 7.14681\n",
      "val Class Accuracy: [0.854,0.990,0.684,0.605,0.593,0.755,0.739,0.458,0.674,0.078]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [125][96/97], lr: 0.01000\tTime 0.310 (0.321)\tData 0.000 (0.020)\tLoss 2.4001 (1.9964)\tPrec@1 83.051 (88.078)\tPrec@5 98.305 (99.299))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.1987 (7.9288)\tPrec@1 64.000 (61.780)\tPrec@5 98.000 (94.275)\n",
      "val Results: Prec@1 61.590 Prec@5 94.280 Loss 7.97311\n",
      "val Class Accuracy: [0.838,0.948,0.747,0.258,0.849,0.617,0.887,0.472,0.364,0.179]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [126][96/97], lr: 0.01000\tTime 0.307 (0.318)\tData 0.000 (0.020)\tLoss 2.3116 (2.0227)\tPrec@1 85.593 (87.683)\tPrec@5 98.305 (99.299))\n",
      "Test: [90/100]\tTime 0.074 (0.075)\tLoss 8.5525 (8.8225)\tPrec@1 63.000 (58.308)\tPrec@5 92.000 (93.330))\n",
      "val Results: Prec@1 58.170 Prec@5 93.390 Loss 8.84341\n",
      "val Class Accuracy: [0.962,0.984,0.781,0.752,0.621,0.573,0.453,0.555,0.028,0.108]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [127][96/97], lr: 0.01000\tTime 0.312 (0.321)\tData 0.000 (0.021)\tLoss 2.5322 (1.9469)\tPrec@1 83.898 (88.272)\tPrec@5 97.458 (99.339))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.2067 (8.6488)\tPrec@1 63.000 (58.473)\tPrec@5 98.000 (94.802))\n",
      "val Results: Prec@1 58.570 Prec@5 94.810 Loss 8.64874\n",
      "val Class Accuracy: [0.957,0.986,0.811,0.601,0.697,0.665,0.404,0.414,0.292,0.030]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [128][96/97], lr: 0.01000\tTime 0.307 (0.319)\tData 0.000 (0.021)\tLoss 1.9101 (1.9638)\tPrec@1 87.288 (88.304)\tPrec@5 98.305 (99.363))\n",
      "Test: [90/100]\tTime 0.074 (0.075)\tLoss 7.8041 (7.9472)\tPrec@1 66.000 (62.440)\tPrec@5 94.000 (93.615)\n",
      "val Results: Prec@1 62.650 Prec@5 93.560 Loss 7.93543\n",
      "val Class Accuracy: [0.906,0.950,0.689,0.849,0.685,0.602,0.566,0.329,0.681,0.008]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [129][96/97], lr: 0.01000\tTime 0.308 (0.321)\tData 0.000 (0.020)\tLoss 1.8840 (1.9637)\tPrec@1 87.288 (87.877)\tPrec@5 100.000 (99.331)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.9839 (7.9480)\tPrec@1 63.000 (61.659)\tPrec@5 99.000 (96.516)\n",
      "val Results: Prec@1 61.570 Prec@5 96.550 Loss 7.95630\n",
      "val Class Accuracy: [0.940,0.995,0.768,0.846,0.777,0.345,0.360,0.518,0.483,0.125]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [130][96/97], lr: 0.01000\tTime 0.306 (0.318)\tData 0.000 (0.020)\tLoss 2.1062 (1.9774)\tPrec@1 84.746 (88.046)\tPrec@5 99.153 (99.355))\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 9.2728 (9.2576)\tPrec@1 56.000 (55.945)\tPrec@5 98.000 (95.187))\n",
      "val Results: Prec@1 55.980 Prec@5 95.110 Loss 9.27028\n",
      "val Class Accuracy: [0.968,0.974,0.780,0.808,0.830,0.233,0.389,0.201,0.238,0.177]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [131][96/97], lr: 0.01000\tTime 0.311 (0.321)\tData 0.000 (0.021)\tLoss 1.5867 (2.0198)\tPrec@1 88.983 (87.828)\tPrec@5 100.000 (99.226)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.1535 (8.7472)\tPrec@1 55.000 (57.396)\tPrec@5 87.000 (88.473)\n",
      "val Results: Prec@1 57.540 Prec@5 88.480 Loss 8.73878\n",
      "val Class Accuracy: [0.739,0.892,0.714,0.564,0.740,0.831,0.211,0.587,0.397,0.079]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [132][96/97], lr: 0.01000\tTime 0.307 (0.319)\tData 0.000 (0.020)\tLoss 1.7563 (1.9686)\tPrec@1 91.525 (88.119)\tPrec@5 98.305 (99.291))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.9847 (8.3023)\tPrec@1 54.000 (59.593)\tPrec@5 99.000 (95.692)\n",
      "val Results: Prec@1 59.830 Prec@5 95.600 Loss 8.29114\n",
      "val Class Accuracy: [0.896,0.970,0.697,0.899,0.537,0.428,0.736,0.392,0.363,0.065]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [133][96/97], lr: 0.01000\tTime 0.307 (0.319)\tData 0.000 (0.020)\tLoss 2.1079 (1.9021)\tPrec@1 90.678 (88.602)\tPrec@5 100.000 (99.387)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.8308 (8.1654)\tPrec@1 59.000 (59.451)\tPrec@5 97.000 (95.044)\n",
      "val Results: Prec@1 59.620 Prec@5 95.090 Loss 8.15580\n",
      "val Class Accuracy: [0.983,0.961,0.776,0.665,0.474,0.281,0.743,0.338,0.529,0.212]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [134][96/97], lr: 0.01000\tTime 0.309 (0.322)\tData 0.000 (0.021)\tLoss 2.1010 (1.9581)\tPrec@1 86.441 (88.135)\tPrec@5 100.000 (99.371)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.9557 (7.9374)\tPrec@1 61.000 (62.769)\tPrec@5 96.000 (93.132)\n",
      "val Results: Prec@1 62.900 Prec@5 93.120 Loss 7.93157\n",
      "val Class Accuracy: [0.881,0.949,0.758,0.801,0.574,0.627,0.739,0.294,0.644,0.023]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [135][96/97], lr: 0.01000\tTime 0.309 (0.319)\tData 0.000 (0.020)\tLoss 1.8783 (1.9480)\tPrec@1 88.983 (88.086)\tPrec@5 98.305 (99.291))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.1333 (7.8765)\tPrec@1 67.000 (62.000)\tPrec@5 98.000 (96.571)\n",
      "val Results: Prec@1 61.960 Prec@5 96.570 Loss 7.88250\n",
      "val Class Accuracy: [0.946,0.991,0.834,0.742,0.491,0.557,0.548,0.650,0.246,0.191]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [136][96/97], lr: 0.01000\tTime 0.306 (0.319)\tData 0.000 (0.020)\tLoss 2.2939 (1.9352)\tPrec@1 89.831 (88.385)\tPrec@5 100.000 (99.420)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.1862 (8.3398)\tPrec@1 62.000 (59.747)\tPrec@5 97.000 (95.055)\n",
      "val Results: Prec@1 59.820 Prec@5 95.090 Loss 8.34705\n",
      "val Class Accuracy: [0.947,0.994,0.821,0.780,0.562,0.510,0.525,0.360,0.401,0.082]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [137][96/97], lr: 0.01000\tTime 0.309 (0.320)\tData 0.000 (0.021)\tLoss 1.7438 (1.9498)\tPrec@1 89.831 (88.127)\tPrec@5 100.000 (99.428)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.1750 (7.1821)\tPrec@1 66.000 (65.582)\tPrec@5 95.000 (95.593))\n",
      "val Results: Prec@1 65.530 Prec@5 95.580 Loss 7.19179\n",
      "val Class Accuracy: [0.932,0.926,0.861,0.549,0.668,0.676,0.755,0.608,0.449,0.129]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [138][96/97], lr: 0.01000\tTime 0.309 (0.320)\tData 0.000 (0.020)\tLoss 2.1526 (1.9488)\tPrec@1 88.983 (88.046)\tPrec@5 98.305 (99.428))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.7299 (8.6433)\tPrec@1 60.000 (57.769)\tPrec@5 98.000 (96.099)\n",
      "val Results: Prec@1 57.760 Prec@5 96.080 Loss 8.66000\n",
      "val Class Accuracy: [0.887,0.972,0.926,0.394,0.419,0.414,0.747,0.425,0.487,0.105]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [139][96/97], lr: 0.01000\tTime 0.307 (0.319)\tData 0.000 (0.020)\tLoss 2.1103 (1.9690)\tPrec@1 88.136 (88.151)\tPrec@5 99.153 (99.275))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.6216 (8.7743)\tPrec@1 58.000 (56.341)\tPrec@5 97.000 (94.659)\n",
      "val Results: Prec@1 56.360 Prec@5 94.790 Loss 8.77641\n",
      "val Class Accuracy: [0.918,0.990,0.892,0.508,0.400,0.374,0.686,0.434,0.317,0.117]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [140][96/97], lr: 0.01000\tTime 0.310 (0.320)\tData 0.000 (0.021)\tLoss 1.9799 (2.0005)\tPrec@1 88.136 (87.804)\tPrec@5 99.153 (99.202))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.5058 (9.3510)\tPrec@1 51.000 (55.747)\tPrec@5 95.000 (92.824)\n",
      "val Results: Prec@1 56.010 Prec@5 92.850 Loss 9.33381\n",
      "val Class Accuracy: [0.905,0.979,0.686,0.924,0.530,0.422,0.320,0.568,0.264,0.003]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [141][96/97], lr: 0.01000\tTime 0.309 (0.320)\tData 0.000 (0.020)\tLoss 1.2420 (1.9034)\tPrec@1 92.373 (88.473)\tPrec@5 99.153 (99.436))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.4747 (8.6445)\tPrec@1 60.000 (59.198)\tPrec@5 97.000 (93.714)\n",
      "val Results: Prec@1 59.200 Prec@5 93.770 Loss 8.66221\n",
      "val Class Accuracy: [0.797,0.960,0.833,0.509,0.934,0.575,0.374,0.301,0.603,0.034]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [142][96/97], lr: 0.01000\tTime 0.308 (0.321)\tData 0.000 (0.021)\tLoss 2.3231 (1.8922)\tPrec@1 85.593 (88.514)\tPrec@5 98.305 (99.371))\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 7.3772 (7.4736)\tPrec@1 63.000 (64.780)\tPrec@5 99.000 (95.604)\n",
      "val Results: Prec@1 64.800 Prec@5 95.620 Loss 7.48509\n",
      "val Class Accuracy: [0.953,0.984,0.741,0.530,0.810,0.809,0.724,0.392,0.412,0.125]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [143][96/97], lr: 0.01000\tTime 0.309 (0.319)\tData 0.000 (0.021)\tLoss 1.9793 (1.9283)\tPrec@1 87.288 (88.159)\tPrec@5 100.000 (99.371)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.9037 (9.3283)\tPrec@1 57.000 (55.473)\tPrec@5 96.000 (93.231))\n",
      "val Results: Prec@1 55.450 Prec@5 93.190 Loss 9.33606\n",
      "val Class Accuracy: [0.959,0.943,0.792,0.158,0.607,0.584,0.833,0.425,0.219,0.025]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [144][96/97], lr: 0.01000\tTime 0.308 (0.320)\tData 0.000 (0.021)\tLoss 2.2822 (1.9222)\tPrec@1 82.203 (88.304)\tPrec@5 99.153 (99.323))\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 8.2041 (8.7521)\tPrec@1 61.000 (57.275)\tPrec@5 96.000 (95.209)\n",
      "val Results: Prec@1 57.160 Prec@5 95.190 Loss 8.76736\n",
      "val Class Accuracy: [0.969,0.992,0.802,0.496,0.542,0.397,0.755,0.450,0.226,0.087]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [145][96/97], lr: 0.01000\tTime 0.308 (0.319)\tData 0.000 (0.020)\tLoss 2.0763 (1.8856)\tPrec@1 87.288 (88.747)\tPrec@5 98.305 (99.404))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.1878 (8.2852)\tPrec@1 61.000 (61.022)\tPrec@5 94.000 (94.462)\n",
      "val Results: Prec@1 61.130 Prec@5 94.480 Loss 8.27920\n",
      "val Class Accuracy: [0.940,0.969,0.785,0.816,0.647,0.532,0.461,0.505,0.138,0.320]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [146][96/97], lr: 0.01000\tTime 0.309 (0.321)\tData 0.000 (0.020)\tLoss 1.8303 (1.9135)\tPrec@1 88.983 (88.336)\tPrec@5 100.000 (99.291)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.8646 (8.6767)\tPrec@1 56.000 (58.110)\tPrec@5 95.000 (94.286))\n",
      "val Results: Prec@1 58.260 Prec@5 94.270 Loss 8.66895\n",
      "val Class Accuracy: [0.939,0.973,0.756,0.777,0.514,0.720,0.176,0.339,0.491,0.141]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [147][96/97], lr: 0.01000\tTime 0.309 (0.319)\tData 0.000 (0.020)\tLoss 2.3382 (1.9611)\tPrec@1 85.593 (88.151)\tPrec@5 99.153 (99.250))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.7682 (8.0994)\tPrec@1 64.000 (61.527)\tPrec@5 99.000 (94.956))\n",
      "val Results: Prec@1 61.430 Prec@5 94.980 Loss 8.11878\n",
      "val Class Accuracy: [0.909,0.976,0.923,0.487,0.529,0.531,0.612,0.717,0.277,0.182]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [148][96/97], lr: 0.01000\tTime 0.311 (0.321)\tData 0.000 (0.020)\tLoss 2.3879 (1.9348)\tPrec@1 86.441 (88.320)\tPrec@5 99.153 (99.283))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.6004 (9.6097)\tPrec@1 55.000 (53.154)\tPrec@5 96.000 (94.527))\n",
      "val Results: Prec@1 53.280 Prec@5 94.520 Loss 9.60477\n",
      "val Class Accuracy: [0.882,0.983,0.928,0.608,0.265,0.392,0.314,0.583,0.146,0.227]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [149][96/97], lr: 0.01000\tTime 0.307 (0.320)\tData 0.000 (0.021)\tLoss 2.0387 (1.8783)\tPrec@1 85.593 (88.546)\tPrec@5 100.000 (99.347)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.9373 (7.6363)\tPrec@1 69.000 (63.901)\tPrec@5 96.000 (96.055)\n",
      "val Results: Prec@1 63.920 Prec@5 96.080 Loss 7.64033\n",
      "val Class Accuracy: [0.956,0.991,0.766,0.612,0.539,0.731,0.609,0.395,0.372,0.421]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [150][96/97], lr: 0.01000\tTime 0.309 (0.321)\tData 0.000 (0.021)\tLoss 1.4675 (1.9697)\tPrec@1 92.373 (87.957)\tPrec@5 99.153 (99.323))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.1672 (8.4599)\tPrec@1 60.000 (59.374)\tPrec@5 97.000 (92.802)\n",
      "val Results: Prec@1 59.220 Prec@5 92.680 Loss 8.50094\n",
      "val Class Accuracy: [0.974,0.991,0.736,0.632,0.856,0.361,0.765,0.050,0.362,0.195]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [151][96/97], lr: 0.01000\tTime 0.307 (0.319)\tData 0.000 (0.020)\tLoss 2.1365 (1.9098)\tPrec@1 88.136 (88.417)\tPrec@5 97.458 (99.387))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.2595 (8.3061)\tPrec@1 62.000 (59.681)\tPrec@5 97.000 (96.077)\n",
      "val Results: Prec@1 59.670 Prec@5 96.130 Loss 8.30734\n",
      "val Class Accuracy: [0.965,0.984,0.845,0.639,0.586,0.382,0.494,0.507,0.384,0.181]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [152][96/97], lr: 0.01000\tTime 0.308 (0.322)\tData 0.000 (0.021)\tLoss 1.9839 (1.9071)\tPrec@1 86.441 (88.385)\tPrec@5 100.000 (99.476)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.0045 (7.5132)\tPrec@1 69.000 (63.758)\tPrec@5 99.000 (95.802)\n",
      "val Results: Prec@1 63.970 Prec@5 95.670 Loss 7.49488\n",
      "val Class Accuracy: [0.940,0.984,0.723,0.795,0.700,0.396,0.789,0.396,0.526,0.148]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [153][96/97], lr: 0.01000\tTime 0.308 (0.319)\tData 0.000 (0.020)\tLoss 2.4446 (1.8817)\tPrec@1 85.593 (88.707)\tPrec@5 98.305 (99.379))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.3512 (7.6688)\tPrec@1 59.000 (63.560)\tPrec@5 98.000 (96.264)\n",
      "val Results: Prec@1 63.590 Prec@5 96.180 Loss 7.68287\n",
      "val Class Accuracy: [0.944,0.980,0.788,0.749,0.802,0.496,0.563,0.377,0.550,0.110]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [154][96/97], lr: 0.01000\tTime 0.309 (0.321)\tData 0.000 (0.021)\tLoss 2.3004 (1.9372)\tPrec@1 84.746 (88.231)\tPrec@5 99.153 (99.412))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.0159 (7.2613)\tPrec@1 76.000 (65.989)\tPrec@5 97.000 (95.418)\n",
      "val Results: Prec@1 65.940 Prec@5 95.500 Loss 7.27028\n",
      "val Class Accuracy: [0.975,0.978,0.757,0.431,0.809,0.627,0.538,0.625,0.231,0.623]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [155][96/97], lr: 0.01000\tTime 0.308 (0.320)\tData 0.000 (0.021)\tLoss 1.7973 (1.8809)\tPrec@1 90.678 (88.772)\tPrec@5 99.153 (99.307))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.3388 (8.2419)\tPrec@1 67.000 (60.626)\tPrec@5 99.000 (95.363)\n",
      "val Results: Prec@1 60.710 Prec@5 95.380 Loss 8.24694\n",
      "val Class Accuracy: [0.954,0.970,0.876,0.732,0.494,0.545,0.353,0.503,0.384,0.260]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [156][96/97], lr: 0.01000\tTime 0.307 (0.321)\tData 0.000 (0.021)\tLoss 2.5772 (1.8478)\tPrec@1 84.746 (88.659)\tPrec@5 99.153 (99.428))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.3077 (7.4628)\tPrec@1 64.000 (64.769)\tPrec@5 99.000 (95.560))\n",
      "val Results: Prec@1 64.500 Prec@5 95.520 Loss 7.52094\n",
      "val Class Accuracy: [0.934,0.960,0.763,0.680,0.864,0.390,0.690,0.466,0.490,0.213]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [157][96/97], lr: 0.01000\tTime 0.308 (0.319)\tData 0.000 (0.020)\tLoss 1.7142 (1.9298)\tPrec@1 88.983 (88.530)\tPrec@5 100.000 (99.339)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.6650 (8.1143)\tPrec@1 62.000 (61.385)\tPrec@5 99.000 (93.297)\n",
      "val Results: Prec@1 61.270 Prec@5 93.260 Loss 8.14504\n",
      "val Class Accuracy: [0.817,0.911,0.907,0.586,0.633,0.679,0.285,0.528,0.629,0.152]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [158][96/97], lr: 0.01000\tTime 0.308 (0.321)\tData 0.000 (0.021)\tLoss 2.8666 (1.9052)\tPrec@1 82.203 (88.401)\tPrec@5 98.305 (99.387))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.3028 (7.4558)\tPrec@1 67.000 (65.000)\tPrec@5 97.000 (95.670)\n",
      "val Results: Prec@1 64.920 Prec@5 95.650 Loss 7.47363\n",
      "val Class Accuracy: [0.850,0.982,0.828,0.659,0.763,0.495,0.831,0.436,0.440,0.208]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [159][96/97], lr: 0.01000\tTime 0.316 (0.319)\tData 0.000 (0.020)\tLoss 1.2454 (1.8297)\tPrec@1 93.220 (88.812)\tPrec@5 99.153 (99.444))\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 9.6541 (8.8940)\tPrec@1 54.000 (58.615)\tPrec@5 99.000 (95.132)\n",
      "val Results: Prec@1 58.730 Prec@5 95.110 Loss 8.89642\n",
      "val Class Accuracy: [0.944,0.983,0.827,0.803,0.607,0.467,0.430,0.504,0.301,0.007]\n",
      "Best Prec@1: 67.590\n",
      "\n",
      "Epoch: [160][96/97], lr: 0.00010\tTime 0.309 (0.321)\tData 0.000 (0.021)\tLoss 5.7025 (4.7617)\tPrec@1 88.136 (89.005)\tPrec@5 99.153 (99.444)))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.7483 (7.3032)\tPrec@1 84.000 (76.813)\tPrec@5 100.000 (98.319)\n",
      "val Results: Prec@1 76.840 Prec@5 98.380 Loss 7.32244\n",
      "val Class Accuracy: [0.926,0.951,0.789,0.671,0.770,0.697,0.767,0.696,0.657,0.760]\n",
      "Best Prec@1: 76.840\n",
      "\n",
      "Epoch: [161][96/97], lr: 0.00010\tTime 0.306 (0.318)\tData 0.000 (0.020)\tLoss 3.0004 (3.5555)\tPrec@1 85.593 (87.861)\tPrec@5 98.305 (99.299))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.9715 (6.7033)\tPrec@1 86.000 (78.275)\tPrec@5 100.000 (98.429)\n",
      "val Results: Prec@1 78.270 Prec@5 98.490 Loss 6.72745\n",
      "val Class Accuracy: [0.933,0.945,0.786,0.683,0.791,0.726,0.803,0.700,0.665,0.795]\n",
      "Best Prec@1: 78.270\n",
      "\n",
      "Epoch: [162][96/97], lr: 0.00010\tTime 0.308 (0.320)\tData 0.000 (0.020)\tLoss 1.3431 (3.3232)\tPrec@1 91.525 (87.506)\tPrec@5 99.153 (99.242))\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 4.7354 (5.7077)\tPrec@1 86.000 (79.220)\tPrec@5 100.000 (98.473)\n",
      "val Results: Prec@1 79.250 Prec@5 98.540 Loss 5.73462\n",
      "val Class Accuracy: [0.918,0.926,0.787,0.685,0.803,0.735,0.813,0.690,0.732,0.836]\n",
      "Best Prec@1: 79.250\n",
      "\n",
      "Epoch: [163][96/97], lr: 0.00010\tTime 0.307 (0.319)\tData 0.000 (0.021)\tLoss 1.5449 (3.2709)\tPrec@1 93.220 (87.756)\tPrec@5 100.000 (99.323)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.1828 (5.8698)\tPrec@1 88.000 (79.835)\tPrec@5 100.000 (98.571)\n",
      "val Results: Prec@1 79.840 Prec@5 98.640 Loss 5.89515\n",
      "val Class Accuracy: [0.906,0.954,0.793,0.702,0.794,0.742,0.804,0.718,0.763,0.808]\n",
      "Best Prec@1: 79.840\n",
      "\n",
      "Epoch: [164][96/97], lr: 0.00010\tTime 0.309 (0.320)\tData 0.000 (0.020)\tLoss 2.2862 (3.1790)\tPrec@1 85.593 (87.450)\tPrec@5 100.000 (99.299)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.2014 (5.7928)\tPrec@1 89.000 (80.044)\tPrec@5 100.000 (98.549)\n",
      "val Results: Prec@1 80.020 Prec@5 98.600 Loss 5.82064\n",
      "val Class Accuracy: [0.919,0.950,0.801,0.704,0.789,0.743,0.807,0.728,0.754,0.807]\n",
      "Best Prec@1: 80.020\n",
      "\n",
      "Epoch: [165][96/97], lr: 0.00010\tTime 0.311 (0.320)\tData 0.000 (0.021)\tLoss 2.9221 (3.0634)\tPrec@1 83.051 (87.425)\tPrec@5 98.305 (99.266))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.2522 (5.8102)\tPrec@1 86.000 (80.088)\tPrec@5 100.000 (98.626)\n",
      "val Results: Prec@1 80.080 Prec@5 98.670 Loss 5.83465\n",
      "val Class Accuracy: [0.908,0.955,0.805,0.693,0.806,0.728,0.821,0.728,0.759,0.805]\n",
      "Best Prec@1: 80.080\n",
      "\n",
      "Epoch: [166][96/97], lr: 0.00010\tTime 0.307 (0.320)\tData 0.000 (0.021)\tLoss 4.9589 (3.0120)\tPrec@1 86.441 (88.336)\tPrec@5 98.305 (99.331))\n",
      "Test: [90/100]\tTime 0.074 (0.075)\tLoss 4.5306 (5.4710)\tPrec@1 89.000 (80.352)\tPrec@5 100.000 (98.637)\n",
      "val Results: Prec@1 80.380 Prec@5 98.690 Loss 5.48648\n",
      "val Class Accuracy: [0.907,0.948,0.793,0.711,0.812,0.740,0.819,0.705,0.760,0.843]\n",
      "Best Prec@1: 80.380\n",
      "\n",
      "Epoch: [167][96/97], lr: 0.00010\tTime 0.310 (0.321)\tData 0.000 (0.020)\tLoss 2.0995 (2.9622)\tPrec@1 90.678 (88.272)\tPrec@5 99.153 (99.283))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.0479 (5.6125)\tPrec@1 88.000 (80.637)\tPrec@5 100.000 (98.571)\n",
      "val Results: Prec@1 80.580 Prec@5 98.620 Loss 5.63681\n",
      "val Class Accuracy: [0.915,0.951,0.796,0.692,0.814,0.752,0.839,0.720,0.767,0.812]\n",
      "Best Prec@1: 80.580\n",
      "\n",
      "Epoch: [168][96/97], lr: 0.00010\tTime 0.307 (0.319)\tData 0.000 (0.021)\tLoss 2.3321 (2.8314)\tPrec@1 88.136 (89.013)\tPrec@5 97.458 (99.291))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.9632 (5.4594)\tPrec@1 89.000 (80.791)\tPrec@5 100.000 (98.538)\n",
      "val Results: Prec@1 80.740 Prec@5 98.600 Loss 5.48087\n",
      "val Class Accuracy: [0.907,0.954,0.807,0.713,0.824,0.740,0.808,0.711,0.785,0.825]\n",
      "Best Prec@1: 80.740\n",
      "\n",
      "Epoch: [169][96/97], lr: 0.00010\tTime 0.310 (0.322)\tData 0.000 (0.020)\tLoss 1.1290 (2.8668)\tPrec@1 94.068 (88.215)\tPrec@5 100.000 (99.266)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 4.9388 (5.3120)\tPrec@1 87.000 (80.846)\tPrec@5 100.000 (98.571)\n",
      "val Results: Prec@1 80.840 Prec@5 98.630 Loss 5.35224\n",
      "val Class Accuracy: [0.903,0.958,0.805,0.708,0.806,0.744,0.812,0.728,0.790,0.830]\n",
      "Best Prec@1: 80.840\n",
      "\n",
      "Epoch: [170][96/97], lr: 0.00010\tTime 0.305 (0.319)\tData 0.000 (0.020)\tLoss 1.5271 (2.7048)\tPrec@1 93.220 (88.796)\tPrec@5 98.305 (99.355))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.2301 (5.0714)\tPrec@1 89.000 (80.659)\tPrec@5 100.000 (98.659)\n",
      "val Results: Prec@1 80.660 Prec@5 98.700 Loss 5.09851\n",
      "val Class Accuracy: [0.915,0.937,0.796,0.723,0.822,0.739,0.807,0.710,0.762,0.855]\n",
      "Best Prec@1: 80.840\n",
      "\n",
      "Epoch: [171][96/97], lr: 0.00010\tTime 0.308 (0.321)\tData 0.000 (0.021)\tLoss 2.0687 (2.6935)\tPrec@1 87.288 (88.667)\tPrec@5 100.000 (99.291)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 5.0570 (5.4137)\tPrec@1 88.000 (80.967)\tPrec@5 100.000 (98.516)\n",
      "val Results: Prec@1 80.870 Prec@5 98.570 Loss 5.45324\n",
      "val Class Accuracy: [0.900,0.956,0.809,0.731,0.810,0.738,0.819,0.711,0.796,0.817]\n",
      "Best Prec@1: 80.870\n",
      "\n",
      "Epoch: [172][96/97], lr: 0.00010\tTime 0.306 (0.319)\tData 0.000 (0.020)\tLoss 2.8392 (2.7628)\tPrec@1 90.678 (88.514)\tPrec@5 100.000 (99.315)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.8075 (5.9927)\tPrec@1 87.000 (80.582)\tPrec@5 100.000 (98.385)\n",
      "val Results: Prec@1 80.520 Prec@5 98.450 Loss 6.03476\n",
      "val Class Accuracy: [0.926,0.959,0.809,0.701,0.813,0.760,0.822,0.723,0.754,0.785]\n",
      "Best Prec@1: 80.870\n",
      "\n",
      "Epoch: [173][96/97], lr: 0.00010\tTime 0.309 (0.322)\tData 0.000 (0.021)\tLoss 1.9229 (2.6371)\tPrec@1 90.678 (89.078)\tPrec@5 100.000 (99.452)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 4.9897 (5.5065)\tPrec@1 89.000 (80.868)\tPrec@5 100.000 (98.495)\n",
      "val Results: Prec@1 80.770 Prec@5 98.540 Loss 5.55471\n",
      "val Class Accuracy: [0.916,0.955,0.805,0.716,0.820,0.756,0.806,0.710,0.779,0.814]\n",
      "Best Prec@1: 80.870\n",
      "\n",
      "Epoch: [174][96/97], lr: 0.00010\tTime 0.310 (0.319)\tData 0.000 (0.020)\tLoss 3.0835 (2.5871)\tPrec@1 84.746 (89.360)\tPrec@5 100.000 (99.347)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.1567 (5.0992)\tPrec@1 90.000 (81.055)\tPrec@5 100.000 (98.593)\n",
      "val Results: Prec@1 81.010 Prec@5 98.630 Loss 5.10923\n",
      "val Class Accuracy: [0.904,0.952,0.799,0.711,0.816,0.747,0.822,0.746,0.749,0.855]\n",
      "Best Prec@1: 81.010\n",
      "\n",
      "Epoch: [175][96/97], lr: 0.00010\tTime 0.310 (0.320)\tData 0.000 (0.020)\tLoss 1.4203 (2.5699)\tPrec@1 87.288 (89.150)\tPrec@5 100.000 (99.420)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 5.2055 (5.5874)\tPrec@1 86.000 (80.945)\tPrec@5 100.000 (98.440)\n",
      "val Results: Prec@1 80.860 Prec@5 98.470 Loss 5.61477\n",
      "val Class Accuracy: [0.919,0.962,0.800,0.727,0.816,0.733,0.810,0.751,0.754,0.814]\n",
      "Best Prec@1: 81.010\n",
      "\n",
      "Epoch: [176][96/97], lr: 0.00010\tTime 0.307 (0.319)\tData 0.000 (0.021)\tLoss 3.2785 (2.5798)\tPrec@1 83.051 (89.142)\tPrec@5 100.000 (99.460)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.7324 (5.5322)\tPrec@1 86.000 (80.747)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 80.720 Prec@5 98.510 Loss 5.56510\n",
      "val Class Accuracy: [0.911,0.963,0.785,0.717,0.811,0.775,0.818,0.721,0.760,0.811]\n",
      "Best Prec@1: 81.010\n",
      "\n",
      "Epoch: [177][96/97], lr: 0.00010\tTime 0.309 (0.321)\tData 0.000 (0.021)\tLoss 2.9319 (2.6410)\tPrec@1 90.678 (89.134)\tPrec@5 100.000 (99.484)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.9380 (5.5818)\tPrec@1 88.000 (80.989)\tPrec@5 100.000 (98.407)\n",
      "val Results: Prec@1 80.930 Prec@5 98.440 Loss 5.61480\n",
      "val Class Accuracy: [0.907,0.966,0.808,0.720,0.813,0.762,0.818,0.719,0.777,0.803]\n",
      "Best Prec@1: 81.010\n",
      "\n",
      "Epoch: [178][96/97], lr: 0.00010\tTime 0.306 (0.319)\tData 0.000 (0.021)\tLoss 3.8403 (2.4548)\tPrec@1 88.136 (89.392)\tPrec@5 99.153 (99.371))\n",
      "Test: [90/100]\tTime 0.074 (0.075)\tLoss 5.5143 (5.9988)\tPrec@1 86.000 (80.648)\tPrec@5 100.000 (98.407)\n",
      "val Results: Prec@1 80.520 Prec@5 98.440 Loss 6.04862\n",
      "val Class Accuracy: [0.914,0.971,0.806,0.713,0.821,0.757,0.819,0.730,0.756,0.765]\n",
      "Best Prec@1: 81.010\n",
      "\n",
      "Epoch: [179][96/97], lr: 0.00010\tTime 0.311 (0.321)\tData 0.000 (0.021)\tLoss 2.0575 (2.6356)\tPrec@1 90.678 (89.191)\tPrec@5 99.153 (99.323))\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 4.2126 (5.1610)\tPrec@1 87.000 (81.220)\tPrec@5 100.000 (98.571)\n",
      "val Results: Prec@1 81.170 Prec@5 98.610 Loss 5.17289\n",
      "val Class Accuracy: [0.910,0.952,0.801,0.725,0.803,0.761,0.799,0.759,0.770,0.837]\n",
      "Best Prec@1: 81.170\n",
      "\n",
      "Epoch: [180][96/97], lr: 0.00000\tTime 0.307 (0.319)\tData 0.000 (0.020)\tLoss 3.8502 (2.5441)\tPrec@1 88.983 (88.772)\tPrec@5 99.153 (99.387))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.4588 (5.3530)\tPrec@1 87.000 (81.121)\tPrec@5 100.000 (98.527)\n",
      "val Results: Prec@1 81.060 Prec@5 98.570 Loss 5.36647\n",
      "val Class Accuracy: [0.909,0.958,0.808,0.737,0.802,0.757,0.798,0.747,0.763,0.827]\n",
      "Best Prec@1: 81.170\n",
      "\n",
      "Epoch: [181][96/97], lr: 0.00000\tTime 0.309 (0.322)\tData 0.000 (0.021)\tLoss 1.9884 (2.4234)\tPrec@1 87.288 (89.118)\tPrec@5 98.305 (99.436))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.6417 (5.4401)\tPrec@1 89.000 (81.110)\tPrec@5 100.000 (98.527)\n",
      "val Results: Prec@1 81.090 Prec@5 98.550 Loss 5.45353\n",
      "val Class Accuracy: [0.902,0.965,0.815,0.724,0.808,0.754,0.806,0.761,0.756,0.818]\n",
      "Best Prec@1: 81.170\n",
      "\n",
      "Epoch: [182][96/97], lr: 0.00000\tTime 0.307 (0.320)\tData 0.000 (0.021)\tLoss 1.3478 (2.4241)\tPrec@1 88.983 (89.110)\tPrec@5 99.153 (99.347))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.6003 (5.4674)\tPrec@1 88.000 (80.967)\tPrec@5 100.000 (98.505)\n",
      "val Results: Prec@1 80.960 Prec@5 98.540 Loss 5.48091\n",
      "val Class Accuracy: [0.909,0.961,0.813,0.733,0.803,0.754,0.798,0.753,0.751,0.821]\n",
      "Best Prec@1: 81.170\n",
      "\n",
      "Epoch: [183][96/97], lr: 0.00000\tTime 0.309 (0.322)\tData 0.000 (0.021)\tLoss 3.5834 (2.4546)\tPrec@1 89.831 (89.191)\tPrec@5 100.000 (99.420)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 4.7610 (5.4894)\tPrec@1 86.000 (81.033)\tPrec@5 100.000 (98.505)\n",
      "val Results: Prec@1 80.970 Prec@5 98.530 Loss 5.50942\n",
      "val Class Accuracy: [0.911,0.963,0.807,0.729,0.812,0.740,0.804,0.756,0.760,0.815]\n",
      "Best Prec@1: 81.170\n",
      "\n",
      "Epoch: [184][96/97], lr: 0.00000\tTime 0.307 (0.320)\tData 0.000 (0.021)\tLoss 2.4895 (2.4636)\tPrec@1 90.678 (88.997)\tPrec@5 99.153 (99.226))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.5160 (5.3242)\tPrec@1 88.000 (81.308)\tPrec@5 100.000 (98.527)\n",
      "val Results: Prec@1 81.210 Prec@5 98.550 Loss 5.34081\n",
      "val Class Accuracy: [0.904,0.954,0.806,0.732,0.819,0.753,0.813,0.745,0.772,0.823]\n",
      "Best Prec@1: 81.210\n",
      "\n",
      "Epoch: [185][96/97], lr: 0.00000\tTime 0.308 (0.320)\tData 0.000 (0.020)\tLoss 2.2523 (2.5938)\tPrec@1 86.441 (89.239)\tPrec@5 100.000 (99.404)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.8560 (5.5271)\tPrec@1 88.000 (81.044)\tPrec@5 100.000 (98.473)\n",
      "val Results: Prec@1 81.000 Prec@5 98.500 Loss 5.54908\n",
      "val Class Accuracy: [0.907,0.964,0.816,0.719,0.807,0.755,0.811,0.747,0.764,0.810]\n",
      "Best Prec@1: 81.210\n",
      "\n",
      "Epoch: [186][96/97], lr: 0.00000\tTime 0.309 (0.321)\tData 0.000 (0.021)\tLoss 2.4041 (2.5219)\tPrec@1 88.983 (89.062)\tPrec@5 99.153 (99.331))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.6414 (5.5168)\tPrec@1 88.000 (81.165)\tPrec@5 100.000 (98.505)\n",
      "val Results: Prec@1 81.100 Prec@5 98.540 Loss 5.53235\n",
      "val Class Accuracy: [0.907,0.960,0.820,0.734,0.807,0.756,0.810,0.741,0.761,0.814]\n",
      "Best Prec@1: 81.210\n",
      "\n",
      "Epoch: [187][96/97], lr: 0.00000\tTime 0.310 (0.320)\tData 0.000 (0.020)\tLoss 4.1351 (2.5496)\tPrec@1 84.746 (89.038)\tPrec@5 99.153 (99.404))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.6290 (5.4683)\tPrec@1 89.000 (80.967)\tPrec@5 100.000 (98.560)\n",
      "val Results: Prec@1 80.910 Prec@5 98.590 Loss 5.48159\n",
      "val Class Accuracy: [0.907,0.960,0.816,0.733,0.806,0.752,0.800,0.741,0.754,0.822]\n",
      "Best Prec@1: 81.210\n",
      "\n",
      "Epoch: [188][96/97], lr: 0.00000\tTime 0.310 (0.321)\tData 0.000 (0.020)\tLoss 0.8649 (2.4487)\tPrec@1 92.373 (88.909)\tPrec@5 100.000 (99.355)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 4.6701 (5.4714)\tPrec@1 88.000 (81.176)\tPrec@5 100.000 (98.484)\n",
      "val Results: Prec@1 81.110 Prec@5 98.510 Loss 5.49265\n",
      "val Class Accuracy: [0.911,0.961,0.818,0.730,0.805,0.747,0.803,0.748,0.777,0.811]\n",
      "Best Prec@1: 81.210\n",
      "\n",
      "Epoch: [189][96/97], lr: 0.00000\tTime 0.308 (0.319)\tData 0.000 (0.020)\tLoss 1.9085 (2.4337)\tPrec@1 90.678 (89.271)\tPrec@5 100.000 (99.412)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.7897 (5.5022)\tPrec@1 88.000 (80.967)\tPrec@5 100.000 (98.538)\n",
      "val Results: Prec@1 80.900 Prec@5 98.570 Loss 5.52253\n",
      "val Class Accuracy: [0.910,0.965,0.807,0.723,0.808,0.748,0.805,0.741,0.771,0.812]\n",
      "Best Prec@1: 81.210\n",
      "\n",
      "Epoch: [190][96/97], lr: 0.00000\tTime 0.309 (0.320)\tData 0.000 (0.020)\tLoss 1.5462 (2.3511)\tPrec@1 89.831 (89.352)\tPrec@5 100.000 (99.371)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.9689 (5.6251)\tPrec@1 87.000 (81.143)\tPrec@5 100.000 (98.473)\n",
      "val Results: Prec@1 81.070 Prec@5 98.500 Loss 5.64454\n",
      "val Class Accuracy: [0.905,0.963,0.812,0.732,0.806,0.764,0.816,0.739,0.767,0.803]\n",
      "Best Prec@1: 81.210\n",
      "\n",
      "Epoch: [191][96/97], lr: 0.00000\tTime 0.314 (0.320)\tData 0.000 (0.020)\tLoss 3.7156 (2.4887)\tPrec@1 88.136 (89.699)\tPrec@5 100.000 (99.436)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.8256 (5.5234)\tPrec@1 88.000 (81.275)\tPrec@5 100.000 (98.495)\n",
      "val Results: Prec@1 81.200 Prec@5 98.530 Loss 5.54498\n",
      "val Class Accuracy: [0.903,0.965,0.805,0.747,0.825,0.741,0.811,0.741,0.771,0.811]\n",
      "Best Prec@1: 81.210\n",
      "\n",
      "Epoch: [192][96/97], lr: 0.00000\tTime 0.309 (0.321)\tData 0.000 (0.021)\tLoss 1.7453 (2.4586)\tPrec@1 91.525 (89.537)\tPrec@5 100.000 (99.395)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.5661 (5.4025)\tPrec@1 87.000 (81.154)\tPrec@5 100.000 (98.560)\n",
      "val Results: Prec@1 81.090 Prec@5 98.590 Loss 5.41677\n",
      "val Class Accuracy: [0.909,0.960,0.802,0.739,0.803,0.756,0.811,0.748,0.757,0.824]\n",
      "Best Prec@1: 81.210\n",
      "\n",
      "Epoch: [193][96/97], lr: 0.00000\tTime 0.307 (0.320)\tData 0.000 (0.020)\tLoss 1.9915 (2.4402)\tPrec@1 93.220 (89.263)\tPrec@5 100.000 (99.363)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.1128 (5.6816)\tPrec@1 88.000 (80.945)\tPrec@5 100.000 (98.495)\n",
      "val Results: Prec@1 80.890 Prec@5 98.520 Loss 5.70714\n",
      "val Class Accuracy: [0.910,0.967,0.814,0.738,0.809,0.741,0.806,0.748,0.761,0.795]\n",
      "Best Prec@1: 81.210\n",
      "\n",
      "Epoch: [194][96/97], lr: 0.00000\tTime 0.307 (0.321)\tData 0.000 (0.020)\tLoss 3.9596 (2.5514)\tPrec@1 86.441 (89.497)\tPrec@5 100.000 (99.549)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 4.9543 (5.5746)\tPrec@1 88.000 (81.165)\tPrec@5 100.000 (98.473)\n",
      "val Results: Prec@1 81.090 Prec@5 98.520 Loss 5.59513\n",
      "val Class Accuracy: [0.905,0.963,0.816,0.725,0.802,0.756,0.825,0.738,0.776,0.803]\n",
      "Best Prec@1: 81.210\n",
      "\n",
      "Epoch: [195][96/97], lr: 0.00000\tTime 0.306 (0.320)\tData 0.000 (0.021)\tLoss 3.1497 (2.4225)\tPrec@1 84.746 (89.521)\tPrec@5 100.000 (99.524)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.7542 (5.4362)\tPrec@1 89.000 (81.154)\tPrec@5 100.000 (98.538)\n",
      "val Results: Prec@1 81.060 Prec@5 98.570 Loss 5.45369\n",
      "val Class Accuracy: [0.908,0.964,0.814,0.732,0.824,0.739,0.798,0.740,0.767,0.820]\n",
      "Best Prec@1: 81.210\n",
      "\n",
      "Epoch: [196][96/97], lr: 0.00000\tTime 0.310 (0.321)\tData 0.000 (0.020)\tLoss 1.9881 (2.4724)\tPrec@1 85.593 (88.909)\tPrec@5 100.000 (99.395)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.1485 (5.6336)\tPrec@1 86.000 (81.044)\tPrec@5 100.000 (98.495)\n",
      "val Results: Prec@1 81.040 Prec@5 98.530 Loss 5.65425\n",
      "val Class Accuracy: [0.905,0.968,0.819,0.745,0.798,0.736,0.810,0.756,0.769,0.798]\n",
      "Best Prec@1: 81.210\n",
      "\n",
      "Epoch: [197][96/97], lr: 0.00000\tTime 0.308 (0.321)\tData 0.000 (0.021)\tLoss 2.2622 (2.5443)\tPrec@1 90.678 (89.465)\tPrec@5 100.000 (99.452)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.7312 (5.4900)\tPrec@1 89.000 (81.000)\tPrec@5 100.000 (98.549)\n",
      "val Results: Prec@1 80.960 Prec@5 98.580 Loss 5.50395\n",
      "val Class Accuracy: [0.912,0.963,0.809,0.742,0.805,0.740,0.808,0.746,0.750,0.821]\n",
      "Best Prec@1: 81.210\n",
      "\n",
      "Epoch: [198][96/97], lr: 0.00000\tTime 0.307 (0.321)\tData 0.000 (0.020)\tLoss 2.0953 (2.4216)\tPrec@1 89.831 (89.473)\tPrec@5 99.153 (99.412))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.0598 (5.5736)\tPrec@1 88.000 (81.110)\tPrec@5 100.000 (98.495)\n",
      "val Results: Prec@1 81.000 Prec@5 98.530 Loss 5.59658\n",
      "val Class Accuracy: [0.904,0.966,0.815,0.729,0.815,0.749,0.805,0.734,0.779,0.804]\n",
      "Best Prec@1: 81.210\n",
      "\n",
      "Epoch: [199][96/97], lr: 0.00000\tTime 0.309 (0.320)\tData 0.000 (0.020)\tLoss 1.5556 (2.4099)\tPrec@1 91.525 (89.223)\tPrec@5 100.000 (99.404)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.7100 (5.4071)\tPrec@1 89.000 (81.253)\tPrec@5 100.000 (98.505)\n",
      "val Results: Prec@1 81.190 Prec@5 98.540 Loss 5.42513\n",
      "val Class Accuracy: [0.906,0.964,0.808,0.743,0.808,0.738,0.806,0.748,0.782,0.816]\n",
      "Best Prec@1: 81.210\n",
      "\n",
      "Epoch: [200][96/97], lr: 0.00000\tTime 0.307 (0.321)\tData 0.000 (0.021)\tLoss 3.8367 (2.4356)\tPrec@1 89.831 (89.747)\tPrec@5 99.153 (99.492))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.7635 (5.4824)\tPrec@1 88.000 (81.143)\tPrec@5 100.000 (98.505)\n",
      "val Results: Prec@1 81.060 Prec@5 98.530 Loss 5.50211\n",
      "val Class Accuracy: [0.912,0.958,0.815,0.715,0.811,0.751,0.816,0.751,0.763,0.814]\n",
      "Best Prec@1: 81.210\n",
      "\n",
      "Epoch: [201][96/97], lr: 0.00000\tTime 0.305 (0.320)\tData 0.000 (0.020)\tLoss 2.4780 (2.5037)\tPrec@1 90.678 (89.618)\tPrec@5 100.000 (99.428)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.9930 (5.5759)\tPrec@1 87.000 (81.242)\tPrec@5 100.000 (98.495)\n",
      "val Results: Prec@1 81.140 Prec@5 98.530 Loss 5.60160\n",
      "val Class Accuracy: [0.911,0.963,0.807,0.716,0.824,0.759,0.816,0.745,0.771,0.802]\n",
      "Best Prec@1: 81.210\n",
      "\n",
      "Epoch: [202][96/97], lr: 0.00000\tTime 0.310 (0.321)\tData 0.000 (0.020)\tLoss 2.5520 (2.4688)\tPrec@1 93.220 (89.215)\tPrec@5 100.000 (99.412)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.9620 (5.5485)\tPrec@1 87.000 (81.253)\tPrec@5 100.000 (98.495)\n",
      "val Results: Prec@1 81.160 Prec@5 98.520 Loss 5.57417\n",
      "val Class Accuracy: [0.908,0.967,0.802,0.741,0.807,0.750,0.809,0.752,0.779,0.801]\n",
      "Best Prec@1: 81.210\n",
      "\n",
      "Epoch: [203][96/97], lr: 0.00000\tTime 0.307 (0.320)\tData 0.000 (0.021)\tLoss 3.4928 (2.4831)\tPrec@1 85.593 (89.086)\tPrec@5 100.000 (99.291)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.8180 (5.4693)\tPrec@1 88.000 (81.220)\tPrec@5 100.000 (98.484)\n",
      "val Results: Prec@1 81.160 Prec@5 98.530 Loss 5.49191\n",
      "val Class Accuracy: [0.908,0.964,0.810,0.743,0.803,0.747,0.789,0.757,0.784,0.811]\n",
      "Best Prec@1: 81.210\n",
      "\n",
      "Epoch: [204][96/97], lr: 0.00000\tTime 0.307 (0.321)\tData 0.000 (0.020)\tLoss 1.5211 (2.5189)\tPrec@1 92.373 (89.433)\tPrec@5 98.305 (99.468))\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 5.0323 (5.6183)\tPrec@1 87.000 (81.165)\tPrec@5 100.000 (98.429)\n",
      "val Results: Prec@1 81.120 Prec@5 98.460 Loss 5.63861\n",
      "val Class Accuracy: [0.913,0.966,0.805,0.732,0.809,0.761,0.807,0.752,0.760,0.807]\n",
      "Best Prec@1: 81.210\n",
      "\n",
      "Epoch: [205][96/97], lr: 0.00000\tTime 0.307 (0.319)\tData 0.000 (0.021)\tLoss 2.8859 (2.3305)\tPrec@1 89.831 (89.642)\tPrec@5 98.305 (99.452))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.7550 (5.3977)\tPrec@1 89.000 (81.319)\tPrec@5 100.000 (98.495)\n",
      "val Results: Prec@1 81.220 Prec@5 98.520 Loss 5.41645\n",
      "val Class Accuracy: [0.902,0.961,0.811,0.725,0.814,0.750,0.812,0.753,0.777,0.817]\n",
      "Best Prec@1: 81.220\n",
      "\n",
      "Epoch: [206][96/97], lr: 0.00000\tTime 0.309 (0.323)\tData 0.000 (0.021)\tLoss 2.6621 (2.5137)\tPrec@1 87.288 (89.433)\tPrec@5 99.153 (99.452))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.0644 (5.5809)\tPrec@1 88.000 (81.121)\tPrec@5 100.000 (98.484)\n",
      "val Results: Prec@1 81.050 Prec@5 98.510 Loss 5.60612\n",
      "val Class Accuracy: [0.902,0.967,0.816,0.721,0.805,0.757,0.810,0.754,0.780,0.793]\n",
      "Best Prec@1: 81.220\n",
      "\n",
      "Epoch: [207][96/97], lr: 0.00000\tTime 0.307 (0.320)\tData 0.000 (0.020)\tLoss 3.6603 (2.5285)\tPrec@1 87.288 (89.086)\tPrec@5 99.153 (99.436))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.1869 (5.7014)\tPrec@1 88.000 (80.956)\tPrec@5 100.000 (98.495)\n",
      "val Results: Prec@1 80.870 Prec@5 98.520 Loss 5.72721\n",
      "val Class Accuracy: [0.905,0.970,0.813,0.735,0.809,0.742,0.809,0.739,0.778,0.787]\n",
      "Best Prec@1: 81.220\n",
      "\n",
      "Epoch: [208][96/97], lr: 0.00000\tTime 0.310 (0.322)\tData 0.000 (0.021)\tLoss 2.2401 (2.5259)\tPrec@1 88.136 (89.175)\tPrec@5 100.000 (99.395)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.1347 (5.7007)\tPrec@1 88.000 (81.099)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 81.000 Prec@5 98.510 Loss 5.72429\n",
      "val Class Accuracy: [0.910,0.967,0.812,0.734,0.822,0.743,0.816,0.729,0.770,0.797]\n",
      "Best Prec@1: 81.220\n",
      "\n",
      "Epoch: [209][96/97], lr: 0.00000\tTime 0.307 (0.320)\tData 0.000 (0.021)\tLoss 2.9394 (2.4977)\tPrec@1 88.983 (89.199)\tPrec@5 100.000 (99.307)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 4.9757 (5.6050)\tPrec@1 87.000 (81.099)\tPrec@5 100.000 (98.484)\n",
      "val Results: Prec@1 81.010 Prec@5 98.510 Loss 5.62978\n",
      "val Class Accuracy: [0.914,0.963,0.806,0.736,0.809,0.742,0.813,0.742,0.768,0.808]\n",
      "Best Prec@1: 81.220\n",
      "\n",
      "Epoch: [210][96/97], lr: 0.00000\tTime 0.310 (0.321)\tData 0.000 (0.020)\tLoss 2.0739 (2.4774)\tPrec@1 86.441 (89.142)\tPrec@5 100.000 (99.476)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.3443 (5.8189)\tPrec@1 87.000 (80.978)\tPrec@5 100.000 (98.495)\n",
      "val Results: Prec@1 80.880 Prec@5 98.530 Loss 5.84626\n",
      "val Class Accuracy: [0.907,0.971,0.813,0.745,0.802,0.755,0.795,0.742,0.777,0.781]\n",
      "Best Prec@1: 81.220\n",
      "\n",
      "Epoch: [211][96/97], lr: 0.00000\tTime 0.306 (0.319)\tData 0.000 (0.021)\tLoss 3.1581 (2.5572)\tPrec@1 89.831 (89.376)\tPrec@5 99.153 (99.355))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.0245 (5.6458)\tPrec@1 87.000 (81.033)\tPrec@5 100.000 (98.538)\n",
      "val Results: Prec@1 80.990 Prec@5 98.560 Loss 5.66612\n",
      "val Class Accuracy: [0.914,0.966,0.805,0.727,0.810,0.766,0.804,0.748,0.757,0.802]\n",
      "Best Prec@1: 81.220\n",
      "\n",
      "Epoch: [212][96/97], lr: 0.00000\tTime 0.311 (0.321)\tData 0.000 (0.020)\tLoss 4.7214 (2.4713)\tPrec@1 85.593 (89.086)\tPrec@5 100.000 (99.323)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 5.0774 (5.5861)\tPrec@1 87.000 (81.154)\tPrec@5 100.000 (98.516)\n",
      "val Results: Prec@1 81.130 Prec@5 98.550 Loss 5.61043\n",
      "val Class Accuracy: [0.906,0.968,0.814,0.719,0.806,0.756,0.806,0.765,0.779,0.794]\n",
      "Best Prec@1: 81.220\n",
      "\n",
      "Epoch: [213][96/97], lr: 0.00000\tTime 0.306 (0.319)\tData 0.000 (0.020)\tLoss 1.5680 (2.3891)\tPrec@1 89.831 (89.545)\tPrec@5 97.458 (99.299))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.9254 (5.5926)\tPrec@1 88.000 (81.066)\tPrec@5 100.000 (98.527)\n",
      "val Results: Prec@1 81.040 Prec@5 98.560 Loss 5.61031\n",
      "val Class Accuracy: [0.910,0.966,0.801,0.751,0.799,0.749,0.814,0.742,0.761,0.811]\n",
      "Best Prec@1: 81.220\n",
      "\n",
      "Epoch: [214][96/97], lr: 0.00000\tTime 0.309 (0.321)\tData 0.000 (0.021)\tLoss 1.1876 (2.4225)\tPrec@1 94.068 (89.553)\tPrec@5 100.000 (99.412)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 5.0848 (5.6244)\tPrec@1 88.000 (81.066)\tPrec@5 100.000 (98.495)\n",
      "val Results: Prec@1 80.980 Prec@5 98.520 Loss 5.64730\n",
      "val Class Accuracy: [0.901,0.967,0.813,0.726,0.813,0.761,0.804,0.744,0.773,0.796]\n",
      "Best Prec@1: 81.220\n",
      "\n",
      "Epoch: [215][96/97], lr: 0.00000\tTime 0.308 (0.320)\tData 0.000 (0.020)\tLoss 0.5011 (2.5701)\tPrec@1 94.068 (89.497)\tPrec@5 99.153 (99.347))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.1827 (5.7373)\tPrec@1 87.000 (80.934)\tPrec@5 100.000 (98.429)\n",
      "val Results: Prec@1 80.870 Prec@5 98.460 Loss 5.76111\n",
      "val Class Accuracy: [0.910,0.969,0.807,0.742,0.804,0.758,0.805,0.737,0.767,0.788]\n",
      "Best Prec@1: 81.220\n",
      "\n",
      "Epoch: [216][96/97], lr: 0.00000\tTime 0.311 (0.322)\tData 0.000 (0.021)\tLoss 3.6873 (2.3966)\tPrec@1 88.983 (89.537)\tPrec@5 98.305 (99.484))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.2095 (5.6824)\tPrec@1 87.000 (81.187)\tPrec@5 100.000 (98.473)\n",
      "val Results: Prec@1 81.090 Prec@5 98.500 Loss 5.71092\n",
      "val Class Accuracy: [0.908,0.967,0.806,0.730,0.804,0.760,0.819,0.746,0.785,0.784]\n",
      "Best Prec@1: 81.220\n",
      "\n",
      "Epoch: [217][96/97], lr: 0.00000\tTime 0.306 (0.319)\tData 0.000 (0.021)\tLoss 2.3862 (2.4262)\tPrec@1 83.898 (89.827)\tPrec@5 99.153 (99.428))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.1965 (5.7162)\tPrec@1 87.000 (81.077)\tPrec@5 100.000 (98.495)\n",
      "val Results: Prec@1 81.000 Prec@5 98.520 Loss 5.73891\n",
      "val Class Accuracy: [0.911,0.964,0.808,0.737,0.813,0.750,0.813,0.744,0.765,0.795]\n",
      "Best Prec@1: 81.220\n",
      "\n",
      "Epoch: [218][96/97], lr: 0.00000\tTime 0.312 (0.321)\tData 0.000 (0.020)\tLoss 3.0981 (2.4614)\tPrec@1 87.288 (89.674)\tPrec@5 100.000 (99.452)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.0760 (5.5793)\tPrec@1 87.000 (81.165)\tPrec@5 100.000 (98.495)\n",
      "val Results: Prec@1 81.110 Prec@5 98.520 Loss 5.60094\n",
      "val Class Accuracy: [0.904,0.968,0.815,0.746,0.809,0.735,0.807,0.754,0.776,0.797]\n",
      "Best Prec@1: 81.220\n",
      "\n",
      "Epoch: [219][96/97], lr: 0.00000\tTime 0.309 (0.319)\tData 0.000 (0.020)\tLoss 3.4069 (2.4926)\tPrec@1 88.983 (89.441)\tPrec@5 99.153 (99.404))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.0093 (5.5403)\tPrec@1 87.000 (81.143)\tPrec@5 100.000 (98.429)\n",
      "val Results: Prec@1 81.090 Prec@5 98.460 Loss 5.56430\n",
      "val Class Accuracy: [0.899,0.967,0.810,0.735,0.809,0.747,0.812,0.750,0.782,0.798]\n",
      "Best Prec@1: 81.220\n",
      "\n",
      "Epoch: [220][96/97], lr: 0.00000\tTime 0.308 (0.321)\tData 0.000 (0.021)\tLoss 1.3675 (2.4963)\tPrec@1 90.678 (89.158)\tPrec@5 100.000 (99.476)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.1675 (5.6375)\tPrec@1 86.000 (81.176)\tPrec@5 100.000 (98.538)\n",
      "val Results: Prec@1 81.070 Prec@5 98.570 Loss 5.66184\n",
      "val Class Accuracy: [0.910,0.968,0.805,0.735,0.815,0.741,0.811,0.758,0.772,0.792]\n",
      "Best Prec@1: 81.220\n",
      "\n",
      "Epoch: [221][96/97], lr: 0.00000\tTime 0.308 (0.320)\tData 0.000 (0.021)\tLoss 3.7084 (2.4374)\tPrec@1 82.203 (89.449)\tPrec@5 100.000 (99.476)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.2797 (5.7880)\tPrec@1 88.000 (80.868)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 80.800 Prec@5 98.500 Loss 5.81280\n",
      "val Class Accuracy: [0.909,0.969,0.811,0.733,0.816,0.753,0.795,0.747,0.761,0.786]\n",
      "Best Prec@1: 81.220\n",
      "\n",
      "Epoch: [222][96/97], lr: 0.00000\tTime 0.306 (0.320)\tData 0.000 (0.020)\tLoss 5.5744 (2.4862)\tPrec@1 90.678 (89.384)\tPrec@5 99.153 (99.387))\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 5.3027 (5.6822)\tPrec@1 87.000 (81.209)\tPrec@5 100.000 (98.473)\n",
      "val Results: Prec@1 81.170 Prec@5 98.500 Loss 5.70892\n",
      "val Class Accuracy: [0.911,0.970,0.818,0.746,0.804,0.731,0.816,0.753,0.781,0.787]\n",
      "Best Prec@1: 81.220\n",
      "\n",
      "Epoch: [223][96/97], lr: 0.00000\tTime 0.310 (0.321)\tData 0.000 (0.021)\tLoss 6.5395 (2.4205)\tPrec@1 93.220 (89.755)\tPrec@5 98.305 (99.387))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.1106 (5.6327)\tPrec@1 87.000 (81.198)\tPrec@5 100.000 (98.484)\n",
      "val Results: Prec@1 81.140 Prec@5 98.520 Loss 5.66005\n",
      "val Class Accuracy: [0.914,0.966,0.802,0.725,0.815,0.755,0.814,0.760,0.772,0.791]\n",
      "Best Prec@1: 81.220\n",
      "\n",
      "Epoch: [224][96/97], lr: 0.00000\tTime 0.309 (0.320)\tData 0.000 (0.020)\tLoss 2.4933 (2.4352)\tPrec@1 88.136 (89.441)\tPrec@5 99.153 (99.347))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.8652 (5.4619)\tPrec@1 88.000 (81.341)\tPrec@5 100.000 (98.495)\n",
      "val Results: Prec@1 81.250 Prec@5 98.530 Loss 5.48632\n",
      "val Class Accuracy: [0.907,0.963,0.812,0.724,0.818,0.751,0.804,0.753,0.784,0.809]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [225][96/97], lr: 0.00000\tTime 0.309 (0.321)\tData 0.000 (0.020)\tLoss 2.7963 (2.3826)\tPrec@1 87.288 (89.481)\tPrec@5 99.153 (99.468))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.0949 (5.6341)\tPrec@1 88.000 (81.165)\tPrec@5 100.000 (98.473)\n",
      "val Results: Prec@1 81.040 Prec@5 98.510 Loss 5.66263\n",
      "val Class Accuracy: [0.912,0.964,0.811,0.724,0.823,0.742,0.813,0.748,0.777,0.790]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [226][96/97], lr: 0.00000\tTime 0.307 (0.319)\tData 0.000 (0.020)\tLoss 3.4097 (2.4314)\tPrec@1 85.593 (89.054)\tPrec@5 97.458 (99.492))\n",
      "Test: [90/100]\tTime 0.074 (0.075)\tLoss 5.2182 (5.6799)\tPrec@1 88.000 (81.187)\tPrec@5 100.000 (98.527)\n",
      "val Results: Prec@1 81.100 Prec@5 98.550 Loss 5.70840\n",
      "val Class Accuracy: [0.911,0.968,0.812,0.736,0.814,0.740,0.818,0.746,0.777,0.788]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [227][96/97], lr: 0.00000\tTime 0.309 (0.321)\tData 0.000 (0.020)\tLoss 0.9189 (2.4606)\tPrec@1 98.305 (89.529)\tPrec@5 99.153 (99.379))\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 5.1603 (5.6201)\tPrec@1 87.000 (81.165)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 81.070 Prec@5 98.490 Loss 5.64747\n",
      "val Class Accuracy: [0.912,0.969,0.805,0.720,0.811,0.748,0.819,0.750,0.784,0.789]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [228][96/97], lr: 0.00000\tTime 0.306 (0.318)\tData 0.000 (0.020)\tLoss 4.2320 (2.4516)\tPrec@1 91.525 (89.707)\tPrec@5 100.000 (99.428)\n",
      "Test: [90/100]\tTime 0.074 (0.075)\tLoss 5.1317 (5.6677)\tPrec@1 87.000 (81.011)\tPrec@5 100.000 (98.484)\n",
      "val Results: Prec@1 80.980 Prec@5 98.510 Loss 5.68523\n",
      "val Class Accuracy: [0.911,0.968,0.805,0.738,0.816,0.753,0.803,0.744,0.758,0.802]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [229][96/97], lr: 0.00000\tTime 0.309 (0.321)\tData 0.000 (0.020)\tLoss 1.5138 (2.4056)\tPrec@1 91.525 (89.521)\tPrec@5 100.000 (99.331)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.0809 (5.7074)\tPrec@1 87.000 (80.890)\tPrec@5 100.000 (98.505)\n",
      "val Results: Prec@1 80.820 Prec@5 98.540 Loss 5.73233\n",
      "val Class Accuracy: [0.916,0.968,0.800,0.738,0.809,0.751,0.804,0.744,0.765,0.787]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [230][96/97], lr: 0.00000\tTime 0.305 (0.318)\tData 0.000 (0.020)\tLoss 2.9413 (2.5713)\tPrec@1 93.220 (89.545)\tPrec@5 100.000 (99.468)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 5.1257 (5.6340)\tPrec@1 86.000 (81.077)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 81.030 Prec@5 98.500 Loss 5.65859\n",
      "val Class Accuracy: [0.907,0.967,0.809,0.738,0.800,0.756,0.810,0.745,0.775,0.796]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [231][96/97], lr: 0.00000\tTime 0.312 (0.321)\tData 0.000 (0.021)\tLoss 1.8019 (2.3497)\tPrec@1 88.983 (89.650)\tPrec@5 98.305 (99.395))\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 5.2442 (5.6449)\tPrec@1 87.000 (81.209)\tPrec@5 100.000 (98.440)\n",
      "val Results: Prec@1 81.120 Prec@5 98.470 Loss 5.67291\n",
      "val Class Accuracy: [0.906,0.969,0.811,0.737,0.810,0.748,0.808,0.746,0.787,0.790]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [232][96/97], lr: 0.00000\tTime 0.306 (0.319)\tData 0.000 (0.020)\tLoss 2.6517 (2.3134)\tPrec@1 89.831 (89.578)\tPrec@5 98.305 (99.404))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.9287 (5.5767)\tPrec@1 89.000 (81.121)\tPrec@5 100.000 (98.484)\n",
      "val Results: Prec@1 81.010 Prec@5 98.520 Loss 5.60156\n",
      "val Class Accuracy: [0.909,0.962,0.811,0.727,0.819,0.750,0.801,0.747,0.771,0.804]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [233][96/97], lr: 0.00000\tTime 0.308 (0.320)\tData 0.000 (0.020)\tLoss 3.5522 (2.5211)\tPrec@1 88.983 (89.134)\tPrec@5 100.000 (99.347)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.2127 (5.7185)\tPrec@1 88.000 (80.956)\tPrec@5 100.000 (98.484)\n",
      "val Results: Prec@1 80.880 Prec@5 98.500 Loss 5.74480\n",
      "val Class Accuracy: [0.909,0.969,0.813,0.749,0.808,0.738,0.800,0.747,0.771,0.784]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [234][96/97], lr: 0.00000\tTime 0.309 (0.320)\tData 0.000 (0.020)\tLoss 2.4643 (2.4169)\tPrec@1 89.831 (89.650)\tPrec@5 100.000 (99.436)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.4886 (5.8252)\tPrec@1 87.000 (81.055)\tPrec@5 100.000 (98.484)\n",
      "val Results: Prec@1 80.930 Prec@5 98.520 Loss 5.85150\n",
      "val Class Accuracy: [0.908,0.970,0.806,0.736,0.818,0.748,0.817,0.745,0.769,0.776]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [235][96/97], lr: 0.00000\tTime 0.309 (0.321)\tData 0.000 (0.021)\tLoss 1.9736 (2.3914)\tPrec@1 88.136 (89.618)\tPrec@5 100.000 (99.557)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 5.2670 (5.7314)\tPrec@1 87.000 (80.945)\tPrec@5 100.000 (98.451)\n",
      "val Results: Prec@1 80.870 Prec@5 98.490 Loss 5.75613\n",
      "val Class Accuracy: [0.909,0.969,0.808,0.738,0.803,0.743,0.811,0.757,0.765,0.784]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [236][96/97], lr: 0.00000\tTime 0.305 (0.319)\tData 0.000 (0.021)\tLoss 1.9258 (2.5326)\tPrec@1 93.220 (89.634)\tPrec@5 99.153 (99.331))\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 5.1891 (5.7024)\tPrec@1 88.000 (81.165)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 81.040 Prec@5 98.490 Loss 5.73172\n",
      "val Class Accuracy: [0.912,0.967,0.811,0.724,0.814,0.745,0.823,0.748,0.775,0.785]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [237][96/97], lr: 0.00000\tTime 0.311 (0.321)\tData 0.000 (0.021)\tLoss 1.8522 (2.5349)\tPrec@1 87.288 (89.473)\tPrec@5 98.305 (99.444))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.2486 (5.6580)\tPrec@1 87.000 (81.187)\tPrec@5 100.000 (98.484)\n",
      "val Results: Prec@1 81.090 Prec@5 98.510 Loss 5.68817\n",
      "val Class Accuracy: [0.909,0.966,0.805,0.745,0.815,0.738,0.822,0.739,0.787,0.783]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [238][96/97], lr: 0.00000\tTime 0.306 (0.320)\tData 0.000 (0.020)\tLoss 1.9114 (2.4946)\tPrec@1 88.136 (89.392)\tPrec@5 100.000 (99.484)\n",
      "Test: [90/100]\tTime 0.074 (0.075)\tLoss 5.0845 (5.6511)\tPrec@1 88.000 (81.132)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 81.020 Prec@5 98.490 Loss 5.68201\n",
      "val Class Accuracy: [0.908,0.964,0.818,0.721,0.812,0.752,0.815,0.736,0.787,0.789]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [239][96/97], lr: 0.00000\tTime 0.310 (0.322)\tData 0.000 (0.020)\tLoss 1.3057 (2.5005)\tPrec@1 90.678 (89.223)\tPrec@5 100.000 (99.363)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 5.0643 (5.6509)\tPrec@1 88.000 (81.088)\tPrec@5 100.000 (98.527)\n",
      "val Results: Prec@1 81.010 Prec@5 98.550 Loss 5.67383\n",
      "val Class Accuracy: [0.909,0.965,0.821,0.732,0.807,0.743,0.822,0.735,0.769,0.798]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [240][96/97], lr: 0.00000\tTime 0.307 (0.319)\tData 0.000 (0.020)\tLoss 1.6867 (2.4144)\tPrec@1 88.983 (89.868)\tPrec@5 100.000 (99.573)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.3878 (5.8348)\tPrec@1 87.000 (80.901)\tPrec@5 100.000 (98.516)\n",
      "val Results: Prec@1 80.820 Prec@5 98.530 Loss 5.85455\n",
      "val Class Accuracy: [0.908,0.969,0.808,0.735,0.801,0.760,0.821,0.741,0.753,0.786]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [241][96/97], lr: 0.00000\tTime 0.309 (0.321)\tData 0.000 (0.020)\tLoss 2.0797 (2.2403)\tPrec@1 83.898 (89.932)\tPrec@5 99.153 (99.379))\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 5.2507 (5.7286)\tPrec@1 87.000 (81.077)\tPrec@5 100.000 (98.440)\n",
      "val Results: Prec@1 80.990 Prec@5 98.480 Loss 5.75958\n",
      "val Class Accuracy: [0.911,0.970,0.806,0.732,0.813,0.749,0.808,0.744,0.784,0.782]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [242][96/97], lr: 0.00000\tTime 0.305 (0.318)\tData 0.000 (0.020)\tLoss 2.1263 (2.4625)\tPrec@1 88.136 (89.376)\tPrec@5 98.305 (99.363))\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 4.8437 (5.5140)\tPrec@1 87.000 (81.165)\tPrec@5 100.000 (98.429)\n",
      "val Results: Prec@1 81.050 Prec@5 98.460 Loss 5.54079\n",
      "val Class Accuracy: [0.909,0.961,0.813,0.705,0.814,0.759,0.814,0.749,0.775,0.806]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [243][96/97], lr: 0.00000\tTime 0.307 (0.321)\tData 0.000 (0.020)\tLoss 1.9762 (2.4330)\tPrec@1 90.678 (89.521)\tPrec@5 100.000 (99.331)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.0506 (5.6609)\tPrec@1 87.000 (81.088)\tPrec@5 100.000 (98.484)\n",
      "val Results: Prec@1 81.020 Prec@5 98.510 Loss 5.68833\n",
      "val Class Accuracy: [0.915,0.965,0.807,0.727,0.807,0.761,0.807,0.746,0.767,0.800]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [244][96/97], lr: 0.00000\tTime 0.307 (0.319)\tData 0.000 (0.021)\tLoss 2.6370 (2.4334)\tPrec@1 94.068 (89.513)\tPrec@5 100.000 (99.444)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 5.2617 (5.6815)\tPrec@1 87.000 (81.121)\tPrec@5 100.000 (98.505)\n",
      "val Results: Prec@1 81.060 Prec@5 98.530 Loss 5.70422\n",
      "val Class Accuracy: [0.909,0.970,0.802,0.735,0.808,0.750,0.823,0.754,0.767,0.788]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [245][96/97], lr: 0.00000\tTime 0.312 (0.323)\tData 0.000 (0.021)\tLoss 1.2329 (2.3372)\tPrec@1 90.678 (89.852)\tPrec@5 99.153 (99.460))\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 5.2980 (5.7172)\tPrec@1 88.000 (81.066)\tPrec@5 100.000 (98.451)\n",
      "val Results: Prec@1 80.950 Prec@5 98.490 Loss 5.74815\n",
      "val Class Accuracy: [0.907,0.969,0.809,0.738,0.819,0.742,0.801,0.743,0.785,0.782]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [246][96/97], lr: 0.00000\tTime 0.307 (0.319)\tData 0.000 (0.020)\tLoss 3.1194 (2.3118)\tPrec@1 90.678 (89.827)\tPrec@5 100.000 (99.460)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.2611 (5.7316)\tPrec@1 88.000 (80.956)\tPrec@5 100.000 (98.473)\n",
      "val Results: Prec@1 80.860 Prec@5 98.500 Loss 5.75838\n",
      "val Class Accuracy: [0.908,0.965,0.819,0.734,0.805,0.740,0.812,0.742,0.777,0.784]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [247][96/97], lr: 0.00000\tTime 0.310 (0.321)\tData 0.000 (0.020)\tLoss 4.1026 (2.4436)\tPrec@1 91.525 (89.739)\tPrec@5 100.000 (99.355)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 5.0413 (5.7184)\tPrec@1 87.000 (81.000)\tPrec@5 100.000 (98.505)\n",
      "val Results: Prec@1 80.940 Prec@5 98.520 Loss 5.74111\n",
      "val Class Accuracy: [0.913,0.964,0.808,0.732,0.803,0.763,0.812,0.744,0.754,0.801]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [248][96/97], lr: 0.00000\tTime 0.305 (0.320)\tData 0.000 (0.021)\tLoss 2.1557 (2.5038)\tPrec@1 87.288 (89.771)\tPrec@5 100.000 (99.395)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.4552 (5.8630)\tPrec@1 88.000 (80.945)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 80.880 Prec@5 98.490 Loss 5.89513\n",
      "val Class Accuracy: [0.913,0.971,0.813,0.720,0.820,0.749,0.811,0.742,0.779,0.770]\n",
      "Best Prec@1: 81.250\n",
      "\n",
      "Epoch: [249][96/97], lr: 0.00000\tTime 0.309 (0.320)\tData 0.000 (0.020)\tLoss 3.2880 (2.3481)\tPrec@1 91.525 (89.328)\tPrec@5 99.153 (99.460))\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.1256 (5.6100)\tPrec@1 88.000 (81.286)\tPrec@5 100.000 (98.505)\n",
      "val Results: Prec@1 81.170 Prec@5 98.530 Loss 5.63555\n",
      "val Class Accuracy: [0.908,0.966,0.813,0.729,0.820,0.745,0.821,0.742,0.780,0.793]\n",
      "Best Prec@1: 81.250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python cifar_train_lorot-E.py --gpu 0 --imb_type exp --exp_str flipscldam --imb_factor 0.01 --loss_type LDAM --train_rule DRW -m \"fliplr sc\" --epochs 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDAM-DRW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar_train_lorot-E.py:175: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.\n",
      "  warnings.warn(\n",
      "Use GPU: 0 for training\n",
      "=> creating model 'resnet32'\n",
      "num_trans : 16\n",
      "num_flipped : 4\n",
      "num shuffled channel : 24\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[5000, 2997, 1796, 1077, 645, 387, 232, 139, 83, 50]\n",
      "/media/aristo/Data A/documents/kuliah/Project/Localizable-Rotation/Imbalanced/losses.py:49: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/TensorCompare.cpp:402.)\n",
      "  output = torch.where(index, x_m, x)\n",
      "Epoch: [0][0/97], lr: 0.00200\tTime 2.203 (2.203)\tData 0.163 (0.163)\tLoss 12.3502 (12.3502)\tPrec@1 12.500 (12.500)\tPrec@5 18.750 (18.750)\n",
      "Epoch: [0][10/97], lr: 0.00200\tTime 0.313 (0.461)\tData 0.000 (0.015)\tLoss 7.5919 (8.7141)\tPrec@1 47.656 (39.773)\tPrec@5 86.719 (82.315)\n",
      "Epoch: [0][20/97], lr: 0.00200\tTime 0.308 (0.390)\tData 0.000 (0.015)\tLoss 6.9558 (8.0534)\tPrec@1 41.406 (41.518)\tPrec@5 91.406 (84.673)\n",
      "Epoch: [0][30/97], lr: 0.00200\tTime 0.302 (0.362)\tData 0.000 (0.016)\tLoss 6.0736 (7.4949)\tPrec@1 54.688 (44.430)\tPrec@5 90.625 (86.190)\n",
      "Epoch: [0][40/97], lr: 0.00200\tTime 0.313 (0.349)\tData 0.000 (0.016)\tLoss 6.4598 (7.1422)\tPrec@1 50.781 (46.513)\tPrec@5 94.531 (87.100)\n",
      "Epoch: [0][50/97], lr: 0.00200\tTime 0.318 (0.341)\tData 0.000 (0.016)\tLoss 5.5058 (6.8738)\tPrec@1 60.938 (48.208)\tPrec@5 93.750 (88.128)\n",
      "Epoch: [0][60/97], lr: 0.00200\tTime 0.315 (0.338)\tData 0.000 (0.016)\tLoss 5.0531 (6.6882)\tPrec@1 63.281 (49.616)\tPrec@5 93.750 (88.973)\n",
      "Epoch: [0][70/97], lr: 0.00200\tTime 0.310 (0.334)\tData 0.000 (0.016)\tLoss 6.5841 (6.5150)\tPrec@1 47.656 (50.803)\tPrec@5 91.406 (89.646)\n",
      "Epoch: [0][80/97], lr: 0.00200\tTime 0.301 (0.331)\tData 0.000 (0.016)\tLoss 4.8578 (6.4364)\tPrec@1 64.062 (51.591)\tPrec@5 96.094 (90.037)\n",
      "Epoch: [0][90/97], lr: 0.00200\tTime 0.300 (0.328)\tData 0.000 (0.016)\tLoss 5.7059 (6.3308)\tPrec@1 58.594 (52.412)\tPrec@5 96.094 (90.428)\n",
      "Epoch: [0][96/97], lr: 0.00200\tTime 0.862 (0.332)\tData 0.000 (0.017)\tLoss 4.6113 (6.2501)\tPrec@1 67.797 (52.966)\tPrec@5 94.915 (90.730)\n",
      "Test: [0/100]\tTime 0.402 (0.402)\tLoss 16.5344 (16.5344)\tPrec@1 19.000 (19.000)\tPrec@5 69.000 (69.000)\n",
      "Test: [10/100]\tTime 0.072 (0.102)\tLoss 12.9634 (15.5302)\tPrec@1 31.000 (22.000)\tPrec@5 72.000 (70.727)\n",
      "Test: [20/100]\tTime 0.072 (0.088)\tLoss 13.7395 (15.5065)\tPrec@1 25.000 (22.143)\tPrec@5 74.000 (71.000)\n",
      "Test: [30/100]\tTime 0.072 (0.083)\tLoss 13.8239 (15.4327)\tPrec@1 24.000 (22.548)\tPrec@5 71.000 (70.613)\n",
      "Test: [40/100]\tTime 0.072 (0.080)\tLoss 15.6480 (15.5529)\tPrec@1 18.000 (22.512)\tPrec@5 70.000 (70.024)\n",
      "Test: [50/100]\tTime 0.072 (0.079)\tLoss 15.3203 (15.4588)\tPrec@1 22.000 (22.765)\tPrec@5 69.000 (70.176)\n",
      "Test: [60/100]\tTime 0.072 (0.078)\tLoss 12.2573 (15.3639)\tPrec@1 25.000 (22.656)\tPrec@5 79.000 (69.918)\n",
      "Test: [70/100]\tTime 0.072 (0.077)\tLoss 15.3445 (15.3105)\tPrec@1 25.000 (22.803)\tPrec@5 72.000 (70.113)\n",
      "Test: [80/100]\tTime 0.072 (0.076)\tLoss 14.5489 (15.2679)\tPrec@1 31.000 (23.198)\tPrec@5 67.000 (70.309)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 15.5695 (15.2922)\tPrec@1 29.000 (22.978)\tPrec@5 71.000 (70.297)\n",
      "val Results: Prec@1 22.990 Prec@5 70.210 Loss 15.33439\n",
      "val Class Accuracy: [0.809,0.868,0.359,0.263,0.000,0.000,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 22.990\n",
      "\n",
      "Epoch: [1][0/97], lr: 0.00400\tTime 0.480 (0.480)\tData 0.299 (0.299)\tLoss 5.6121 (5.6121)\tPrec@1 59.375 (59.375)\tPrec@5 92.188 (92.188)\n",
      "Epoch: [1][10/97], lr: 0.00400\tTime 0.311 (0.328)\tData 0.000 (0.041)\tLoss 6.2492 (5.6556)\tPrec@1 52.344 (57.386)\tPrec@5 89.844 (91.406)\n",
      "Epoch: [1][20/97], lr: 0.00400\tTime 0.308 (0.321)\tData 0.000 (0.029)\tLoss 5.2111 (5.5998)\tPrec@1 58.594 (57.478)\tPrec@5 95.312 (92.188)\n",
      "Epoch: [1][30/97], lr: 0.00400\tTime 0.306 (0.318)\tData 0.000 (0.025)\tLoss 4.9816 (5.6746)\tPrec@1 62.500 (57.283)\tPrec@5 92.969 (91.885)\n",
      "Epoch: [1][40/97], lr: 0.00400\tTime 0.307 (0.316)\tData 0.000 (0.023)\tLoss 5.3888 (5.6865)\tPrec@1 53.906 (57.336)\tPrec@5 93.750 (91.578)\n",
      "Epoch: [1][50/97], lr: 0.00400\tTime 0.307 (0.314)\tData 0.000 (0.022)\tLoss 5.8214 (5.6772)\tPrec@1 55.469 (57.690)\tPrec@5 88.281 (91.759)\n",
      "Epoch: [1][60/97], lr: 0.00400\tTime 0.309 (0.315)\tData 0.000 (0.021)\tLoss 4.7990 (5.6338)\tPrec@1 66.406 (58.197)\tPrec@5 91.406 (91.842)\n",
      "Epoch: [1][70/97], lr: 0.00400\tTime 0.310 (0.314)\tData 0.000 (0.020)\tLoss 5.3049 (5.5704)\tPrec@1 63.281 (58.781)\tPrec@5 95.312 (92.132)\n",
      "Epoch: [1][80/97], lr: 0.00400\tTime 0.317 (0.314)\tData 0.000 (0.020)\tLoss 5.1448 (5.5141)\tPrec@1 59.375 (59.269)\tPrec@5 93.750 (92.313)\n",
      "Epoch: [1][90/97], lr: 0.00400\tTime 0.296 (0.313)\tData 0.000 (0.019)\tLoss 5.3164 (5.4842)\tPrec@1 62.500 (59.401)\tPrec@5 93.750 (92.428)\n",
      "Epoch: [1][96/97], lr: 0.00400\tTime 0.292 (0.311)\tData 0.000 (0.020)\tLoss 4.1029 (5.4439)\tPrec@1 72.034 (59.786)\tPrec@5 97.458 (92.520)\n",
      "Test: [0/100]\tTime 0.274 (0.274)\tLoss 14.3628 (14.3628)\tPrec@1 19.000 (19.000)\tPrec@5 72.000 (72.000)\n",
      "Test: [10/100]\tTime 0.072 (0.091)\tLoss 11.7424 (13.1957)\tPrec@1 24.000 (23.545)\tPrec@5 75.000 (71.636)\n",
      "Test: [20/100]\tTime 0.072 (0.082)\tLoss 12.2506 (13.2671)\tPrec@1 27.000 (23.619)\tPrec@5 79.000 (70.810)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 12.1392 (13.1314)\tPrec@1 25.000 (24.355)\tPrec@5 78.000 (70.839)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 13.6910 (13.1731)\tPrec@1 19.000 (23.780)\tPrec@5 70.000 (70.195)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 12.6882 (13.0831)\tPrec@1 29.000 (24.059)\tPrec@5 74.000 (70.843)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 11.2026 (13.0422)\tPrec@1 33.000 (23.770)\tPrec@5 78.000 (70.984)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 12.6380 (12.9964)\tPrec@1 24.000 (23.972)\tPrec@5 73.000 (70.944)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 11.9192 (12.9464)\tPrec@1 29.000 (24.296)\tPrec@5 74.000 (71.123)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 12.6282 (12.9935)\tPrec@1 34.000 (24.220)\tPrec@5 66.000 (70.901)\n",
      "val Results: Prec@1 24.260 Prec@5 70.620 Loss 13.01742\n",
      "val Class Accuracy: [0.850,0.736,0.718,0.122,0.000,0.000,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 24.260\n",
      "\n",
      "Epoch: [2][0/97], lr: 0.00600\tTime 0.384 (0.384)\tData 0.213 (0.213)\tLoss 4.4222 (4.4222)\tPrec@1 67.969 (67.969)\tPrec@5 93.750 (93.750)\n",
      "Epoch: [2][10/97], lr: 0.00600\tTime 0.322 (0.340)\tData 0.001 (0.033)\tLoss 5.2706 (4.9867)\tPrec@1 63.281 (62.855)\tPrec@5 92.188 (93.750)\n",
      "Epoch: [2][20/97], lr: 0.00600\tTime 0.318 (0.326)\tData 0.000 (0.024)\tLoss 5.3616 (5.0005)\tPrec@1 60.938 (63.356)\tPrec@5 91.406 (94.122)\n",
      "Epoch: [2][30/97], lr: 0.00600\tTime 0.309 (0.321)\tData 0.000 (0.022)\tLoss 5.6550 (4.9928)\tPrec@1 55.469 (63.533)\tPrec@5 92.188 (93.800)\n",
      "Epoch: [2][40/97], lr: 0.00600\tTime 0.299 (0.316)\tData 0.000 (0.020)\tLoss 5.1436 (5.0336)\tPrec@1 59.375 (63.224)\tPrec@5 91.406 (93.845)\n",
      "Epoch: [2][50/97], lr: 0.00600\tTime 0.298 (0.312)\tData 0.000 (0.020)\tLoss 5.1109 (5.0584)\tPrec@1 66.406 (63.189)\tPrec@5 92.969 (93.719)\n",
      "Epoch: [2][60/97], lr: 0.00600\tTime 0.297 (0.310)\tData 0.000 (0.019)\tLoss 5.2910 (5.1161)\tPrec@1 59.375 (62.807)\tPrec@5 91.406 (93.404)\n",
      "Epoch: [2][70/97], lr: 0.00600\tTime 0.298 (0.308)\tData 0.000 (0.019)\tLoss 4.4890 (5.1401)\tPrec@1 64.844 (62.599)\tPrec@5 96.875 (93.398)\n",
      "Epoch: [2][80/97], lr: 0.00600\tTime 0.298 (0.307)\tData 0.000 (0.019)\tLoss 5.1035 (5.1501)\tPrec@1 61.719 (62.365)\tPrec@5 92.188 (93.181)\n",
      "Epoch: [2][90/97], lr: 0.00600\tTime 0.286 (0.306)\tData 0.000 (0.018)\tLoss 6.0813 (5.1344)\tPrec@1 55.469 (62.491)\tPrec@5 95.312 (93.063)\n",
      "Epoch: [2][96/97], lr: 0.00600\tTime 0.292 (0.305)\tData 0.000 (0.019)\tLoss 5.2585 (5.1174)\tPrec@1 62.712 (62.567)\tPrec@5 88.136 (93.100)\n",
      "Test: [0/100]\tTime 0.233 (0.233)\tLoss 12.4636 (12.4636)\tPrec@1 24.000 (24.000)\tPrec@5 83.000 (83.000)\n",
      "Test: [10/100]\tTime 0.072 (0.087)\tLoss 10.8432 (11.8209)\tPrec@1 30.000 (25.818)\tPrec@5 87.000 (82.909)\n",
      "Test: [20/100]\tTime 0.072 (0.080)\tLoss 10.7477 (11.8039)\tPrec@1 29.000 (25.857)\tPrec@5 90.000 (83.524)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 11.2798 (11.7666)\tPrec@1 28.000 (25.516)\tPrec@5 80.000 (82.645)\n",
      "Test: [40/100]\tTime 0.073 (0.076)\tLoss 12.3014 (11.7832)\tPrec@1 22.000 (25.878)\tPrec@5 78.000 (82.171)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 11.0469 (11.7037)\tPrec@1 31.000 (26.098)\tPrec@5 84.000 (82.294)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 10.0929 (11.6624)\tPrec@1 33.000 (26.016)\tPrec@5 88.000 (81.984)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 11.5121 (11.6307)\tPrec@1 28.000 (26.296)\tPrec@5 84.000 (81.944)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 10.6157 (11.5881)\tPrec@1 37.000 (26.667)\tPrec@5 79.000 (81.975)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 11.7149 (11.6336)\tPrec@1 30.000 (26.593)\tPrec@5 81.000 (81.857)\n",
      "val Results: Prec@1 26.650 Prec@5 81.660 Loss 11.64593\n",
      "val Class Accuracy: [0.832,0.750,0.447,0.553,0.083,0.000,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 26.650\n",
      "\n",
      "Epoch: [3][0/97], lr: 0.00800\tTime 0.634 (0.634)\tData 0.412 (0.412)\tLoss 5.5951 (5.5951)\tPrec@1 59.375 (59.375)\tPrec@5 88.281 (88.281)\n",
      "Epoch: [3][10/97], lr: 0.00800\tTime 0.324 (0.361)\tData 0.000 (0.051)\tLoss 5.1221 (5.1826)\tPrec@1 64.062 (62.571)\tPrec@5 96.094 (92.543)\n",
      "Epoch: [3][20/97], lr: 0.00800\tTime 0.315 (0.344)\tData 0.000 (0.034)\tLoss 5.8162 (5.2404)\tPrec@1 56.250 (62.388)\tPrec@5 89.844 (91.890)\n",
      "Epoch: [3][30/97], lr: 0.00800\tTime 0.321 (0.340)\tData 0.001 (0.028)\tLoss 4.6771 (5.1291)\tPrec@1 64.062 (63.155)\tPrec@5 95.312 (92.566)\n",
      "Epoch: [3][40/97], lr: 0.00800\tTime 0.295 (0.332)\tData 0.000 (0.025)\tLoss 4.3959 (5.0069)\tPrec@1 68.750 (63.796)\tPrec@5 89.844 (93.331)\n",
      "Epoch: [3][50/97], lr: 0.00800\tTime 0.297 (0.326)\tData 0.000 (0.024)\tLoss 4.7057 (4.9879)\tPrec@1 70.312 (63.741)\tPrec@5 92.969 (93.781)\n",
      "Epoch: [3][60/97], lr: 0.00800\tTime 0.293 (0.321)\tData 0.000 (0.023)\tLoss 4.8330 (4.9664)\tPrec@1 68.750 (64.101)\tPrec@5 95.312 (94.032)\n",
      "Epoch: [3][70/97], lr: 0.00800\tTime 0.294 (0.317)\tData 0.000 (0.022)\tLoss 4.4340 (4.9493)\tPrec@1 68.750 (64.239)\tPrec@5 95.312 (94.157)\n",
      "Epoch: [3][80/97], lr: 0.00800\tTime 0.298 (0.315)\tData 0.000 (0.021)\tLoss 4.6853 (4.9056)\tPrec@1 66.406 (64.574)\tPrec@5 92.188 (94.252)\n",
      "Epoch: [3][90/97], lr: 0.00800\tTime 0.294 (0.312)\tData 0.000 (0.021)\tLoss 4.8040 (4.8621)\tPrec@1 69.531 (65.067)\tPrec@5 92.969 (94.291)\n",
      "Epoch: [3][96/97], lr: 0.00800\tTime 0.287 (0.311)\tData 0.000 (0.021)\tLoss 4.7162 (4.8549)\tPrec@1 65.254 (65.098)\tPrec@5 94.068 (94.301)\n",
      "Test: [0/100]\tTime 0.244 (0.244)\tLoss 12.3967 (12.3967)\tPrec@1 25.000 (25.000)\tPrec@5 89.000 (89.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 10.1740 (11.5819)\tPrec@1 39.000 (30.091)\tPrec@5 87.000 (86.000)\n",
      "Test: [20/100]\tTime 0.072 (0.080)\tLoss 10.7550 (11.6889)\tPrec@1 34.000 (29.000)\tPrec@5 89.000 (86.714)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 10.8702 (11.6441)\tPrec@1 33.000 (28.645)\tPrec@5 84.000 (86.258)\n",
      "Test: [40/100]\tTime 0.072 (0.076)\tLoss 12.2673 (11.6969)\tPrec@1 23.000 (28.220)\tPrec@5 82.000 (85.659)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 11.5147 (11.6068)\tPrec@1 29.000 (28.745)\tPrec@5 88.000 (85.902)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 9.7795 (11.5497)\tPrec@1 33.000 (28.557)\tPrec@5 90.000 (86.016)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 11.5404 (11.5153)\tPrec@1 29.000 (28.690)\tPrec@5 86.000 (85.803)\n",
      "Test: [80/100]\tTime 0.072 (0.074)\tLoss 10.3517 (11.4615)\tPrec@1 39.000 (28.988)\tPrec@5 85.000 (86.012)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 11.6538 (11.5110)\tPrec@1 30.000 (28.736)\tPrec@5 92.000 (85.978)\n",
      "val Results: Prec@1 28.650 Prec@5 85.880 Loss 11.54006\n",
      "val Class Accuracy: [0.939,0.782,0.423,0.621,0.001,0.000,0.098,0.001,0.000,0.000]\n",
      "Best Prec@1: 28.650\n",
      "\n",
      "Epoch: [4][0/97], lr: 0.01000\tTime 0.349 (0.349)\tData 0.209 (0.209)\tLoss 4.4224 (4.4224)\tPrec@1 70.312 (70.312)\tPrec@5 92.969 (92.969)\n",
      "Epoch: [4][10/97], lr: 0.01000\tTime 0.293 (0.301)\tData 0.000 (0.034)\tLoss 4.6086 (4.7668)\tPrec@1 65.625 (66.335)\tPrec@5 96.875 (94.957)\n",
      "Epoch: [4][20/97], lr: 0.01000\tTime 0.296 (0.298)\tData 0.000 (0.026)\tLoss 4.4341 (4.6589)\tPrec@1 67.188 (66.555)\tPrec@5 94.531 (94.792)\n",
      "Epoch: [4][30/97], lr: 0.01000\tTime 0.293 (0.297)\tData 0.000 (0.023)\tLoss 5.0053 (4.6801)\tPrec@1 63.281 (66.154)\tPrec@5 92.188 (94.657)\n",
      "Epoch: [4][40/97], lr: 0.01000\tTime 0.287 (0.297)\tData 0.000 (0.022)\tLoss 4.9624 (4.7182)\tPrec@1 62.500 (65.663)\tPrec@5 94.531 (94.550)\n",
      "Epoch: [4][50/97], lr: 0.01000\tTime 0.298 (0.297)\tData 0.000 (0.021)\tLoss 5.5284 (4.7404)\tPrec@1 64.062 (65.472)\tPrec@5 94.531 (94.424)\n",
      "Epoch: [4][60/97], lr: 0.01000\tTime 0.296 (0.297)\tData 0.000 (0.020)\tLoss 4.5984 (4.7096)\tPrec@1 69.531 (65.638)\tPrec@5 94.531 (94.518)\n",
      "Epoch: [4][70/97], lr: 0.01000\tTime 0.294 (0.297)\tData 0.000 (0.020)\tLoss 5.1244 (4.7686)\tPrec@1 62.500 (65.493)\tPrec@5 96.875 (94.630)\n",
      "Epoch: [4][80/97], lr: 0.01000\tTime 0.295 (0.297)\tData 0.000 (0.020)\tLoss 4.5446 (4.7252)\tPrec@1 69.531 (65.818)\tPrec@5 93.750 (94.743)\n",
      "Epoch: [4][90/97], lr: 0.01000\tTime 0.293 (0.297)\tData 0.000 (0.019)\tLoss 4.2303 (4.6963)\tPrec@1 75.000 (66.140)\tPrec@5 96.094 (94.840)\n",
      "Epoch: [4][96/97], lr: 0.01000\tTime 0.286 (0.296)\tData 0.000 (0.020)\tLoss 4.6821 (4.6915)\tPrec@1 62.712 (66.105)\tPrec@5 93.220 (94.873)\n",
      "Test: [0/100]\tTime 0.244 (0.244)\tLoss 12.6254 (12.6254)\tPrec@1 25.000 (25.000)\tPrec@5 89.000 (89.000)\n",
      "Test: [10/100]\tTime 0.072 (0.088)\tLoss 10.0081 (11.6888)\tPrec@1 39.000 (29.182)\tPrec@5 87.000 (84.364)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 10.5545 (11.7588)\tPrec@1 31.000 (28.000)\tPrec@5 90.000 (84.857)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 10.8959 (11.7038)\tPrec@1 32.000 (28.452)\tPrec@5 82.000 (84.419)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 12.4459 (11.7562)\tPrec@1 21.000 (28.195)\tPrec@5 83.000 (84.415)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 11.7548 (11.6642)\tPrec@1 28.000 (28.804)\tPrec@5 87.000 (84.941)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 9.7717 (11.6220)\tPrec@1 36.000 (28.410)\tPrec@5 91.000 (84.820)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 11.1645 (11.5721)\tPrec@1 33.000 (28.606)\tPrec@5 86.000 (84.704)\n",
      "Test: [80/100]\tTime 0.072 (0.074)\tLoss 10.5754 (11.5309)\tPrec@1 39.000 (28.938)\tPrec@5 79.000 (84.852)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 11.4126 (11.5780)\tPrec@1 36.000 (28.714)\tPrec@5 90.000 (84.879)\n",
      "val Results: Prec@1 28.690 Prec@5 84.770 Loss 11.59863\n",
      "val Class Accuracy: [0.917,0.790,0.704,0.250,0.000,0.000,0.208,0.000,0.000,0.000]\n",
      "Best Prec@1: 28.690\n",
      "\n",
      "Epoch: [5][0/97], lr: 0.01000\tTime 0.367 (0.367)\tData 0.213 (0.213)\tLoss 4.1175 (4.1175)\tPrec@1 69.531 (69.531)\tPrec@5 94.531 (94.531)\n",
      "Epoch: [5][10/97], lr: 0.01000\tTime 0.297 (0.305)\tData 0.000 (0.035)\tLoss 3.5907 (4.5816)\tPrec@1 75.000 (67.045)\tPrec@5 97.656 (95.099)\n",
      "Epoch: [5][20/97], lr: 0.01000\tTime 0.293 (0.300)\tData 0.000 (0.026)\tLoss 4.5196 (4.6260)\tPrec@1 66.406 (67.150)\tPrec@5 97.656 (94.978)\n",
      "Epoch: [5][30/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.023)\tLoss 5.0256 (4.5650)\tPrec@1 61.719 (67.389)\tPrec@5 95.312 (95.212)\n",
      "Epoch: [5][40/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.022)\tLoss 4.6999 (4.5693)\tPrec@1 67.188 (67.207)\tPrec@5 91.406 (94.970)\n",
      "Epoch: [5][50/97], lr: 0.01000\tTime 0.293 (0.299)\tData 0.000 (0.021)\tLoss 5.0187 (4.5849)\tPrec@1 61.719 (67.065)\tPrec@5 94.531 (94.975)\n",
      "Epoch: [5][60/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.020)\tLoss 4.7051 (4.5421)\tPrec@1 67.188 (67.252)\tPrec@5 95.312 (95.159)\n",
      "Epoch: [5][70/97], lr: 0.01000\tTime 0.293 (0.298)\tData 0.000 (0.020)\tLoss 4.5860 (4.5250)\tPrec@1 66.406 (67.331)\tPrec@5 95.312 (95.268)\n",
      "Epoch: [5][80/97], lr: 0.01000\tTime 0.299 (0.298)\tData 0.000 (0.020)\tLoss 4.3815 (4.5056)\tPrec@1 69.531 (67.448)\tPrec@5 96.094 (95.332)\n",
      "Epoch: [5][90/97], lr: 0.01000\tTime 0.304 (0.299)\tData 0.000 (0.019)\tLoss 5.2042 (4.5019)\tPrec@1 62.500 (67.608)\tPrec@5 94.531 (95.355)\n",
      "Epoch: [5][96/97], lr: 0.01000\tTime 0.365 (0.301)\tData 0.000 (0.020)\tLoss 5.1726 (4.5047)\tPrec@1 59.322 (67.516)\tPrec@5 91.525 (95.260)\n",
      "Test: [0/100]\tTime 0.652 (0.652)\tLoss 12.3447 (12.3447)\tPrec@1 28.000 (28.000)\tPrec@5 89.000 (89.000)\n",
      "Test: [10/100]\tTime 0.074 (0.126)\tLoss 9.8051 (11.4528)\tPrec@1 42.000 (31.636)\tPrec@5 92.000 (85.727)\n",
      "Test: [20/100]\tTime 0.074 (0.102)\tLoss 10.2101 (11.5003)\tPrec@1 39.000 (31.238)\tPrec@5 93.000 (87.000)\n",
      "Test: [30/100]\tTime 0.074 (0.093)\tLoss 10.7273 (11.4221)\tPrec@1 31.000 (31.161)\tPrec@5 84.000 (86.581)\n",
      "Test: [40/100]\tTime 0.074 (0.089)\tLoss 11.8833 (11.4411)\tPrec@1 25.000 (30.976)\tPrec@5 84.000 (86.512)\n",
      "Test: [50/100]\tTime 0.075 (0.086)\tLoss 11.2037 (11.3660)\tPrec@1 32.000 (31.294)\tPrec@5 89.000 (86.863)\n",
      "Test: [60/100]\tTime 0.080 (0.084)\tLoss 9.5221 (11.3052)\tPrec@1 33.000 (30.885)\tPrec@5 89.000 (86.689)\n",
      "Test: [70/100]\tTime 0.076 (0.083)\tLoss 11.0155 (11.2697)\tPrec@1 34.000 (31.211)\tPrec@5 90.000 (86.521)\n",
      "Test: [80/100]\tTime 0.074 (0.082)\tLoss 10.1921 (11.2302)\tPrec@1 42.000 (31.506)\tPrec@5 82.000 (86.580)\n",
      "Test: [90/100]\tTime 0.077 (0.082)\tLoss 11.2594 (11.2855)\tPrec@1 36.000 (31.330)\tPrec@5 92.000 (86.516)\n",
      "val Results: Prec@1 31.310 Prec@5 86.520 Loss 11.31018\n",
      "val Class Accuracy: [0.918,0.846,0.565,0.589,0.013,0.040,0.101,0.059,0.000,0.000]\n",
      "Best Prec@1: 31.310\n",
      "\n",
      "Epoch: [6][0/97], lr: 0.01000\tTime 1.304 (1.304)\tData 0.921 (0.921)\tLoss 4.2639 (4.2639)\tPrec@1 67.969 (67.969)\tPrec@5 92.969 (92.969)\n",
      "Epoch: [6][10/97], lr: 0.01000\tTime 0.296 (0.434)\tData 0.000 (0.091)\tLoss 4.4217 (4.2825)\tPrec@1 65.625 (69.744)\tPrec@5 94.531 (96.023)\n",
      "Epoch: [6][20/97], lr: 0.01000\tTime 0.296 (0.377)\tData 0.000 (0.056)\tLoss 5.3084 (4.4607)\tPrec@1 62.500 (68.192)\tPrec@5 91.406 (95.796)\n",
      "Epoch: [6][30/97], lr: 0.01000\tTime 0.323 (0.353)\tData 0.000 (0.043)\tLoss 4.5185 (4.4031)\tPrec@1 68.750 (68.775)\tPrec@5 93.750 (95.842)\n",
      "Epoch: [6][40/97], lr: 0.01000\tTime 0.299 (0.340)\tData 0.000 (0.037)\tLoss 4.6174 (4.3694)\tPrec@1 67.188 (68.998)\tPrec@5 92.969 (95.884)\n",
      "Epoch: [6][50/97], lr: 0.01000\tTime 0.299 (0.332)\tData 0.000 (0.033)\tLoss 4.5377 (4.3738)\tPrec@1 66.406 (69.041)\tPrec@5 96.094 (95.757)\n",
      "Epoch: [6][60/97], lr: 0.01000\tTime 0.303 (0.327)\tData 0.000 (0.030)\tLoss 4.1288 (4.3531)\tPrec@1 73.438 (69.352)\tPrec@5 97.656 (95.902)\n",
      "Epoch: [6][70/97], lr: 0.01000\tTime 0.296 (0.324)\tData 0.000 (0.028)\tLoss 4.0473 (4.3071)\tPrec@1 75.781 (69.817)\tPrec@5 96.094 (96.017)\n",
      "Epoch: [6][80/97], lr: 0.01000\tTime 0.298 (0.321)\tData 0.000 (0.027)\tLoss 3.9024 (4.3047)\tPrec@1 71.875 (69.936)\tPrec@5 95.312 (95.901)\n",
      "Epoch: [6][90/97], lr: 0.01000\tTime 0.298 (0.319)\tData 0.000 (0.026)\tLoss 4.9662 (4.3089)\tPrec@1 67.969 (69.909)\tPrec@5 97.656 (96.016)\n",
      "Epoch: [6][96/97], lr: 0.01000\tTime 0.288 (0.317)\tData 0.000 (0.026)\tLoss 4.2880 (4.3207)\tPrec@1 70.339 (69.740)\tPrec@5 94.915 (96.074)\n",
      "Test: [0/100]\tTime 0.304 (0.304)\tLoss 12.1562 (12.1562)\tPrec@1 32.000 (32.000)\tPrec@5 90.000 (90.000)\n",
      "Test: [10/100]\tTime 0.073 (0.094)\tLoss 9.7062 (11.3721)\tPrec@1 43.000 (32.000)\tPrec@5 91.000 (89.000)\n",
      "Test: [20/100]\tTime 0.072 (0.084)\tLoss 9.8427 (11.4107)\tPrec@1 42.000 (31.905)\tPrec@5 93.000 (89.095)\n",
      "Test: [30/100]\tTime 0.072 (0.080)\tLoss 10.7304 (11.3612)\tPrec@1 30.000 (32.065)\tPrec@5 89.000 (88.968)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 11.8939 (11.3966)\tPrec@1 30.000 (32.098)\tPrec@5 85.000 (88.756)\n",
      "Test: [50/100]\tTime 0.072 (0.077)\tLoss 11.1304 (11.3239)\tPrec@1 34.000 (32.098)\tPrec@5 85.000 (88.863)\n",
      "Test: [60/100]\tTime 0.072 (0.077)\tLoss 9.5426 (11.2779)\tPrec@1 38.000 (31.902)\tPrec@5 93.000 (88.787)\n",
      "Test: [70/100]\tTime 0.072 (0.076)\tLoss 10.7240 (11.2198)\tPrec@1 38.000 (32.254)\tPrec@5 93.000 (88.789)\n",
      "Test: [80/100]\tTime 0.072 (0.076)\tLoss 10.0747 (11.1792)\tPrec@1 42.000 (32.617)\tPrec@5 87.000 (88.889)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.2744 (11.2272)\tPrec@1 37.000 (32.396)\tPrec@5 92.000 (88.857)\n",
      "val Results: Prec@1 32.370 Prec@5 88.800 Loss 11.25514\n",
      "val Class Accuracy: [0.901,0.929,0.624,0.286,0.020,0.005,0.458,0.014,0.000,0.000]\n",
      "Best Prec@1: 32.370\n",
      "\n",
      "Epoch: [7][0/97], lr: 0.01000\tTime 0.528 (0.528)\tData 0.337 (0.337)\tLoss 4.3311 (4.3311)\tPrec@1 68.750 (68.750)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [7][10/97], lr: 0.01000\tTime 0.294 (0.325)\tData 0.000 (0.045)\tLoss 4.8571 (4.2591)\tPrec@1 65.625 (69.673)\tPrec@5 92.969 (96.307)\n",
      "Epoch: [7][20/97], lr: 0.01000\tTime 0.307 (0.312)\tData 0.000 (0.032)\tLoss 4.1888 (4.3318)\tPrec@1 71.094 (68.936)\tPrec@5 95.312 (96.168)\n",
      "Epoch: [7][30/97], lr: 0.01000\tTime 0.309 (0.312)\tData 0.000 (0.027)\tLoss 4.4042 (4.3015)\tPrec@1 64.844 (69.078)\tPrec@5 97.656 (96.069)\n",
      "Epoch: [7][40/97], lr: 0.01000\tTime 0.295 (0.309)\tData 0.000 (0.024)\tLoss 3.8296 (4.2844)\tPrec@1 71.875 (69.226)\tPrec@5 99.219 (96.189)\n",
      "Epoch: [7][50/97], lr: 0.01000\tTime 0.294 (0.307)\tData 0.000 (0.023)\tLoss 4.2263 (4.2287)\tPrec@1 66.406 (69.700)\tPrec@5 97.656 (96.324)\n",
      "Epoch: [7][60/97], lr: 0.01000\tTime 0.301 (0.305)\tData 0.000 (0.022)\tLoss 4.4560 (4.1809)\tPrec@1 68.750 (70.018)\tPrec@5 96.094 (96.465)\n",
      "Epoch: [7][70/97], lr: 0.01000\tTime 0.294 (0.304)\tData 0.000 (0.021)\tLoss 4.2139 (4.1674)\tPrec@1 70.312 (70.401)\tPrec@5 93.750 (96.325)\n",
      "Epoch: [7][80/97], lr: 0.01000\tTime 0.296 (0.303)\tData 0.000 (0.021)\tLoss 4.1052 (4.1563)\tPrec@1 69.531 (70.486)\tPrec@5 93.750 (96.267)\n",
      "Epoch: [7][90/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.020)\tLoss 4.5511 (4.1860)\tPrec@1 67.188 (70.407)\tPrec@5 93.750 (96.308)\n",
      "Epoch: [7][96/97], lr: 0.01000\tTime 0.290 (0.302)\tData 0.000 (0.021)\tLoss 5.1465 (4.1933)\tPrec@1 64.407 (70.321)\tPrec@5 97.458 (96.340)\n",
      "Test: [0/100]\tTime 0.254 (0.254)\tLoss 15.3487 (15.3487)\tPrec@1 18.000 (18.000)\tPrec@5 82.000 (82.000)\n",
      "Test: [10/100]\tTime 0.072 (0.089)\tLoss 12.1692 (13.9753)\tPrec@1 32.000 (24.909)\tPrec@5 82.000 (82.636)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 12.6726 (13.9921)\tPrec@1 27.000 (24.762)\tPrec@5 85.000 (83.286)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 12.9804 (13.8571)\tPrec@1 28.000 (25.323)\tPrec@5 81.000 (83.194)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 15.0510 (13.8992)\tPrec@1 19.000 (25.220)\tPrec@5 79.000 (83.024)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 14.0597 (13.8123)\tPrec@1 26.000 (25.490)\tPrec@5 77.000 (83.196)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 11.8688 (13.8235)\tPrec@1 31.000 (25.049)\tPrec@5 88.000 (82.852)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 13.2388 (13.7752)\tPrec@1 28.000 (25.338)\tPrec@5 83.000 (82.761)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 12.3098 (13.7067)\tPrec@1 32.000 (25.605)\tPrec@5 79.000 (82.901)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 12.8370 (13.7621)\tPrec@1 34.000 (25.440)\tPrec@5 88.000 (82.901)\n",
      "val Results: Prec@1 25.490 Prec@5 82.880 Loss 13.77209\n",
      "val Class Accuracy: [0.938,0.936,0.649,0.026,0.000,0.000,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 32.370\n",
      "\n",
      "Epoch: [8][0/97], lr: 0.01000\tTime 0.355 (0.355)\tData 0.198 (0.198)\tLoss 3.8267 (3.8267)\tPrec@1 70.312 (70.312)\tPrec@5 94.531 (94.531)\n",
      "Epoch: [8][10/97], lr: 0.01000\tTime 0.378 (0.348)\tData 0.001 (0.033)\tLoss 4.3048 (4.2960)\tPrec@1 71.875 (68.892)\tPrec@5 98.438 (96.662)\n",
      "Epoch: [8][20/97], lr: 0.01000\tTime 0.380 (0.355)\tData 0.001 (0.023)\tLoss 4.0424 (4.3312)\tPrec@1 70.312 (68.899)\tPrec@5 95.312 (96.280)\n",
      "Epoch: [8][30/97], lr: 0.01000\tTime 0.372 (0.359)\tData 0.007 (0.019)\tLoss 4.1754 (4.2675)\tPrec@1 71.094 (69.657)\tPrec@5 96.875 (96.270)\n",
      "Epoch: [8][40/97], lr: 0.01000\tTime 0.325 (0.359)\tData 0.000 (0.018)\tLoss 4.1816 (4.1922)\tPrec@1 71.094 (70.255)\tPrec@5 95.312 (96.418)\n",
      "Epoch: [8][50/97], lr: 0.01000\tTime 0.326 (0.355)\tData 0.001 (0.017)\tLoss 4.7609 (4.1617)\tPrec@1 70.312 (70.711)\tPrec@5 96.094 (96.461)\n",
      "Epoch: [8][60/97], lr: 0.01000\tTime 0.352 (0.353)\tData 0.001 (0.017)\tLoss 3.4504 (4.1224)\tPrec@1 75.000 (70.889)\tPrec@5 99.219 (96.632)\n",
      "Epoch: [8][70/97], lr: 0.01000\tTime 0.333 (0.351)\tData 0.001 (0.016)\tLoss 3.3545 (4.0864)\tPrec@1 76.562 (71.094)\tPrec@5 98.438 (96.710)\n",
      "Epoch: [8][80/97], lr: 0.01000\tTime 0.344 (0.351)\tData 0.001 (0.016)\tLoss 3.9559 (4.0482)\tPrec@1 73.438 (71.528)\tPrec@5 98.438 (96.759)\n",
      "Epoch: [8][90/97], lr: 0.01000\tTime 0.302 (0.349)\tData 0.000 (0.016)\tLoss 3.9428 (4.0574)\tPrec@1 74.219 (71.420)\tPrec@5 98.438 (96.823)\n",
      "Epoch: [8][96/97], lr: 0.01000\tTime 0.320 (0.347)\tData 0.000 (0.017)\tLoss 3.7614 (4.0706)\tPrec@1 72.034 (71.328)\tPrec@5 97.458 (96.824)\n",
      "Test: [0/100]\tTime 0.405 (0.405)\tLoss 12.7914 (12.7914)\tPrec@1 31.000 (31.000)\tPrec@5 83.000 (83.000)\n",
      "Test: [10/100]\tTime 0.074 (0.104)\tLoss 9.6179 (11.8566)\tPrec@1 46.000 (34.636)\tPrec@5 90.000 (83.545)\n",
      "Test: [20/100]\tTime 0.082 (0.090)\tLoss 10.4053 (11.9354)\tPrec@1 39.000 (33.000)\tPrec@5 88.000 (83.381)\n",
      "Test: [30/100]\tTime 0.073 (0.085)\tLoss 11.5577 (11.8814)\tPrec@1 35.000 (33.484)\tPrec@5 83.000 (83.194)\n",
      "Test: [40/100]\tTime 0.074 (0.082)\tLoss 12.7212 (11.9533)\tPrec@1 26.000 (33.268)\tPrec@5 77.000 (83.049)\n",
      "Test: [50/100]\tTime 0.073 (0.081)\tLoss 11.8284 (11.8804)\tPrec@1 35.000 (33.451)\tPrec@5 78.000 (83.314)\n",
      "Test: [60/100]\tTime 0.073 (0.079)\tLoss 10.1449 (11.8534)\tPrec@1 38.000 (32.934)\tPrec@5 88.000 (83.230)\n",
      "Test: [70/100]\tTime 0.073 (0.078)\tLoss 11.3205 (11.8137)\tPrec@1 40.000 (33.099)\tPrec@5 80.000 (83.268)\n",
      "Test: [80/100]\tTime 0.072 (0.078)\tLoss 10.6930 (11.7689)\tPrec@1 43.000 (33.321)\tPrec@5 79.000 (83.383)\n",
      "Test: [90/100]\tTime 0.072 (0.077)\tLoss 12.0831 (11.8069)\tPrec@1 32.000 (33.264)\tPrec@5 90.000 (83.473)\n",
      "val Results: Prec@1 33.210 Prec@5 83.430 Loss 11.83302\n",
      "val Class Accuracy: [0.875,0.985,0.522,0.754,0.002,0.002,0.181,0.000,0.000,0.000]\n",
      "Best Prec@1: 33.210\n",
      "\n",
      "Epoch: [9][0/97], lr: 0.01000\tTime 0.387 (0.387)\tData 0.236 (0.236)\tLoss 4.3034 (4.3034)\tPrec@1 68.750 (68.750)\tPrec@5 96.875 (96.875)\n",
      "Epoch: [9][10/97], lr: 0.01000\tTime 0.329 (0.327)\tData 0.000 (0.036)\tLoss 3.3448 (3.8391)\tPrec@1 75.000 (72.443)\tPrec@5 98.438 (96.591)\n",
      "Epoch: [9][20/97], lr: 0.01000\tTime 0.326 (0.343)\tData 0.001 (0.026)\tLoss 4.6523 (3.9311)\tPrec@1 71.094 (72.173)\tPrec@5 95.312 (96.689)\n",
      "Epoch: [9][30/97], lr: 0.01000\tTime 0.370 (0.347)\tData 0.000 (0.022)\tLoss 4.1401 (3.9545)\tPrec@1 67.969 (71.875)\tPrec@5 97.656 (96.346)\n",
      "Epoch: [9][40/97], lr: 0.01000\tTime 0.349 (0.352)\tData 0.001 (0.020)\tLoss 3.5870 (3.9526)\tPrec@1 77.344 (72.046)\tPrec@5 96.875 (96.361)\n",
      "Epoch: [9][50/97], lr: 0.01000\tTime 0.378 (0.356)\tData 0.001 (0.019)\tLoss 3.1806 (3.9797)\tPrec@1 80.469 (72.166)\tPrec@5 99.219 (96.492)\n",
      "Epoch: [9][60/97], lr: 0.01000\tTime 0.317 (0.355)\tData 0.000 (0.018)\tLoss 4.2040 (3.9884)\tPrec@1 69.531 (72.221)\tPrec@5 96.875 (96.606)\n",
      "Epoch: [9][70/97], lr: 0.01000\tTime 0.313 (0.353)\tData 0.001 (0.017)\tLoss 3.4858 (3.9355)\tPrec@1 76.562 (72.678)\tPrec@5 99.219 (96.710)\n",
      "Epoch: [9][80/97], lr: 0.01000\tTime 0.305 (0.350)\tData 0.000 (0.017)\tLoss 4.0749 (3.9322)\tPrec@1 72.656 (72.907)\tPrec@5 96.875 (96.730)\n",
      "Epoch: [9][90/97], lr: 0.01000\tTime 0.298 (0.345)\tData 0.000 (0.017)\tLoss 4.1703 (3.9322)\tPrec@1 71.094 (73.051)\tPrec@5 95.312 (96.746)\n",
      "Epoch: [9][96/97], lr: 0.01000\tTime 0.288 (0.342)\tData 0.000 (0.018)\tLoss 4.0365 (3.9316)\tPrec@1 72.881 (73.053)\tPrec@5 95.763 (96.735)\n",
      "Test: [0/100]\tTime 0.248 (0.248)\tLoss 12.3531 (12.3531)\tPrec@1 24.000 (24.000)\tPrec@5 90.000 (90.000)\n",
      "Test: [10/100]\tTime 0.072 (0.088)\tLoss 9.3727 (11.0725)\tPrec@1 38.000 (33.545)\tPrec@5 91.000 (90.000)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 10.1660 (11.0723)\tPrec@1 35.000 (33.810)\tPrec@5 93.000 (90.810)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 10.4312 (10.9825)\tPrec@1 34.000 (34.935)\tPrec@5 91.000 (90.355)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 11.1619 (11.0084)\tPrec@1 33.000 (34.707)\tPrec@5 94.000 (90.512)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 10.7003 (10.9166)\tPrec@1 36.000 (35.235)\tPrec@5 87.000 (90.510)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 9.3196 (10.8654)\tPrec@1 40.000 (35.262)\tPrec@5 93.000 (90.475)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 10.4030 (10.8493)\tPrec@1 37.000 (35.211)\tPrec@5 94.000 (90.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.6411 (10.8071)\tPrec@1 46.000 (35.494)\tPrec@5 86.000 (90.407)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 10.9550 (10.8760)\tPrec@1 34.000 (35.088)\tPrec@5 91.000 (90.264)\n",
      "val Results: Prec@1 34.980 Prec@5 90.260 Loss 10.89588\n",
      "val Class Accuracy: [0.918,0.890,0.511,0.367,0.768,0.017,0.017,0.010,0.000,0.000]\n",
      "Best Prec@1: 34.980\n",
      "\n",
      "Epoch: [10][0/97], lr: 0.01000\tTime 0.540 (0.540)\tData 0.338 (0.338)\tLoss 4.2383 (4.2383)\tPrec@1 70.312 (70.312)\tPrec@5 96.094 (96.094)\n",
      "Epoch: [10][10/97], lr: 0.01000\tTime 0.296 (0.326)\tData 0.000 (0.042)\tLoss 3.6231 (3.8430)\tPrec@1 75.000 (74.006)\tPrec@5 96.875 (96.662)\n",
      "Epoch: [10][20/97], lr: 0.01000\tTime 0.295 (0.311)\tData 0.000 (0.030)\tLoss 3.4796 (3.9210)\tPrec@1 78.125 (73.549)\tPrec@5 97.656 (96.652)\n",
      "Epoch: [10][30/97], lr: 0.01000\tTime 0.294 (0.307)\tData 0.000 (0.026)\tLoss 4.3263 (3.8658)\tPrec@1 66.406 (73.841)\tPrec@5 97.656 (96.850)\n",
      "Epoch: [10][40/97], lr: 0.01000\tTime 0.290 (0.304)\tData 0.000 (0.024)\tLoss 3.4322 (3.8662)\tPrec@1 75.781 (73.590)\tPrec@5 96.094 (96.742)\n",
      "Epoch: [10][50/97], lr: 0.01000\tTime 0.294 (0.302)\tData 0.000 (0.023)\tLoss 3.9916 (3.8746)\tPrec@1 75.000 (73.392)\tPrec@5 93.750 (96.706)\n",
      "Epoch: [10][60/97], lr: 0.01000\tTime 0.294 (0.301)\tData 0.000 (0.022)\tLoss 3.9621 (3.8724)\tPrec@1 74.219 (73.450)\tPrec@5 93.750 (96.721)\n",
      "Epoch: [10][70/97], lr: 0.01000\tTime 0.294 (0.300)\tData 0.000 (0.021)\tLoss 4.6494 (3.8704)\tPrec@1 73.438 (73.581)\tPrec@5 94.531 (96.765)\n",
      "Epoch: [10][80/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 3.8710 (3.8708)\tPrec@1 74.219 (73.524)\tPrec@5 96.875 (96.605)\n",
      "Epoch: [10][90/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 4.0147 (3.8672)\tPrec@1 75.000 (73.566)\tPrec@5 96.875 (96.660)\n",
      "Epoch: [10][96/97], lr: 0.01000\tTime 0.290 (0.299)\tData 0.000 (0.021)\tLoss 4.6186 (3.8833)\tPrec@1 64.407 (73.424)\tPrec@5 96.610 (96.703)\n",
      "Test: [0/100]\tTime 0.240 (0.240)\tLoss 13.3497 (13.3497)\tPrec@1 27.000 (27.000)\tPrec@5 83.000 (83.000)\n",
      "Test: [10/100]\tTime 0.072 (0.088)\tLoss 10.8275 (12.5858)\tPrec@1 36.000 (29.909)\tPrec@5 87.000 (82.909)\n",
      "Test: [20/100]\tTime 0.072 (0.080)\tLoss 11.4490 (12.5530)\tPrec@1 34.000 (29.762)\tPrec@5 87.000 (83.048)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 11.5464 (12.4442)\tPrec@1 31.000 (30.194)\tPrec@5 84.000 (82.839)\n",
      "Test: [40/100]\tTime 0.072 (0.076)\tLoss 13.0684 (12.5182)\tPrec@1 24.000 (30.122)\tPrec@5 73.000 (82.268)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 12.5084 (12.4717)\tPrec@1 34.000 (30.412)\tPrec@5 77.000 (82.373)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 10.4460 (12.4877)\tPrec@1 38.000 (30.066)\tPrec@5 88.000 (82.213)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 11.7089 (12.4488)\tPrec@1 36.000 (30.282)\tPrec@5 83.000 (82.211)\n",
      "Test: [80/100]\tTime 0.072 (0.074)\tLoss 11.5753 (12.4203)\tPrec@1 39.000 (30.642)\tPrec@5 76.000 (82.272)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 11.9617 (12.4616)\tPrec@1 35.000 (30.516)\tPrec@5 89.000 (82.374)\n",
      "val Results: Prec@1 30.490 Prec@5 82.310 Loss 12.48643\n",
      "val Class Accuracy: [0.873,0.975,0.770,0.324,0.035,0.000,0.072,0.000,0.000,0.000]\n",
      "Best Prec@1: 34.980\n",
      "\n",
      "Epoch: [11][0/97], lr: 0.01000\tTime 0.332 (0.332)\tData 0.195 (0.195)\tLoss 3.7210 (3.7210)\tPrec@1 73.438 (73.438)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [11][10/97], lr: 0.01000\tTime 0.294 (0.301)\tData 0.000 (0.033)\tLoss 3.5216 (3.8626)\tPrec@1 73.438 (72.230)\tPrec@5 98.438 (96.307)\n",
      "Epoch: [11][20/97], lr: 0.01000\tTime 0.293 (0.298)\tData 0.000 (0.025)\tLoss 3.9356 (3.7079)\tPrec@1 71.094 (74.628)\tPrec@5 95.312 (97.061)\n",
      "Epoch: [11][30/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.023)\tLoss 3.2479 (3.7263)\tPrec@1 77.344 (74.723)\tPrec@5 98.438 (97.077)\n",
      "Epoch: [11][40/97], lr: 0.01000\tTime 0.293 (0.298)\tData 0.000 (0.021)\tLoss 3.9398 (3.7582)\tPrec@1 77.344 (74.695)\tPrec@5 96.094 (96.951)\n",
      "Epoch: [11][50/97], lr: 0.01000\tTime 0.298 (0.297)\tData 0.000 (0.021)\tLoss 4.5758 (3.7880)\tPrec@1 69.531 (74.617)\tPrec@5 96.094 (96.875)\n",
      "Epoch: [11][60/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.020)\tLoss 3.6447 (3.7754)\tPrec@1 75.781 (74.693)\tPrec@5 97.656 (96.952)\n",
      "Epoch: [11][70/97], lr: 0.01000\tTime 0.294 (0.297)\tData 0.000 (0.020)\tLoss 3.5582 (3.8085)\tPrec@1 72.656 (74.406)\tPrec@5 96.875 (97.007)\n",
      "Epoch: [11][80/97], lr: 0.01000\tTime 0.294 (0.297)\tData 0.000 (0.019)\tLoss 3.3484 (3.7987)\tPrec@1 78.906 (74.547)\tPrec@5 96.875 (97.000)\n",
      "Epoch: [11][90/97], lr: 0.01000\tTime 0.292 (0.297)\tData 0.000 (0.019)\tLoss 3.2324 (3.7665)\tPrec@1 81.250 (74.760)\tPrec@5 96.094 (97.038)\n",
      "Epoch: [11][96/97], lr: 0.01000\tTime 0.286 (0.297)\tData 0.000 (0.020)\tLoss 4.2016 (3.7601)\tPrec@1 69.492 (74.746)\tPrec@5 94.915 (97.090)\n",
      "Test: [0/100]\tTime 0.260 (0.260)\tLoss 12.9982 (12.9982)\tPrec@1 26.000 (26.000)\tPrec@5 80.000 (80.000)\n",
      "Test: [10/100]\tTime 0.072 (0.089)\tLoss 10.3764 (12.0001)\tPrec@1 39.000 (32.273)\tPrec@5 85.000 (86.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 10.8737 (12.0181)\tPrec@1 35.000 (31.381)\tPrec@5 90.000 (86.905)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 11.2875 (11.9605)\tPrec@1 35.000 (31.806)\tPrec@5 88.000 (87.258)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 12.6166 (12.0412)\tPrec@1 28.000 (31.561)\tPrec@5 88.000 (87.244)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 11.7249 (11.9393)\tPrec@1 35.000 (32.078)\tPrec@5 89.000 (87.314)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 10.2689 (11.9288)\tPrec@1 39.000 (31.902)\tPrec@5 91.000 (87.311)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 11.2004 (11.9159)\tPrec@1 37.000 (31.915)\tPrec@5 90.000 (87.324)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 10.6853 (11.8776)\tPrec@1 40.000 (32.074)\tPrec@5 83.000 (87.481)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 12.3417 (11.9344)\tPrec@1 28.000 (31.824)\tPrec@5 94.000 (87.308)\n",
      "val Results: Prec@1 31.660 Prec@5 87.330 Loss 11.96429\n",
      "val Class Accuracy: [0.949,0.902,0.382,0.886,0.043,0.003,0.001,0.000,0.000,0.000]\n",
      "Best Prec@1: 34.980\n",
      "\n",
      "Epoch: [12][0/97], lr: 0.01000\tTime 0.484 (0.484)\tData 0.331 (0.331)\tLoss 3.8807 (3.8807)\tPrec@1 75.000 (75.000)\tPrec@5 96.094 (96.094)\n",
      "Epoch: [12][10/97], lr: 0.01000\tTime 0.295 (0.315)\tData 0.000 (0.045)\tLoss 3.7950 (3.7255)\tPrec@1 73.438 (75.071)\tPrec@5 96.875 (97.585)\n",
      "Epoch: [12][20/97], lr: 0.01000\tTime 0.294 (0.306)\tData 0.000 (0.032)\tLoss 4.1280 (3.5984)\tPrec@1 71.875 (76.190)\tPrec@5 95.312 (97.656)\n",
      "Epoch: [12][30/97], lr: 0.01000\tTime 0.294 (0.303)\tData 0.000 (0.027)\tLoss 3.1936 (3.5240)\tPrec@1 80.469 (77.067)\tPrec@5 98.438 (97.530)\n",
      "Epoch: [12][40/97], lr: 0.01000\tTime 0.289 (0.301)\tData 0.000 (0.025)\tLoss 4.2758 (3.5714)\tPrec@1 69.531 (76.220)\tPrec@5 98.438 (97.428)\n",
      "Epoch: [12][50/97], lr: 0.01000\tTime 0.293 (0.300)\tData 0.000 (0.023)\tLoss 3.5746 (3.6090)\tPrec@1 74.219 (75.919)\tPrec@5 96.094 (97.319)\n",
      "Epoch: [12][60/97], lr: 0.01000\tTime 0.294 (0.300)\tData 0.000 (0.022)\tLoss 3.5314 (3.6560)\tPrec@1 74.219 (75.499)\tPrec@5 98.438 (97.426)\n",
      "Epoch: [12][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 3.7619 (3.6949)\tPrec@1 75.000 (75.143)\tPrec@5 98.438 (97.414)\n",
      "Epoch: [12][80/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.021)\tLoss 3.6897 (3.6902)\tPrec@1 71.094 (75.125)\tPrec@5 96.875 (97.348)\n",
      "Epoch: [12][90/97], lr: 0.01000\tTime 0.302 (0.299)\tData 0.000 (0.020)\tLoss 3.3583 (3.6629)\tPrec@1 79.688 (75.318)\tPrec@5 96.875 (97.364)\n",
      "Epoch: [12][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.021)\tLoss 3.8036 (3.6931)\tPrec@1 76.271 (75.149)\tPrec@5 97.458 (97.364)\n",
      "Test: [0/100]\tTime 0.252 (0.252)\tLoss 11.7424 (11.7424)\tPrec@1 36.000 (36.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.072 (0.089)\tLoss 9.3589 (10.8321)\tPrec@1 50.000 (39.909)\tPrec@5 91.000 (92.273)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 9.5735 (10.9341)\tPrec@1 44.000 (38.095)\tPrec@5 98.000 (92.905)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 10.2754 (10.8805)\tPrec@1 40.000 (38.065)\tPrec@5 92.000 (92.935)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 11.4812 (10.9132)\tPrec@1 33.000 (37.878)\tPrec@5 91.000 (92.610)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 10.7558 (10.8539)\tPrec@1 34.000 (37.804)\tPrec@5 95.000 (92.667)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 9.4357 (10.8228)\tPrec@1 44.000 (37.803)\tPrec@5 92.000 (92.393)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 10.5529 (10.7943)\tPrec@1 36.000 (37.873)\tPrec@5 89.000 (92.141)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.7062 (10.7582)\tPrec@1 44.000 (37.963)\tPrec@5 90.000 (92.247)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 10.4044 (10.8298)\tPrec@1 39.000 (37.462)\tPrec@5 98.000 (92.253)\n",
      "val Results: Prec@1 37.360 Prec@5 92.370 Loss 10.85160\n",
      "val Class Accuracy: [0.971,0.938,0.605,0.249,0.476,0.160,0.109,0.218,0.000,0.010]\n",
      "Best Prec@1: 37.360\n",
      "\n",
      "Epoch: [13][0/97], lr: 0.01000\tTime 0.359 (0.359)\tData 0.217 (0.217)\tLoss 3.3074 (3.3074)\tPrec@1 78.906 (78.906)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [13][10/97], lr: 0.01000\tTime 0.298 (0.303)\tData 0.000 (0.035)\tLoss 4.2838 (3.7158)\tPrec@1 71.094 (75.355)\tPrec@5 96.875 (97.656)\n",
      "Epoch: [13][20/97], lr: 0.01000\tTime 0.294 (0.300)\tData 0.000 (0.026)\tLoss 3.0703 (3.7231)\tPrec@1 78.125 (75.372)\tPrec@5 98.438 (97.582)\n",
      "Epoch: [13][30/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.023)\tLoss 3.5585 (3.7359)\tPrec@1 74.219 (75.025)\tPrec@5 95.312 (97.480)\n",
      "Epoch: [13][40/97], lr: 0.01000\tTime 0.290 (0.298)\tData 0.000 (0.022)\tLoss 4.4316 (3.7435)\tPrec@1 64.062 (74.600)\tPrec@5 94.531 (97.466)\n",
      "Epoch: [13][50/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.021)\tLoss 3.5362 (3.6437)\tPrec@1 76.562 (75.337)\tPrec@5 97.656 (97.641)\n",
      "Epoch: [13][60/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.020)\tLoss 3.3743 (3.6532)\tPrec@1 73.438 (75.282)\tPrec@5 96.875 (97.605)\n",
      "Epoch: [13][70/97], lr: 0.01000\tTime 0.301 (0.297)\tData 0.000 (0.020)\tLoss 3.8723 (3.6853)\tPrec@1 72.656 (75.132)\tPrec@5 92.969 (97.392)\n",
      "Epoch: [13][80/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.019)\tLoss 3.8285 (3.6806)\tPrec@1 74.219 (75.087)\tPrec@5 96.875 (97.463)\n",
      "Epoch: [13][90/97], lr: 0.01000\tTime 0.294 (0.297)\tData 0.000 (0.019)\tLoss 3.4939 (3.6806)\tPrec@1 78.125 (75.180)\tPrec@5 96.875 (97.519)\n",
      "Epoch: [13][96/97], lr: 0.01000\tTime 0.287 (0.297)\tData 0.000 (0.020)\tLoss 2.9563 (3.6630)\tPrec@1 81.356 (75.335)\tPrec@5 100.000 (97.574)\n",
      "Test: [0/100]\tTime 0.318 (0.318)\tLoss 11.3927 (11.3927)\tPrec@1 39.000 (39.000)\tPrec@5 89.000 (89.000)\n",
      "Test: [10/100]\tTime 0.072 (0.095)\tLoss 9.1871 (10.7516)\tPrec@1 47.000 (39.909)\tPrec@5 92.000 (89.182)\n",
      "Test: [20/100]\tTime 0.072 (0.084)\tLoss 9.6420 (10.7394)\tPrec@1 45.000 (39.857)\tPrec@5 91.000 (89.429)\n",
      "Test: [30/100]\tTime 0.072 (0.080)\tLoss 10.2664 (10.7317)\tPrec@1 42.000 (39.806)\tPrec@5 86.000 (88.774)\n",
      "Test: [40/100]\tTime 0.072 (0.078)\tLoss 11.3149 (10.7827)\tPrec@1 35.000 (39.610)\tPrec@5 85.000 (88.439)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 10.2505 (10.6794)\tPrec@1 44.000 (40.412)\tPrec@5 84.000 (88.549)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 9.5284 (10.6506)\tPrec@1 40.000 (40.311)\tPrec@5 93.000 (88.639)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 9.9105 (10.6301)\tPrec@1 46.000 (40.282)\tPrec@5 93.000 (88.718)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.5702 (10.6041)\tPrec@1 51.000 (40.309)\tPrec@5 86.000 (88.728)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.2164 (10.6668)\tPrec@1 37.000 (39.901)\tPrec@5 88.000 (88.769)\n",
      "val Results: Prec@1 39.870 Prec@5 88.660 Loss 10.69264\n",
      "val Class Accuracy: [0.916,0.952,0.511,0.741,0.473,0.046,0.337,0.011,0.000,0.000]\n",
      "Best Prec@1: 39.870\n",
      "\n",
      "Epoch: [14][0/97], lr: 0.01000\tTime 0.388 (0.388)\tData 0.231 (0.231)\tLoss 2.6345 (2.6345)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [14][10/97], lr: 0.01000\tTime 0.294 (0.307)\tData 0.000 (0.036)\tLoss 3.2728 (3.2714)\tPrec@1 78.125 (79.261)\tPrec@5 100.000 (98.864)\n",
      "Epoch: [14][20/97], lr: 0.01000\tTime 0.294 (0.302)\tData 0.000 (0.027)\tLoss 3.9185 (3.4476)\tPrec@1 76.562 (77.418)\tPrec@5 95.312 (98.103)\n",
      "Epoch: [14][30/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.024)\tLoss 3.4993 (3.4650)\tPrec@1 75.781 (77.193)\tPrec@5 96.875 (97.782)\n",
      "Epoch: [14][40/97], lr: 0.01000\tTime 0.291 (0.300)\tData 0.000 (0.022)\tLoss 3.5344 (3.4955)\tPrec@1 75.781 (76.715)\tPrec@5 97.656 (97.809)\n",
      "Epoch: [14][50/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 3.4241 (3.4968)\tPrec@1 77.344 (76.976)\tPrec@5 96.094 (97.794)\n",
      "Epoch: [14][60/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.021)\tLoss 4.5334 (3.5075)\tPrec@1 73.438 (76.844)\tPrec@5 95.312 (97.733)\n",
      "Epoch: [14][70/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.020)\tLoss 2.8920 (3.5377)\tPrec@1 82.031 (76.695)\tPrec@5 100.000 (97.711)\n",
      "Epoch: [14][80/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.020)\tLoss 4.4313 (3.5701)\tPrec@1 73.438 (76.505)\tPrec@5 93.750 (97.637)\n",
      "Epoch: [14][90/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.019)\tLoss 4.1118 (3.5537)\tPrec@1 69.531 (76.631)\tPrec@5 97.656 (97.699)\n",
      "Epoch: [14][96/97], lr: 0.01000\tTime 0.287 (0.298)\tData 0.000 (0.020)\tLoss 3.8954 (3.5681)\tPrec@1 75.424 (76.544)\tPrec@5 97.458 (97.711)\n",
      "Test: [0/100]\tTime 0.243 (0.243)\tLoss 12.7257 (12.7257)\tPrec@1 31.000 (31.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.072 (0.088)\tLoss 9.8423 (11.4733)\tPrec@1 45.000 (37.545)\tPrec@5 95.000 (91.182)\n",
      "Test: [20/100]\tTime 0.072 (0.080)\tLoss 10.8504 (11.4415)\tPrec@1 39.000 (36.952)\tPrec@5 94.000 (92.000)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 10.5987 (11.3012)\tPrec@1 39.000 (37.774)\tPrec@5 91.000 (91.839)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 12.0012 (11.3657)\tPrec@1 32.000 (37.268)\tPrec@5 92.000 (91.805)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 11.3258 (11.2871)\tPrec@1 36.000 (37.549)\tPrec@5 92.000 (91.784)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 9.9565 (11.2952)\tPrec@1 43.000 (36.984)\tPrec@5 93.000 (91.607)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 10.4610 (11.2737)\tPrec@1 39.000 (37.113)\tPrec@5 95.000 (91.620)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.7408 (11.2280)\tPrec@1 52.000 (37.383)\tPrec@5 89.000 (91.556)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 10.2637 (11.2727)\tPrec@1 43.000 (37.231)\tPrec@5 94.000 (91.527)\n",
      "val Results: Prec@1 37.110 Prec@5 91.610 Loss 11.30692\n",
      "val Class Accuracy: [0.869,0.951,0.854,0.385,0.515,0.001,0.136,0.000,0.000,0.000]\n",
      "Best Prec@1: 39.870\n",
      "\n",
      "Epoch: [15][0/97], lr: 0.01000\tTime 0.589 (0.589)\tData 0.361 (0.361)\tLoss 4.1432 (4.1432)\tPrec@1 74.219 (74.219)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [15][10/97], lr: 0.01000\tTime 0.317 (0.358)\tData 0.000 (0.045)\tLoss 3.7993 (3.5559)\tPrec@1 75.000 (76.847)\tPrec@5 97.656 (98.011)\n",
      "Epoch: [15][20/97], lr: 0.01000\tTime 0.318 (0.341)\tData 0.000 (0.031)\tLoss 3.7627 (3.5628)\tPrec@1 73.438 (76.972)\tPrec@5 97.656 (97.879)\n",
      "Epoch: [15][30/97], lr: 0.01000\tTime 0.320 (0.335)\tData 0.000 (0.026)\tLoss 3.0721 (3.5286)\tPrec@1 79.688 (77.369)\tPrec@5 98.438 (97.908)\n",
      "Epoch: [15][40/97], lr: 0.01000\tTime 0.301 (0.328)\tData 0.001 (0.023)\tLoss 2.9241 (3.5251)\tPrec@1 80.469 (77.420)\tPrec@5 99.219 (97.923)\n",
      "Epoch: [15][50/97], lr: 0.01000\tTime 0.301 (0.324)\tData 0.000 (0.022)\tLoss 3.5384 (3.5463)\tPrec@1 78.125 (77.175)\tPrec@5 93.750 (97.595)\n",
      "Epoch: [15][60/97], lr: 0.01000\tTime 0.301 (0.321)\tData 0.000 (0.021)\tLoss 3.6851 (3.5199)\tPrec@1 71.875 (77.203)\tPrec@5 98.438 (97.656)\n",
      "Epoch: [15][70/97], lr: 0.01000\tTime 0.301 (0.318)\tData 0.000 (0.021)\tLoss 3.4692 (3.4981)\tPrec@1 78.125 (77.234)\tPrec@5 97.656 (97.667)\n",
      "Epoch: [15][80/97], lr: 0.01000\tTime 0.302 (0.317)\tData 0.000 (0.020)\tLoss 3.9240 (3.4894)\tPrec@1 79.688 (77.257)\tPrec@5 97.656 (97.704)\n",
      "Epoch: [15][90/97], lr: 0.01000\tTime 0.298 (0.316)\tData 0.000 (0.020)\tLoss 2.5934 (3.4646)\tPrec@1 80.469 (77.352)\tPrec@5 99.219 (97.759)\n",
      "Epoch: [15][96/97], lr: 0.01000\tTime 0.295 (0.315)\tData 0.000 (0.020)\tLoss 2.6086 (3.4493)\tPrec@1 84.746 (77.487)\tPrec@5 98.305 (97.775)\n",
      "Test: [0/100]\tTime 0.299 (0.299)\tLoss 13.0708 (13.0708)\tPrec@1 32.000 (32.000)\tPrec@5 83.000 (83.000)\n",
      "Test: [10/100]\tTime 0.072 (0.093)\tLoss 10.4573 (11.8468)\tPrec@1 42.000 (37.091)\tPrec@5 83.000 (87.091)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 10.7713 (11.9837)\tPrec@1 44.000 (36.048)\tPrec@5 92.000 (87.762)\n",
      "Test: [30/100]\tTime 0.072 (0.080)\tLoss 10.8054 (11.8813)\tPrec@1 40.000 (36.290)\tPrec@5 87.000 (88.032)\n",
      "Test: [40/100]\tTime 0.072 (0.078)\tLoss 12.6604 (11.9276)\tPrec@1 35.000 (36.146)\tPrec@5 84.000 (88.024)\n",
      "Test: [50/100]\tTime 0.072 (0.077)\tLoss 11.7753 (11.8483)\tPrec@1 36.000 (36.765)\tPrec@5 90.000 (88.176)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 10.6431 (11.8414)\tPrec@1 39.000 (36.393)\tPrec@5 85.000 (87.984)\n",
      "Test: [70/100]\tTime 0.072 (0.076)\tLoss 11.6098 (11.8249)\tPrec@1 35.000 (36.479)\tPrec@5 91.000 (87.901)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 10.3969 (11.7795)\tPrec@1 46.000 (36.654)\tPrec@5 93.000 (88.012)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 11.8078 (11.8447)\tPrec@1 35.000 (36.330)\tPrec@5 93.000 (87.736)\n",
      "val Results: Prec@1 36.250 Prec@5 87.730 Loss 11.86904\n",
      "val Class Accuracy: [0.969,0.937,0.582,0.661,0.386,0.000,0.008,0.081,0.000,0.001]\n",
      "Best Prec@1: 39.870\n",
      "\n",
      "Epoch: [16][0/97], lr: 0.01000\tTime 0.502 (0.502)\tData 0.321 (0.321)\tLoss 4.2695 (4.2695)\tPrec@1 75.781 (75.781)\tPrec@5 96.094 (96.094)\n",
      "Epoch: [16][10/97], lr: 0.01000\tTime 0.300 (0.323)\tData 0.000 (0.044)\tLoss 3.1524 (3.3780)\tPrec@1 83.594 (78.551)\tPrec@5 98.438 (97.869)\n",
      "Epoch: [16][20/97], lr: 0.01000\tTime 0.301 (0.313)\tData 0.000 (0.031)\tLoss 3.1865 (3.3062)\tPrec@1 80.469 (78.720)\tPrec@5 98.438 (97.842)\n",
      "Epoch: [16][30/97], lr: 0.01000\tTime 0.305 (0.311)\tData 0.000 (0.026)\tLoss 2.9394 (3.2940)\tPrec@1 78.906 (78.478)\tPrec@5 96.875 (97.908)\n",
      "Epoch: [16][40/97], lr: 0.01000\tTime 0.303 (0.309)\tData 0.000 (0.024)\tLoss 4.8126 (3.4174)\tPrec@1 69.531 (77.706)\tPrec@5 96.875 (97.732)\n",
      "Epoch: [16][50/97], lr: 0.01000\tTime 0.301 (0.308)\tData 0.000 (0.023)\tLoss 3.0830 (3.4454)\tPrec@1 78.906 (77.482)\tPrec@5 98.438 (97.794)\n",
      "Epoch: [16][60/97], lr: 0.01000\tTime 0.300 (0.307)\tData 0.000 (0.022)\tLoss 4.0401 (3.4577)\tPrec@1 75.000 (77.408)\tPrec@5 98.438 (97.759)\n",
      "Epoch: [16][70/97], lr: 0.01000\tTime 0.299 (0.306)\tData 0.000 (0.021)\tLoss 3.3768 (3.4440)\tPrec@1 80.469 (77.476)\tPrec@5 97.656 (97.854)\n",
      "Epoch: [16][80/97], lr: 0.01000\tTime 0.301 (0.306)\tData 0.000 (0.020)\tLoss 3.4493 (3.4194)\tPrec@1 75.781 (77.469)\tPrec@5 99.219 (97.811)\n",
      "Epoch: [16][90/97], lr: 0.01000\tTime 0.301 (0.306)\tData 0.000 (0.020)\tLoss 3.7359 (3.4329)\tPrec@1 77.344 (77.541)\tPrec@5 94.531 (97.776)\n",
      "Epoch: [16][96/97], lr: 0.01000\tTime 0.293 (0.305)\tData 0.000 (0.021)\tLoss 2.9596 (3.4115)\tPrec@1 77.966 (77.608)\tPrec@5 97.458 (97.824)\n",
      "Test: [0/100]\tTime 0.297 (0.297)\tLoss 12.1854 (12.1854)\tPrec@1 34.000 (34.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.072 (0.093)\tLoss 9.5018 (10.9928)\tPrec@1 47.000 (39.364)\tPrec@5 94.000 (91.636)\n",
      "Test: [20/100]\tTime 0.072 (0.083)\tLoss 9.5469 (10.9018)\tPrec@1 44.000 (40.190)\tPrec@5 96.000 (92.048)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 9.9198 (10.8522)\tPrec@1 43.000 (40.226)\tPrec@5 88.000 (91.613)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 11.6073 (10.9714)\tPrec@1 35.000 (39.585)\tPrec@5 90.000 (91.610)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 10.5320 (10.8502)\tPrec@1 41.000 (40.216)\tPrec@5 91.000 (91.843)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 9.5658 (10.8348)\tPrec@1 46.000 (40.066)\tPrec@5 94.000 (91.967)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 10.3124 (10.8211)\tPrec@1 46.000 (40.225)\tPrec@5 93.000 (91.901)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.9631 (10.7853)\tPrec@1 48.000 (40.395)\tPrec@5 83.000 (91.840)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.2872 (10.8484)\tPrec@1 39.000 (40.011)\tPrec@5 91.000 (91.835)\n",
      "val Results: Prec@1 39.890 Prec@5 91.860 Loss 10.87780\n",
      "val Class Accuracy: [0.920,0.949,0.596,0.800,0.579,0.039,0.096,0.010,0.000,0.000]\n",
      "Best Prec@1: 39.890\n",
      "\n",
      "Epoch: [17][0/97], lr: 0.01000\tTime 0.514 (0.514)\tData 0.325 (0.325)\tLoss 3.4759 (3.4759)\tPrec@1 75.781 (75.781)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [17][10/97], lr: 0.01000\tTime 0.301 (0.325)\tData 0.000 (0.044)\tLoss 3.6313 (3.4950)\tPrec@1 77.344 (76.918)\tPrec@5 98.438 (97.798)\n",
      "Epoch: [17][20/97], lr: 0.01000\tTime 0.301 (0.315)\tData 0.000 (0.031)\tLoss 3.0688 (3.3443)\tPrec@1 82.812 (78.051)\tPrec@5 100.000 (97.842)\n",
      "Epoch: [17][30/97], lr: 0.01000\tTime 0.303 (0.313)\tData 0.000 (0.027)\tLoss 3.8312 (3.3426)\tPrec@1 73.438 (78.201)\tPrec@5 99.219 (98.110)\n",
      "Epoch: [17][40/97], lr: 0.01000\tTime 0.302 (0.311)\tData 0.000 (0.024)\tLoss 4.6978 (3.3701)\tPrec@1 71.875 (78.011)\tPrec@5 97.656 (98.209)\n",
      "Epoch: [17][50/97], lr: 0.01000\tTime 0.301 (0.309)\tData 0.000 (0.023)\tLoss 2.9635 (3.3928)\tPrec@1 81.250 (77.941)\tPrec@5 99.219 (98.100)\n",
      "Epoch: [17][60/97], lr: 0.01000\tTime 0.301 (0.308)\tData 0.000 (0.022)\tLoss 3.4960 (3.3925)\tPrec@1 75.781 (77.894)\tPrec@5 96.094 (97.989)\n",
      "Epoch: [17][70/97], lr: 0.01000\tTime 0.299 (0.307)\tData 0.000 (0.021)\tLoss 3.7898 (3.3542)\tPrec@1 73.438 (78.015)\tPrec@5 97.656 (97.997)\n",
      "Epoch: [17][80/97], lr: 0.01000\tTime 0.300 (0.307)\tData 0.000 (0.021)\tLoss 4.0629 (3.3716)\tPrec@1 71.875 (77.855)\tPrec@5 97.656 (97.936)\n",
      "Epoch: [17][90/97], lr: 0.01000\tTime 0.316 (0.307)\tData 0.000 (0.020)\tLoss 4.1520 (3.3857)\tPrec@1 71.875 (77.713)\tPrec@5 94.531 (97.897)\n",
      "Epoch: [17][96/97], lr: 0.01000\tTime 0.298 (0.306)\tData 0.000 (0.021)\tLoss 2.6146 (3.3749)\tPrec@1 86.441 (77.866)\tPrec@5 99.153 (97.920)\n",
      "Test: [0/100]\tTime 0.300 (0.300)\tLoss 11.7535 (11.7535)\tPrec@1 41.000 (41.000)\tPrec@5 89.000 (89.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 8.4839 (10.4301)\tPrec@1 51.000 (43.636)\tPrec@5 95.000 (90.364)\n",
      "Test: [20/100]\tTime 0.072 (0.083)\tLoss 9.5865 (10.3920)\tPrec@1 46.000 (44.000)\tPrec@5 95.000 (91.000)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 9.5660 (10.3353)\tPrec@1 49.000 (44.871)\tPrec@5 91.000 (91.032)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 10.5394 (10.4228)\tPrec@1 41.000 (44.098)\tPrec@5 91.000 (90.927)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 9.6622 (10.3150)\tPrec@1 48.000 (44.784)\tPrec@5 94.000 (91.196)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.9602 (10.2977)\tPrec@1 45.000 (44.754)\tPrec@5 95.000 (91.098)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 9.6397 (10.2767)\tPrec@1 52.000 (44.789)\tPrec@5 92.000 (90.972)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.0076 (10.2504)\tPrec@1 55.000 (44.790)\tPrec@5 90.000 (91.062)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.5463 (10.2899)\tPrec@1 48.000 (44.703)\tPrec@5 91.000 (91.055)\n",
      "val Results: Prec@1 44.550 Prec@5 91.070 Loss 10.32986\n",
      "val Class Accuracy: [0.883,0.983,0.614,0.705,0.740,0.095,0.427,0.008,0.000,0.000]\n",
      "Best Prec@1: 44.550\n",
      "\n",
      "Epoch: [18][0/97], lr: 0.01000\tTime 0.517 (0.517)\tData 0.327 (0.327)\tLoss 2.8061 (2.8061)\tPrec@1 82.812 (82.812)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [18][10/97], lr: 0.01000\tTime 0.302 (0.326)\tData 0.000 (0.044)\tLoss 3.6312 (3.2136)\tPrec@1 75.781 (78.977)\tPrec@5 99.219 (98.295)\n",
      "Epoch: [18][20/97], lr: 0.01000\tTime 0.304 (0.315)\tData 0.000 (0.031)\tLoss 2.9765 (3.1718)\tPrec@1 81.250 (79.204)\tPrec@5 98.438 (97.954)\n",
      "Epoch: [18][30/97], lr: 0.01000\tTime 0.302 (0.312)\tData 0.000 (0.026)\tLoss 2.8257 (3.1870)\tPrec@1 82.812 (78.982)\tPrec@5 98.438 (98.135)\n",
      "Epoch: [18][40/97], lr: 0.01000\tTime 0.307 (0.310)\tData 0.000 (0.024)\tLoss 2.7856 (3.1901)\tPrec@1 82.812 (79.211)\tPrec@5 98.438 (98.037)\n",
      "Epoch: [18][50/97], lr: 0.01000\tTime 0.306 (0.310)\tData 0.000 (0.023)\tLoss 4.4060 (3.2280)\tPrec@1 71.094 (79.105)\tPrec@5 99.219 (97.978)\n",
      "Epoch: [18][60/97], lr: 0.01000\tTime 0.303 (0.309)\tData 0.000 (0.022)\tLoss 4.6335 (3.2885)\tPrec@1 69.531 (78.714)\tPrec@5 97.656 (97.925)\n",
      "Epoch: [18][70/97], lr: 0.01000\tTime 0.300 (0.308)\tData 0.000 (0.021)\tLoss 2.7393 (3.2900)\tPrec@1 79.688 (78.510)\tPrec@5 97.656 (97.920)\n",
      "Epoch: [18][80/97], lr: 0.01000\tTime 0.299 (0.308)\tData 0.000 (0.021)\tLoss 3.1971 (3.3029)\tPrec@1 78.906 (78.366)\tPrec@5 98.438 (97.975)\n",
      "Epoch: [18][90/97], lr: 0.01000\tTime 0.308 (0.308)\tData 0.000 (0.020)\tLoss 2.9952 (3.3050)\tPrec@1 81.250 (78.314)\tPrec@5 100.000 (98.060)\n",
      "Epoch: [18][96/97], lr: 0.01000\tTime 0.294 (0.307)\tData 0.000 (0.021)\tLoss 3.2270 (3.3178)\tPrec@1 75.424 (78.277)\tPrec@5 100.000 (98.025)\n",
      "Test: [0/100]\tTime 0.348 (0.348)\tLoss 13.2260 (13.2260)\tPrec@1 28.000 (28.000)\tPrec@5 89.000 (89.000)\n",
      "Test: [10/100]\tTime 0.073 (0.098)\tLoss 10.4195 (12.0459)\tPrec@1 42.000 (34.545)\tPrec@5 87.000 (90.545)\n",
      "Test: [20/100]\tTime 0.073 (0.086)\tLoss 10.7119 (12.1964)\tPrec@1 38.000 (33.143)\tPrec@5 91.000 (89.333)\n",
      "Test: [30/100]\tTime 0.073 (0.082)\tLoss 11.0684 (12.1103)\tPrec@1 38.000 (33.484)\tPrec@5 90.000 (89.452)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 12.6296 (12.1745)\tPrec@1 32.000 (33.171)\tPrec@5 87.000 (89.341)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 11.9541 (12.0799)\tPrec@1 36.000 (33.765)\tPrec@5 88.000 (89.627)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 10.4194 (12.0702)\tPrec@1 38.000 (33.377)\tPrec@5 90.000 (89.475)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 11.7407 (12.0262)\tPrec@1 34.000 (33.606)\tPrec@5 88.000 (89.380)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 10.6822 (11.9744)\tPrec@1 45.000 (33.914)\tPrec@5 90.000 (89.556)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 12.0859 (12.0298)\tPrec@1 33.000 (33.637)\tPrec@5 93.000 (89.363)\n",
      "val Results: Prec@1 33.720 Prec@5 89.460 Loss 12.05497\n",
      "val Class Accuracy: [0.972,0.991,0.499,0.559,0.137,0.039,0.080,0.095,0.000,0.000]\n",
      "Best Prec@1: 44.550\n",
      "\n",
      "Epoch: [19][0/97], lr: 0.01000\tTime 0.598 (0.598)\tData 0.404 (0.404)\tLoss 3.6644 (3.6644)\tPrec@1 78.125 (78.125)\tPrec@5 96.094 (96.094)\n",
      "Epoch: [19][10/97], lr: 0.01000\tTime 0.301 (0.332)\tData 0.000 (0.050)\tLoss 3.0781 (3.1548)\tPrec@1 80.469 (79.688)\tPrec@5 99.219 (98.082)\n",
      "Epoch: [19][20/97], lr: 0.01000\tTime 0.302 (0.319)\tData 0.000 (0.034)\tLoss 2.7817 (3.1622)\tPrec@1 82.031 (79.799)\tPrec@5 99.219 (98.326)\n",
      "Epoch: [19][30/97], lr: 0.01000\tTime 0.296 (0.314)\tData 0.000 (0.029)\tLoss 3.7279 (3.1426)\tPrec@1 75.000 (79.889)\tPrec@5 100.000 (98.337)\n",
      "Epoch: [19][40/97], lr: 0.01000\tTime 0.301 (0.311)\tData 0.000 (0.026)\tLoss 3.8323 (3.2707)\tPrec@1 74.219 (78.811)\tPrec@5 97.656 (98.075)\n",
      "Epoch: [19][50/97], lr: 0.01000\tTime 0.301 (0.310)\tData 0.000 (0.024)\tLoss 2.7743 (3.2964)\tPrec@1 84.375 (78.370)\tPrec@5 98.438 (97.993)\n",
      "Epoch: [19][60/97], lr: 0.01000\tTime 0.304 (0.309)\tData 0.000 (0.023)\tLoss 3.6856 (3.3348)\tPrec@1 71.875 (78.035)\tPrec@5 97.656 (97.861)\n",
      "Epoch: [19][70/97], lr: 0.01000\tTime 0.304 (0.308)\tData 0.000 (0.022)\tLoss 2.8050 (3.3392)\tPrec@1 83.594 (77.949)\tPrec@5 95.312 (97.876)\n",
      "Epoch: [19][80/97], lr: 0.01000\tTime 0.299 (0.308)\tData 0.000 (0.021)\tLoss 4.0316 (3.3363)\tPrec@1 71.094 (78.038)\tPrec@5 98.438 (97.917)\n",
      "Epoch: [19][90/97], lr: 0.01000\tTime 0.301 (0.308)\tData 0.000 (0.021)\tLoss 2.7505 (3.3327)\tPrec@1 82.812 (78.159)\tPrec@5 99.219 (97.948)\n",
      "Epoch: [19][96/97], lr: 0.01000\tTime 0.294 (0.307)\tData 0.000 (0.021)\tLoss 4.2964 (3.3294)\tPrec@1 71.186 (78.099)\tPrec@5 96.610 (97.936)\n",
      "Test: [0/100]\tTime 0.326 (0.326)\tLoss 11.4441 (11.4441)\tPrec@1 42.000 (42.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.072 (0.096)\tLoss 8.7668 (10.5774)\tPrec@1 55.000 (44.455)\tPrec@5 93.000 (93.545)\n",
      "Test: [20/100]\tTime 0.072 (0.085)\tLoss 9.8428 (10.4944)\tPrec@1 47.000 (44.238)\tPrec@5 95.000 (93.905)\n",
      "Test: [30/100]\tTime 0.072 (0.081)\tLoss 9.4642 (10.5194)\tPrec@1 49.000 (43.710)\tPrec@5 93.000 (93.710)\n",
      "Test: [40/100]\tTime 0.072 (0.079)\tLoss 10.6249 (10.5574)\tPrec@1 40.000 (43.244)\tPrec@5 94.000 (93.951)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 9.5918 (10.4139)\tPrec@1 52.000 (44.059)\tPrec@5 98.000 (94.235)\n",
      "Test: [60/100]\tTime 0.072 (0.077)\tLoss 9.6168 (10.3934)\tPrec@1 45.000 (43.918)\tPrec@5 93.000 (94.295)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 9.7808 (10.3829)\tPrec@1 52.000 (44.042)\tPrec@5 96.000 (94.282)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 9.4400 (10.3650)\tPrec@1 51.000 (43.938)\tPrec@5 89.000 (94.185)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.8903 (10.4127)\tPrec@1 42.000 (43.758)\tPrec@5 95.000 (94.066)\n",
      "val Results: Prec@1 43.720 Prec@5 94.110 Loss 10.44629\n",
      "val Class Accuracy: [0.854,0.990,0.498,0.648,0.800,0.146,0.391,0.043,0.001,0.001]\n",
      "Best Prec@1: 44.550\n",
      "\n",
      "Epoch: [20][0/97], lr: 0.01000\tTime 0.454 (0.454)\tData 0.283 (0.283)\tLoss 2.8391 (2.8391)\tPrec@1 81.250 (81.250)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [20][10/97], lr: 0.01000\tTime 0.301 (0.320)\tData 0.000 (0.041)\tLoss 3.3868 (3.0882)\tPrec@1 78.125 (79.403)\tPrec@5 99.219 (98.011)\n",
      "Epoch: [20][20/97], lr: 0.01000\tTime 0.301 (0.313)\tData 0.000 (0.029)\tLoss 3.0836 (3.2153)\tPrec@1 80.469 (78.943)\tPrec@5 98.438 (97.842)\n",
      "Epoch: [20][30/97], lr: 0.01000\tTime 0.304 (0.311)\tData 0.000 (0.025)\tLoss 2.9538 (3.1993)\tPrec@1 82.031 (79.083)\tPrec@5 99.219 (97.908)\n",
      "Epoch: [20][40/97], lr: 0.01000\tTime 0.303 (0.309)\tData 0.000 (0.023)\tLoss 3.3890 (3.1725)\tPrec@1 79.688 (79.173)\tPrec@5 99.219 (98.152)\n",
      "Epoch: [20][50/97], lr: 0.01000\tTime 0.301 (0.308)\tData 0.000 (0.022)\tLoss 3.6197 (3.1838)\tPrec@1 76.562 (79.167)\tPrec@5 97.656 (98.116)\n",
      "Epoch: [20][60/97], lr: 0.01000\tTime 0.305 (0.308)\tData 0.000 (0.021)\tLoss 2.8998 (3.2156)\tPrec@1 82.031 (79.162)\tPrec@5 97.656 (98.079)\n",
      "Epoch: [20][70/97], lr: 0.01000\tTime 0.308 (0.307)\tData 0.000 (0.021)\tLoss 2.5263 (3.2406)\tPrec@1 82.812 (78.917)\tPrec@5 100.000 (98.140)\n",
      "Epoch: [20][80/97], lr: 0.01000\tTime 0.294 (0.307)\tData 0.000 (0.020)\tLoss 3.9430 (3.2218)\tPrec@1 71.875 (79.003)\tPrec@5 95.312 (98.187)\n",
      "Epoch: [20][90/97], lr: 0.01000\tTime 0.298 (0.306)\tData 0.000 (0.020)\tLoss 3.1245 (3.2303)\tPrec@1 79.688 (78.992)\tPrec@5 97.656 (98.189)\n",
      "Epoch: [20][96/97], lr: 0.01000\tTime 0.295 (0.305)\tData 0.000 (0.020)\tLoss 2.5688 (3.2103)\tPrec@1 82.203 (79.067)\tPrec@5 99.153 (98.211)\n",
      "Test: [0/100]\tTime 0.221 (0.221)\tLoss 13.4430 (13.4430)\tPrec@1 32.000 (32.000)\tPrec@5 88.000 (88.000)\n",
      "Test: [10/100]\tTime 0.072 (0.086)\tLoss 10.4599 (12.4449)\tPrec@1 40.000 (34.636)\tPrec@5 92.000 (88.909)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 10.9350 (12.5537)\tPrec@1 41.000 (33.286)\tPrec@5 89.000 (88.190)\n",
      "Test: [30/100]\tTime 0.073 (0.077)\tLoss 11.6853 (12.4714)\tPrec@1 35.000 (33.581)\tPrec@5 86.000 (87.548)\n",
      "Test: [40/100]\tTime 0.072 (0.076)\tLoss 13.5740 (12.5499)\tPrec@1 27.000 (33.268)\tPrec@5 82.000 (86.780)\n",
      "Test: [50/100]\tTime 0.072 (0.075)\tLoss 12.2398 (12.4534)\tPrec@1 37.000 (33.902)\tPrec@5 87.000 (86.902)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 10.7612 (12.4366)\tPrec@1 39.000 (33.836)\tPrec@5 88.000 (86.803)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 11.9550 (12.4171)\tPrec@1 35.000 (33.901)\tPrec@5 87.000 (86.944)\n",
      "Test: [80/100]\tTime 0.073 (0.074)\tLoss 10.8612 (12.3672)\tPrec@1 45.000 (34.111)\tPrec@5 79.000 (87.074)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 12.4082 (12.4190)\tPrec@1 35.000 (33.901)\tPrec@5 88.000 (87.165)\n",
      "val Results: Prec@1 33.890 Prec@5 87.310 Loss 12.43305\n",
      "val Class Accuracy: [0.966,0.961,0.740,0.538,0.027,0.070,0.081,0.006,0.000,0.000]\n",
      "Best Prec@1: 44.550\n",
      "\n",
      "Epoch: [21][0/97], lr: 0.01000\tTime 0.421 (0.421)\tData 0.261 (0.261)\tLoss 2.7534 (2.7534)\tPrec@1 84.375 (84.375)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [21][10/97], lr: 0.01000\tTime 0.295 (0.309)\tData 0.000 (0.039)\tLoss 3.6766 (3.1976)\tPrec@1 75.781 (79.545)\tPrec@5 98.438 (98.082)\n",
      "Epoch: [21][20/97], lr: 0.01000\tTime 0.294 (0.303)\tData 0.000 (0.028)\tLoss 3.7431 (3.3193)\tPrec@1 75.000 (78.869)\tPrec@5 96.094 (97.879)\n",
      "Epoch: [21][30/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.025)\tLoss 2.5602 (3.3163)\tPrec@1 83.594 (78.528)\tPrec@5 99.219 (97.959)\n",
      "Epoch: [21][40/97], lr: 0.01000\tTime 0.287 (0.300)\tData 0.000 (0.023)\tLoss 2.9117 (3.3066)\tPrec@1 81.250 (78.506)\tPrec@5 97.656 (97.942)\n",
      "Epoch: [21][50/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.022)\tLoss 3.4741 (3.2691)\tPrec@1 77.344 (78.707)\tPrec@5 97.656 (97.978)\n",
      "Epoch: [21][60/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 3.5027 (3.2444)\tPrec@1 76.562 (78.740)\tPrec@5 98.438 (98.181)\n",
      "Epoch: [21][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 3.5878 (3.2196)\tPrec@1 75.000 (78.895)\tPrec@5 97.656 (98.107)\n",
      "Epoch: [21][80/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.020)\tLoss 2.6622 (3.2110)\tPrec@1 82.031 (79.080)\tPrec@5 99.219 (98.090)\n",
      "Epoch: [21][90/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.020)\tLoss 3.0975 (3.1742)\tPrec@1 78.906 (79.344)\tPrec@5 98.438 (98.060)\n",
      "Epoch: [21][96/97], lr: 0.01000\tTime 0.288 (0.298)\tData 0.000 (0.020)\tLoss 2.6255 (3.1684)\tPrec@1 83.898 (79.397)\tPrec@5 97.458 (98.106)\n",
      "Test: [0/100]\tTime 0.219 (0.219)\tLoss 12.2707 (12.2707)\tPrec@1 34.000 (34.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.072 (0.086)\tLoss 9.1183 (11.2801)\tPrec@1 53.000 (41.273)\tPrec@5 96.000 (92.636)\n",
      "Test: [20/100]\tTime 0.072 (0.079)\tLoss 11.2092 (11.1227)\tPrec@1 38.000 (41.762)\tPrec@5 90.000 (92.714)\n",
      "Test: [30/100]\tTime 0.072 (0.077)\tLoss 10.2012 (11.1377)\tPrec@1 45.000 (41.452)\tPrec@5 94.000 (92.258)\n",
      "Test: [40/100]\tTime 0.073 (0.076)\tLoss 11.5677 (11.2320)\tPrec@1 40.000 (41.000)\tPrec@5 91.000 (91.829)\n",
      "Test: [50/100]\tTime 0.073 (0.075)\tLoss 10.6037 (11.0975)\tPrec@1 42.000 (41.784)\tPrec@5 94.000 (92.157)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 10.1230 (11.1012)\tPrec@1 46.000 (41.705)\tPrec@5 93.000 (92.066)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 10.3954 (11.0995)\tPrec@1 49.000 (41.606)\tPrec@5 90.000 (91.930)\n",
      "Test: [80/100]\tTime 0.073 (0.074)\tLoss 10.2787 (11.1046)\tPrec@1 50.000 (41.407)\tPrec@5 90.000 (91.988)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 11.2784 (11.1418)\tPrec@1 39.000 (41.319)\tPrec@5 92.000 (91.967)\n",
      "val Results: Prec@1 41.190 Prec@5 91.880 Loss 11.17743\n",
      "val Class Accuracy: [0.829,0.822,0.368,0.799,0.866,0.049,0.336,0.002,0.000,0.048]\n",
      "Best Prec@1: 44.550\n",
      "\n",
      "Epoch: [22][0/97], lr: 0.01000\tTime 0.342 (0.342)\tData 0.185 (0.185)\tLoss 3.5325 (3.5325)\tPrec@1 75.781 (75.781)\tPrec@5 94.531 (94.531)\n",
      "Epoch: [22][10/97], lr: 0.01000\tTime 0.296 (0.303)\tData 0.000 (0.031)\tLoss 2.9485 (3.0870)\tPrec@1 82.031 (79.616)\tPrec@5 100.000 (97.869)\n",
      "Epoch: [22][20/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.024)\tLoss 2.8910 (3.2098)\tPrec@1 83.594 (78.906)\tPrec@5 99.219 (97.954)\n",
      "Epoch: [22][30/97], lr: 0.01000\tTime 0.303 (0.300)\tData 0.000 (0.022)\tLoss 2.4475 (3.1493)\tPrec@1 84.375 (79.511)\tPrec@5 100.000 (98.211)\n",
      "Epoch: [22][40/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.021)\tLoss 3.0252 (3.1204)\tPrec@1 78.906 (79.821)\tPrec@5 98.438 (98.285)\n",
      "Epoch: [22][50/97], lr: 0.01000\tTime 0.300 (0.299)\tData 0.000 (0.020)\tLoss 3.6938 (3.1578)\tPrec@1 76.562 (79.626)\tPrec@5 97.656 (98.376)\n",
      "Epoch: [22][60/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 3.0461 (3.1495)\tPrec@1 78.906 (79.649)\tPrec@5 98.438 (98.373)\n",
      "Epoch: [22][70/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.019)\tLoss 3.4623 (3.1502)\tPrec@1 76.562 (79.665)\tPrec@5 99.219 (98.360)\n",
      "Epoch: [22][80/97], lr: 0.01000\tTime 0.296 (0.298)\tData 0.000 (0.019)\tLoss 2.4978 (3.1286)\tPrec@1 84.375 (79.803)\tPrec@5 98.438 (98.399)\n",
      "Epoch: [22][90/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.019)\tLoss 2.9661 (3.1157)\tPrec@1 78.906 (79.868)\tPrec@5 99.219 (98.412)\n",
      "Epoch: [22][96/97], lr: 0.01000\tTime 0.287 (0.298)\tData 0.000 (0.019)\tLoss 2.5876 (3.1043)\tPrec@1 79.661 (79.873)\tPrec@5 99.153 (98.428)\n",
      "Test: [0/100]\tTime 0.235 (0.235)\tLoss 12.1423 (12.1423)\tPrec@1 34.000 (34.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 8.5415 (10.7583)\tPrec@1 53.000 (42.545)\tPrec@5 98.000 (94.091)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 10.1913 (10.7570)\tPrec@1 46.000 (42.667)\tPrec@5 93.000 (93.952)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.6362 (10.6855)\tPrec@1 47.000 (43.097)\tPrec@5 97.000 (93.903)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 11.2931 (10.7239)\tPrec@1 40.000 (42.805)\tPrec@5 91.000 (93.634)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 10.4785 (10.6137)\tPrec@1 43.000 (43.235)\tPrec@5 92.000 (93.804)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 9.0587 (10.5805)\tPrec@1 51.000 (43.508)\tPrec@5 96.000 (94.033)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 10.0511 (10.5664)\tPrec@1 49.000 (43.648)\tPrec@5 94.000 (94.028)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 9.7047 (10.5565)\tPrec@1 50.000 (43.519)\tPrec@5 91.000 (94.049)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 10.6611 (10.6071)\tPrec@1 44.000 (43.363)\tPrec@5 99.000 (94.099)\n",
      "val Results: Prec@1 43.420 Prec@5 94.180 Loss 10.63005\n",
      "val Class Accuracy: [0.961,0.927,0.554,0.703,0.746,0.134,0.280,0.030,0.005,0.002]\n",
      "Best Prec@1: 44.550\n",
      "\n",
      "Epoch: [23][0/97], lr: 0.01000\tTime 0.426 (0.426)\tData 0.268 (0.268)\tLoss 3.0544 (3.0544)\tPrec@1 78.906 (78.906)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [23][10/97], lr: 0.01000\tTime 0.296 (0.312)\tData 0.000 (0.040)\tLoss 3.2694 (3.0825)\tPrec@1 76.562 (79.901)\tPrec@5 99.219 (98.295)\n",
      "Epoch: [23][20/97], lr: 0.01000\tTime 0.298 (0.305)\tData 0.000 (0.029)\tLoss 3.8580 (3.1500)\tPrec@1 73.438 (79.650)\tPrec@5 97.656 (98.400)\n",
      "Epoch: [23][30/97], lr: 0.01000\tTime 0.292 (0.303)\tData 0.000 (0.025)\tLoss 3.1319 (3.1280)\tPrec@1 79.688 (80.116)\tPrec@5 99.219 (98.488)\n",
      "Epoch: [23][40/97], lr: 0.01000\tTime 0.290 (0.301)\tData 0.000 (0.023)\tLoss 3.2826 (3.1484)\tPrec@1 77.344 (79.897)\tPrec@5 97.656 (98.380)\n",
      "Epoch: [23][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.022)\tLoss 2.2413 (3.1137)\tPrec@1 86.719 (80.132)\tPrec@5 99.219 (98.346)\n",
      "Epoch: [23][60/97], lr: 0.01000\tTime 0.299 (0.300)\tData 0.000 (0.021)\tLoss 2.8595 (3.0891)\tPrec@1 82.031 (80.418)\tPrec@5 100.000 (98.284)\n",
      "Epoch: [23][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 2.8760 (3.0629)\tPrec@1 79.688 (80.414)\tPrec@5 97.656 (98.272)\n",
      "Epoch: [23][80/97], lr: 0.01000\tTime 0.302 (0.299)\tData 0.000 (0.020)\tLoss 3.4108 (3.0682)\tPrec@1 78.125 (80.507)\tPrec@5 97.656 (98.264)\n",
      "Epoch: [23][90/97], lr: 0.01000\tTime 0.298 (0.299)\tData 0.000 (0.020)\tLoss 3.4130 (3.0764)\tPrec@1 76.562 (80.366)\tPrec@5 97.656 (98.257)\n",
      "Epoch: [23][96/97], lr: 0.01000\tTime 0.292 (0.299)\tData 0.000 (0.020)\tLoss 2.9603 (3.1039)\tPrec@1 81.356 (80.155)\tPrec@5 99.153 (98.251)\n",
      "Test: [0/100]\tTime 0.236 (0.236)\tLoss 12.0970 (12.0970)\tPrec@1 36.000 (36.000)\tPrec@5 90.000 (90.000)\n",
      "Test: [10/100]\tTime 0.072 (0.087)\tLoss 9.3418 (10.7466)\tPrec@1 49.000 (41.455)\tPrec@5 87.000 (92.545)\n",
      "Test: [20/100]\tTime 0.072 (0.080)\tLoss 9.8610 (10.8266)\tPrec@1 41.000 (40.286)\tPrec@5 93.000 (92.238)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 10.1320 (10.7461)\tPrec@1 42.000 (40.516)\tPrec@5 92.000 (92.516)\n",
      "Test: [40/100]\tTime 0.072 (0.076)\tLoss 11.0954 (10.7494)\tPrec@1 38.000 (40.585)\tPrec@5 89.000 (92.293)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 10.4844 (10.6897)\tPrec@1 43.000 (41.098)\tPrec@5 93.000 (92.471)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 8.8848 (10.6810)\tPrec@1 48.000 (40.721)\tPrec@5 95.000 (92.426)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 10.4744 (10.6722)\tPrec@1 42.000 (40.648)\tPrec@5 97.000 (92.423)\n",
      "Test: [80/100]\tTime 0.072 (0.074)\tLoss 9.4569 (10.6417)\tPrec@1 51.000 (40.741)\tPrec@5 94.000 (92.580)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 10.6212 (10.7080)\tPrec@1 41.000 (40.385)\tPrec@5 96.000 (92.440)\n",
      "val Results: Prec@1 40.330 Prec@5 92.560 Loss 10.74186\n",
      "val Class Accuracy: [0.969,0.955,0.652,0.466,0.510,0.223,0.134,0.119,0.002,0.003]\n",
      "Best Prec@1: 44.550\n",
      "\n",
      "Epoch: [24][0/97], lr: 0.01000\tTime 0.376 (0.376)\tData 0.220 (0.220)\tLoss 3.6494 (3.6494)\tPrec@1 75.000 (75.000)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [24][10/97], lr: 0.01000\tTime 0.295 (0.307)\tData 0.000 (0.035)\tLoss 2.7880 (2.9874)\tPrec@1 82.031 (80.327)\tPrec@5 98.438 (98.224)\n",
      "Epoch: [24][20/97], lr: 0.01000\tTime 0.294 (0.302)\tData 0.000 (0.027)\tLoss 3.1801 (3.0665)\tPrec@1 75.781 (80.283)\tPrec@5 99.219 (98.586)\n",
      "Epoch: [24][30/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.024)\tLoss 3.5844 (3.1199)\tPrec@1 76.562 (80.166)\tPrec@5 98.438 (98.463)\n",
      "Epoch: [24][40/97], lr: 0.01000\tTime 0.289 (0.300)\tData 0.000 (0.022)\tLoss 3.1990 (3.1116)\tPrec@1 79.688 (80.221)\tPrec@5 94.531 (98.304)\n",
      "Epoch: [24][50/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 3.2434 (3.0949)\tPrec@1 81.250 (80.423)\tPrec@5 96.094 (98.055)\n",
      "Epoch: [24][60/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 3.6619 (3.1140)\tPrec@1 75.781 (80.213)\tPrec@5 99.219 (98.079)\n",
      "Epoch: [24][70/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.020)\tLoss 2.2264 (3.0746)\tPrec@1 85.938 (80.425)\tPrec@5 99.219 (98.206)\n",
      "Epoch: [24][80/97], lr: 0.01000\tTime 0.297 (0.298)\tData 0.000 (0.020)\tLoss 3.6532 (3.0712)\tPrec@1 75.781 (80.449)\tPrec@5 98.438 (98.264)\n",
      "Epoch: [24][90/97], lr: 0.01000\tTime 0.293 (0.298)\tData 0.000 (0.019)\tLoss 3.1121 (3.0809)\tPrec@1 78.906 (80.340)\tPrec@5 100.000 (98.300)\n",
      "Epoch: [24][96/97], lr: 0.01000\tTime 0.287 (0.298)\tData 0.000 (0.020)\tLoss 2.5821 (3.0659)\tPrec@1 81.356 (80.437)\tPrec@5 99.153 (98.283)\n",
      "Test: [0/100]\tTime 0.249 (0.249)\tLoss 11.0334 (11.0334)\tPrec@1 48.000 (48.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.072 (0.089)\tLoss 7.4628 (9.8563)\tPrec@1 62.000 (48.909)\tPrec@5 93.000 (93.545)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 9.3785 (9.9036)\tPrec@1 49.000 (48.095)\tPrec@5 97.000 (93.714)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.1378 (9.8933)\tPrec@1 53.000 (48.097)\tPrec@5 93.000 (93.161)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 10.1249 (9.9941)\tPrec@1 44.000 (47.439)\tPrec@5 90.000 (93.073)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.4981 (9.9241)\tPrec@1 52.000 (47.843)\tPrec@5 92.000 (93.098)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 8.6510 (9.8792)\tPrec@1 50.000 (47.951)\tPrec@5 95.000 (93.016)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 8.8686 (9.8455)\tPrec@1 53.000 (48.070)\tPrec@5 96.000 (93.141)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 9.0345 (9.8275)\tPrec@1 53.000 (48.185)\tPrec@5 89.000 (93.123)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 10.3571 (9.8608)\tPrec@1 45.000 (48.209)\tPrec@5 98.000 (93.143)\n",
      "val Results: Prec@1 48.160 Prec@5 93.150 Loss 9.89019\n",
      "val Class Accuracy: [0.904,0.985,0.541,0.503,0.788,0.341,0.713,0.025,0.016,0.000]\n",
      "Best Prec@1: 48.160\n",
      "\n",
      "Epoch: [25][0/97], lr: 0.01000\tTime 0.344 (0.344)\tData 0.204 (0.204)\tLoss 2.9526 (2.9526)\tPrec@1 80.469 (80.469)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [25][10/97], lr: 0.01000\tTime 0.295 (0.303)\tData 0.000 (0.034)\tLoss 3.2117 (3.0857)\tPrec@1 81.250 (80.185)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [25][20/97], lr: 0.01000\tTime 0.293 (0.299)\tData 0.000 (0.026)\tLoss 1.9505 (3.0110)\tPrec@1 88.281 (80.915)\tPrec@5 100.000 (98.438)\n",
      "Epoch: [25][30/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.023)\tLoss 3.0428 (3.0858)\tPrec@1 78.906 (80.166)\tPrec@5 97.656 (98.160)\n",
      "Epoch: [25][40/97], lr: 0.01000\tTime 0.296 (0.298)\tData 0.000 (0.022)\tLoss 2.6349 (3.0590)\tPrec@1 82.812 (80.373)\tPrec@5 100.000 (98.304)\n",
      "Epoch: [25][50/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.021)\tLoss 3.1930 (3.0417)\tPrec@1 79.688 (80.316)\tPrec@5 97.656 (98.407)\n",
      "Epoch: [25][60/97], lr: 0.01000\tTime 0.296 (0.298)\tData 0.000 (0.020)\tLoss 2.5161 (3.0363)\tPrec@1 83.594 (80.315)\tPrec@5 99.219 (98.373)\n",
      "Epoch: [25][70/97], lr: 0.01000\tTime 0.295 (0.297)\tData 0.000 (0.020)\tLoss 2.6672 (2.9791)\tPrec@1 83.594 (80.843)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [25][80/97], lr: 0.01000\tTime 0.294 (0.297)\tData 0.000 (0.019)\tLoss 3.8076 (2.9874)\tPrec@1 75.781 (80.883)\tPrec@5 96.875 (98.457)\n",
      "Epoch: [25][90/97], lr: 0.01000\tTime 0.293 (0.297)\tData 0.000 (0.019)\tLoss 2.7152 (3.0063)\tPrec@1 83.594 (80.743)\tPrec@5 99.219 (98.455)\n",
      "Epoch: [25][96/97], lr: 0.01000\tTime 0.288 (0.297)\tData 0.000 (0.020)\tLoss 2.9629 (2.9959)\tPrec@1 81.356 (80.848)\tPrec@5 99.153 (98.460)\n",
      "Test: [0/100]\tTime 0.250 (0.250)\tLoss 11.1596 (11.1596)\tPrec@1 44.000 (44.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.072 (0.089)\tLoss 7.1146 (10.3324)\tPrec@1 61.000 (45.545)\tPrec@5 95.000 (92.182)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 10.7566 (10.4421)\tPrec@1 40.000 (45.190)\tPrec@5 92.000 (91.952)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.7447 (10.5051)\tPrec@1 50.000 (44.806)\tPrec@5 89.000 (91.419)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 11.1323 (10.5789)\tPrec@1 43.000 (44.366)\tPrec@5 83.000 (91.098)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 10.1776 (10.4579)\tPrec@1 48.000 (45.039)\tPrec@5 87.000 (91.078)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 9.3623 (10.4134)\tPrec@1 45.000 (45.230)\tPrec@5 93.000 (91.082)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 10.1478 (10.4071)\tPrec@1 45.000 (45.310)\tPrec@5 89.000 (91.113)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.4933 (10.4043)\tPrec@1 49.000 (45.222)\tPrec@5 88.000 (91.173)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.6698 (10.4574)\tPrec@1 44.000 (44.945)\tPrec@5 91.000 (91.220)\n",
      "val Results: Prec@1 44.820 Prec@5 91.200 Loss 10.48872\n",
      "val Class Accuracy: [0.948,0.920,0.591,0.397,0.630,0.119,0.853,0.011,0.000,0.013]\n",
      "Best Prec@1: 48.160\n",
      "\n",
      "Epoch: [26][0/97], lr: 0.01000\tTime 0.386 (0.386)\tData 0.229 (0.229)\tLoss 3.2979 (3.2979)\tPrec@1 74.219 (74.219)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [26][10/97], lr: 0.01000\tTime 0.297 (0.308)\tData 0.000 (0.036)\tLoss 2.5843 (2.8555)\tPrec@1 82.812 (81.605)\tPrec@5 97.656 (98.366)\n",
      "Epoch: [26][20/97], lr: 0.01000\tTime 0.296 (0.303)\tData 0.000 (0.027)\tLoss 2.5252 (3.0420)\tPrec@1 85.156 (80.357)\tPrec@5 98.438 (98.177)\n",
      "Epoch: [26][30/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.024)\tLoss 2.9289 (3.0839)\tPrec@1 80.469 (79.940)\tPrec@5 99.219 (98.286)\n",
      "Epoch: [26][40/97], lr: 0.01000\tTime 0.299 (0.301)\tData 0.000 (0.022)\tLoss 2.4519 (3.0525)\tPrec@1 84.375 (80.221)\tPrec@5 100.000 (98.457)\n",
      "Epoch: [26][50/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 3.4115 (3.0022)\tPrec@1 78.125 (80.790)\tPrec@5 96.875 (98.468)\n",
      "Epoch: [26][60/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.021)\tLoss 2.8309 (2.9880)\tPrec@1 78.906 (80.840)\tPrec@5 100.000 (98.540)\n",
      "Epoch: [26][70/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 2.3287 (2.9662)\tPrec@1 86.719 (80.909)\tPrec@5 98.438 (98.493)\n",
      "Epoch: [26][80/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 3.2471 (2.9956)\tPrec@1 78.906 (80.662)\tPrec@5 99.219 (98.457)\n",
      "Epoch: [26][90/97], lr: 0.01000\tTime 0.298 (0.299)\tData 0.000 (0.020)\tLoss 2.0192 (2.9659)\tPrec@1 88.281 (80.907)\tPrec@5 98.438 (98.498)\n",
      "Epoch: [26][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 2.6617 (2.9678)\tPrec@1 82.203 (80.945)\tPrec@5 100.000 (98.517)\n",
      "Test: [0/100]\tTime 0.257 (0.257)\tLoss 11.8977 (11.8977)\tPrec@1 41.000 (41.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 8.0980 (10.5479)\tPrec@1 57.000 (45.364)\tPrec@5 97.000 (94.000)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 10.7197 (10.5415)\tPrec@1 42.000 (45.190)\tPrec@5 96.000 (94.476)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.9461 (10.4851)\tPrec@1 54.000 (45.452)\tPrec@5 94.000 (94.516)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.8624 (10.5241)\tPrec@1 44.000 (45.439)\tPrec@5 94.000 (94.561)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 10.2972 (10.4283)\tPrec@1 47.000 (45.706)\tPrec@5 93.000 (94.686)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 9.5383 (10.3860)\tPrec@1 52.000 (45.918)\tPrec@5 96.000 (94.557)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 9.3281 (10.3673)\tPrec@1 52.000 (45.958)\tPrec@5 95.000 (94.451)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 9.2059 (10.3317)\tPrec@1 49.000 (46.074)\tPrec@5 93.000 (94.370)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 10.0857 (10.3860)\tPrec@1 47.000 (45.780)\tPrec@5 96.000 (94.385)\n",
      "val Results: Prec@1 45.660 Prec@5 94.320 Loss 10.42486\n",
      "val Class Accuracy: [0.896,0.988,0.725,0.520,0.790,0.101,0.485,0.051,0.000,0.010]\n",
      "Best Prec@1: 48.160\n",
      "\n",
      "Epoch: [27][0/97], lr: 0.01000\tTime 0.344 (0.344)\tData 0.196 (0.196)\tLoss 2.9171 (2.9171)\tPrec@1 81.250 (81.250)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [27][10/97], lr: 0.01000\tTime 0.295 (0.304)\tData 0.000 (0.032)\tLoss 2.0788 (2.9317)\tPrec@1 87.500 (81.250)\tPrec@5 99.219 (98.509)\n",
      "Epoch: [27][20/97], lr: 0.01000\tTime 0.294 (0.301)\tData 0.000 (0.025)\tLoss 3.1136 (2.9454)\tPrec@1 78.125 (81.324)\tPrec@5 96.875 (98.661)\n",
      "Epoch: [27][30/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.023)\tLoss 2.2546 (2.9157)\tPrec@1 85.156 (81.452)\tPrec@5 99.219 (98.538)\n",
      "Epoch: [27][40/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 2.8032 (2.9450)\tPrec@1 84.375 (81.402)\tPrec@5 98.438 (98.380)\n",
      "Epoch: [27][50/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 3.6075 (3.0001)\tPrec@1 77.344 (80.959)\tPrec@5 98.438 (98.315)\n",
      "Epoch: [27][60/97], lr: 0.01000\tTime 0.298 (0.298)\tData 0.000 (0.020)\tLoss 3.0336 (2.9957)\tPrec@1 79.688 (80.904)\tPrec@5 98.438 (98.258)\n",
      "Epoch: [27][70/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.020)\tLoss 2.8625 (2.9609)\tPrec@1 79.688 (81.041)\tPrec@5 99.219 (98.283)\n",
      "Epoch: [27][80/97], lr: 0.01000\tTime 0.296 (0.298)\tData 0.000 (0.019)\tLoss 2.9591 (2.9290)\tPrec@1 81.250 (81.250)\tPrec@5 100.000 (98.399)\n",
      "Epoch: [27][90/97], lr: 0.01000\tTime 0.297 (0.298)\tData 0.000 (0.019)\tLoss 2.6767 (2.9135)\tPrec@1 85.938 (81.362)\tPrec@5 97.656 (98.429)\n",
      "Epoch: [27][96/97], lr: 0.01000\tTime 0.290 (0.298)\tData 0.000 (0.020)\tLoss 3.3790 (2.9248)\tPrec@1 76.271 (81.275)\tPrec@5 100.000 (98.452)\n",
      "Test: [0/100]\tTime 0.245 (0.245)\tLoss 10.2541 (10.2541)\tPrec@1 44.000 (44.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 6.9246 (9.2773)\tPrec@1 65.000 (51.000)\tPrec@5 97.000 (95.727)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 9.1453 (9.2976)\tPrec@1 49.000 (50.571)\tPrec@5 93.000 (95.286)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.1621 (9.3063)\tPrec@1 51.000 (50.355)\tPrec@5 92.000 (94.871)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.4991 (9.4128)\tPrec@1 50.000 (49.610)\tPrec@5 95.000 (94.878)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.2970 (9.3241)\tPrec@1 49.000 (49.804)\tPrec@5 94.000 (95.196)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 8.3518 (9.3144)\tPrec@1 54.000 (49.918)\tPrec@5 95.000 (95.279)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.8298 (9.2985)\tPrec@1 49.000 (49.944)\tPrec@5 97.000 (95.282)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.3787 (9.2800)\tPrec@1 56.000 (50.111)\tPrec@5 97.000 (95.395)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 9.2700 (9.2995)\tPrec@1 53.000 (50.088)\tPrec@5 94.000 (95.330)\n",
      "val Results: Prec@1 50.040 Prec@5 95.330 Loss 9.31285\n",
      "val Class Accuracy: [0.880,0.969,0.739,0.698,0.456,0.412,0.681,0.045,0.057,0.067]\n",
      "Best Prec@1: 50.040\n",
      "\n",
      "Epoch: [28][0/97], lr: 0.01000\tTime 0.356 (0.356)\tData 0.204 (0.204)\tLoss 2.6994 (2.6994)\tPrec@1 84.375 (84.375)\tPrec@5 96.875 (96.875)\n",
      "Epoch: [28][10/97], lr: 0.01000\tTime 0.294 (0.304)\tData 0.000 (0.034)\tLoss 2.3776 (2.9814)\tPrec@1 85.156 (81.463)\tPrec@5 100.000 (98.011)\n",
      "Epoch: [28][20/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.026)\tLoss 2.1956 (2.9080)\tPrec@1 86.719 (81.882)\tPrec@5 100.000 (98.438)\n",
      "Epoch: [28][30/97], lr: 0.01000\tTime 0.291 (0.300)\tData 0.000 (0.023)\tLoss 3.5789 (2.8935)\tPrec@1 78.125 (81.552)\tPrec@5 98.438 (98.513)\n",
      "Epoch: [28][40/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.022)\tLoss 2.7553 (2.8244)\tPrec@1 81.250 (81.993)\tPrec@5 98.438 (98.628)\n",
      "Epoch: [28][50/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 3.0172 (2.8884)\tPrec@1 80.469 (81.449)\tPrec@5 99.219 (98.591)\n",
      "Epoch: [28][60/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.020)\tLoss 2.3444 (2.9329)\tPrec@1 85.156 (81.007)\tPrec@5 99.219 (98.617)\n",
      "Epoch: [28][70/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.020)\tLoss 2.7186 (2.9458)\tPrec@1 82.031 (80.887)\tPrec@5 99.219 (98.636)\n",
      "Epoch: [28][80/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.019)\tLoss 3.4185 (2.9598)\tPrec@1 76.562 (80.903)\tPrec@5 98.438 (98.630)\n",
      "Epoch: [28][90/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.019)\tLoss 3.3338 (2.9652)\tPrec@1 78.125 (80.846)\tPrec@5 98.438 (98.635)\n",
      "Epoch: [28][96/97], lr: 0.01000\tTime 0.286 (0.298)\tData 0.000 (0.020)\tLoss 3.3885 (2.9584)\tPrec@1 79.661 (80.904)\tPrec@5 97.458 (98.630)\n",
      "Test: [0/100]\tTime 0.319 (0.319)\tLoss 12.6445 (12.6445)\tPrec@1 35.000 (35.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.073 (0.095)\tLoss 9.4439 (11.0585)\tPrec@1 49.000 (42.182)\tPrec@5 91.000 (92.818)\n",
      "Test: [20/100]\tTime 0.072 (0.084)\tLoss 9.5896 (11.1054)\tPrec@1 50.000 (41.286)\tPrec@5 95.000 (92.762)\n",
      "Test: [30/100]\tTime 0.072 (0.080)\tLoss 10.4615 (11.0332)\tPrec@1 45.000 (41.871)\tPrec@5 90.000 (92.710)\n",
      "Test: [40/100]\tTime 0.072 (0.078)\tLoss 11.9073 (11.0544)\tPrec@1 35.000 (42.146)\tPrec@5 87.000 (92.366)\n",
      "Test: [50/100]\tTime 0.072 (0.077)\tLoss 10.7421 (10.9803)\tPrec@1 46.000 (42.510)\tPrec@5 93.000 (92.667)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 10.0176 (10.9846)\tPrec@1 44.000 (42.213)\tPrec@5 93.000 (92.623)\n",
      "Test: [70/100]\tTime 0.072 (0.076)\tLoss 10.5244 (10.9519)\tPrec@1 44.000 (42.380)\tPrec@5 95.000 (92.620)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 10.0048 (10.9089)\tPrec@1 49.000 (42.667)\tPrec@5 91.000 (92.654)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.7267 (10.9480)\tPrec@1 44.000 (42.582)\tPrec@5 94.000 (92.516)\n",
      "val Results: Prec@1 42.560 Prec@5 92.450 Loss 10.97006\n",
      "val Class Accuracy: [0.911,0.997,0.762,0.669,0.382,0.234,0.198,0.103,0.000,0.000]\n",
      "Best Prec@1: 50.040\n",
      "\n",
      "Epoch: [29][0/97], lr: 0.01000\tTime 0.369 (0.369)\tData 0.231 (0.231)\tLoss 2.4787 (2.4787)\tPrec@1 84.375 (84.375)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [29][10/97], lr: 0.01000\tTime 0.295 (0.306)\tData 0.000 (0.036)\tLoss 3.2588 (2.9656)\tPrec@1 78.906 (80.469)\tPrec@5 96.875 (98.651)\n",
      "Epoch: [29][20/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.027)\tLoss 2.8832 (2.9144)\tPrec@1 84.375 (81.213)\tPrec@5 98.438 (98.735)\n",
      "Epoch: [29][30/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.024)\tLoss 2.4323 (2.8393)\tPrec@1 88.281 (82.056)\tPrec@5 98.438 (98.690)\n",
      "Epoch: [29][40/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.022)\tLoss 2.3660 (2.8111)\tPrec@1 85.938 (82.260)\tPrec@5 100.000 (98.704)\n",
      "Epoch: [29][50/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.021)\tLoss 2.8562 (2.8329)\tPrec@1 81.250 (82.001)\tPrec@5 100.000 (98.790)\n",
      "Epoch: [29][60/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 3.0753 (2.8511)\tPrec@1 82.031 (81.826)\tPrec@5 96.094 (98.719)\n",
      "Epoch: [29][70/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.020)\tLoss 3.3572 (2.9037)\tPrec@1 78.906 (81.525)\tPrec@5 98.438 (98.713)\n",
      "Epoch: [29][80/97], lr: 0.01000\tTime 0.298 (0.299)\tData 0.000 (0.020)\tLoss 2.8507 (2.8816)\tPrec@1 80.469 (81.607)\tPrec@5 99.219 (98.717)\n",
      "Epoch: [29][90/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 3.1161 (2.8800)\tPrec@1 79.688 (81.576)\tPrec@5 96.875 (98.661)\n",
      "Epoch: [29][96/97], lr: 0.01000\tTime 0.291 (0.298)\tData 0.000 (0.020)\tLoss 2.4240 (2.8668)\tPrec@1 84.746 (81.694)\tPrec@5 98.305 (98.638)\n",
      "Test: [0/100]\tTime 0.256 (0.256)\tLoss 11.3715 (11.3715)\tPrec@1 43.000 (43.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.072 (0.089)\tLoss 8.1102 (10.0663)\tPrec@1 58.000 (47.545)\tPrec@5 93.000 (94.818)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 9.5999 (10.0617)\tPrec@1 46.000 (47.476)\tPrec@5 96.000 (95.190)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 9.4315 (10.0456)\tPrec@1 49.000 (47.871)\tPrec@5 98.000 (95.129)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 10.1670 (10.0958)\tPrec@1 48.000 (47.829)\tPrec@5 98.000 (94.902)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.3279 (9.9795)\tPrec@1 49.000 (48.490)\tPrec@5 97.000 (95.098)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 8.5673 (9.9364)\tPrec@1 54.000 (48.475)\tPrec@5 97.000 (95.098)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 9.0599 (9.9452)\tPrec@1 53.000 (48.310)\tPrec@5 98.000 (95.127)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.2580 (9.9475)\tPrec@1 54.000 (48.198)\tPrec@5 94.000 (95.173)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 10.3637 (9.9870)\tPrec@1 44.000 (47.978)\tPrec@5 97.000 (95.077)\n",
      "val Results: Prec@1 47.920 Prec@5 95.100 Loss 10.01761\n",
      "val Class Accuracy: [0.952,0.918,0.601,0.769,0.778,0.165,0.500,0.083,0.011,0.015]\n",
      "Best Prec@1: 50.040\n",
      "\n",
      "Epoch: [30][0/97], lr: 0.01000\tTime 0.327 (0.327)\tData 0.188 (0.188)\tLoss 2.7196 (2.7196)\tPrec@1 81.250 (81.250)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [30][10/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.032)\tLoss 2.2652 (2.5472)\tPrec@1 89.844 (83.452)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [30][20/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.025)\tLoss 3.1349 (2.7708)\tPrec@1 82.812 (82.292)\tPrec@5 100.000 (98.512)\n",
      "Epoch: [30][30/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.022)\tLoss 3.0457 (2.8148)\tPrec@1 79.688 (82.031)\tPrec@5 99.219 (98.538)\n",
      "Epoch: [30][40/97], lr: 0.01000\tTime 0.299 (0.298)\tData 0.000 (0.021)\tLoss 2.7398 (2.8329)\tPrec@1 82.031 (81.860)\tPrec@5 99.219 (98.628)\n",
      "Epoch: [30][50/97], lr: 0.01000\tTime 0.298 (0.298)\tData 0.000 (0.020)\tLoss 2.6568 (2.8446)\tPrec@1 85.156 (81.664)\tPrec@5 99.219 (98.545)\n",
      "Epoch: [30][60/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.020)\tLoss 2.1505 (2.8107)\tPrec@1 86.719 (81.954)\tPrec@5 98.438 (98.604)\n",
      "Epoch: [30][70/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.020)\tLoss 2.6932 (2.8313)\tPrec@1 82.812 (81.943)\tPrec@5 97.656 (98.603)\n",
      "Epoch: [30][80/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.019)\tLoss 2.9554 (2.8426)\tPrec@1 78.906 (81.809)\tPrec@5 98.438 (98.582)\n",
      "Epoch: [30][90/97], lr: 0.01000\tTime 0.296 (0.298)\tData 0.000 (0.019)\tLoss 2.7085 (2.8274)\tPrec@1 82.812 (81.937)\tPrec@5 99.219 (98.609)\n",
      "Epoch: [30][96/97], lr: 0.01000\tTime 0.289 (0.298)\tData 0.000 (0.020)\tLoss 3.9069 (2.8522)\tPrec@1 76.271 (81.767)\tPrec@5 100.000 (98.581)\n",
      "Test: [0/100]\tTime 0.239 (0.239)\tLoss 11.2854 (11.2854)\tPrec@1 44.000 (44.000)\tPrec@5 86.000 (86.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 8.4357 (10.3265)\tPrec@1 56.000 (48.091)\tPrec@5 93.000 (90.000)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 9.6635 (10.3249)\tPrec@1 52.000 (47.762)\tPrec@5 91.000 (90.524)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.0659 (10.2826)\tPrec@1 54.000 (48.226)\tPrec@5 87.000 (90.129)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 11.0081 (10.3793)\tPrec@1 43.000 (48.146)\tPrec@5 84.000 (89.585)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.5668 (10.2926)\tPrec@1 59.000 (48.608)\tPrec@5 86.000 (89.765)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 9.2173 (10.3004)\tPrec@1 52.000 (48.230)\tPrec@5 91.000 (89.902)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.3058 (10.2851)\tPrec@1 53.000 (48.282)\tPrec@5 88.000 (89.873)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.9464 (10.2681)\tPrec@1 51.000 (48.358)\tPrec@5 81.000 (89.901)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 10.7255 (10.3020)\tPrec@1 44.000 (48.044)\tPrec@5 94.000 (89.879)\n",
      "val Results: Prec@1 48.010 Prec@5 89.840 Loss 10.34474\n",
      "val Class Accuracy: [0.912,0.993,0.619,0.729,0.800,0.236,0.478,0.001,0.031,0.002]\n",
      "Best Prec@1: 50.040\n",
      "\n",
      "Epoch: [31][0/97], lr: 0.01000\tTime 0.380 (0.380)\tData 0.237 (0.237)\tLoss 2.0653 (2.0653)\tPrec@1 86.719 (86.719)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [31][10/97], lr: 0.01000\tTime 0.297 (0.308)\tData 0.000 (0.037)\tLoss 2.7161 (2.7155)\tPrec@1 84.375 (83.026)\tPrec@5 99.219 (98.864)\n",
      "Epoch: [31][20/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.028)\tLoss 2.6275 (2.7430)\tPrec@1 84.375 (82.887)\tPrec@5 97.656 (98.586)\n",
      "Epoch: [31][30/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.024)\tLoss 3.3490 (2.7592)\tPrec@1 78.125 (82.560)\tPrec@5 97.656 (98.690)\n",
      "Epoch: [31][40/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.023)\tLoss 2.4897 (2.7548)\tPrec@1 84.375 (82.470)\tPrec@5 98.438 (98.742)\n",
      "Epoch: [31][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 2.2700 (2.7701)\tPrec@1 81.250 (82.399)\tPrec@5 100.000 (98.713)\n",
      "Epoch: [31][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 2.8987 (2.8335)\tPrec@1 83.594 (82.057)\tPrec@5 100.000 (98.630)\n",
      "Epoch: [31][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.8543 (2.8008)\tPrec@1 82.031 (82.405)\tPrec@5 96.875 (98.625)\n",
      "Epoch: [31][80/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 3.1993 (2.8266)\tPrec@1 80.469 (82.301)\tPrec@5 98.438 (98.592)\n",
      "Epoch: [31][90/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 3.8120 (2.8598)\tPrec@1 78.906 (82.005)\tPrec@5 97.656 (98.583)\n",
      "Epoch: [31][96/97], lr: 0.01000\tTime 0.289 (0.298)\tData 0.000 (0.020)\tLoss 2.9845 (2.8553)\tPrec@1 80.508 (82.041)\tPrec@5 98.305 (98.614)\n",
      "Test: [0/100]\tTime 0.241 (0.241)\tLoss 10.9192 (10.9192)\tPrec@1 45.000 (45.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.072 (0.088)\tLoss 7.8149 (10.1245)\tPrec@1 60.000 (47.545)\tPrec@5 97.000 (92.909)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 8.7695 (10.1527)\tPrec@1 53.000 (47.524)\tPrec@5 95.000 (93.286)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.3247 (10.1515)\tPrec@1 48.000 (47.548)\tPrec@5 98.000 (92.968)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 10.4449 (10.2693)\tPrec@1 45.000 (46.805)\tPrec@5 93.000 (92.951)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 10.0307 (10.1589)\tPrec@1 53.000 (47.627)\tPrec@5 92.000 (93.255)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 8.3292 (10.0862)\tPrec@1 55.000 (47.754)\tPrec@5 96.000 (93.344)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.6952 (10.0825)\tPrec@1 50.000 (47.803)\tPrec@5 96.000 (93.296)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 9.4525 (10.0395)\tPrec@1 52.000 (48.148)\tPrec@5 90.000 (93.481)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 10.4499 (10.0630)\tPrec@1 49.000 (48.033)\tPrec@5 93.000 (93.418)\n",
      "val Results: Prec@1 47.960 Prec@5 93.290 Loss 10.08778\n",
      "val Class Accuracy: [0.938,0.971,0.690,0.647,0.346,0.511,0.641,0.048,0.004,0.000]\n",
      "Best Prec@1: 50.040\n",
      "\n",
      "Epoch: [32][0/97], lr: 0.01000\tTime 0.341 (0.341)\tData 0.188 (0.188)\tLoss 3.2382 (3.2382)\tPrec@1 80.469 (80.469)\tPrec@5 96.875 (96.875)\n",
      "Epoch: [32][10/97], lr: 0.01000\tTime 0.295 (0.303)\tData 0.000 (0.032)\tLoss 3.3430 (2.7238)\tPrec@1 74.219 (82.670)\tPrec@5 99.219 (98.580)\n",
      "Epoch: [32][20/97], lr: 0.01000\tTime 0.301 (0.300)\tData 0.000 (0.025)\tLoss 1.9802 (2.7874)\tPrec@1 88.281 (82.366)\tPrec@5 99.219 (98.549)\n",
      "Epoch: [32][30/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.023)\tLoss 2.4268 (2.7230)\tPrec@1 85.156 (82.686)\tPrec@5 100.000 (98.790)\n",
      "Epoch: [32][40/97], lr: 0.01000\tTime 0.296 (0.298)\tData 0.000 (0.021)\tLoss 2.5293 (2.7678)\tPrec@1 85.156 (82.336)\tPrec@5 97.656 (98.723)\n",
      "Epoch: [32][50/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.020)\tLoss 2.8909 (2.8023)\tPrec@1 83.594 (82.184)\tPrec@5 100.000 (98.713)\n",
      "Epoch: [32][60/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.020)\tLoss 3.1147 (2.8030)\tPrec@1 82.031 (82.300)\tPrec@5 96.875 (98.642)\n",
      "Epoch: [32][70/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.020)\tLoss 2.4531 (2.7956)\tPrec@1 85.156 (82.273)\tPrec@5 100.000 (98.746)\n",
      "Epoch: [32][80/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.019)\tLoss 2.7604 (2.8073)\tPrec@1 82.812 (82.176)\tPrec@5 98.438 (98.688)\n",
      "Epoch: [32][90/97], lr: 0.01000\tTime 0.296 (0.298)\tData 0.000 (0.019)\tLoss 3.6298 (2.8132)\tPrec@1 78.125 (82.100)\tPrec@5 95.312 (98.661)\n",
      "Epoch: [32][96/97], lr: 0.01000\tTime 0.287 (0.297)\tData 0.000 (0.020)\tLoss 2.8540 (2.8072)\tPrec@1 83.898 (82.194)\tPrec@5 100.000 (98.654)\n",
      "Test: [0/100]\tTime 0.227 (0.227)\tLoss 12.1690 (12.1690)\tPrec@1 36.000 (36.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 8.6266 (10.6877)\tPrec@1 57.000 (43.818)\tPrec@5 96.000 (94.545)\n",
      "Test: [20/100]\tTime 0.072 (0.080)\tLoss 9.5295 (10.7419)\tPrec@1 48.000 (43.000)\tPrec@5 93.000 (94.238)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 10.2419 (10.6573)\tPrec@1 43.000 (43.806)\tPrec@5 96.000 (93.839)\n",
      "Test: [40/100]\tTime 0.073 (0.076)\tLoss 11.1871 (10.6943)\tPrec@1 37.000 (43.659)\tPrec@5 93.000 (93.927)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 10.3969 (10.5775)\tPrec@1 47.000 (44.216)\tPrec@5 94.000 (94.216)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 8.8588 (10.5504)\tPrec@1 52.000 (44.115)\tPrec@5 96.000 (94.393)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.8979 (10.5032)\tPrec@1 45.000 (44.394)\tPrec@5 94.000 (94.268)\n",
      "Test: [80/100]\tTime 0.073 (0.074)\tLoss 9.6570 (10.4654)\tPrec@1 50.000 (44.691)\tPrec@5 92.000 (94.296)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 10.4181 (10.5144)\tPrec@1 46.000 (44.560)\tPrec@5 93.000 (94.165)\n",
      "val Results: Prec@1 44.440 Prec@5 94.230 Loss 10.54333\n",
      "val Class Accuracy: [0.912,0.995,0.773,0.422,0.424,0.386,0.470,0.056,0.006,0.000]\n",
      "Best Prec@1: 50.040\n",
      "\n",
      "Epoch: [33][0/97], lr: 0.01000\tTime 0.384 (0.384)\tData 0.228 (0.228)\tLoss 3.2860 (3.2860)\tPrec@1 78.906 (78.906)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [33][10/97], lr: 0.01000\tTime 0.291 (0.306)\tData 0.000 (0.036)\tLoss 2.7482 (2.6557)\tPrec@1 82.812 (83.026)\tPrec@5 97.656 (98.864)\n",
      "Epoch: [33][20/97], lr: 0.01000\tTime 0.299 (0.302)\tData 0.000 (0.027)\tLoss 2.5018 (2.7119)\tPrec@1 85.156 (83.073)\tPrec@5 98.438 (98.735)\n",
      "Epoch: [33][30/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.024)\tLoss 2.1902 (2.6831)\tPrec@1 87.500 (83.518)\tPrec@5 100.000 (98.614)\n",
      "Epoch: [33][40/97], lr: 0.01000\tTime 0.292 (0.300)\tData 0.000 (0.022)\tLoss 1.9821 (2.7157)\tPrec@1 87.500 (83.155)\tPrec@5 99.219 (98.647)\n",
      "Epoch: [33][50/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 2.9246 (2.6680)\tPrec@1 82.812 (83.333)\tPrec@5 100.000 (98.698)\n",
      "Epoch: [33][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 3.2465 (2.6704)\tPrec@1 82.031 (83.184)\tPrec@5 100.000 (98.732)\n",
      "Epoch: [33][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 3.1340 (2.7035)\tPrec@1 78.906 (82.923)\tPrec@5 98.438 (98.702)\n",
      "Epoch: [33][80/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.020)\tLoss 1.9299 (2.7071)\tPrec@1 89.062 (82.861)\tPrec@5 98.438 (98.708)\n",
      "Epoch: [33][90/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.019)\tLoss 2.6472 (2.7156)\tPrec@1 84.375 (82.761)\tPrec@5 100.000 (98.755)\n",
      "Epoch: [33][96/97], lr: 0.01000\tTime 0.291 (0.298)\tData 0.000 (0.020)\tLoss 3.3368 (2.7345)\tPrec@1 77.119 (82.589)\tPrec@5 99.153 (98.791)\n",
      "Test: [0/100]\tTime 0.221 (0.221)\tLoss 11.9012 (11.9012)\tPrec@1 42.000 (42.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.072 (0.086)\tLoss 9.0918 (10.9218)\tPrec@1 53.000 (44.636)\tPrec@5 93.000 (94.182)\n",
      "Test: [20/100]\tTime 0.072 (0.079)\tLoss 9.6997 (10.7001)\tPrec@1 48.000 (45.000)\tPrec@5 98.000 (94.333)\n",
      "Test: [30/100]\tTime 0.072 (0.077)\tLoss 10.1158 (10.7147)\tPrec@1 44.000 (44.871)\tPrec@5 97.000 (93.968)\n",
      "Test: [40/100]\tTime 0.072 (0.076)\tLoss 10.6398 (10.7247)\tPrec@1 44.000 (44.976)\tPrec@5 94.000 (93.805)\n",
      "Test: [50/100]\tTime 0.073 (0.075)\tLoss 10.4164 (10.6155)\tPrec@1 48.000 (45.490)\tPrec@5 94.000 (94.137)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 9.8650 (10.6562)\tPrec@1 51.000 (45.230)\tPrec@5 95.000 (94.377)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.2277 (10.6176)\tPrec@1 55.000 (45.394)\tPrec@5 99.000 (94.366)\n",
      "Test: [80/100]\tTime 0.073 (0.074)\tLoss 10.0551 (10.5896)\tPrec@1 45.000 (45.568)\tPrec@5 89.000 (94.395)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 10.8574 (10.6403)\tPrec@1 49.000 (45.418)\tPrec@5 96.000 (94.297)\n",
      "val Results: Prec@1 45.400 Prec@5 94.290 Loss 10.65525\n",
      "val Class Accuracy: [0.839,0.977,0.709,0.886,0.537,0.219,0.109,0.068,0.193,0.003]\n",
      "Best Prec@1: 50.040\n",
      "\n",
      "Epoch: [34][0/97], lr: 0.01000\tTime 0.325 (0.325)\tData 0.187 (0.187)\tLoss 2.4126 (2.4126)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [34][10/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.032)\tLoss 2.6032 (2.5958)\tPrec@1 82.031 (82.955)\tPrec@5 99.219 (99.006)\n",
      "Epoch: [34][20/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.025)\tLoss 3.2195 (2.6080)\tPrec@1 79.688 (83.333)\tPrec@5 99.219 (99.070)\n",
      "Epoch: [34][30/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.023)\tLoss 2.7109 (2.6407)\tPrec@1 82.812 (83.266)\tPrec@5 96.875 (98.916)\n",
      "Epoch: [34][40/97], lr: 0.01000\tTime 0.299 (0.300)\tData 0.000 (0.021)\tLoss 3.1135 (2.6676)\tPrec@1 79.688 (83.194)\tPrec@5 97.656 (98.914)\n",
      "Epoch: [34][50/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 3.0821 (2.6795)\tPrec@1 80.469 (83.104)\tPrec@5 97.656 (98.897)\n",
      "Epoch: [34][60/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.020)\tLoss 3.1315 (2.7009)\tPrec@1 79.688 (83.017)\tPrec@5 99.219 (98.886)\n",
      "Epoch: [34][70/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 2.6719 (2.6837)\tPrec@1 83.594 (83.154)\tPrec@5 99.219 (98.944)\n",
      "Epoch: [34][80/97], lr: 0.01000\tTime 0.298 (0.299)\tData 0.000 (0.019)\tLoss 3.1679 (2.7235)\tPrec@1 79.688 (82.870)\tPrec@5 100.000 (98.958)\n",
      "Epoch: [34][90/97], lr: 0.01000\tTime 0.304 (0.299)\tData 0.000 (0.019)\tLoss 2.3852 (2.7324)\tPrec@1 86.719 (82.795)\tPrec@5 99.219 (98.935)\n",
      "Epoch: [34][96/97], lr: 0.01000\tTime 0.290 (0.299)\tData 0.000 (0.020)\tLoss 2.5219 (2.7214)\tPrec@1 83.051 (82.839)\tPrec@5 99.153 (98.936)\n",
      "Test: [0/100]\tTime 0.242 (0.242)\tLoss 10.7988 (10.7988)\tPrec@1 45.000 (45.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 7.5987 (9.8825)\tPrec@1 58.000 (48.818)\tPrec@5 97.000 (94.818)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 9.7293 (9.9606)\tPrec@1 45.000 (47.619)\tPrec@5 94.000 (94.333)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 8.8493 (9.9135)\tPrec@1 52.000 (48.258)\tPrec@5 94.000 (94.097)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.5297 (9.9842)\tPrec@1 43.000 (47.951)\tPrec@5 93.000 (93.707)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.5707 (9.9205)\tPrec@1 51.000 (48.333)\tPrec@5 91.000 (93.843)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 8.1114 (9.8810)\tPrec@1 57.000 (48.344)\tPrec@5 94.000 (93.672)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.1711 (9.8887)\tPrec@1 52.000 (48.225)\tPrec@5 96.000 (93.704)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 9.1823 (9.8765)\tPrec@1 52.000 (48.272)\tPrec@5 93.000 (93.815)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 9.7159 (9.9177)\tPrec@1 49.000 (48.110)\tPrec@5 96.000 (93.923)\n",
      "val Results: Prec@1 48.020 Prec@5 93.910 Loss 9.93778\n",
      "val Class Accuracy: [0.978,0.927,0.759,0.656,0.721,0.123,0.548,0.055,0.029,0.006]\n",
      "Best Prec@1: 50.040\n",
      "\n",
      "Epoch: [35][0/97], lr: 0.01000\tTime 0.372 (0.372)\tData 0.223 (0.223)\tLoss 1.7071 (1.7071)\tPrec@1 91.406 (91.406)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [35][10/97], lr: 0.01000\tTime 0.294 (0.305)\tData 0.000 (0.035)\tLoss 2.9698 (2.6009)\tPrec@1 82.812 (83.878)\tPrec@5 96.094 (98.580)\n",
      "Epoch: [35][20/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.027)\tLoss 2.6930 (2.8166)\tPrec@1 83.594 (82.068)\tPrec@5 99.219 (98.624)\n",
      "Epoch: [35][30/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.024)\tLoss 2.8536 (2.7913)\tPrec@1 83.594 (82.056)\tPrec@5 100.000 (98.790)\n",
      "Epoch: [35][40/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.022)\tLoss 2.6946 (2.8161)\tPrec@1 82.812 (82.031)\tPrec@5 98.438 (98.819)\n",
      "Epoch: [35][50/97], lr: 0.01000\tTime 0.292 (0.298)\tData 0.000 (0.021)\tLoss 1.8666 (2.8124)\tPrec@1 89.844 (81.955)\tPrec@5 100.000 (98.836)\n",
      "Epoch: [35][60/97], lr: 0.01000\tTime 0.296 (0.298)\tData 0.000 (0.021)\tLoss 2.3195 (2.7892)\tPrec@1 88.281 (82.134)\tPrec@5 98.438 (98.809)\n",
      "Epoch: [35][70/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.020)\tLoss 2.4703 (2.7842)\tPrec@1 84.375 (82.240)\tPrec@5 99.219 (98.812)\n",
      "Epoch: [35][80/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.020)\tLoss 1.9336 (2.7748)\tPrec@1 88.281 (82.243)\tPrec@5 99.219 (98.872)\n",
      "Epoch: [35][90/97], lr: 0.01000\tTime 0.293 (0.297)\tData 0.000 (0.020)\tLoss 2.7928 (2.7615)\tPrec@1 82.812 (82.375)\tPrec@5 100.000 (98.910)\n",
      "Epoch: [35][96/97], lr: 0.01000\tTime 0.288 (0.297)\tData 0.000 (0.020)\tLoss 2.6601 (2.7719)\tPrec@1 83.898 (82.283)\tPrec@5 100.000 (98.831)\n",
      "Test: [0/100]\tTime 0.251 (0.251)\tLoss 10.8645 (10.8645)\tPrec@1 42.000 (42.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.072 (0.089)\tLoss 7.4166 (9.8294)\tPrec@1 64.000 (48.727)\tPrec@5 95.000 (93.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 8.4784 (9.9203)\tPrec@1 56.000 (47.619)\tPrec@5 97.000 (93.286)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.3882 (9.8350)\tPrec@1 48.000 (48.452)\tPrec@5 94.000 (93.161)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.6937 (9.8934)\tPrec@1 43.000 (48.341)\tPrec@5 95.000 (93.366)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.2952 (9.7968)\tPrec@1 51.000 (48.961)\tPrec@5 97.000 (93.471)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.2928 (9.7810)\tPrec@1 51.000 (48.721)\tPrec@5 94.000 (93.459)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.5296 (9.7804)\tPrec@1 50.000 (48.873)\tPrec@5 92.000 (93.324)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.5734 (9.7228)\tPrec@1 61.000 (49.185)\tPrec@5 90.000 (93.531)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.6047 (9.7816)\tPrec@1 52.000 (48.901)\tPrec@5 96.000 (93.418)\n",
      "val Results: Prec@1 48.890 Prec@5 93.360 Loss 9.79330\n",
      "val Class Accuracy: [0.949,0.966,0.800,0.479,0.261,0.585,0.578,0.206,0.008,0.057]\n",
      "Best Prec@1: 50.040\n",
      "\n",
      "Epoch: [36][0/97], lr: 0.01000\tTime 0.368 (0.368)\tData 0.214 (0.214)\tLoss 2.0368 (2.0368)\tPrec@1 86.719 (86.719)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [36][10/97], lr: 0.01000\tTime 0.297 (0.306)\tData 0.000 (0.035)\tLoss 2.7533 (2.8292)\tPrec@1 80.469 (81.960)\tPrec@5 100.000 (98.722)\n",
      "Epoch: [36][20/97], lr: 0.01000\tTime 0.297 (0.302)\tData 0.000 (0.026)\tLoss 3.3701 (2.7784)\tPrec@1 77.344 (82.217)\tPrec@5 96.875 (98.772)\n",
      "Epoch: [36][30/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.024)\tLoss 2.9555 (2.7392)\tPrec@1 82.031 (82.661)\tPrec@5 100.000 (98.916)\n",
      "Epoch: [36][40/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.022)\tLoss 2.9100 (2.7444)\tPrec@1 82.812 (82.717)\tPrec@5 100.000 (98.952)\n",
      "Epoch: [36][50/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 2.7925 (2.7400)\tPrec@1 84.375 (82.782)\tPrec@5 100.000 (98.974)\n",
      "Epoch: [36][60/97], lr: 0.01000\tTime 0.302 (0.300)\tData 0.000 (0.020)\tLoss 3.7444 (2.7415)\tPrec@1 75.781 (82.697)\tPrec@5 99.219 (98.963)\n",
      "Epoch: [36][70/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 3.1111 (2.7613)\tPrec@1 81.250 (82.570)\tPrec@5 96.875 (98.922)\n",
      "Epoch: [36][80/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 2.7960 (2.7520)\tPrec@1 82.031 (82.533)\tPrec@5 98.438 (98.939)\n",
      "Epoch: [36][90/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.019)\tLoss 2.1158 (2.7487)\tPrec@1 85.156 (82.546)\tPrec@5 100.000 (98.901)\n",
      "Epoch: [36][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 2.5372 (2.7429)\tPrec@1 86.441 (82.565)\tPrec@5 99.153 (98.888)\n",
      "Test: [0/100]\tTime 0.239 (0.239)\tLoss 9.7375 (9.7375)\tPrec@1 51.000 (51.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.072 (0.088)\tLoss 7.3063 (9.1365)\tPrec@1 63.000 (54.545)\tPrec@5 95.000 (96.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 8.0477 (9.0341)\tPrec@1 60.000 (55.524)\tPrec@5 97.000 (96.524)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 7.9904 (9.0547)\tPrec@1 59.000 (54.871)\tPrec@5 95.000 (96.032)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.3680 (9.1294)\tPrec@1 55.000 (54.854)\tPrec@5 97.000 (95.976)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.9224 (9.0365)\tPrec@1 56.000 (55.235)\tPrec@5 99.000 (96.216)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 7.3891 (9.0107)\tPrec@1 65.000 (55.066)\tPrec@5 99.000 (96.246)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.4284 (9.0010)\tPrec@1 56.000 (55.028)\tPrec@5 98.000 (96.282)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.6343 (8.9661)\tPrec@1 56.000 (55.235)\tPrec@5 94.000 (96.259)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 9.5244 (9.0012)\tPrec@1 52.000 (55.000)\tPrec@5 98.000 (96.231)\n",
      "val Results: Prec@1 55.000 Prec@5 96.280 Loss 9.01039\n",
      "val Class Accuracy: [0.894,0.992,0.747,0.811,0.608,0.364,0.490,0.366,0.219,0.009]\n",
      "Best Prec@1: 55.000\n",
      "\n",
      "Epoch: [37][0/97], lr: 0.01000\tTime 0.369 (0.369)\tData 0.212 (0.212)\tLoss 1.8076 (1.8076)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [37][10/97], lr: 0.01000\tTime 0.294 (0.305)\tData 0.000 (0.034)\tLoss 2.4734 (2.4970)\tPrec@1 85.938 (84.943)\tPrec@5 100.000 (98.722)\n",
      "Epoch: [37][20/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.026)\tLoss 2.7582 (2.5705)\tPrec@1 80.469 (83.929)\tPrec@5 99.219 (98.921)\n",
      "Epoch: [37][30/97], lr: 0.01000\tTime 0.294 (0.300)\tData 0.000 (0.023)\tLoss 2.6600 (2.6140)\tPrec@1 85.156 (83.695)\tPrec@5 100.000 (98.967)\n",
      "Epoch: [37][40/97], lr: 0.01000\tTime 0.293 (0.299)\tData 0.000 (0.022)\tLoss 2.3238 (2.6233)\tPrec@1 86.719 (83.537)\tPrec@5 98.438 (98.876)\n",
      "Epoch: [37][50/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.021)\tLoss 3.3675 (2.6436)\tPrec@1 78.906 (83.379)\tPrec@5 100.000 (98.836)\n",
      "Epoch: [37][60/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.020)\tLoss 2.4277 (2.5950)\tPrec@1 84.375 (83.824)\tPrec@5 100.000 (98.911)\n",
      "Epoch: [37][70/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.020)\tLoss 2.6604 (2.5492)\tPrec@1 83.594 (84.199)\tPrec@5 98.438 (99.010)\n",
      "Epoch: [37][80/97], lr: 0.01000\tTime 0.294 (0.297)\tData 0.000 (0.020)\tLoss 2.5181 (2.5636)\tPrec@1 82.031 (84.134)\tPrec@5 98.438 (98.939)\n",
      "Epoch: [37][90/97], lr: 0.01000\tTime 0.293 (0.297)\tData 0.000 (0.019)\tLoss 1.8814 (2.5498)\tPrec@1 88.281 (84.195)\tPrec@5 100.000 (98.875)\n",
      "Epoch: [37][96/97], lr: 0.01000\tTime 0.287 (0.297)\tData 0.000 (0.020)\tLoss 3.5047 (2.5734)\tPrec@1 77.119 (84.000)\tPrec@5 99.153 (98.888)\n",
      "Test: [0/100]\tTime 0.245 (0.245)\tLoss 10.8779 (10.8779)\tPrec@1 48.000 (48.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 6.3727 (9.6429)\tPrec@1 69.000 (52.273)\tPrec@5 93.000 (92.636)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 10.4126 (9.6368)\tPrec@1 41.000 (51.333)\tPrec@5 98.000 (93.476)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.8557 (9.6649)\tPrec@1 49.000 (51.161)\tPrec@5 90.000 (92.935)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.8962 (9.7367)\tPrec@1 51.000 (50.927)\tPrec@5 88.000 (92.659)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.3055 (9.6696)\tPrec@1 55.000 (51.333)\tPrec@5 92.000 (92.706)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.3748 (9.6438)\tPrec@1 60.000 (51.377)\tPrec@5 92.000 (92.656)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.0002 (9.6056)\tPrec@1 59.000 (51.718)\tPrec@5 91.000 (92.620)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.6164 (9.5991)\tPrec@1 54.000 (51.741)\tPrec@5 89.000 (92.716)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.4243 (9.6588)\tPrec@1 46.000 (51.374)\tPrec@5 93.000 (92.681)\n",
      "val Results: Prec@1 51.280 Prec@5 92.710 Loss 9.68578\n",
      "val Class Accuracy: [0.932,0.972,0.648,0.494,0.691,0.404,0.848,0.026,0.079,0.034]\n",
      "Best Prec@1: 55.000\n",
      "\n",
      "Epoch: [38][0/97], lr: 0.01000\tTime 0.378 (0.378)\tData 0.216 (0.216)\tLoss 2.6901 (2.6901)\tPrec@1 82.812 (82.812)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [38][10/97], lr: 0.01000\tTime 0.298 (0.307)\tData 0.000 (0.035)\tLoss 2.5785 (2.7964)\tPrec@1 83.594 (82.528)\tPrec@5 99.219 (98.651)\n",
      "Epoch: [38][20/97], lr: 0.01000\tTime 0.298 (0.303)\tData 0.000 (0.026)\tLoss 2.5666 (2.8108)\tPrec@1 82.031 (82.254)\tPrec@5 99.219 (98.698)\n",
      "Epoch: [38][30/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.023)\tLoss 2.7435 (2.6858)\tPrec@1 84.375 (83.090)\tPrec@5 99.219 (98.740)\n",
      "Epoch: [38][40/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.022)\tLoss 2.3600 (2.7268)\tPrec@1 86.719 (82.793)\tPrec@5 100.000 (98.685)\n",
      "Epoch: [38][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 3.6769 (2.7377)\tPrec@1 75.781 (82.705)\tPrec@5 96.875 (98.729)\n",
      "Epoch: [38][60/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.020)\tLoss 2.8732 (2.7160)\tPrec@1 78.906 (82.889)\tPrec@5 100.000 (98.809)\n",
      "Epoch: [38][70/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 2.9177 (2.7027)\tPrec@1 82.031 (82.978)\tPrec@5 97.656 (98.735)\n",
      "Epoch: [38][80/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.020)\tLoss 2.7780 (2.6984)\tPrec@1 83.594 (83.015)\tPrec@5 98.438 (98.775)\n",
      "Epoch: [38][90/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.019)\tLoss 2.6866 (2.6852)\tPrec@1 84.375 (83.104)\tPrec@5 100.000 (98.772)\n",
      "Epoch: [38][96/97], lr: 0.01000\tTime 0.291 (0.300)\tData 0.000 (0.020)\tLoss 3.3794 (2.6911)\tPrec@1 78.814 (83.040)\tPrec@5 97.458 (98.791)\n",
      "Test: [0/100]\tTime 0.244 (0.244)\tLoss 10.5988 (10.5988)\tPrec@1 47.000 (47.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 7.6180 (9.6684)\tPrec@1 62.000 (48.727)\tPrec@5 95.000 (93.636)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 8.2682 (9.5262)\tPrec@1 54.000 (49.476)\tPrec@5 93.000 (93.952)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.4773 (9.6079)\tPrec@1 50.000 (49.129)\tPrec@5 91.000 (93.645)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.9479 (9.7166)\tPrec@1 48.000 (48.634)\tPrec@5 96.000 (93.659)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 9.2747 (9.6349)\tPrec@1 53.000 (48.922)\tPrec@5 95.000 (93.745)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 7.9465 (9.5737)\tPrec@1 60.000 (49.230)\tPrec@5 94.000 (94.016)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.8945 (9.5651)\tPrec@1 52.000 (49.239)\tPrec@5 98.000 (94.070)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 9.5891 (9.5493)\tPrec@1 49.000 (49.358)\tPrec@5 92.000 (94.049)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 9.5986 (9.6101)\tPrec@1 49.000 (48.868)\tPrec@5 95.000 (93.901)\n",
      "val Results: Prec@1 48.980 Prec@5 93.940 Loss 9.60434\n",
      "val Class Accuracy: [0.809,0.901,0.515,0.336,0.392,0.941,0.582,0.191,0.097,0.134]\n",
      "Best Prec@1: 55.000\n",
      "\n",
      "Epoch: [39][0/97], lr: 0.01000\tTime 0.359 (0.359)\tData 0.206 (0.206)\tLoss 2.3705 (2.3705)\tPrec@1 84.375 (84.375)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [39][10/97], lr: 0.01000\tTime 0.297 (0.306)\tData 0.000 (0.034)\tLoss 2.9325 (2.5292)\tPrec@1 82.031 (84.446)\tPrec@5 98.438 (98.651)\n",
      "Epoch: [39][20/97], lr: 0.01000\tTime 0.294 (0.302)\tData 0.000 (0.026)\tLoss 2.9122 (2.6151)\tPrec@1 82.812 (83.743)\tPrec@5 98.438 (98.624)\n",
      "Epoch: [39][30/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.023)\tLoss 2.1304 (2.5592)\tPrec@1 86.719 (83.972)\tPrec@5 100.000 (98.740)\n",
      "Epoch: [39][40/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.022)\tLoss 2.7748 (2.6006)\tPrec@1 82.031 (83.975)\tPrec@5 100.000 (98.647)\n",
      "Epoch: [39][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 2.7155 (2.5913)\tPrec@1 82.031 (84.053)\tPrec@5 99.219 (98.667)\n",
      "Epoch: [39][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 2.4729 (2.6233)\tPrec@1 85.938 (83.722)\tPrec@5 98.438 (98.655)\n",
      "Epoch: [39][70/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 2.3197 (2.6403)\tPrec@1 86.719 (83.517)\tPrec@5 98.438 (98.636)\n",
      "Epoch: [39][80/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.020)\tLoss 2.5008 (2.6116)\tPrec@1 82.031 (83.738)\tPrec@5 98.438 (98.659)\n",
      "Epoch: [39][90/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.019)\tLoss 2.3986 (2.5916)\tPrec@1 85.156 (83.920)\tPrec@5 98.438 (98.695)\n",
      "Epoch: [39][96/97], lr: 0.01000\tTime 0.288 (0.299)\tData 0.000 (0.020)\tLoss 3.0387 (2.6017)\tPrec@1 80.508 (83.798)\tPrec@5 99.153 (98.726)\n",
      "Test: [0/100]\tTime 0.253 (0.253)\tLoss 10.4298 (10.4298)\tPrec@1 47.000 (47.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.0371 (9.0578)\tPrec@1 66.000 (54.636)\tPrec@5 94.000 (93.909)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 8.2195 (8.8628)\tPrec@1 58.000 (54.905)\tPrec@5 97.000 (94.810)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 8.3940 (8.8828)\tPrec@1 57.000 (55.387)\tPrec@5 94.000 (94.484)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.0454 (8.9365)\tPrec@1 51.000 (55.024)\tPrec@5 94.000 (94.268)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 8.7198 (8.8310)\tPrec@1 53.000 (55.412)\tPrec@5 94.000 (94.490)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 6.9831 (8.8063)\tPrec@1 66.000 (55.492)\tPrec@5 98.000 (94.623)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 8.4181 (8.7843)\tPrec@1 57.000 (55.521)\tPrec@5 96.000 (94.592)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 8.6427 (8.7561)\tPrec@1 55.000 (55.667)\tPrec@5 90.000 (94.568)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.6090 (8.7842)\tPrec@1 56.000 (55.473)\tPrec@5 98.000 (94.560)\n",
      "val Results: Prec@1 55.490 Prec@5 94.530 Loss 8.79737\n",
      "val Class Accuracy: [0.907,0.956,0.679,0.736,0.732,0.631,0.441,0.077,0.349,0.041]\n",
      "Best Prec@1: 55.490\n",
      "\n",
      "Epoch: [40][0/97], lr: 0.01000\tTime 0.367 (0.367)\tData 0.212 (0.212)\tLoss 2.6494 (2.6494)\tPrec@1 84.375 (84.375)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [40][10/97], lr: 0.01000\tTime 0.294 (0.306)\tData 0.000 (0.035)\tLoss 3.1280 (2.6138)\tPrec@1 81.250 (84.730)\tPrec@5 97.656 (98.864)\n",
      "Epoch: [40][20/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.026)\tLoss 2.2563 (2.5862)\tPrec@1 85.938 (84.189)\tPrec@5 100.000 (98.810)\n",
      "Epoch: [40][30/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.023)\tLoss 2.0639 (2.5468)\tPrec@1 85.938 (84.350)\tPrec@5 98.438 (98.866)\n",
      "Epoch: [40][40/97], lr: 0.01000\tTime 0.291 (0.300)\tData 0.000 (0.022)\tLoss 2.2643 (2.5236)\tPrec@1 85.156 (84.413)\tPrec@5 98.438 (98.895)\n",
      "Epoch: [40][50/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 3.0021 (2.5710)\tPrec@1 82.031 (84.222)\tPrec@5 99.219 (98.759)\n",
      "Epoch: [40][60/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.9428 (2.5782)\tPrec@1 83.594 (84.311)\tPrec@5 97.656 (98.745)\n",
      "Epoch: [40][70/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.020)\tLoss 2.2738 (2.6004)\tPrec@1 86.719 (84.078)\tPrec@5 97.656 (98.779)\n",
      "Epoch: [40][80/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.020)\tLoss 1.5304 (2.5831)\tPrec@1 92.188 (84.134)\tPrec@5 100.000 (98.756)\n",
      "Epoch: [40][90/97], lr: 0.01000\tTime 0.293 (0.298)\tData 0.000 (0.019)\tLoss 2.8620 (2.5938)\tPrec@1 81.250 (83.946)\tPrec@5 98.438 (98.755)\n",
      "Epoch: [40][96/97], lr: 0.01000\tTime 0.288 (0.298)\tData 0.000 (0.020)\tLoss 2.7229 (2.6073)\tPrec@1 80.508 (83.822)\tPrec@5 99.153 (98.775)\n",
      "Test: [0/100]\tTime 0.231 (0.231)\tLoss 10.1571 (10.1571)\tPrec@1 52.000 (52.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 6.6786 (9.2494)\tPrec@1 63.000 (52.182)\tPrec@5 93.000 (93.273)\n",
      "Test: [20/100]\tTime 0.072 (0.080)\tLoss 7.9745 (9.2139)\tPrec@1 59.000 (52.905)\tPrec@5 95.000 (93.381)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 8.8128 (9.2623)\tPrec@1 52.000 (52.452)\tPrec@5 89.000 (92.613)\n",
      "Test: [40/100]\tTime 0.072 (0.076)\tLoss 9.4139 (9.3161)\tPrec@1 50.000 (52.195)\tPrec@5 90.000 (92.463)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.2467 (9.1952)\tPrec@1 62.000 (53.020)\tPrec@5 91.000 (92.451)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 7.9043 (9.1900)\tPrec@1 60.000 (53.066)\tPrec@5 94.000 (92.328)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.5410 (9.1438)\tPrec@1 58.000 (53.310)\tPrec@5 93.000 (92.352)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.8710 (9.1295)\tPrec@1 54.000 (53.346)\tPrec@5 90.000 (92.407)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 9.0593 (9.1556)\tPrec@1 55.000 (53.330)\tPrec@5 95.000 (92.516)\n",
      "val Results: Prec@1 53.360 Prec@5 92.500 Loss 9.16748\n",
      "val Class Accuracy: [0.939,0.986,0.616,0.715,0.429,0.644,0.669,0.069,0.244,0.025]\n",
      "Best Prec@1: 55.490\n",
      "\n",
      "Epoch: [41][0/97], lr: 0.01000\tTime 0.352 (0.352)\tData 0.214 (0.214)\tLoss 2.3444 (2.3444)\tPrec@1 85.938 (85.938)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [41][10/97], lr: 0.01000\tTime 0.296 (0.305)\tData 0.000 (0.035)\tLoss 2.7872 (2.5939)\tPrec@1 82.812 (84.304)\tPrec@5 100.000 (99.006)\n",
      "Epoch: [41][20/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.026)\tLoss 1.5901 (2.5335)\tPrec@1 90.625 (84.152)\tPrec@5 99.219 (98.772)\n",
      "Epoch: [41][30/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.023)\tLoss 2.8837 (2.4537)\tPrec@1 79.688 (84.476)\tPrec@5 99.219 (98.866)\n",
      "Epoch: [41][40/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.022)\tLoss 2.4089 (2.4384)\tPrec@1 85.156 (84.661)\tPrec@5 100.000 (98.933)\n",
      "Epoch: [41][50/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 3.5590 (2.4836)\tPrec@1 79.688 (84.406)\tPrec@5 97.656 (98.928)\n",
      "Epoch: [41][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 2.3768 (2.5083)\tPrec@1 83.594 (84.221)\tPrec@5 97.656 (98.988)\n",
      "Epoch: [41][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.1895 (2.5576)\tPrec@1 87.500 (83.946)\tPrec@5 99.219 (98.933)\n",
      "Epoch: [41][80/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.020)\tLoss 2.0774 (2.5783)\tPrec@1 88.281 (83.960)\tPrec@5 100.000 (98.929)\n",
      "Epoch: [41][90/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.019)\tLoss 2.3330 (2.5758)\tPrec@1 85.938 (83.971)\tPrec@5 97.656 (98.867)\n",
      "Epoch: [41][96/97], lr: 0.01000\tTime 0.288 (0.298)\tData 0.000 (0.020)\tLoss 2.7290 (2.5608)\tPrec@1 83.051 (84.080)\tPrec@5 99.153 (98.872)\n",
      "Test: [0/100]\tTime 0.243 (0.243)\tLoss 12.0888 (12.0888)\tPrec@1 40.000 (40.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.072 (0.088)\tLoss 8.2508 (10.0692)\tPrec@1 58.000 (49.091)\tPrec@5 96.000 (93.727)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 8.5347 (10.0668)\tPrec@1 55.000 (48.429)\tPrec@5 97.000 (94.000)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 9.6234 (9.9846)\tPrec@1 47.000 (49.258)\tPrec@5 92.000 (93.548)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 10.5818 (10.0534)\tPrec@1 48.000 (49.171)\tPrec@5 93.000 (93.659)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 9.2007 (9.9388)\tPrec@1 55.000 (49.961)\tPrec@5 92.000 (93.647)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 8.4200 (9.9189)\tPrec@1 57.000 (49.984)\tPrec@5 95.000 (93.656)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 9.7211 (9.8908)\tPrec@1 48.000 (50.028)\tPrec@5 97.000 (93.789)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.0125 (9.8571)\tPrec@1 56.000 (50.198)\tPrec@5 91.000 (93.963)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 9.4011 (9.9096)\tPrec@1 54.000 (50.022)\tPrec@5 96.000 (93.945)\n",
      "val Results: Prec@1 50.070 Prec@5 93.850 Loss 9.93050\n",
      "val Class Accuracy: [0.966,0.991,0.796,0.572,0.408,0.657,0.312,0.271,0.025,0.009]\n",
      "Best Prec@1: 55.490\n",
      "\n",
      "Epoch: [42][0/97], lr: 0.01000\tTime 0.351 (0.351)\tData 0.200 (0.200)\tLoss 2.5988 (2.5988)\tPrec@1 85.156 (85.156)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [42][10/97], lr: 0.01000\tTime 0.297 (0.306)\tData 0.000 (0.034)\tLoss 3.1557 (2.5335)\tPrec@1 81.250 (84.375)\tPrec@5 95.312 (98.580)\n",
      "Epoch: [42][20/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.026)\tLoss 2.6682 (2.4688)\tPrec@1 84.375 (84.635)\tPrec@5 99.219 (98.810)\n",
      "Epoch: [42][30/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.023)\tLoss 2.4408 (2.4549)\tPrec@1 84.375 (84.904)\tPrec@5 97.656 (98.841)\n",
      "Epoch: [42][40/97], lr: 0.01000\tTime 0.286 (0.301)\tData 0.000 (0.022)\tLoss 2.9282 (2.4684)\tPrec@1 83.594 (84.851)\tPrec@5 99.219 (98.838)\n",
      "Epoch: [42][50/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 2.2292 (2.4690)\tPrec@1 85.156 (84.988)\tPrec@5 100.000 (98.790)\n",
      "Epoch: [42][60/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.020)\tLoss 2.0673 (2.5215)\tPrec@1 87.500 (84.772)\tPrec@5 99.219 (98.732)\n",
      "Epoch: [42][70/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 2.7718 (2.5735)\tPrec@1 82.031 (84.463)\tPrec@5 100.000 (98.669)\n",
      "Epoch: [42][80/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 2.3415 (2.5648)\tPrec@1 85.156 (84.404)\tPrec@5 98.438 (98.698)\n",
      "Epoch: [42][90/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.019)\tLoss 2.5520 (2.5592)\tPrec@1 84.375 (84.366)\tPrec@5 100.000 (98.781)\n",
      "Epoch: [42][96/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.8583 (2.5477)\tPrec@1 89.831 (84.419)\tPrec@5 100.000 (98.791)\n",
      "Test: [0/100]\tTime 0.266 (0.266)\tLoss 12.6861 (12.6861)\tPrec@1 38.000 (38.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.072 (0.090)\tLoss 9.8453 (11.6719)\tPrec@1 49.000 (40.909)\tPrec@5 93.000 (92.909)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 10.2292 (11.6295)\tPrec@1 46.000 (41.190)\tPrec@5 96.000 (93.048)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 11.2120 (11.5179)\tPrec@1 42.000 (41.645)\tPrec@5 91.000 (92.839)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 12.2127 (11.5655)\tPrec@1 39.000 (41.683)\tPrec@5 93.000 (92.610)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 11.4762 (11.4954)\tPrec@1 42.000 (42.196)\tPrec@5 90.000 (92.529)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 10.2337 (11.5257)\tPrec@1 47.000 (41.885)\tPrec@5 97.000 (92.492)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 10.8028 (11.4963)\tPrec@1 45.000 (42.028)\tPrec@5 94.000 (92.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 10.4684 (11.4505)\tPrec@1 52.000 (42.333)\tPrec@5 88.000 (92.506)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.6759 (11.4931)\tPrec@1 46.000 (42.165)\tPrec@5 96.000 (92.538)\n",
      "val Results: Prec@1 42.130 Prec@5 92.540 Loss 11.50343\n",
      "val Class Accuracy: [0.908,0.994,0.907,0.644,0.355,0.103,0.062,0.095,0.141,0.004]\n",
      "Best Prec@1: 55.490\n",
      "\n",
      "Epoch: [43][0/97], lr: 0.01000\tTime 0.371 (0.371)\tData 0.212 (0.212)\tLoss 1.5834 (1.5834)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [43][10/97], lr: 0.01000\tTime 0.295 (0.307)\tData 0.000 (0.034)\tLoss 3.0622 (2.7850)\tPrec@1 80.469 (82.244)\tPrec@5 99.219 (98.438)\n",
      "Epoch: [43][20/97], lr: 0.01000\tTime 0.297 (0.302)\tData 0.000 (0.026)\tLoss 2.7256 (2.7352)\tPrec@1 82.812 (82.515)\tPrec@5 98.438 (98.661)\n",
      "Epoch: [43][30/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.023)\tLoss 2.9583 (2.6774)\tPrec@1 81.250 (83.014)\tPrec@5 99.219 (98.841)\n",
      "Epoch: [43][40/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.022)\tLoss 3.2824 (2.6535)\tPrec@1 80.469 (83.327)\tPrec@5 97.656 (98.819)\n",
      "Epoch: [43][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 2.4476 (2.6180)\tPrec@1 85.156 (83.594)\tPrec@5 100.000 (98.897)\n",
      "Epoch: [43][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 2.1198 (2.5939)\tPrec@1 86.719 (83.722)\tPrec@5 99.219 (98.911)\n",
      "Epoch: [43][70/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 3.6604 (2.6060)\tPrec@1 75.000 (83.627)\tPrec@5 98.438 (98.889)\n",
      "Epoch: [43][80/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 2.4958 (2.5762)\tPrec@1 85.156 (83.825)\tPrec@5 100.000 (98.900)\n",
      "Epoch: [43][90/97], lr: 0.01000\tTime 0.302 (0.299)\tData 0.000 (0.019)\tLoss 2.0091 (2.5564)\tPrec@1 87.500 (84.014)\tPrec@5 100.000 (98.893)\n",
      "Epoch: [43][96/97], lr: 0.01000\tTime 0.288 (0.298)\tData 0.000 (0.020)\tLoss 2.0637 (2.5477)\tPrec@1 86.441 (84.072)\tPrec@5 98.305 (98.904)\n",
      "Test: [0/100]\tTime 0.257 (0.257)\tLoss 10.6475 (10.6475)\tPrec@1 45.000 (45.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.072 (0.089)\tLoss 7.6281 (9.3789)\tPrec@1 61.000 (52.818)\tPrec@5 95.000 (95.182)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 8.4410 (9.3520)\tPrec@1 54.000 (52.095)\tPrec@5 95.000 (95.810)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 8.3525 (9.3142)\tPrec@1 57.000 (51.903)\tPrec@5 96.000 (95.613)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 9.6899 (9.3510)\tPrec@1 49.000 (52.049)\tPrec@5 93.000 (95.293)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 8.9745 (9.2633)\tPrec@1 55.000 (52.686)\tPrec@5 95.000 (95.451)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 7.5204 (9.2385)\tPrec@1 61.000 (52.836)\tPrec@5 95.000 (95.492)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 8.9069 (9.2444)\tPrec@1 58.000 (52.859)\tPrec@5 95.000 (95.451)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.0721 (9.2216)\tPrec@1 54.000 (52.914)\tPrec@5 91.000 (95.420)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 8.6315 (9.2882)\tPrec@1 56.000 (52.516)\tPrec@5 96.000 (95.341)\n",
      "val Results: Prec@1 52.450 Prec@5 95.280 Loss 9.30952\n",
      "val Class Accuracy: [0.935,0.851,0.858,0.627,0.723,0.451,0.398,0.221,0.138,0.043]\n",
      "Best Prec@1: 55.490\n",
      "\n",
      "Epoch: [44][0/97], lr: 0.01000\tTime 0.406 (0.406)\tData 0.249 (0.249)\tLoss 2.5163 (2.5163)\tPrec@1 82.812 (82.812)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [44][10/97], lr: 0.01000\tTime 0.293 (0.310)\tData 0.000 (0.037)\tLoss 2.4523 (2.5150)\tPrec@1 85.938 (84.517)\tPrec@5 99.219 (98.935)\n",
      "Epoch: [44][20/97], lr: 0.01000\tTime 0.294 (0.304)\tData 0.000 (0.028)\tLoss 2.0431 (2.4825)\tPrec@1 87.500 (84.710)\tPrec@5 100.000 (99.144)\n",
      "Epoch: [44][30/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.024)\tLoss 3.4993 (2.5825)\tPrec@1 81.250 (84.022)\tPrec@5 100.000 (98.992)\n",
      "Epoch: [44][40/97], lr: 0.01000\tTime 0.287 (0.301)\tData 0.000 (0.023)\tLoss 2.1474 (2.5749)\tPrec@1 87.500 (83.994)\tPrec@5 99.219 (98.971)\n",
      "Epoch: [44][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.022)\tLoss 3.0468 (2.5793)\tPrec@1 82.812 (83.931)\tPrec@5 96.875 (98.897)\n",
      "Epoch: [44][60/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 2.9608 (2.5598)\tPrec@1 83.594 (84.106)\tPrec@5 100.000 (98.988)\n",
      "Epoch: [44][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.1674 (2.5479)\tPrec@1 87.500 (84.144)\tPrec@5 97.656 (99.010)\n",
      "Epoch: [44][80/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.2991 (2.5640)\tPrec@1 85.156 (83.912)\tPrec@5 98.438 (98.929)\n",
      "Epoch: [44][90/97], lr: 0.01000\tTime 0.293 (0.298)\tData 0.000 (0.020)\tLoss 3.2591 (2.6091)\tPrec@1 77.344 (83.620)\tPrec@5 97.656 (98.901)\n",
      "Epoch: [44][96/97], lr: 0.01000\tTime 0.288 (0.298)\tData 0.000 (0.020)\tLoss 2.6740 (2.6151)\tPrec@1 83.051 (83.597)\tPrec@5 98.305 (98.872)\n",
      "Test: [0/100]\tTime 0.270 (0.270)\tLoss 9.7345 (9.7345)\tPrec@1 48.000 (48.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.072 (0.090)\tLoss 7.1282 (8.8815)\tPrec@1 59.000 (54.273)\tPrec@5 94.000 (95.000)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 8.4170 (8.7971)\tPrec@1 51.000 (54.810)\tPrec@5 97.000 (95.429)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.7894 (8.7597)\tPrec@1 62.000 (55.226)\tPrec@5 92.000 (95.226)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.7813 (8.8109)\tPrec@1 62.000 (54.927)\tPrec@5 95.000 (95.171)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.3062 (8.7616)\tPrec@1 60.000 (55.059)\tPrec@5 95.000 (95.216)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.5623 (8.7260)\tPrec@1 60.000 (55.164)\tPrec@5 97.000 (95.262)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.5326 (8.6978)\tPrec@1 63.000 (55.282)\tPrec@5 97.000 (95.127)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.4641 (8.6898)\tPrec@1 61.000 (55.259)\tPrec@5 91.000 (95.247)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.3733 (8.7280)\tPrec@1 61.000 (55.165)\tPrec@5 99.000 (95.187)\n",
      "val Results: Prec@1 55.160 Prec@5 95.100 Loss 8.75187\n",
      "val Class Accuracy: [0.923,0.977,0.706,0.597,0.598,0.060,0.752,0.658,0.231,0.014]\n",
      "Best Prec@1: 55.490\n",
      "\n",
      "Epoch: [45][0/97], lr: 0.01000\tTime 0.347 (0.347)\tData 0.210 (0.210)\tLoss 2.4915 (2.4915)\tPrec@1 82.031 (82.031)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [45][10/97], lr: 0.01000\tTime 0.298 (0.305)\tData 0.000 (0.035)\tLoss 3.0851 (2.4602)\tPrec@1 81.250 (84.588)\tPrec@5 99.219 (98.580)\n",
      "Epoch: [45][20/97], lr: 0.01000\tTime 0.297 (0.302)\tData 0.000 (0.026)\tLoss 2.2635 (2.4755)\tPrec@1 84.375 (84.747)\tPrec@5 100.000 (98.884)\n",
      "Epoch: [45][30/97], lr: 0.01000\tTime 0.298 (0.302)\tData 0.000 (0.024)\tLoss 2.3560 (2.4771)\tPrec@1 82.812 (84.602)\tPrec@5 96.875 (98.765)\n",
      "Epoch: [45][40/97], lr: 0.01000\tTime 0.302 (0.302)\tData 0.000 (0.022)\tLoss 1.7696 (2.5035)\tPrec@1 89.844 (84.451)\tPrec@5 99.219 (98.876)\n",
      "Epoch: [45][50/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.021)\tLoss 2.3615 (2.5251)\tPrec@1 83.594 (84.023)\tPrec@5 99.219 (98.882)\n",
      "Epoch: [45][60/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.021)\tLoss 2.9025 (2.5148)\tPrec@1 84.375 (84.144)\tPrec@5 97.656 (98.886)\n",
      "Epoch: [45][70/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.020)\tLoss 2.3868 (2.5203)\tPrec@1 84.375 (84.133)\tPrec@5 99.219 (98.900)\n",
      "Epoch: [45][80/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.020)\tLoss 3.3196 (2.5289)\tPrec@1 78.906 (84.086)\tPrec@5 98.438 (98.872)\n",
      "Epoch: [45][90/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.020)\tLoss 2.7568 (2.5485)\tPrec@1 82.812 (84.006)\tPrec@5 99.219 (98.807)\n",
      "Epoch: [45][96/97], lr: 0.01000\tTime 0.290 (0.301)\tData 0.000 (0.020)\tLoss 3.0530 (2.5501)\tPrec@1 77.966 (84.008)\tPrec@5 100.000 (98.863)\n",
      "Test: [0/100]\tTime 0.252 (0.252)\tLoss 11.0408 (11.0408)\tPrec@1 49.000 (49.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 8.6127 (10.5208)\tPrec@1 58.000 (49.091)\tPrec@5 92.000 (91.636)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 9.5420 (10.5119)\tPrec@1 53.000 (48.810)\tPrec@5 94.000 (92.190)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 10.1777 (10.6113)\tPrec@1 48.000 (48.032)\tPrec@5 93.000 (91.806)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 11.2719 (10.6875)\tPrec@1 43.000 (47.707)\tPrec@5 89.000 (91.659)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 10.8702 (10.5910)\tPrec@1 47.000 (48.078)\tPrec@5 92.000 (91.922)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.8220 (10.5529)\tPrec@1 56.000 (48.082)\tPrec@5 94.000 (91.869)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 9.8952 (10.5202)\tPrec@1 49.000 (48.085)\tPrec@5 96.000 (92.028)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.7418 (10.4823)\tPrec@1 53.000 (48.259)\tPrec@5 91.000 (92.025)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.3358 (10.4948)\tPrec@1 54.000 (48.275)\tPrec@5 94.000 (91.989)\n",
      "val Results: Prec@1 48.230 Prec@5 91.890 Loss 10.51662\n",
      "val Class Accuracy: [0.970,0.966,0.694,0.850,0.421,0.132,0.535,0.229,0.025,0.001]\n",
      "Best Prec@1: 55.490\n",
      "\n",
      "Epoch: [46][0/97], lr: 0.01000\tTime 0.345 (0.345)\tData 0.200 (0.200)\tLoss 2.5992 (2.5992)\tPrec@1 82.812 (82.812)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [46][10/97], lr: 0.01000\tTime 0.297 (0.303)\tData 0.000 (0.033)\tLoss 3.1893 (2.5177)\tPrec@1 80.469 (84.659)\tPrec@5 97.656 (99.077)\n",
      "Epoch: [46][20/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.026)\tLoss 2.4812 (2.4117)\tPrec@1 82.812 (85.082)\tPrec@5 100.000 (99.144)\n",
      "Epoch: [46][30/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.023)\tLoss 2.1314 (2.4398)\tPrec@1 87.500 (84.829)\tPrec@5 98.438 (99.017)\n",
      "Epoch: [46][40/97], lr: 0.01000\tTime 0.301 (0.299)\tData 0.000 (0.022)\tLoss 1.8341 (2.4602)\tPrec@1 89.844 (84.699)\tPrec@5 100.000 (99.028)\n",
      "Epoch: [46][50/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 2.2238 (2.4850)\tPrec@1 87.500 (84.743)\tPrec@5 99.219 (99.035)\n",
      "Epoch: [46][60/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 2.5828 (2.5010)\tPrec@1 83.594 (84.541)\tPrec@5 99.219 (99.052)\n",
      "Epoch: [46][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.7748 (2.5008)\tPrec@1 84.375 (84.463)\tPrec@5 96.094 (98.944)\n",
      "Epoch: [46][80/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.019)\tLoss 2.9354 (2.4866)\tPrec@1 79.688 (84.500)\tPrec@5 99.219 (98.939)\n",
      "Epoch: [46][90/97], lr: 0.01000\tTime 0.293 (0.298)\tData 0.000 (0.019)\tLoss 2.0775 (2.5067)\tPrec@1 85.938 (84.332)\tPrec@5 99.219 (98.935)\n",
      "Epoch: [46][96/97], lr: 0.01000\tTime 0.292 (0.298)\tData 0.000 (0.020)\tLoss 2.6434 (2.4885)\tPrec@1 84.746 (84.524)\tPrec@5 99.153 (98.936)\n",
      "Test: [0/100]\tTime 0.279 (0.279)\tLoss 8.8786 (8.8786)\tPrec@1 55.000 (55.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 6.3455 (8.0519)\tPrec@1 68.000 (59.909)\tPrec@5 94.000 (95.545)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 7.3102 (7.9872)\tPrec@1 63.000 (60.619)\tPrec@5 96.000 (95.476)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.4150 (8.0352)\tPrec@1 63.000 (60.419)\tPrec@5 93.000 (95.548)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 8.1339 (8.1081)\tPrec@1 59.000 (59.854)\tPrec@5 95.000 (95.561)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.6773 (8.0136)\tPrec@1 71.000 (60.353)\tPrec@5 96.000 (95.824)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 7.2687 (8.0227)\tPrec@1 63.000 (60.279)\tPrec@5 95.000 (95.656)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 7.4814 (7.9913)\tPrec@1 67.000 (60.563)\tPrec@5 98.000 (95.704)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.6823 (7.9776)\tPrec@1 61.000 (60.531)\tPrec@5 96.000 (95.716)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.8094 (8.0177)\tPrec@1 61.000 (60.352)\tPrec@5 98.000 (95.758)\n",
      "val Results: Prec@1 60.150 Prec@5 95.760 Loss 8.05012\n",
      "val Class Accuracy: [0.951,0.996,0.587,0.607,0.683,0.620,0.726,0.421,0.345,0.079]\n",
      "Best Prec@1: 60.150\n",
      "\n",
      "Epoch: [47][0/97], lr: 0.01000\tTime 0.382 (0.382)\tData 0.222 (0.222)\tLoss 1.6518 (1.6518)\tPrec@1 88.281 (88.281)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [47][10/97], lr: 0.01000\tTime 0.295 (0.307)\tData 0.000 (0.035)\tLoss 1.8679 (2.3201)\tPrec@1 88.281 (85.156)\tPrec@5 98.438 (98.651)\n",
      "Epoch: [47][20/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.027)\tLoss 2.5511 (2.3431)\tPrec@1 84.375 (85.082)\tPrec@5 99.219 (98.921)\n",
      "Epoch: [47][30/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.024)\tLoss 2.4116 (2.3937)\tPrec@1 87.500 (85.081)\tPrec@5 100.000 (98.942)\n",
      "Epoch: [47][40/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.022)\tLoss 2.7860 (2.3801)\tPrec@1 82.812 (85.290)\tPrec@5 99.219 (98.914)\n",
      "Epoch: [47][50/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 2.4725 (2.3773)\tPrec@1 83.594 (85.110)\tPrec@5 99.219 (98.958)\n",
      "Epoch: [47][60/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.8664 (2.3719)\tPrec@1 90.625 (85.259)\tPrec@5 100.000 (98.988)\n",
      "Epoch: [47][70/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 3.0930 (2.4409)\tPrec@1 78.906 (84.848)\tPrec@5 100.000 (98.944)\n",
      "Epoch: [47][80/97], lr: 0.01000\tTime 0.296 (0.298)\tData 0.000 (0.020)\tLoss 2.3421 (2.4594)\tPrec@1 85.156 (84.674)\tPrec@5 99.219 (98.929)\n",
      "Epoch: [47][90/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.019)\tLoss 2.0267 (2.4657)\tPrec@1 88.281 (84.718)\tPrec@5 99.219 (98.918)\n",
      "Epoch: [47][96/97], lr: 0.01000\tTime 0.288 (0.298)\tData 0.000 (0.020)\tLoss 1.7549 (2.4606)\tPrec@1 89.831 (84.733)\tPrec@5 100.000 (98.944)\n",
      "Test: [0/100]\tTime 0.248 (0.248)\tLoss 10.4934 (10.4934)\tPrec@1 47.000 (47.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.6075 (9.1881)\tPrec@1 62.000 (54.545)\tPrec@5 95.000 (95.818)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 7.7703 (9.1187)\tPrec@1 59.000 (53.810)\tPrec@5 96.000 (95.429)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 8.6572 (9.1765)\tPrec@1 55.000 (53.226)\tPrec@5 94.000 (95.194)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.2965 (9.2316)\tPrec@1 57.000 (53.049)\tPrec@5 97.000 (95.073)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.1766 (9.1341)\tPrec@1 59.000 (53.667)\tPrec@5 94.000 (95.275)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 7.7855 (9.1306)\tPrec@1 56.000 (53.475)\tPrec@5 97.000 (95.230)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.8863 (9.0873)\tPrec@1 56.000 (53.620)\tPrec@5 96.000 (95.437)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 8.4195 (9.0521)\tPrec@1 60.000 (53.827)\tPrec@5 93.000 (95.531)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.5612 (9.0904)\tPrec@1 53.000 (53.604)\tPrec@5 98.000 (95.593)\n",
      "val Results: Prec@1 53.530 Prec@5 95.590 Loss 9.11233\n",
      "val Class Accuracy: [0.957,0.986,0.806,0.362,0.467,0.666,0.510,0.293,0.286,0.020]\n",
      "Best Prec@1: 60.150\n",
      "\n",
      "Epoch: [48][0/97], lr: 0.01000\tTime 0.343 (0.343)\tData 0.189 (0.189)\tLoss 1.5976 (1.5976)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [48][10/97], lr: 0.01000\tTime 0.297 (0.305)\tData 0.000 (0.032)\tLoss 1.9845 (2.3553)\tPrec@1 88.281 (86.222)\tPrec@5 99.219 (98.935)\n",
      "Epoch: [48][20/97], lr: 0.01000\tTime 0.294 (0.301)\tData 0.000 (0.025)\tLoss 2.4959 (2.5022)\tPrec@1 82.812 (85.007)\tPrec@5 98.438 (98.772)\n",
      "Epoch: [48][30/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.023)\tLoss 1.4547 (2.4320)\tPrec@1 89.844 (85.358)\tPrec@5 99.219 (98.891)\n",
      "Epoch: [48][40/97], lr: 0.01000\tTime 0.292 (0.299)\tData 0.000 (0.021)\tLoss 2.0972 (2.4851)\tPrec@1 87.500 (84.794)\tPrec@5 99.219 (98.857)\n",
      "Epoch: [48][50/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.8457 (2.4787)\tPrec@1 89.844 (84.789)\tPrec@5 99.219 (98.851)\n",
      "Epoch: [48][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 2.8965 (2.4958)\tPrec@1 83.594 (84.644)\tPrec@5 97.656 (98.860)\n",
      "Epoch: [48][70/97], lr: 0.01000\tTime 0.299 (0.298)\tData 0.000 (0.020)\tLoss 2.2205 (2.5092)\tPrec@1 84.375 (84.518)\tPrec@5 100.000 (98.911)\n",
      "Epoch: [48][80/97], lr: 0.01000\tTime 0.298 (0.300)\tData 0.000 (0.019)\tLoss 2.6054 (2.4910)\tPrec@1 85.938 (84.713)\tPrec@5 98.438 (98.910)\n",
      "Epoch: [48][90/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.019)\tLoss 3.3607 (2.4992)\tPrec@1 79.688 (84.598)\tPrec@5 98.438 (98.918)\n",
      "Epoch: [48][96/97], lr: 0.01000\tTime 0.288 (0.299)\tData 0.000 (0.020)\tLoss 1.7559 (2.4909)\tPrec@1 87.288 (84.604)\tPrec@5 100.000 (98.952)\n",
      "Test: [0/100]\tTime 0.282 (0.282)\tLoss 10.4230 (10.4230)\tPrec@1 47.000 (47.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.072 (0.091)\tLoss 7.1018 (9.6106)\tPrec@1 66.000 (51.455)\tPrec@5 94.000 (93.364)\n",
      "Test: [20/100]\tTime 0.072 (0.082)\tLoss 8.3198 (9.5849)\tPrec@1 59.000 (51.095)\tPrec@5 94.000 (92.905)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 9.2098 (9.5516)\tPrec@1 50.000 (50.903)\tPrec@5 90.000 (92.419)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 10.2671 (9.6175)\tPrec@1 48.000 (50.683)\tPrec@5 90.000 (92.146)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 9.4257 (9.5492)\tPrec@1 55.000 (51.137)\tPrec@5 90.000 (92.176)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 8.0349 (9.5510)\tPrec@1 58.000 (51.049)\tPrec@5 94.000 (92.082)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.0052 (9.4901)\tPrec@1 52.000 (51.380)\tPrec@5 92.000 (92.141)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.9805 (9.4586)\tPrec@1 53.000 (51.420)\tPrec@5 91.000 (92.235)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.5614 (9.4773)\tPrec@1 60.000 (51.516)\tPrec@5 93.000 (92.242)\n",
      "val Results: Prec@1 51.520 Prec@5 92.230 Loss 9.47758\n",
      "val Class Accuracy: [0.950,0.993,0.843,0.563,0.300,0.455,0.611,0.117,0.311,0.009]\n",
      "Best Prec@1: 60.150\n",
      "\n",
      "Epoch: [49][0/97], lr: 0.01000\tTime 0.397 (0.397)\tData 0.251 (0.251)\tLoss 2.0230 (2.0230)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [49][10/97], lr: 0.01000\tTime 0.297 (0.310)\tData 0.000 (0.038)\tLoss 2.5455 (2.1860)\tPrec@1 83.594 (87.003)\tPrec@5 98.438 (99.148)\n",
      "Epoch: [49][20/97], lr: 0.01000\tTime 0.296 (0.305)\tData 0.000 (0.028)\tLoss 1.9087 (2.2649)\tPrec@1 89.844 (86.384)\tPrec@5 99.219 (99.107)\n",
      "Epoch: [49][30/97], lr: 0.01000\tTime 0.299 (0.303)\tData 0.000 (0.025)\tLoss 2.2747 (2.3491)\tPrec@1 85.156 (85.534)\tPrec@5 97.656 (98.916)\n",
      "Epoch: [49][40/97], lr: 0.01000\tTime 0.298 (0.302)\tData 0.000 (0.023)\tLoss 2.3065 (2.3931)\tPrec@1 87.500 (85.080)\tPrec@5 99.219 (98.971)\n",
      "Epoch: [49][50/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.022)\tLoss 1.9892 (2.3664)\tPrec@1 87.500 (85.218)\tPrec@5 99.219 (99.050)\n",
      "Epoch: [49][60/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.021)\tLoss 2.4299 (2.3724)\tPrec@1 85.156 (85.207)\tPrec@5 99.219 (99.001)\n",
      "Epoch: [49][70/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 2.9802 (2.4409)\tPrec@1 82.031 (84.837)\tPrec@5 100.000 (98.977)\n",
      "Epoch: [49][80/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.020)\tLoss 2.3629 (2.4527)\tPrec@1 86.719 (84.790)\tPrec@5 99.219 (99.016)\n",
      "Epoch: [49][90/97], lr: 0.01000\tTime 0.304 (0.301)\tData 0.000 (0.020)\tLoss 2.4744 (2.4387)\tPrec@1 86.719 (84.847)\tPrec@5 99.219 (99.021)\n",
      "Epoch: [49][96/97], lr: 0.01000\tTime 0.289 (0.300)\tData 0.000 (0.020)\tLoss 2.8691 (2.4219)\tPrec@1 80.508 (84.894)\tPrec@5 98.305 (99.041)\n",
      "Test: [0/100]\tTime 0.265 (0.265)\tLoss 11.4285 (11.4285)\tPrec@1 43.000 (43.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 8.7571 (10.7040)\tPrec@1 59.000 (48.000)\tPrec@5 90.000 (90.636)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 8.8035 (10.5631)\tPrec@1 52.000 (48.476)\tPrec@5 94.000 (91.524)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 9.8859 (10.5340)\tPrec@1 51.000 (48.387)\tPrec@5 91.000 (90.871)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.9656 (10.5847)\tPrec@1 48.000 (48.390)\tPrec@5 90.000 (90.659)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 10.2505 (10.5453)\tPrec@1 49.000 (48.490)\tPrec@5 91.000 (91.039)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.6633 (10.5413)\tPrec@1 56.000 (48.377)\tPrec@5 94.000 (91.213)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 9.8914 (10.5283)\tPrec@1 51.000 (48.324)\tPrec@5 93.000 (91.254)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.8014 (10.4797)\tPrec@1 53.000 (48.531)\tPrec@5 87.000 (91.420)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.5291 (10.5150)\tPrec@1 51.000 (48.407)\tPrec@5 92.000 (91.396)\n",
      "val Results: Prec@1 48.340 Prec@5 91.310 Loss 10.53064\n",
      "val Class Accuracy: [0.984,0.939,0.743,0.602,0.640,0.242,0.316,0.198,0.170,0.000]\n",
      "Best Prec@1: 60.150\n",
      "\n",
      "Epoch: [50][0/97], lr: 0.01000\tTime 0.345 (0.345)\tData 0.192 (0.192)\tLoss 2.3983 (2.3983)\tPrec@1 83.594 (83.594)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [50][10/97], lr: 0.01000\tTime 0.297 (0.304)\tData 0.000 (0.033)\tLoss 2.8501 (2.3427)\tPrec@1 82.812 (85.369)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [50][20/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.025)\tLoss 2.6547 (2.3867)\tPrec@1 83.594 (85.268)\tPrec@5 100.000 (99.182)\n",
      "Epoch: [50][30/97], lr: 0.01000\tTime 0.299 (0.300)\tData 0.000 (0.023)\tLoss 2.5710 (2.3748)\tPrec@1 85.938 (85.282)\tPrec@5 98.438 (99.068)\n",
      "Epoch: [50][40/97], lr: 0.01000\tTime 0.300 (0.300)\tData 0.000 (0.021)\tLoss 2.2773 (2.3986)\tPrec@1 87.500 (85.213)\tPrec@5 98.438 (99.009)\n",
      "Epoch: [50][50/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 1.7978 (2.3632)\tPrec@1 89.062 (85.432)\tPrec@5 99.219 (98.989)\n",
      "Epoch: [50][60/97], lr: 0.01000\tTime 0.301 (0.299)\tData 0.000 (0.020)\tLoss 2.7707 (2.4169)\tPrec@1 80.469 (85.041)\tPrec@5 99.219 (98.924)\n",
      "Epoch: [50][70/97], lr: 0.01000\tTime 0.293 (0.299)\tData 0.000 (0.020)\tLoss 1.7525 (2.3838)\tPrec@1 87.500 (85.266)\tPrec@5 97.656 (98.977)\n",
      "Epoch: [50][80/97], lr: 0.01000\tTime 0.304 (0.299)\tData 0.000 (0.019)\tLoss 2.3299 (2.3975)\tPrec@1 83.594 (85.204)\tPrec@5 100.000 (99.007)\n",
      "Epoch: [50][90/97], lr: 0.01000\tTime 0.302 (0.299)\tData 0.000 (0.019)\tLoss 2.5857 (2.4130)\tPrec@1 85.156 (85.070)\tPrec@5 96.875 (98.918)\n",
      "Epoch: [50][96/97], lr: 0.01000\tTime 0.291 (0.299)\tData 0.000 (0.020)\tLoss 2.6742 (2.4229)\tPrec@1 82.203 (84.927)\tPrec@5 99.153 (98.928)\n",
      "Test: [0/100]\tTime 0.243 (0.243)\tLoss 10.5038 (10.5038)\tPrec@1 54.000 (54.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 7.6394 (9.1619)\tPrec@1 63.000 (56.545)\tPrec@5 93.000 (96.000)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.1977 (9.1223)\tPrec@1 63.000 (55.952)\tPrec@5 96.000 (95.524)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.0745 (9.1189)\tPrec@1 55.000 (56.065)\tPrec@5 93.000 (95.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.1359 (9.1853)\tPrec@1 58.000 (55.902)\tPrec@5 96.000 (95.390)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.4996 (9.0776)\tPrec@1 68.000 (56.333)\tPrec@5 96.000 (95.588)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 8.1013 (9.0650)\tPrec@1 57.000 (56.344)\tPrec@5 94.000 (95.656)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.8357 (9.0237)\tPrec@1 56.000 (56.479)\tPrec@5 96.000 (95.676)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.6363 (8.9892)\tPrec@1 54.000 (56.531)\tPrec@5 96.000 (95.778)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 8.8581 (9.0461)\tPrec@1 59.000 (56.198)\tPrec@5 95.000 (95.692)\n",
      "val Results: Prec@1 56.030 Prec@5 95.660 Loss 9.07485\n",
      "val Class Accuracy: [0.912,0.983,0.635,0.452,0.598,0.877,0.567,0.317,0.180,0.082]\n",
      "Best Prec@1: 60.150\n",
      "\n",
      "Epoch: [51][0/97], lr: 0.01000\tTime 0.362 (0.362)\tData 0.206 (0.206)\tLoss 2.5912 (2.5912)\tPrec@1 83.594 (83.594)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [51][10/97], lr: 0.01000\tTime 0.296 (0.306)\tData 0.000 (0.034)\tLoss 2.3115 (2.2993)\tPrec@1 82.031 (85.582)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [51][20/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.026)\tLoss 1.9196 (2.3305)\tPrec@1 87.500 (85.268)\tPrec@5 98.438 (99.107)\n",
      "Epoch: [51][30/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.023)\tLoss 2.7324 (2.3437)\tPrec@1 82.812 (85.282)\tPrec@5 98.438 (99.118)\n",
      "Epoch: [51][40/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.022)\tLoss 2.5014 (2.3584)\tPrec@1 85.938 (85.175)\tPrec@5 99.219 (99.047)\n",
      "Epoch: [51][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 2.4647 (2.3800)\tPrec@1 85.156 (85.095)\tPrec@5 100.000 (99.020)\n",
      "Epoch: [51][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 2.2266 (2.3777)\tPrec@1 89.062 (85.233)\tPrec@5 100.000 (99.039)\n",
      "Epoch: [51][70/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 2.1477 (2.3751)\tPrec@1 87.500 (85.277)\tPrec@5 100.000 (99.076)\n",
      "Epoch: [51][80/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 3.2387 (2.3643)\tPrec@1 76.562 (85.340)\tPrec@5 98.438 (99.093)\n",
      "Epoch: [51][90/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.019)\tLoss 1.7097 (2.3632)\tPrec@1 89.062 (85.337)\tPrec@5 100.000 (99.064)\n",
      "Epoch: [51][96/97], lr: 0.01000\tTime 0.288 (0.298)\tData 0.000 (0.020)\tLoss 3.0013 (2.3657)\tPrec@1 81.356 (85.330)\tPrec@5 99.153 (99.089)\n",
      "Test: [0/100]\tTime 0.241 (0.241)\tLoss 9.7414 (9.7414)\tPrec@1 54.000 (54.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 7.1141 (9.0009)\tPrec@1 61.000 (55.273)\tPrec@5 94.000 (94.000)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.6490 (9.0400)\tPrec@1 61.000 (54.238)\tPrec@5 94.000 (93.714)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 8.0036 (8.9932)\tPrec@1 60.000 (55.000)\tPrec@5 96.000 (93.774)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 9.0168 (9.0031)\tPrec@1 55.000 (54.951)\tPrec@5 91.000 (93.561)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 8.0907 (8.9105)\tPrec@1 65.000 (55.529)\tPrec@5 94.000 (93.667)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 7.2307 (8.9412)\tPrec@1 65.000 (55.246)\tPrec@5 95.000 (93.672)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 8.7962 (8.8980)\tPrec@1 58.000 (55.451)\tPrec@5 95.000 (93.803)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.3497 (8.8733)\tPrec@1 59.000 (55.543)\tPrec@5 94.000 (93.963)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 8.2322 (8.9100)\tPrec@1 58.000 (55.341)\tPrec@5 94.000 (93.956)\n",
      "val Results: Prec@1 55.300 Prec@5 93.960 Loss 8.92305\n",
      "val Class Accuracy: [0.966,0.979,0.742,0.532,0.650,0.488,0.435,0.282,0.449,0.007]\n",
      "Best Prec@1: 60.150\n",
      "\n",
      "Epoch: [52][0/97], lr: 0.01000\tTime 0.342 (0.342)\tData 0.197 (0.197)\tLoss 2.2027 (2.2027)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [52][10/97], lr: 0.01000\tTime 0.296 (0.303)\tData 0.000 (0.033)\tLoss 2.2904 (2.2836)\tPrec@1 85.156 (85.440)\tPrec@5 100.000 (99.077)\n",
      "Epoch: [52][20/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.026)\tLoss 2.8845 (2.3093)\tPrec@1 81.250 (85.417)\tPrec@5 99.219 (99.182)\n",
      "Epoch: [52][30/97], lr: 0.01000\tTime 0.300 (0.300)\tData 0.000 (0.023)\tLoss 2.5248 (2.3441)\tPrec@1 85.156 (85.307)\tPrec@5 99.219 (99.093)\n",
      "Epoch: [52][40/97], lr: 0.01000\tTime 0.298 (0.299)\tData 0.000 (0.022)\tLoss 2.1974 (2.3591)\tPrec@1 86.719 (85.156)\tPrec@5 100.000 (99.047)\n",
      "Epoch: [52][50/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 2.3517 (2.3515)\tPrec@1 85.938 (85.202)\tPrec@5 99.219 (99.050)\n",
      "Epoch: [52][60/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.4586 (2.3361)\tPrec@1 83.594 (85.259)\tPrec@5 100.000 (99.142)\n",
      "Epoch: [52][70/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 2.4884 (2.3451)\tPrec@1 85.156 (85.244)\tPrec@5 99.219 (99.142)\n",
      "Epoch: [52][80/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.019)\tLoss 2.3160 (2.3841)\tPrec@1 84.375 (85.021)\tPrec@5 99.219 (99.093)\n",
      "Epoch: [52][90/97], lr: 0.01000\tTime 0.308 (0.299)\tData 0.000 (0.019)\tLoss 2.2627 (2.3912)\tPrec@1 85.156 (84.993)\tPrec@5 99.219 (99.107)\n",
      "Epoch: [52][96/97], lr: 0.01000\tTime 0.288 (0.298)\tData 0.000 (0.020)\tLoss 2.6539 (2.3891)\tPrec@1 83.898 (84.999)\tPrec@5 99.153 (99.121)\n",
      "Test: [0/100]\tTime 0.267 (0.267)\tLoss 9.1313 (9.1313)\tPrec@1 57.000 (57.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.0977 (8.2795)\tPrec@1 68.000 (59.909)\tPrec@5 94.000 (95.909)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.2242 (8.3033)\tPrec@1 65.000 (59.429)\tPrec@5 98.000 (95.905)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.7318 (8.3024)\tPrec@1 59.000 (59.226)\tPrec@5 94.000 (95.613)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.5421 (8.3500)\tPrec@1 61.000 (59.439)\tPrec@5 96.000 (95.732)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.6476 (8.2888)\tPrec@1 66.000 (59.706)\tPrec@5 98.000 (95.863)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.2090 (8.3172)\tPrec@1 62.000 (59.393)\tPrec@5 97.000 (95.934)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.8698 (8.2690)\tPrec@1 61.000 (59.634)\tPrec@5 94.000 (95.944)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 8.5935 (8.2593)\tPrec@1 54.000 (59.630)\tPrec@5 97.000 (96.086)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.3989 (8.2917)\tPrec@1 60.000 (59.505)\tPrec@5 93.000 (96.011)\n",
      "val Results: Prec@1 59.520 Prec@5 95.950 Loss 8.30625\n",
      "val Class Accuracy: [0.938,0.986,0.754,0.814,0.658,0.445,0.621,0.438,0.290,0.008]\n",
      "Best Prec@1: 60.150\n",
      "\n",
      "Epoch: [53][0/97], lr: 0.01000\tTime 0.411 (0.411)\tData 0.247 (0.247)\tLoss 2.0467 (2.0467)\tPrec@1 87.500 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [53][10/97], lr: 0.01000\tTime 0.295 (0.308)\tData 0.000 (0.037)\tLoss 2.3874 (2.3705)\tPrec@1 86.719 (85.653)\tPrec@5 100.000 (98.935)\n",
      "Epoch: [53][20/97], lr: 0.01000\tTime 0.294 (0.302)\tData 0.000 (0.028)\tLoss 1.7070 (2.2953)\tPrec@1 89.062 (86.235)\tPrec@5 98.438 (99.033)\n",
      "Epoch: [53][30/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.024)\tLoss 2.8483 (2.3355)\tPrec@1 81.250 (85.887)\tPrec@5 98.438 (99.093)\n",
      "Epoch: [53][40/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.022)\tLoss 2.6023 (2.3058)\tPrec@1 82.812 (85.823)\tPrec@5 99.219 (99.066)\n",
      "Epoch: [53][50/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.021)\tLoss 2.0054 (2.3425)\tPrec@1 85.156 (85.616)\tPrec@5 99.219 (99.081)\n",
      "Epoch: [53][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 1.7194 (2.3730)\tPrec@1 89.844 (85.451)\tPrec@5 98.438 (98.988)\n",
      "Epoch: [53][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 3.3036 (2.3907)\tPrec@1 79.688 (85.332)\tPrec@5 99.219 (98.966)\n",
      "Epoch: [53][80/97], lr: 0.01000\tTime 0.296 (0.298)\tData 0.000 (0.020)\tLoss 2.4717 (2.3842)\tPrec@1 84.375 (85.388)\tPrec@5 97.656 (98.939)\n",
      "Epoch: [53][90/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.020)\tLoss 3.3897 (2.4078)\tPrec@1 79.688 (85.251)\tPrec@5 99.219 (98.884)\n",
      "Epoch: [53][96/97], lr: 0.01000\tTime 0.302 (0.298)\tData 0.000 (0.020)\tLoss 2.3301 (2.3977)\tPrec@1 86.441 (85.330)\tPrec@5 99.153 (98.880)\n",
      "Test: [0/100]\tTime 0.273 (0.273)\tLoss 13.1291 (13.1291)\tPrec@1 33.000 (33.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.072 (0.091)\tLoss 10.0900 (11.7333)\tPrec@1 49.000 (42.273)\tPrec@5 93.000 (94.364)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 10.4230 (11.7739)\tPrec@1 49.000 (42.333)\tPrec@5 98.000 (94.667)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 11.2101 (11.6665)\tPrec@1 45.000 (42.742)\tPrec@5 92.000 (94.645)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 12.4750 (11.6928)\tPrec@1 37.000 (42.585)\tPrec@5 93.000 (94.341)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 11.3732 (11.6502)\tPrec@1 44.000 (42.549)\tPrec@5 96.000 (94.647)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 10.3656 (11.7019)\tPrec@1 47.000 (42.115)\tPrec@5 95.000 (94.787)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 11.1288 (11.7225)\tPrec@1 49.000 (42.000)\tPrec@5 96.000 (94.831)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 10.2662 (11.6897)\tPrec@1 52.000 (42.160)\tPrec@5 95.000 (94.889)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.2029 (11.7592)\tPrec@1 44.000 (41.901)\tPrec@5 99.000 (94.923)\n",
      "val Results: Prec@1 41.820 Prec@5 94.910 Loss 11.77212\n",
      "val Class Accuracy: [0.860,0.990,0.948,0.436,0.255,0.109,0.270,0.197,0.115,0.002]\n",
      "Best Prec@1: 60.150\n",
      "\n",
      "Epoch: [54][0/97], lr: 0.01000\tTime 0.404 (0.404)\tData 0.257 (0.257)\tLoss 1.7785 (1.7785)\tPrec@1 87.500 (87.500)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [54][10/97], lr: 0.01000\tTime 0.296 (0.310)\tData 0.000 (0.039)\tLoss 2.3134 (2.1008)\tPrec@1 89.062 (86.790)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [54][20/97], lr: 0.01000\tTime 0.296 (0.304)\tData 0.000 (0.028)\tLoss 1.7677 (2.0798)\tPrec@1 92.188 (87.388)\tPrec@5 100.000 (99.702)\n",
      "Epoch: [54][30/97], lr: 0.01000\tTime 0.297 (0.302)\tData 0.000 (0.025)\tLoss 2.6080 (2.2808)\tPrec@1 82.031 (85.963)\tPrec@5 98.438 (99.521)\n",
      "Epoch: [54][40/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.023)\tLoss 1.3265 (2.2952)\tPrec@1 92.188 (85.766)\tPrec@5 100.000 (99.390)\n",
      "Epoch: [54][50/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.022)\tLoss 2.5306 (2.3482)\tPrec@1 84.375 (85.478)\tPrec@5 100.000 (99.295)\n",
      "Epoch: [54][60/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.021)\tLoss 2.5441 (2.3019)\tPrec@1 88.281 (85.822)\tPrec@5 99.219 (99.308)\n",
      "Epoch: [54][70/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 3.3361 (2.3400)\tPrec@1 79.688 (85.585)\tPrec@5 96.875 (99.186)\n",
      "Epoch: [54][80/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 2.5558 (2.3483)\tPrec@1 84.375 (85.475)\tPrec@5 97.656 (99.132)\n",
      "Epoch: [54][90/97], lr: 0.01000\tTime 0.294 (0.300)\tData 0.000 (0.020)\tLoss 2.5369 (2.3603)\tPrec@1 85.938 (85.457)\tPrec@5 99.219 (99.133)\n",
      "Epoch: [54][96/97], lr: 0.01000\tTime 0.290 (0.300)\tData 0.000 (0.020)\tLoss 2.6353 (2.3710)\tPrec@1 83.051 (85.346)\tPrec@5 99.153 (99.121)\n",
      "Test: [0/100]\tTime 0.272 (0.272)\tLoss 9.8414 (9.8414)\tPrec@1 50.000 (50.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.072 (0.091)\tLoss 7.2430 (8.8625)\tPrec@1 63.000 (55.727)\tPrec@5 96.000 (94.455)\n",
      "Test: [20/100]\tTime 0.072 (0.082)\tLoss 6.8326 (8.7137)\tPrec@1 67.000 (56.238)\tPrec@5 96.000 (94.524)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 9.1063 (8.7471)\tPrec@1 51.000 (56.226)\tPrec@5 94.000 (94.161)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 9.2694 (8.7865)\tPrec@1 55.000 (56.122)\tPrec@5 95.000 (93.951)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.0378 (8.6779)\tPrec@1 59.000 (56.745)\tPrec@5 93.000 (94.020)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.4793 (8.6542)\tPrec@1 58.000 (56.770)\tPrec@5 96.000 (93.885)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.1489 (8.5852)\tPrec@1 59.000 (57.085)\tPrec@5 95.000 (93.845)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.9726 (8.5650)\tPrec@1 55.000 (57.136)\tPrec@5 92.000 (93.901)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.1322 (8.5938)\tPrec@1 60.000 (56.967)\tPrec@5 95.000 (93.967)\n",
      "val Results: Prec@1 57.090 Prec@5 94.040 Loss 8.58545\n",
      "val Class Accuracy: [0.949,0.977,0.699,0.413,0.417,0.846,0.643,0.338,0.410,0.017]\n",
      "Best Prec@1: 60.150\n",
      "\n",
      "Epoch: [55][0/97], lr: 0.01000\tTime 0.429 (0.429)\tData 0.262 (0.262)\tLoss 2.3215 (2.3215)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [55][10/97], lr: 0.01000\tTime 0.295 (0.311)\tData 0.000 (0.038)\tLoss 2.7008 (2.2262)\tPrec@1 82.031 (86.435)\tPrec@5 100.000 (98.935)\n",
      "Epoch: [55][20/97], lr: 0.01000\tTime 0.295 (0.304)\tData 0.000 (0.028)\tLoss 1.8321 (2.2077)\tPrec@1 88.281 (86.347)\tPrec@5 100.000 (99.070)\n",
      "Epoch: [55][30/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.025)\tLoss 2.8220 (2.2706)\tPrec@1 82.031 (85.761)\tPrec@5 100.000 (99.143)\n",
      "Epoch: [55][40/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.023)\tLoss 1.6133 (2.3015)\tPrec@1 89.062 (85.480)\tPrec@5 100.000 (99.200)\n",
      "Epoch: [55][50/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.022)\tLoss 2.7617 (2.2884)\tPrec@1 82.812 (85.708)\tPrec@5 98.438 (99.081)\n",
      "Epoch: [55][60/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 1.9885 (2.3036)\tPrec@1 90.625 (85.604)\tPrec@5 100.000 (99.103)\n",
      "Epoch: [55][70/97], lr: 0.01000\tTime 0.294 (0.300)\tData 0.000 (0.020)\tLoss 1.8756 (2.3076)\tPrec@1 88.281 (85.640)\tPrec@5 98.438 (99.065)\n",
      "Epoch: [55][80/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.1396 (2.2979)\tPrec@1 87.500 (85.754)\tPrec@5 99.219 (99.026)\n",
      "Epoch: [55][90/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 3.0044 (2.3268)\tPrec@1 80.469 (85.568)\tPrec@5 96.875 (99.004)\n",
      "Epoch: [55][96/97], lr: 0.01000\tTime 0.286 (0.299)\tData 0.000 (0.020)\tLoss 2.2416 (2.3404)\tPrec@1 89.831 (85.531)\tPrec@5 99.153 (98.992)\n",
      "Test: [0/100]\tTime 0.292 (0.292)\tLoss 8.1771 (8.1771)\tPrec@1 60.000 (60.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 5.6847 (8.2533)\tPrec@1 71.000 (59.455)\tPrec@5 95.000 (94.182)\n",
      "Test: [20/100]\tTime 0.072 (0.083)\tLoss 7.8898 (8.1902)\tPrec@1 57.000 (58.952)\tPrec@5 95.000 (94.333)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 8.6849 (8.3321)\tPrec@1 54.000 (58.419)\tPrec@5 92.000 (94.355)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 8.4342 (8.4419)\tPrec@1 61.000 (58.171)\tPrec@5 92.000 (94.171)\n",
      "Test: [50/100]\tTime 0.072 (0.077)\tLoss 7.6089 (8.3972)\tPrec@1 61.000 (58.392)\tPrec@5 97.000 (94.294)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.3038 (8.3756)\tPrec@1 61.000 (58.426)\tPrec@5 95.000 (94.213)\n",
      "Test: [70/100]\tTime 0.072 (0.076)\tLoss 8.1248 (8.3570)\tPrec@1 63.000 (58.606)\tPrec@5 97.000 (94.099)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 8.7042 (8.3231)\tPrec@1 58.000 (58.728)\tPrec@5 94.000 (94.185)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.3299 (8.3620)\tPrec@1 62.000 (58.692)\tPrec@5 95.000 (94.132)\n",
      "val Results: Prec@1 58.700 Prec@5 94.170 Loss 8.36436\n",
      "val Class Accuracy: [0.844,0.967,0.649,0.312,0.570,0.852,0.851,0.387,0.375,0.063]\n",
      "Best Prec@1: 60.150\n",
      "\n",
      "Epoch: [56][0/97], lr: 0.01000\tTime 0.387 (0.387)\tData 0.241 (0.241)\tLoss 2.7354 (2.7354)\tPrec@1 82.812 (82.812)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [56][10/97], lr: 0.01000\tTime 0.296 (0.308)\tData 0.000 (0.037)\tLoss 3.9220 (2.5019)\tPrec@1 76.562 (85.085)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [56][20/97], lr: 0.01000\tTime 0.297 (0.303)\tData 0.000 (0.028)\tLoss 1.2868 (2.3597)\tPrec@1 91.406 (85.156)\tPrec@5 100.000 (99.182)\n",
      "Epoch: [56][30/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.024)\tLoss 2.4993 (2.3490)\tPrec@1 85.938 (85.358)\tPrec@5 100.000 (99.118)\n",
      "Epoch: [56][40/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.022)\tLoss 2.3791 (2.3783)\tPrec@1 86.719 (85.366)\tPrec@5 98.438 (99.143)\n",
      "Epoch: [56][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 2.0514 (2.3311)\tPrec@1 86.719 (85.585)\tPrec@5 99.219 (99.188)\n",
      "Epoch: [56][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 2.9506 (2.3534)\tPrec@1 82.031 (85.438)\tPrec@5 98.438 (99.180)\n",
      "Epoch: [56][70/97], lr: 0.01000\tTime 0.303 (0.299)\tData 0.000 (0.020)\tLoss 2.7426 (2.3325)\tPrec@1 82.031 (85.717)\tPrec@5 99.219 (99.153)\n",
      "Epoch: [56][80/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 1.7804 (2.3054)\tPrec@1 89.844 (85.889)\tPrec@5 99.219 (99.161)\n",
      "Epoch: [56][90/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.5110 (2.2977)\tPrec@1 84.375 (86.023)\tPrec@5 98.438 (99.159)\n",
      "Epoch: [56][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 2.5606 (2.3108)\tPrec@1 85.593 (86.015)\tPrec@5 98.305 (99.170)\n",
      "Test: [0/100]\tTime 0.267 (0.267)\tLoss 7.7409 (7.7409)\tPrec@1 65.000 (65.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.072 (0.090)\tLoss 5.7438 (7.7108)\tPrec@1 72.000 (62.636)\tPrec@5 96.000 (95.182)\n",
      "Test: [20/100]\tTime 0.072 (0.082)\tLoss 7.8453 (7.6123)\tPrec@1 57.000 (63.048)\tPrec@5 93.000 (95.238)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 7.0367 (7.5671)\tPrec@1 66.000 (63.387)\tPrec@5 96.000 (95.290)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 7.6022 (7.6352)\tPrec@1 67.000 (63.220)\tPrec@5 96.000 (95.268)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 6.8906 (7.5453)\tPrec@1 69.000 (63.451)\tPrec@5 96.000 (95.451)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.6293 (7.5675)\tPrec@1 65.000 (63.426)\tPrec@5 92.000 (95.328)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.2112 (7.5747)\tPrec@1 66.000 (63.493)\tPrec@5 97.000 (95.408)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.9057 (7.5872)\tPrec@1 63.000 (63.506)\tPrec@5 94.000 (95.469)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.2118 (7.6238)\tPrec@1 59.000 (63.253)\tPrec@5 96.000 (95.407)\n",
      "val Results: Prec@1 63.130 Prec@5 95.280 Loss 7.64695\n",
      "val Class Accuracy: [0.913,0.990,0.656,0.796,0.707,0.489,0.775,0.482,0.439,0.066]\n",
      "Best Prec@1: 63.130\n",
      "\n",
      "Epoch: [57][0/97], lr: 0.01000\tTime 0.389 (0.389)\tData 0.240 (0.240)\tLoss 2.1158 (2.1158)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [57][10/97], lr: 0.01000\tTime 0.298 (0.311)\tData 0.000 (0.037)\tLoss 1.7195 (2.2039)\tPrec@1 90.625 (86.577)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [57][20/97], lr: 0.01000\tTime 0.299 (0.306)\tData 0.000 (0.028)\tLoss 2.0522 (2.1941)\tPrec@1 90.625 (86.979)\tPrec@5 99.219 (99.182)\n",
      "Epoch: [57][30/97], lr: 0.01000\tTime 0.297 (0.303)\tData 0.000 (0.024)\tLoss 2.6303 (2.2031)\tPrec@1 82.812 (86.643)\tPrec@5 98.438 (99.143)\n",
      "Epoch: [57][40/97], lr: 0.01000\tTime 0.300 (0.303)\tData 0.000 (0.023)\tLoss 2.7726 (2.2196)\tPrec@1 82.031 (86.452)\tPrec@5 96.875 (99.047)\n",
      "Epoch: [57][50/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.022)\tLoss 3.4133 (2.2785)\tPrec@1 78.125 (85.999)\tPrec@5 96.094 (99.020)\n",
      "Epoch: [57][60/97], lr: 0.01000\tTime 0.297 (0.302)\tData 0.000 (0.021)\tLoss 2.3545 (2.2942)\tPrec@1 84.375 (85.950)\tPrec@5 99.219 (99.027)\n",
      "Epoch: [57][70/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.020)\tLoss 2.4822 (2.2977)\tPrec@1 86.719 (85.971)\tPrec@5 99.219 (98.944)\n",
      "Epoch: [57][80/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.020)\tLoss 2.2918 (2.3210)\tPrec@1 86.719 (85.745)\tPrec@5 100.000 (98.929)\n",
      "Epoch: [57][90/97], lr: 0.01000\tTime 0.305 (0.301)\tData 0.000 (0.020)\tLoss 1.7273 (2.3036)\tPrec@1 89.062 (85.826)\tPrec@5 100.000 (98.935)\n",
      "Epoch: [57][96/97], lr: 0.01000\tTime 0.288 (0.300)\tData 0.000 (0.020)\tLoss 2.1517 (2.2889)\tPrec@1 87.288 (85.926)\tPrec@5 100.000 (98.952)\n",
      "Test: [0/100]\tTime 0.276 (0.276)\tLoss 9.7167 (9.7167)\tPrec@1 52.000 (52.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 7.0656 (8.4183)\tPrec@1 64.000 (58.545)\tPrec@5 95.000 (94.909)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.1838 (8.2443)\tPrec@1 64.000 (59.619)\tPrec@5 98.000 (95.095)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.0225 (8.2036)\tPrec@1 67.000 (60.032)\tPrec@5 95.000 (94.903)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 8.0367 (8.2008)\tPrec@1 63.000 (60.146)\tPrec@5 95.000 (94.854)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.9074 (8.0886)\tPrec@1 69.000 (60.902)\tPrec@5 98.000 (95.020)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.3810 (8.1024)\tPrec@1 64.000 (60.590)\tPrec@5 94.000 (95.000)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 6.9408 (8.0885)\tPrec@1 70.000 (60.606)\tPrec@5 96.000 (95.028)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 8.2542 (8.0748)\tPrec@1 64.000 (60.741)\tPrec@5 95.000 (95.148)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.0033 (8.1367)\tPrec@1 62.000 (60.385)\tPrec@5 96.000 (95.055)\n",
      "val Results: Prec@1 60.190 Prec@5 95.040 Loss 8.16521\n",
      "val Class Accuracy: [0.887,0.991,0.794,0.702,0.803,0.615,0.443,0.545,0.229,0.010]\n",
      "Best Prec@1: 63.130\n",
      "\n",
      "Epoch: [58][0/97], lr: 0.01000\tTime 0.384 (0.384)\tData 0.231 (0.231)\tLoss 1.2687 (1.2687)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [58][10/97], lr: 0.01000\tTime 0.297 (0.308)\tData 0.000 (0.036)\tLoss 2.7194 (2.0411)\tPrec@1 84.375 (87.784)\tPrec@5 99.219 (99.645)\n",
      "Epoch: [58][20/97], lr: 0.01000\tTime 0.295 (0.303)\tData 0.000 (0.027)\tLoss 2.3407 (2.1141)\tPrec@1 89.844 (87.760)\tPrec@5 97.656 (99.442)\n",
      "Epoch: [58][30/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.024)\tLoss 1.9834 (2.1552)\tPrec@1 87.500 (87.147)\tPrec@5 98.438 (99.269)\n",
      "Epoch: [58][40/97], lr: 0.01000\tTime 0.289 (0.300)\tData 0.000 (0.022)\tLoss 2.2731 (2.2024)\tPrec@1 87.500 (86.890)\tPrec@5 99.219 (99.162)\n",
      "Epoch: [58][50/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 2.5614 (2.2198)\tPrec@1 83.594 (86.688)\tPrec@5 100.000 (99.112)\n",
      "Epoch: [58][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 2.4222 (2.2578)\tPrec@1 84.375 (86.335)\tPrec@5 98.438 (99.078)\n",
      "Epoch: [58][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.3811 (2.3037)\tPrec@1 86.719 (86.103)\tPrec@5 99.219 (99.065)\n",
      "Epoch: [58][80/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 2.4313 (2.2992)\tPrec@1 85.156 (86.082)\tPrec@5 99.219 (98.997)\n",
      "Epoch: [58][90/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.020)\tLoss 1.7497 (2.3322)\tPrec@1 87.500 (85.895)\tPrec@5 99.219 (98.961)\n",
      "Epoch: [58][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 1.8356 (2.3261)\tPrec@1 87.288 (85.878)\tPrec@5 100.000 (98.984)\n",
      "Test: [0/100]\tTime 0.241 (0.241)\tLoss 12.2647 (12.2647)\tPrec@1 37.000 (37.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.072 (0.088)\tLoss 8.4225 (10.3318)\tPrec@1 60.000 (48.727)\tPrec@5 91.000 (89.818)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 8.5433 (10.1864)\tPrec@1 58.000 (49.571)\tPrec@5 94.000 (90.048)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.7894 (10.1779)\tPrec@1 51.000 (49.323)\tPrec@5 87.000 (90.032)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.9400 (10.2122)\tPrec@1 52.000 (49.854)\tPrec@5 90.000 (89.951)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.2415 (10.1029)\tPrec@1 55.000 (50.294)\tPrec@5 92.000 (90.039)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 8.1435 (10.0884)\tPrec@1 59.000 (50.246)\tPrec@5 90.000 (90.098)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.2085 (10.0441)\tPrec@1 56.000 (50.437)\tPrec@5 90.000 (89.930)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 9.9808 (9.9878)\tPrec@1 49.000 (50.741)\tPrec@5 89.000 (90.049)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 10.1574 (10.0424)\tPrec@1 50.000 (50.385)\tPrec@5 91.000 (89.835)\n",
      "val Results: Prec@1 50.390 Prec@5 89.710 Loss 10.06109\n",
      "val Class Accuracy: [0.789,0.991,0.705,0.821,0.724,0.652,0.212,0.068,0.058,0.019]\n",
      "Best Prec@1: 63.130\n",
      "\n",
      "Epoch: [59][0/97], lr: 0.01000\tTime 0.392 (0.392)\tData 0.241 (0.241)\tLoss 2.8924 (2.8924)\tPrec@1 82.031 (82.031)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [59][10/97], lr: 0.01000\tTime 0.296 (0.309)\tData 0.000 (0.036)\tLoss 1.9421 (2.2630)\tPrec@1 91.406 (86.719)\tPrec@5 99.219 (99.361)\n",
      "Epoch: [59][20/97], lr: 0.01000\tTime 0.298 (0.304)\tData 0.000 (0.027)\tLoss 1.7774 (2.1542)\tPrec@1 87.500 (87.091)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [59][30/97], lr: 0.01000\tTime 0.295 (0.303)\tData 0.000 (0.024)\tLoss 2.0751 (2.2107)\tPrec@1 84.375 (86.719)\tPrec@5 98.438 (99.118)\n",
      "Epoch: [59][40/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.022)\tLoss 2.5225 (2.2124)\tPrec@1 84.375 (86.604)\tPrec@5 99.219 (99.066)\n",
      "Epoch: [59][50/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 2.1800 (2.2728)\tPrec@1 85.156 (86.183)\tPrec@5 99.219 (99.020)\n",
      "Epoch: [59][60/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 2.0864 (2.2708)\tPrec@1 88.281 (86.168)\tPrec@5 98.438 (99.027)\n",
      "Epoch: [59][70/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 2.4567 (2.2629)\tPrec@1 85.938 (86.246)\tPrec@5 100.000 (99.087)\n",
      "Epoch: [59][80/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 2.3975 (2.2576)\tPrec@1 86.719 (86.352)\tPrec@5 99.219 (99.074)\n",
      "Epoch: [59][90/97], lr: 0.01000\tTime 0.300 (0.299)\tData 0.000 (0.020)\tLoss 2.3575 (2.2710)\tPrec@1 84.375 (86.221)\tPrec@5 99.219 (99.064)\n",
      "Epoch: [59][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 2.1127 (2.3026)\tPrec@1 85.593 (85.999)\tPrec@5 98.305 (98.960)\n",
      "Test: [0/100]\tTime 0.256 (0.256)\tLoss 11.7094 (11.7094)\tPrec@1 42.000 (42.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.072 (0.089)\tLoss 7.9885 (9.7229)\tPrec@1 61.000 (52.000)\tPrec@5 91.000 (93.000)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 7.4069 (9.5430)\tPrec@1 61.000 (53.286)\tPrec@5 94.000 (93.190)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 8.9262 (9.5381)\tPrec@1 56.000 (53.032)\tPrec@5 92.000 (92.935)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 9.4266 (9.4908)\tPrec@1 57.000 (53.756)\tPrec@5 93.000 (92.805)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.3970 (9.4179)\tPrec@1 51.000 (54.020)\tPrec@5 94.000 (92.804)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 7.5006 (9.4053)\tPrec@1 65.000 (53.754)\tPrec@5 92.000 (92.984)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.2984 (9.4181)\tPrec@1 55.000 (53.690)\tPrec@5 92.000 (93.014)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 9.2816 (9.3886)\tPrec@1 55.000 (53.864)\tPrec@5 91.000 (92.914)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 9.0150 (9.4459)\tPrec@1 56.000 (53.637)\tPrec@5 94.000 (92.835)\n",
      "val Results: Prec@1 53.590 Prec@5 92.790 Loss 9.45501\n",
      "val Class Accuracy: [0.981,0.848,0.704,0.796,0.674,0.480,0.286,0.401,0.171,0.018]\n",
      "Best Prec@1: 63.130\n",
      "\n",
      "Epoch: [60][0/97], lr: 0.01000\tTime 0.365 (0.365)\tData 0.226 (0.226)\tLoss 2.6710 (2.6710)\tPrec@1 82.031 (82.031)\tPrec@5 96.875 (96.875)\n",
      "Epoch: [60][10/97], lr: 0.01000\tTime 0.295 (0.305)\tData 0.000 (0.036)\tLoss 2.0516 (2.4693)\tPrec@1 89.062 (84.943)\tPrec@5 98.438 (98.793)\n",
      "Epoch: [60][20/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.027)\tLoss 1.9025 (2.3279)\tPrec@1 88.281 (85.640)\tPrec@5 100.000 (98.810)\n",
      "Epoch: [60][30/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.024)\tLoss 2.4303 (2.3053)\tPrec@1 85.156 (85.685)\tPrec@5 98.438 (98.942)\n",
      "Epoch: [60][40/97], lr: 0.01000\tTime 0.282 (0.300)\tData 0.000 (0.022)\tLoss 2.1872 (2.2717)\tPrec@1 88.281 (85.938)\tPrec@5 100.000 (98.952)\n",
      "Epoch: [60][50/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 2.5523 (2.2676)\tPrec@1 85.156 (85.968)\tPrec@5 99.219 (98.974)\n",
      "Epoch: [60][60/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 1.9466 (2.2988)\tPrec@1 87.500 (85.822)\tPrec@5 99.219 (98.924)\n",
      "Epoch: [60][70/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 2.6863 (2.3222)\tPrec@1 83.594 (85.761)\tPrec@5 99.219 (98.933)\n",
      "Epoch: [60][80/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.020)\tLoss 2.5185 (2.3081)\tPrec@1 84.375 (85.938)\tPrec@5 96.094 (98.900)\n",
      "Epoch: [60][90/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.019)\tLoss 2.1783 (2.3139)\tPrec@1 86.719 (85.860)\tPrec@5 96.875 (98.901)\n",
      "Epoch: [60][96/97], lr: 0.01000\tTime 0.290 (0.299)\tData 0.000 (0.020)\tLoss 1.9892 (2.3062)\tPrec@1 87.288 (85.870)\tPrec@5 100.000 (98.912)\n",
      "Test: [0/100]\tTime 0.246 (0.246)\tLoss 10.1046 (10.1046)\tPrec@1 50.000 (50.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 7.7428 (8.8804)\tPrec@1 63.000 (56.273)\tPrec@5 91.000 (93.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.5243 (8.8805)\tPrec@1 60.000 (56.095)\tPrec@5 92.000 (93.143)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 8.2470 (8.8682)\tPrec@1 55.000 (55.903)\tPrec@5 91.000 (93.290)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.1133 (8.8748)\tPrec@1 52.000 (56.122)\tPrec@5 94.000 (93.415)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.1996 (8.7839)\tPrec@1 62.000 (56.667)\tPrec@5 93.000 (93.647)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 7.8497 (8.7337)\tPrec@1 60.000 (56.738)\tPrec@5 88.000 (93.574)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 7.9463 (8.7009)\tPrec@1 61.000 (56.958)\tPrec@5 93.000 (93.634)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.0460 (8.6575)\tPrec@1 60.000 (57.185)\tPrec@5 92.000 (93.654)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 8.9153 (8.7307)\tPrec@1 56.000 (56.769)\tPrec@5 95.000 (93.582)\n",
      "val Results: Prec@1 56.790 Prec@5 93.520 Loss 8.74780\n",
      "val Class Accuracy: [0.963,0.992,0.742,0.567,0.756,0.510,0.591,0.504,0.047,0.007]\n",
      "Best Prec@1: 63.130\n",
      "\n",
      "Epoch: [61][0/97], lr: 0.01000\tTime 0.358 (0.358)\tData 0.206 (0.206)\tLoss 2.0553 (2.0553)\tPrec@1 88.281 (88.281)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [61][10/97], lr: 0.01000\tTime 0.297 (0.308)\tData 0.000 (0.034)\tLoss 1.8906 (2.5302)\tPrec@1 91.406 (84.801)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [61][20/97], lr: 0.01000\tTime 0.296 (0.303)\tData 0.000 (0.026)\tLoss 1.9078 (2.3415)\tPrec@1 89.844 (85.863)\tPrec@5 99.219 (99.033)\n",
      "Epoch: [61][30/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.023)\tLoss 1.5884 (2.2320)\tPrec@1 89.062 (86.618)\tPrec@5 99.219 (99.118)\n",
      "Epoch: [61][40/97], lr: 0.01000\tTime 0.300 (0.302)\tData 0.000 (0.022)\tLoss 2.1566 (2.2191)\tPrec@1 87.500 (86.681)\tPrec@5 99.219 (99.238)\n",
      "Epoch: [61][50/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 2.5816 (2.2743)\tPrec@1 82.812 (86.366)\tPrec@5 98.438 (99.142)\n",
      "Epoch: [61][60/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.020)\tLoss 1.6848 (2.2511)\tPrec@1 88.281 (86.411)\tPrec@5 99.219 (99.142)\n",
      "Epoch: [61][70/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 2.5534 (2.2719)\tPrec@1 82.031 (86.224)\tPrec@5 100.000 (99.175)\n",
      "Epoch: [61][80/97], lr: 0.01000\tTime 0.307 (0.300)\tData 0.000 (0.020)\tLoss 2.3142 (2.2848)\tPrec@1 86.719 (86.063)\tPrec@5 96.094 (99.103)\n",
      "Epoch: [61][90/97], lr: 0.01000\tTime 0.308 (0.300)\tData 0.000 (0.019)\tLoss 2.1784 (2.3035)\tPrec@1 87.500 (85.929)\tPrec@5 98.438 (99.107)\n",
      "Epoch: [61][96/97], lr: 0.01000\tTime 0.287 (0.300)\tData 0.000 (0.020)\tLoss 3.0439 (2.3203)\tPrec@1 77.966 (85.765)\tPrec@5 99.153 (99.097)\n",
      "Test: [0/100]\tTime 0.228 (0.228)\tLoss 8.9478 (8.9478)\tPrec@1 54.000 (54.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 7.2567 (8.4909)\tPrec@1 63.000 (56.273)\tPrec@5 94.000 (95.091)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 7.9309 (8.4420)\tPrec@1 57.000 (56.571)\tPrec@5 95.000 (95.286)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 8.4592 (8.5404)\tPrec@1 57.000 (56.065)\tPrec@5 97.000 (95.258)\n",
      "Test: [40/100]\tTime 0.072 (0.076)\tLoss 9.0717 (8.5586)\tPrec@1 55.000 (56.171)\tPrec@5 93.000 (94.976)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.7577 (8.5169)\tPrec@1 53.000 (56.235)\tPrec@5 97.000 (95.157)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 7.4391 (8.5028)\tPrec@1 62.000 (56.344)\tPrec@5 96.000 (95.344)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 8.0625 (8.5129)\tPrec@1 63.000 (56.310)\tPrec@5 95.000 (95.324)\n",
      "Test: [80/100]\tTime 0.072 (0.074)\tLoss 8.3623 (8.4851)\tPrec@1 60.000 (56.556)\tPrec@5 90.000 (95.296)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 8.7158 (8.5341)\tPrec@1 54.000 (56.374)\tPrec@5 97.000 (95.341)\n",
      "val Results: Prec@1 56.440 Prec@5 95.330 Loss 8.53363\n",
      "val Class Accuracy: [0.959,0.902,0.801,0.824,0.490,0.223,0.552,0.552,0.261,0.080]\n",
      "Best Prec@1: 63.130\n",
      "\n",
      "Epoch: [62][0/97], lr: 0.01000\tTime 0.376 (0.376)\tData 0.218 (0.218)\tLoss 1.8949 (1.8949)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [62][10/97], lr: 0.01000\tTime 0.300 (0.307)\tData 0.000 (0.035)\tLoss 1.9366 (2.0252)\tPrec@1 88.281 (87.571)\tPrec@5 97.656 (98.864)\n",
      "Epoch: [62][20/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.027)\tLoss 1.9769 (2.1445)\tPrec@1 89.062 (86.607)\tPrec@5 99.219 (98.996)\n",
      "Epoch: [62][30/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.024)\tLoss 2.2807 (2.2288)\tPrec@1 85.938 (86.316)\tPrec@5 99.219 (98.866)\n",
      "Epoch: [62][40/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.022)\tLoss 2.5210 (2.3017)\tPrec@1 87.500 (85.709)\tPrec@5 100.000 (98.895)\n",
      "Epoch: [62][50/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 2.5995 (2.3513)\tPrec@1 84.375 (85.386)\tPrec@5 99.219 (98.775)\n",
      "Epoch: [62][60/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.6895 (2.3632)\tPrec@1 89.062 (85.297)\tPrec@5 99.219 (98.822)\n",
      "Epoch: [62][70/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 2.4501 (2.3710)\tPrec@1 85.938 (85.321)\tPrec@5 100.000 (98.878)\n",
      "Epoch: [62][80/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.020)\tLoss 1.9189 (2.3599)\tPrec@1 89.062 (85.311)\tPrec@5 98.438 (98.891)\n",
      "Epoch: [62][90/97], lr: 0.01000\tTime 0.293 (0.298)\tData 0.000 (0.019)\tLoss 2.6030 (2.3458)\tPrec@1 85.938 (85.560)\tPrec@5 98.438 (98.918)\n",
      "Epoch: [62][96/97], lr: 0.01000\tTime 0.289 (0.298)\tData 0.000 (0.020)\tLoss 2.0888 (2.3288)\tPrec@1 87.288 (85.652)\tPrec@5 97.458 (98.928)\n",
      "Test: [0/100]\tTime 0.249 (0.249)\tLoss 11.1548 (11.1548)\tPrec@1 51.000 (51.000)\tPrec@5 90.000 (90.000)\n",
      "Test: [10/100]\tTime 0.072 (0.089)\tLoss 7.5726 (9.3900)\tPrec@1 65.000 (56.273)\tPrec@5 92.000 (91.000)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.9943 (9.3467)\tPrec@1 58.000 (56.571)\tPrec@5 96.000 (90.905)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 8.5001 (9.3745)\tPrec@1 59.000 (56.516)\tPrec@5 91.000 (91.290)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.8618 (9.3686)\tPrec@1 55.000 (56.390)\tPrec@5 89.000 (91.220)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.5696 (9.2596)\tPrec@1 60.000 (56.843)\tPrec@5 93.000 (91.235)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.5374 (9.2159)\tPrec@1 63.000 (56.705)\tPrec@5 92.000 (91.361)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.8580 (9.1976)\tPrec@1 57.000 (56.901)\tPrec@5 92.000 (91.282)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.2934 (9.1750)\tPrec@1 60.000 (56.914)\tPrec@5 91.000 (91.383)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.9188 (9.2579)\tPrec@1 54.000 (56.571)\tPrec@5 91.000 (91.264)\n",
      "val Results: Prec@1 56.440 Prec@5 91.180 Loss 9.28473\n",
      "val Class Accuracy: [0.958,0.988,0.663,0.795,0.800,0.551,0.466,0.375,0.043,0.005]\n",
      "Best Prec@1: 63.130\n",
      "\n",
      "Epoch: [63][0/97], lr: 0.01000\tTime 0.392 (0.392)\tData 0.237 (0.237)\tLoss 2.4195 (2.4195)\tPrec@1 86.719 (86.719)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [63][10/97], lr: 0.01000\tTime 0.295 (0.308)\tData 0.000 (0.037)\tLoss 2.1167 (2.3277)\tPrec@1 85.156 (85.582)\tPrec@5 97.656 (98.793)\n",
      "Epoch: [63][20/97], lr: 0.01000\tTime 0.298 (0.304)\tData 0.000 (0.027)\tLoss 1.5555 (2.2892)\tPrec@1 90.625 (85.640)\tPrec@5 100.000 (98.847)\n",
      "Epoch: [63][30/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.024)\tLoss 2.5952 (2.2922)\tPrec@1 85.938 (85.433)\tPrec@5 98.438 (98.916)\n",
      "Epoch: [63][40/97], lr: 0.01000\tTime 0.300 (0.301)\tData 0.000 (0.022)\tLoss 2.0049 (2.3340)\tPrec@1 89.062 (85.252)\tPrec@5 100.000 (98.914)\n",
      "Epoch: [63][50/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.021)\tLoss 2.2535 (2.3193)\tPrec@1 85.938 (85.478)\tPrec@5 99.219 (98.958)\n",
      "Epoch: [63][60/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 2.8930 (2.3031)\tPrec@1 81.250 (85.669)\tPrec@5 97.656 (98.963)\n",
      "Epoch: [63][70/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 1.7283 (2.2688)\tPrec@1 89.844 (85.838)\tPrec@5 98.438 (99.021)\n",
      "Epoch: [63][80/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.020)\tLoss 2.5991 (2.2417)\tPrec@1 83.594 (85.976)\tPrec@5 98.438 (99.045)\n",
      "Epoch: [63][90/97], lr: 0.01000\tTime 0.305 (0.299)\tData 0.000 (0.020)\tLoss 1.1460 (2.2272)\tPrec@1 93.750 (86.212)\tPrec@5 100.000 (99.081)\n",
      "Epoch: [63][96/97], lr: 0.01000\tTime 0.288 (0.299)\tData 0.000 (0.020)\tLoss 2.2047 (2.2470)\tPrec@1 86.441 (86.160)\tPrec@5 99.153 (99.049)\n",
      "Test: [0/100]\tTime 0.247 (0.247)\tLoss 10.6824 (10.6824)\tPrec@1 51.000 (51.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 8.5869 (9.9480)\tPrec@1 57.000 (51.636)\tPrec@5 93.000 (92.636)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 8.7604 (9.8814)\tPrec@1 57.000 (52.476)\tPrec@5 94.000 (92.857)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.7232 (9.9110)\tPrec@1 49.000 (52.000)\tPrec@5 93.000 (92.645)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.7906 (9.9600)\tPrec@1 46.000 (51.829)\tPrec@5 93.000 (92.488)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.7483 (9.9176)\tPrec@1 53.000 (51.941)\tPrec@5 93.000 (92.686)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 7.9903 (9.9127)\tPrec@1 59.000 (51.689)\tPrec@5 95.000 (92.787)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.4241 (9.8894)\tPrec@1 56.000 (51.704)\tPrec@5 96.000 (92.704)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 9.3550 (9.8232)\tPrec@1 56.000 (52.049)\tPrec@5 91.000 (92.691)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.0321 (9.8825)\tPrec@1 48.000 (51.670)\tPrec@5 94.000 (92.593)\n",
      "val Results: Prec@1 51.830 Prec@5 92.660 Loss 9.88931\n",
      "val Class Accuracy: [0.863,0.976,0.868,0.836,0.376,0.271,0.369,0.501,0.123,0.000]\n",
      "Best Prec@1: 63.130\n",
      "\n",
      "Epoch: [64][0/97], lr: 0.01000\tTime 0.401 (0.401)\tData 0.242 (0.242)\tLoss 2.1861 (2.1861)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [64][10/97], lr: 0.01000\tTime 0.295 (0.308)\tData 0.000 (0.037)\tLoss 2.7391 (2.3319)\tPrec@1 82.812 (85.653)\tPrec@5 99.219 (98.722)\n",
      "Epoch: [64][20/97], lr: 0.01000\tTime 0.295 (0.303)\tData 0.000 (0.028)\tLoss 1.9881 (2.2769)\tPrec@1 88.281 (86.161)\tPrec@5 100.000 (98.958)\n",
      "Epoch: [64][30/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.024)\tLoss 1.6316 (2.2335)\tPrec@1 89.844 (86.341)\tPrec@5 99.219 (98.967)\n",
      "Epoch: [64][40/97], lr: 0.01000\tTime 0.299 (0.301)\tData 0.000 (0.023)\tLoss 2.0516 (2.2283)\tPrec@1 86.719 (86.395)\tPrec@5 99.219 (98.990)\n",
      "Epoch: [64][50/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.022)\tLoss 1.8667 (2.2303)\tPrec@1 90.625 (86.428)\tPrec@5 99.219 (99.020)\n",
      "Epoch: [64][60/97], lr: 0.01000\tTime 0.298 (0.300)\tData 0.000 (0.021)\tLoss 2.1665 (2.2294)\tPrec@1 85.938 (86.475)\tPrec@5 98.438 (99.027)\n",
      "Epoch: [64][70/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 1.7604 (2.2165)\tPrec@1 89.844 (86.433)\tPrec@5 98.438 (99.010)\n",
      "Epoch: [64][80/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 2.0538 (2.2159)\tPrec@1 87.500 (86.429)\tPrec@5 100.000 (99.055)\n",
      "Epoch: [64][90/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.9770 (2.2156)\tPrec@1 87.500 (86.487)\tPrec@5 100.000 (99.073)\n",
      "Epoch: [64][96/97], lr: 0.01000\tTime 0.290 (0.299)\tData 0.000 (0.020)\tLoss 2.8106 (2.2429)\tPrec@1 83.051 (86.345)\tPrec@5 99.153 (99.033)\n",
      "Test: [0/100]\tTime 0.259 (0.259)\tLoss 10.5523 (10.5523)\tPrec@1 52.000 (52.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 7.1945 (9.6289)\tPrec@1 64.000 (53.818)\tPrec@5 92.000 (89.273)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.4516 (9.4077)\tPrec@1 61.000 (55.286)\tPrec@5 94.000 (89.762)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 9.3489 (9.4036)\tPrec@1 53.000 (55.226)\tPrec@5 88.000 (89.387)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 9.9424 (9.4504)\tPrec@1 52.000 (55.171)\tPrec@5 91.000 (89.537)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.6789 (9.3174)\tPrec@1 58.000 (55.706)\tPrec@5 92.000 (89.765)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.0043 (9.2656)\tPrec@1 61.000 (55.738)\tPrec@5 93.000 (89.984)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.4923 (9.2353)\tPrec@1 62.000 (55.930)\tPrec@5 97.000 (89.887)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.6063 (9.1766)\tPrec@1 60.000 (56.210)\tPrec@5 88.000 (89.889)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 9.5347 (9.2471)\tPrec@1 54.000 (55.703)\tPrec@5 91.000 (89.747)\n",
      "val Results: Prec@1 55.770 Prec@5 89.690 Loss 9.26257\n",
      "val Class Accuracy: [0.823,0.967,0.812,0.656,0.548,0.781,0.602,0.337,0.049,0.002]\n",
      "Best Prec@1: 63.130\n",
      "\n",
      "Epoch: [65][0/97], lr: 0.01000\tTime 0.410 (0.410)\tData 0.270 (0.270)\tLoss 1.7991 (1.7991)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [65][10/97], lr: 0.01000\tTime 0.294 (0.310)\tData 0.000 (0.040)\tLoss 2.8293 (2.1159)\tPrec@1 82.031 (86.577)\tPrec@5 98.438 (99.290)\n",
      "Epoch: [65][20/97], lr: 0.01000\tTime 0.296 (0.304)\tData 0.000 (0.029)\tLoss 2.7171 (2.1865)\tPrec@1 82.812 (86.421)\tPrec@5 97.656 (99.182)\n",
      "Epoch: [65][30/97], lr: 0.01000\tTime 0.296 (0.303)\tData 0.000 (0.025)\tLoss 2.3571 (2.1913)\tPrec@1 86.719 (86.744)\tPrec@5 100.000 (99.118)\n",
      "Epoch: [65][40/97], lr: 0.01000\tTime 0.293 (0.301)\tData 0.000 (0.023)\tLoss 2.3634 (2.1894)\tPrec@1 85.156 (86.700)\tPrec@5 98.438 (99.143)\n",
      "Epoch: [65][50/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.022)\tLoss 2.7477 (2.2010)\tPrec@1 85.156 (86.688)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [65][60/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 1.9723 (2.2345)\tPrec@1 88.281 (86.463)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [65][70/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 2.0179 (2.2404)\tPrec@1 89.062 (86.466)\tPrec@5 99.219 (99.241)\n",
      "Epoch: [65][80/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 1.6000 (2.2197)\tPrec@1 90.625 (86.487)\tPrec@5 100.000 (99.286)\n",
      "Epoch: [65][90/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.6570 (2.2365)\tPrec@1 85.938 (86.332)\tPrec@5 96.094 (99.193)\n",
      "Epoch: [65][96/97], lr: 0.01000\tTime 0.288 (0.299)\tData 0.000 (0.021)\tLoss 2.3690 (2.2245)\tPrec@1 86.441 (86.418)\tPrec@5 99.153 (99.186)\n",
      "Test: [0/100]\tTime 0.258 (0.258)\tLoss 8.2415 (8.2415)\tPrec@1 64.000 (64.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 6.3429 (7.7584)\tPrec@1 68.000 (62.909)\tPrec@5 97.000 (95.909)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.2817 (7.6399)\tPrec@1 68.000 (62.905)\tPrec@5 98.000 (95.762)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.3116 (7.6571)\tPrec@1 63.000 (62.645)\tPrec@5 95.000 (95.710)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 7.0786 (7.6936)\tPrec@1 66.000 (62.463)\tPrec@5 93.000 (95.707)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.9085 (7.6265)\tPrec@1 65.000 (62.882)\tPrec@5 97.000 (95.882)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.0924 (7.6191)\tPrec@1 66.000 (62.738)\tPrec@5 94.000 (95.836)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 6.6615 (7.6036)\tPrec@1 69.000 (62.817)\tPrec@5 97.000 (95.845)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 7.4309 (7.5788)\tPrec@1 67.000 (62.975)\tPrec@5 96.000 (95.901)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 7.1498 (7.6233)\tPrec@1 70.000 (62.846)\tPrec@5 98.000 (95.780)\n",
      "val Results: Prec@1 62.870 Prec@5 95.740 Loss 7.62644\n",
      "val Class Accuracy: [0.912,0.991,0.811,0.562,0.652,0.549,0.717,0.727,0.339,0.027]\n",
      "Best Prec@1: 63.130\n",
      "\n",
      "Epoch: [66][0/97], lr: 0.01000\tTime 0.558 (0.558)\tData 0.365 (0.365)\tLoss 1.8737 (1.8737)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [66][10/97], lr: 0.01000\tTime 0.299 (0.333)\tData 0.000 (0.046)\tLoss 2.2132 (2.1454)\tPrec@1 84.375 (86.577)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [66][20/97], lr: 0.01000\tTime 0.297 (0.318)\tData 0.000 (0.032)\tLoss 2.2607 (2.1553)\tPrec@1 83.594 (86.682)\tPrec@5 99.219 (99.144)\n",
      "Epoch: [66][30/97], lr: 0.01000\tTime 0.299 (0.313)\tData 0.000 (0.027)\tLoss 1.6094 (2.1384)\tPrec@1 90.625 (86.920)\tPrec@5 100.000 (99.194)\n",
      "Epoch: [66][40/97], lr: 0.01000\tTime 0.294 (0.310)\tData 0.000 (0.025)\tLoss 2.6855 (2.2099)\tPrec@1 83.594 (86.490)\tPrec@5 99.219 (99.143)\n",
      "Epoch: [66][50/97], lr: 0.01000\tTime 0.296 (0.308)\tData 0.000 (0.023)\tLoss 1.4016 (2.1881)\tPrec@1 91.406 (86.612)\tPrec@5 100.000 (99.112)\n",
      "Epoch: [66][60/97], lr: 0.01000\tTime 0.297 (0.306)\tData 0.000 (0.022)\tLoss 2.5459 (2.2121)\tPrec@1 85.938 (86.450)\tPrec@5 97.656 (99.027)\n",
      "Epoch: [66][70/97], lr: 0.01000\tTime 0.296 (0.305)\tData 0.000 (0.022)\tLoss 2.1048 (2.2024)\tPrec@1 85.938 (86.554)\tPrec@5 98.438 (99.065)\n",
      "Epoch: [66][80/97], lr: 0.01000\tTime 0.300 (0.304)\tData 0.000 (0.021)\tLoss 2.0229 (2.1954)\tPrec@1 88.281 (86.651)\tPrec@5 99.219 (99.045)\n",
      "Epoch: [66][90/97], lr: 0.01000\tTime 0.295 (0.304)\tData 0.000 (0.021)\tLoss 2.0891 (2.2092)\tPrec@1 86.719 (86.607)\tPrec@5 100.000 (99.081)\n",
      "Epoch: [66][96/97], lr: 0.01000\tTime 0.291 (0.303)\tData 0.000 (0.021)\tLoss 3.2296 (2.2304)\tPrec@1 82.203 (86.490)\tPrec@5 100.000 (99.073)\n",
      "Test: [0/100]\tTime 0.250 (0.250)\tLoss 8.8624 (8.8624)\tPrec@1 58.000 (58.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.072 (0.089)\tLoss 7.1188 (8.6004)\tPrec@1 67.000 (58.091)\tPrec@5 94.000 (96.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.4924 (8.2992)\tPrec@1 69.000 (59.429)\tPrec@5 96.000 (95.952)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.5151 (8.2282)\tPrec@1 64.000 (59.613)\tPrec@5 97.000 (96.161)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.0910 (8.2498)\tPrec@1 64.000 (59.634)\tPrec@5 93.000 (95.976)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.9769 (8.1584)\tPrec@1 63.000 (60.000)\tPrec@5 96.000 (95.922)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.5223 (8.2167)\tPrec@1 69.000 (59.492)\tPrec@5 97.000 (95.984)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.5582 (8.1928)\tPrec@1 62.000 (59.676)\tPrec@5 96.000 (96.042)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 7.9878 (8.1474)\tPrec@1 64.000 (60.148)\tPrec@5 95.000 (96.062)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.2740 (8.2156)\tPrec@1 58.000 (59.857)\tPrec@5 98.000 (95.956)\n",
      "val Results: Prec@1 59.920 Prec@5 96.030 Loss 8.22527\n",
      "val Class Accuracy: [0.894,0.969,0.676,0.882,0.665,0.569,0.115,0.538,0.486,0.198]\n",
      "Best Prec@1: 63.130\n",
      "\n",
      "Epoch: [67][0/97], lr: 0.01000\tTime 0.439 (0.439)\tData 0.279 (0.279)\tLoss 1.8297 (1.8297)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [67][10/97], lr: 0.01000\tTime 0.295 (0.312)\tData 0.000 (0.040)\tLoss 2.3230 (1.8933)\tPrec@1 85.938 (87.784)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [67][20/97], lr: 0.01000\tTime 0.295 (0.305)\tData 0.000 (0.029)\tLoss 2.2195 (2.0201)\tPrec@1 85.938 (87.463)\tPrec@5 98.438 (99.256)\n",
      "Epoch: [67][30/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.025)\tLoss 1.8578 (2.0780)\tPrec@1 88.281 (87.147)\tPrec@5 99.219 (99.168)\n",
      "Epoch: [67][40/97], lr: 0.01000\tTime 0.294 (0.301)\tData 0.000 (0.023)\tLoss 2.0168 (2.1265)\tPrec@1 87.500 (86.738)\tPrec@5 99.219 (99.104)\n",
      "Epoch: [67][50/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.022)\tLoss 2.2587 (2.1578)\tPrec@1 88.281 (86.566)\tPrec@5 99.219 (99.096)\n",
      "Epoch: [67][60/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 1.8079 (2.1707)\tPrec@1 88.281 (86.322)\tPrec@5 100.000 (99.091)\n",
      "Epoch: [67][70/97], lr: 0.01000\tTime 0.298 (0.300)\tData 0.000 (0.021)\tLoss 2.5743 (2.2252)\tPrec@1 85.938 (86.037)\tPrec@5 98.438 (98.966)\n",
      "Epoch: [67][80/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 2.5662 (2.2269)\tPrec@1 82.812 (86.044)\tPrec@5 99.219 (98.968)\n",
      "Epoch: [67][90/97], lr: 0.01000\tTime 0.292 (0.299)\tData 0.000 (0.020)\tLoss 1.6524 (2.2014)\tPrec@1 90.625 (86.238)\tPrec@5 99.219 (99.021)\n",
      "Epoch: [67][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.021)\tLoss 2.4166 (2.2148)\tPrec@1 84.746 (86.200)\tPrec@5 100.000 (99.017)\n",
      "Test: [0/100]\tTime 0.248 (0.248)\tLoss 10.4891 (10.4891)\tPrec@1 47.000 (47.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 6.8174 (9.4646)\tPrec@1 64.000 (53.818)\tPrec@5 97.000 (93.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 8.6472 (9.1504)\tPrec@1 53.000 (55.190)\tPrec@5 96.000 (93.524)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 8.6096 (9.1193)\tPrec@1 53.000 (55.581)\tPrec@5 95.000 (93.097)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.8796 (9.2213)\tPrec@1 52.000 (55.244)\tPrec@5 92.000 (93.122)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.7247 (9.1804)\tPrec@1 60.000 (55.314)\tPrec@5 93.000 (93.216)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 8.2247 (9.1900)\tPrec@1 60.000 (55.213)\tPrec@5 91.000 (93.148)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.3785 (9.1730)\tPrec@1 59.000 (55.380)\tPrec@5 94.000 (93.113)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 8.6329 (9.1388)\tPrec@1 62.000 (55.667)\tPrec@5 88.000 (93.173)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 8.9392 (9.1982)\tPrec@1 61.000 (55.396)\tPrec@5 96.000 (93.143)\n",
      "val Results: Prec@1 55.390 Prec@5 93.010 Loss 9.23108\n",
      "val Class Accuracy: [0.892,0.941,0.855,0.573,0.820,0.372,0.698,0.139,0.247,0.002]\n",
      "Best Prec@1: 63.130\n",
      "\n",
      "Epoch: [68][0/97], lr: 0.01000\tTime 0.370 (0.370)\tData 0.226 (0.226)\tLoss 1.7559 (1.7559)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [68][10/97], lr: 0.01000\tTime 0.296 (0.306)\tData 0.000 (0.035)\tLoss 2.3284 (2.0521)\tPrec@1 86.719 (87.429)\tPrec@5 99.219 (99.503)\n",
      "Epoch: [68][20/97], lr: 0.01000\tTime 0.303 (0.302)\tData 0.000 (0.027)\tLoss 1.7165 (2.0518)\tPrec@1 88.281 (86.942)\tPrec@5 100.000 (99.368)\n",
      "Epoch: [68][30/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.024)\tLoss 2.2111 (2.1628)\tPrec@1 88.281 (86.316)\tPrec@5 99.219 (99.320)\n",
      "Epoch: [68][40/97], lr: 0.01000\tTime 0.298 (0.300)\tData 0.000 (0.022)\tLoss 2.7231 (2.1978)\tPrec@1 85.156 (86.338)\tPrec@5 98.438 (99.333)\n",
      "Epoch: [68][50/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 2.4772 (2.2092)\tPrec@1 85.938 (86.152)\tPrec@5 98.438 (99.311)\n",
      "Epoch: [68][60/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 2.5727 (2.2249)\tPrec@1 83.594 (86.053)\tPrec@5 100.000 (99.334)\n",
      "Epoch: [68][70/97], lr: 0.01000\tTime 0.298 (0.300)\tData 0.000 (0.020)\tLoss 2.8491 (2.2352)\tPrec@1 82.812 (86.081)\tPrec@5 99.219 (99.263)\n",
      "Epoch: [68][80/97], lr: 0.01000\tTime 0.298 (0.300)\tData 0.000 (0.020)\tLoss 1.3815 (2.2196)\tPrec@1 92.188 (86.236)\tPrec@5 99.219 (99.267)\n",
      "Epoch: [68][90/97], lr: 0.01000\tTime 0.303 (0.299)\tData 0.000 (0.019)\tLoss 1.8658 (2.2350)\tPrec@1 89.844 (86.178)\tPrec@5 98.438 (99.227)\n",
      "Epoch: [68][96/97], lr: 0.01000\tTime 0.290 (0.299)\tData 0.000 (0.020)\tLoss 1.9294 (2.2258)\tPrec@1 90.678 (86.265)\tPrec@5 99.153 (99.226)\n",
      "Test: [0/100]\tTime 0.229 (0.229)\tLoss 8.9554 (8.9554)\tPrec@1 56.000 (56.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 7.2520 (7.7095)\tPrec@1 62.000 (61.364)\tPrec@5 92.000 (95.909)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 6.2394 (7.5837)\tPrec@1 64.000 (61.857)\tPrec@5 97.000 (95.857)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 6.9343 (7.5502)\tPrec@1 62.000 (62.323)\tPrec@5 96.000 (95.677)\n",
      "Test: [40/100]\tTime 0.072 (0.076)\tLoss 6.9645 (7.5794)\tPrec@1 65.000 (62.439)\tPrec@5 94.000 (95.537)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 6.5453 (7.5476)\tPrec@1 68.000 (62.686)\tPrec@5 97.000 (95.765)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 6.6238 (7.5274)\tPrec@1 65.000 (62.803)\tPrec@5 95.000 (95.656)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 6.7456 (7.5259)\tPrec@1 68.000 (62.803)\tPrec@5 97.000 (95.634)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 7.3001 (7.4622)\tPrec@1 65.000 (63.160)\tPrec@5 96.000 (95.728)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 6.5731 (7.4965)\tPrec@1 69.000 (63.110)\tPrec@5 100.000 (95.802)\n",
      "val Results: Prec@1 62.850 Prec@5 95.820 Loss 7.54103\n",
      "val Class Accuracy: [0.956,0.979,0.630,0.483,0.663,0.811,0.529,0.682,0.358,0.194]\n",
      "Best Prec@1: 63.130\n",
      "\n",
      "Epoch: [69][0/97], lr: 0.01000\tTime 0.454 (0.454)\tData 0.291 (0.291)\tLoss 2.6722 (2.6722)\tPrec@1 80.469 (80.469)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [69][10/97], lr: 0.01000\tTime 0.296 (0.314)\tData 0.000 (0.041)\tLoss 2.6160 (2.2661)\tPrec@1 84.375 (85.795)\tPrec@5 97.656 (99.077)\n",
      "Epoch: [69][20/97], lr: 0.01000\tTime 0.297 (0.306)\tData 0.000 (0.030)\tLoss 1.8653 (2.1890)\tPrec@1 85.938 (86.496)\tPrec@5 98.438 (99.256)\n",
      "Epoch: [69][30/97], lr: 0.01000\tTime 0.295 (0.303)\tData 0.000 (0.026)\tLoss 2.0616 (2.1352)\tPrec@1 86.719 (86.643)\tPrec@5 100.000 (99.269)\n",
      "Epoch: [69][40/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.024)\tLoss 1.7324 (2.1605)\tPrec@1 88.281 (86.528)\tPrec@5 100.000 (99.314)\n",
      "Epoch: [69][50/97], lr: 0.01000\tTime 0.294 (0.301)\tData 0.000 (0.022)\tLoss 2.4447 (2.1574)\tPrec@1 85.938 (86.657)\tPrec@5 100.000 (99.249)\n",
      "Epoch: [69][60/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.022)\tLoss 1.8024 (2.1262)\tPrec@1 87.500 (86.975)\tPrec@5 100.000 (99.244)\n",
      "Epoch: [69][70/97], lr: 0.01000\tTime 0.300 (0.300)\tData 0.000 (0.021)\tLoss 1.9736 (2.1271)\tPrec@1 88.281 (86.895)\tPrec@5 99.219 (99.230)\n",
      "Epoch: [69][80/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 2.7018 (2.1489)\tPrec@1 82.812 (86.796)\tPrec@5 97.656 (99.190)\n",
      "Epoch: [69][90/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 2.1129 (2.1727)\tPrec@1 87.500 (86.667)\tPrec@5 99.219 (99.150)\n",
      "Epoch: [69][96/97], lr: 0.01000\tTime 0.290 (0.299)\tData 0.000 (0.021)\tLoss 2.7526 (2.1904)\tPrec@1 83.898 (86.571)\tPrec@5 98.305 (99.154)\n",
      "Test: [0/100]\tTime 0.261 (0.261)\tLoss 10.6766 (10.6766)\tPrec@1 46.000 (46.000)\tPrec@5 90.000 (90.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 8.2504 (10.9568)\tPrec@1 58.000 (45.273)\tPrec@5 95.000 (89.273)\n",
      "Test: [20/100]\tTime 0.072 (0.082)\tLoss 9.9248 (10.7742)\tPrec@1 48.000 (46.238)\tPrec@5 92.000 (89.667)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 10.2993 (10.6781)\tPrec@1 50.000 (47.226)\tPrec@5 88.000 (88.871)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 11.7433 (10.7007)\tPrec@1 42.000 (47.024)\tPrec@5 87.000 (88.659)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 11.0216 (10.6321)\tPrec@1 44.000 (47.235)\tPrec@5 86.000 (88.824)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 10.0120 (10.7158)\tPrec@1 48.000 (46.738)\tPrec@5 88.000 (88.770)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 10.7982 (10.7085)\tPrec@1 44.000 (46.732)\tPrec@5 88.000 (88.887)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 10.9215 (10.6847)\tPrec@1 48.000 (46.901)\tPrec@5 82.000 (88.864)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 9.5249 (10.7171)\tPrec@1 55.000 (46.758)\tPrec@5 91.000 (88.868)\n",
      "val Results: Prec@1 46.740 Prec@5 88.790 Loss 10.72936\n",
      "val Class Accuracy: [0.909,0.795,0.911,0.510,0.233,0.248,0.576,0.068,0.391,0.033]\n",
      "Best Prec@1: 63.130\n",
      "\n",
      "Epoch: [70][0/97], lr: 0.01000\tTime 0.366 (0.366)\tData 0.211 (0.211)\tLoss 2.3397 (2.3397)\tPrec@1 87.500 (87.500)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [70][10/97], lr: 0.01000\tTime 0.295 (0.306)\tData 0.000 (0.034)\tLoss 2.5330 (2.3127)\tPrec@1 84.375 (85.582)\tPrec@5 100.000 (98.864)\n",
      "Epoch: [70][20/97], lr: 0.01000\tTime 0.294 (0.302)\tData 0.000 (0.026)\tLoss 1.6081 (2.2254)\tPrec@1 89.844 (86.049)\tPrec@5 100.000 (98.921)\n",
      "Epoch: [70][30/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.023)\tLoss 2.2710 (2.1942)\tPrec@1 87.500 (86.341)\tPrec@5 100.000 (98.992)\n",
      "Epoch: [70][40/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.022)\tLoss 2.2921 (2.2323)\tPrec@1 86.719 (86.052)\tPrec@5 99.219 (99.047)\n",
      "Epoch: [70][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 1.7659 (2.1957)\tPrec@1 89.844 (86.198)\tPrec@5 99.219 (99.096)\n",
      "Epoch: [70][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 2.4421 (2.2232)\tPrec@1 85.938 (86.155)\tPrec@5 98.438 (99.052)\n",
      "Epoch: [70][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.0924 (2.2233)\tPrec@1 89.844 (86.158)\tPrec@5 99.219 (99.010)\n",
      "Epoch: [70][80/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.1736 (2.1963)\tPrec@1 85.938 (86.304)\tPrec@5 99.219 (99.045)\n",
      "Epoch: [70][90/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.019)\tLoss 2.6459 (2.1981)\tPrec@1 82.031 (86.281)\tPrec@5 99.219 (99.038)\n",
      "Epoch: [70][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 2.3821 (2.2023)\tPrec@1 85.593 (86.241)\tPrec@5 99.153 (99.041)\n",
      "Test: [0/100]\tTime 0.252 (0.252)\tLoss 10.0899 (10.0899)\tPrec@1 51.000 (51.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 8.2771 (9.7716)\tPrec@1 58.000 (50.909)\tPrec@5 89.000 (94.000)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.4913 (9.7180)\tPrec@1 59.000 (50.714)\tPrec@5 94.000 (94.429)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.5715 (9.6508)\tPrec@1 48.000 (51.452)\tPrec@5 95.000 (94.194)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.5801 (9.6362)\tPrec@1 56.000 (51.585)\tPrec@5 94.000 (94.171)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.9057 (9.5945)\tPrec@1 57.000 (51.941)\tPrec@5 92.000 (94.275)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.0355 (9.6015)\tPrec@1 57.000 (51.492)\tPrec@5 94.000 (94.328)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 9.2904 (9.5786)\tPrec@1 52.000 (51.577)\tPrec@5 94.000 (94.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.1143 (9.5301)\tPrec@1 54.000 (51.790)\tPrec@5 93.000 (94.506)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.1793 (9.5860)\tPrec@1 54.000 (51.527)\tPrec@5 97.000 (94.560)\n",
      "val Results: Prec@1 51.630 Prec@5 94.490 Loss 9.60138\n",
      "val Class Accuracy: [0.993,0.975,0.687,0.309,0.497,0.563,0.381,0.457,0.296,0.005]\n",
      "Best Prec@1: 63.130\n",
      "\n",
      "Epoch: [71][0/97], lr: 0.01000\tTime 0.406 (0.406)\tData 0.244 (0.244)\tLoss 1.8081 (1.8081)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [71][10/97], lr: 0.01000\tTime 0.294 (0.309)\tData 0.000 (0.037)\tLoss 2.3388 (2.1456)\tPrec@1 89.844 (86.577)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [71][20/97], lr: 0.01000\tTime 0.296 (0.304)\tData 0.000 (0.028)\tLoss 2.8199 (2.2065)\tPrec@1 85.156 (86.570)\tPrec@5 98.438 (99.182)\n",
      "Epoch: [71][30/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.024)\tLoss 1.8423 (2.2262)\tPrec@1 86.719 (86.416)\tPrec@5 99.219 (99.168)\n",
      "Epoch: [71][40/97], lr: 0.01000\tTime 0.292 (0.301)\tData 0.000 (0.023)\tLoss 1.6749 (2.2404)\tPrec@1 89.844 (86.376)\tPrec@5 99.219 (99.276)\n",
      "Epoch: [71][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.022)\tLoss 2.9730 (2.2722)\tPrec@1 80.469 (86.198)\tPrec@5 99.219 (99.203)\n",
      "Epoch: [71][60/97], lr: 0.01000\tTime 0.298 (0.300)\tData 0.000 (0.021)\tLoss 2.3093 (2.3019)\tPrec@1 85.156 (85.912)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [71][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.0160 (2.2474)\tPrec@1 87.500 (86.213)\tPrec@5 99.219 (99.186)\n",
      "Epoch: [71][80/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.0115 (2.2544)\tPrec@1 90.625 (86.227)\tPrec@5 98.438 (99.199)\n",
      "Epoch: [71][90/97], lr: 0.01000\tTime 0.298 (0.299)\tData 0.000 (0.020)\tLoss 1.9243 (2.2476)\tPrec@1 88.281 (86.272)\tPrec@5 100.000 (99.176)\n",
      "Epoch: [71][96/97], lr: 0.01000\tTime 0.292 (0.299)\tData 0.000 (0.020)\tLoss 1.5324 (2.2524)\tPrec@1 92.373 (86.257)\tPrec@5 100.000 (99.210)\n",
      "Test: [0/100]\tTime 0.259 (0.259)\tLoss 7.8790 (7.8790)\tPrec@1 63.000 (63.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 5.9542 (7.6641)\tPrec@1 70.000 (61.091)\tPrec@5 97.000 (96.909)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.4835 (7.6352)\tPrec@1 67.000 (61.905)\tPrec@5 98.000 (97.000)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.4313 (7.7195)\tPrec@1 66.000 (61.839)\tPrec@5 97.000 (96.774)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.6582 (7.7378)\tPrec@1 65.000 (61.951)\tPrec@5 96.000 (96.780)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 7.1395 (7.6604)\tPrec@1 67.000 (62.490)\tPrec@5 99.000 (96.843)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.9602 (7.6752)\tPrec@1 66.000 (62.246)\tPrec@5 96.000 (96.852)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.5884 (7.6602)\tPrec@1 64.000 (62.225)\tPrec@5 98.000 (96.817)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 7.6596 (7.6490)\tPrec@1 63.000 (62.346)\tPrec@5 98.000 (96.864)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.8143 (7.6766)\tPrec@1 66.000 (62.385)\tPrec@5 95.000 (96.846)\n",
      "val Results: Prec@1 62.300 Prec@5 96.800 Loss 7.68354\n",
      "val Class Accuracy: [0.925,0.991,0.529,0.851,0.672,0.464,0.746,0.588,0.444,0.020]\n",
      "Best Prec@1: 63.130\n",
      "\n",
      "Epoch: [72][0/97], lr: 0.01000\tTime 0.395 (0.395)\tData 0.237 (0.237)\tLoss 2.1141 (2.1141)\tPrec@1 87.500 (87.500)\tPrec@5 96.875 (96.875)\n",
      "Epoch: [72][10/97], lr: 0.01000\tTime 0.297 (0.309)\tData 0.000 (0.037)\tLoss 2.4252 (1.9823)\tPrec@1 83.594 (87.855)\tPrec@5 100.000 (99.077)\n",
      "Epoch: [72][20/97], lr: 0.01000\tTime 0.297 (0.304)\tData 0.000 (0.028)\tLoss 2.4740 (2.1522)\tPrec@1 85.938 (86.942)\tPrec@5 99.219 (98.996)\n",
      "Epoch: [72][30/97], lr: 0.01000\tTime 0.300 (0.302)\tData 0.000 (0.024)\tLoss 1.7689 (2.1060)\tPrec@1 89.062 (87.046)\tPrec@5 100.000 (99.093)\n",
      "Epoch: [72][40/97], lr: 0.01000\tTime 0.304 (0.302)\tData 0.000 (0.023)\tLoss 2.1586 (2.1346)\tPrec@1 85.938 (86.928)\tPrec@5 98.438 (99.123)\n",
      "Epoch: [72][50/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.022)\tLoss 2.0868 (2.1671)\tPrec@1 87.500 (86.703)\tPrec@5 99.219 (99.112)\n",
      "Epoch: [72][60/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.021)\tLoss 2.3156 (2.1576)\tPrec@1 85.938 (86.744)\tPrec@5 100.000 (99.168)\n",
      "Epoch: [72][70/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.020)\tLoss 1.6000 (2.1474)\tPrec@1 91.406 (86.774)\tPrec@5 100.000 (99.153)\n",
      "Epoch: [72][80/97], lr: 0.01000\tTime 0.300 (0.301)\tData 0.000 (0.020)\tLoss 1.6651 (2.1506)\tPrec@1 89.062 (86.796)\tPrec@5 99.219 (99.103)\n",
      "Epoch: [72][90/97], lr: 0.01000\tTime 0.307 (0.300)\tData 0.000 (0.020)\tLoss 2.2119 (2.1544)\tPrec@1 84.375 (86.787)\tPrec@5 98.438 (99.047)\n",
      "Epoch: [72][96/97], lr: 0.01000\tTime 0.291 (0.300)\tData 0.000 (0.020)\tLoss 2.7200 (2.1642)\tPrec@1 84.746 (86.773)\tPrec@5 100.000 (99.057)\n",
      "Test: [0/100]\tTime 0.233 (0.233)\tLoss 8.7445 (8.7445)\tPrec@1 57.000 (57.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 6.1809 (7.6270)\tPrec@1 69.000 (62.818)\tPrec@5 97.000 (95.000)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 6.4801 (7.6117)\tPrec@1 65.000 (62.667)\tPrec@5 97.000 (95.524)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.1292 (7.6266)\tPrec@1 66.000 (62.484)\tPrec@5 92.000 (95.290)\n",
      "Test: [40/100]\tTime 0.072 (0.076)\tLoss 7.3976 (7.6546)\tPrec@1 65.000 (62.512)\tPrec@5 93.000 (95.293)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 7.3234 (7.5962)\tPrec@1 67.000 (62.922)\tPrec@5 97.000 (95.353)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 7.0909 (7.5623)\tPrec@1 61.000 (62.934)\tPrec@5 93.000 (95.295)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 6.9661 (7.5803)\tPrec@1 63.000 (62.803)\tPrec@5 96.000 (95.211)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 7.1751 (7.5335)\tPrec@1 66.000 (63.198)\tPrec@5 97.000 (95.309)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 6.9195 (7.6002)\tPrec@1 65.000 (62.813)\tPrec@5 96.000 (95.264)\n",
      "val Results: Prec@1 62.710 Prec@5 95.260 Loss 7.62875\n",
      "val Class Accuracy: [0.931,0.982,0.791,0.478,0.788,0.696,0.648,0.689,0.201,0.067]\n",
      "Best Prec@1: 63.130\n",
      "\n",
      "Epoch: [73][0/97], lr: 0.01000\tTime 0.392 (0.392)\tData 0.228 (0.228)\tLoss 2.0691 (2.0691)\tPrec@1 86.719 (86.719)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [73][10/97], lr: 0.01000\tTime 0.295 (0.307)\tData 0.000 (0.035)\tLoss 1.4338 (2.0976)\tPrec@1 92.969 (87.216)\tPrec@5 100.000 (98.935)\n",
      "Epoch: [73][20/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.027)\tLoss 1.4753 (2.1774)\tPrec@1 92.188 (86.868)\tPrec@5 99.219 (99.070)\n",
      "Epoch: [73][30/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.024)\tLoss 1.9251 (2.0962)\tPrec@1 87.500 (87.223)\tPrec@5 100.000 (99.294)\n",
      "Epoch: [73][40/97], lr: 0.01000\tTime 0.293 (0.300)\tData 0.000 (0.022)\tLoss 2.3059 (2.0896)\tPrec@1 85.156 (87.309)\tPrec@5 99.219 (99.314)\n",
      "Epoch: [73][50/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 1.9993 (2.1021)\tPrec@1 89.062 (87.286)\tPrec@5 100.000 (99.265)\n",
      "Epoch: [73][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 2.1034 (2.1149)\tPrec@1 85.156 (87.141)\tPrec@5 98.438 (99.193)\n",
      "Epoch: [73][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.3085 (2.1037)\tPrec@1 83.594 (87.148)\tPrec@5 99.219 (99.197)\n",
      "Epoch: [73][80/97], lr: 0.01000\tTime 0.302 (0.299)\tData 0.000 (0.020)\tLoss 3.0992 (2.1208)\tPrec@1 80.469 (86.950)\tPrec@5 97.656 (99.180)\n",
      "Epoch: [73][90/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.019)\tLoss 3.4662 (2.1442)\tPrec@1 77.344 (86.822)\tPrec@5 96.875 (99.099)\n",
      "Epoch: [73][96/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.0139 (2.1497)\tPrec@1 88.983 (86.764)\tPrec@5 99.153 (99.089)\n",
      "Test: [0/100]\tTime 0.244 (0.244)\tLoss 8.2556 (8.2556)\tPrec@1 58.000 (58.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 5.6695 (7.4613)\tPrec@1 74.000 (63.909)\tPrec@5 95.000 (94.727)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.4696 (7.2992)\tPrec@1 68.000 (64.286)\tPrec@5 96.000 (95.190)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 6.6522 (7.2719)\tPrec@1 69.000 (64.581)\tPrec@5 95.000 (94.968)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 6.7116 (7.3462)\tPrec@1 71.000 (64.122)\tPrec@5 93.000 (94.707)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.1907 (7.3443)\tPrec@1 70.000 (64.059)\tPrec@5 94.000 (94.725)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 6.1798 (7.3614)\tPrec@1 72.000 (64.016)\tPrec@5 96.000 (94.705)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 6.4974 (7.3310)\tPrec@1 66.000 (64.042)\tPrec@5 97.000 (94.817)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 7.1785 (7.2897)\tPrec@1 62.000 (64.259)\tPrec@5 91.000 (94.852)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 6.9586 (7.3278)\tPrec@1 66.000 (64.198)\tPrec@5 95.000 (94.791)\n",
      "val Results: Prec@1 64.050 Prec@5 94.760 Loss 7.36158\n",
      "val Class Accuracy: [0.856,0.980,0.851,0.498,0.794,0.674,0.749,0.290,0.606,0.107]\n",
      "Best Prec@1: 64.050\n",
      "\n",
      "Epoch: [74][0/97], lr: 0.01000\tTime 0.368 (0.368)\tData 0.213 (0.213)\tLoss 1.7205 (1.7205)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [74][10/97], lr: 0.01000\tTime 0.296 (0.306)\tData 0.000 (0.034)\tLoss 1.2893 (1.8826)\tPrec@1 93.750 (88.778)\tPrec@5 99.219 (99.716)\n",
      "Epoch: [74][20/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.026)\tLoss 1.7520 (2.0107)\tPrec@1 91.406 (88.207)\tPrec@5 99.219 (99.554)\n",
      "Epoch: [74][30/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.023)\tLoss 2.0615 (2.0151)\tPrec@1 85.938 (88.029)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [74][40/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.022)\tLoss 3.0156 (2.0424)\tPrec@1 83.594 (87.938)\tPrec@5 96.875 (99.200)\n",
      "Epoch: [74][50/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 2.4452 (2.0905)\tPrec@1 86.719 (87.699)\tPrec@5 99.219 (99.112)\n",
      "Epoch: [74][60/97], lr: 0.01000\tTime 0.299 (0.299)\tData 0.000 (0.020)\tLoss 2.3669 (2.1150)\tPrec@1 85.938 (87.398)\tPrec@5 100.000 (99.116)\n",
      "Epoch: [74][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.7884 (2.0800)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (99.142)\n",
      "Epoch: [74][80/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 2.7205 (2.0961)\tPrec@1 85.156 (87.355)\tPrec@5 99.219 (99.180)\n",
      "Epoch: [74][90/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.019)\tLoss 2.2650 (2.1082)\tPrec@1 88.281 (87.285)\tPrec@5 99.219 (99.184)\n",
      "Epoch: [74][96/97], lr: 0.01000\tTime 0.289 (0.298)\tData 0.000 (0.020)\tLoss 2.0476 (2.1010)\tPrec@1 86.441 (87.280)\tPrec@5 100.000 (99.194)\n",
      "Test: [0/100]\tTime 0.270 (0.270)\tLoss 9.2366 (9.2366)\tPrec@1 54.000 (54.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 7.3171 (9.1158)\tPrec@1 61.000 (54.273)\tPrec@5 96.000 (95.455)\n",
      "Test: [20/100]\tTime 0.072 (0.082)\tLoss 8.6035 (9.0672)\tPrec@1 54.000 (55.048)\tPrec@5 95.000 (95.238)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 8.9401 (8.9662)\tPrec@1 55.000 (55.613)\tPrec@5 93.000 (94.871)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.6472 (9.0009)\tPrec@1 50.000 (55.512)\tPrec@5 92.000 (94.659)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.7163 (8.9623)\tPrec@1 56.000 (55.569)\tPrec@5 98.000 (94.804)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.9676 (8.9977)\tPrec@1 58.000 (55.213)\tPrec@5 97.000 (94.918)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.1907 (8.9989)\tPrec@1 60.000 (55.183)\tPrec@5 92.000 (94.873)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.0835 (8.9517)\tPrec@1 61.000 (55.407)\tPrec@5 94.000 (94.901)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.6484 (9.0237)\tPrec@1 57.000 (55.099)\tPrec@5 97.000 (94.901)\n",
      "val Results: Prec@1 55.020 Prec@5 94.810 Loss 9.05165\n",
      "val Class Accuracy: [0.956,0.953,0.909,0.468,0.538,0.219,0.576,0.685,0.134,0.064]\n",
      "Best Prec@1: 64.050\n",
      "\n",
      "Epoch: [75][0/97], lr: 0.01000\tTime 0.406 (0.406)\tData 0.252 (0.252)\tLoss 2.0215 (2.0215)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [75][10/97], lr: 0.01000\tTime 0.296 (0.309)\tData 0.000 (0.038)\tLoss 1.8755 (2.1848)\tPrec@1 88.281 (86.932)\tPrec@5 98.438 (98.935)\n",
      "Epoch: [75][20/97], lr: 0.01000\tTime 0.295 (0.303)\tData 0.000 (0.028)\tLoss 2.0651 (2.1850)\tPrec@1 87.500 (86.644)\tPrec@5 99.219 (99.107)\n",
      "Epoch: [75][30/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.024)\tLoss 2.4317 (2.1304)\tPrec@1 84.375 (86.971)\tPrec@5 100.000 (99.269)\n",
      "Epoch: [75][40/97], lr: 0.01000\tTime 0.290 (0.300)\tData 0.000 (0.023)\tLoss 1.8960 (2.1353)\tPrec@1 89.062 (86.852)\tPrec@5 99.219 (99.238)\n",
      "Epoch: [75][50/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.022)\tLoss 1.8087 (2.1516)\tPrec@1 89.844 (86.673)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [75][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 2.3836 (2.1639)\tPrec@1 85.938 (86.565)\tPrec@5 100.000 (99.180)\n",
      "Epoch: [75][70/97], lr: 0.01000\tTime 0.300 (0.299)\tData 0.000 (0.020)\tLoss 2.4476 (2.1518)\tPrec@1 84.375 (86.598)\tPrec@5 100.000 (99.241)\n",
      "Epoch: [75][80/97], lr: 0.01000\tTime 0.299 (0.299)\tData 0.000 (0.020)\tLoss 2.2995 (2.1585)\tPrec@1 86.719 (86.564)\tPrec@5 99.219 (99.267)\n",
      "Epoch: [75][90/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 2.1870 (2.1604)\tPrec@1 87.500 (86.650)\tPrec@5 99.219 (99.253)\n",
      "Epoch: [75][96/97], lr: 0.01000\tTime 0.291 (0.299)\tData 0.000 (0.020)\tLoss 2.4645 (2.1668)\tPrec@1 83.051 (86.635)\tPrec@5 99.153 (99.226)\n",
      "Test: [0/100]\tTime 0.253 (0.253)\tLoss 8.5406 (8.5406)\tPrec@1 58.000 (58.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.2800 (7.9815)\tPrec@1 62.000 (61.818)\tPrec@5 93.000 (95.636)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.4826 (7.9060)\tPrec@1 64.000 (61.238)\tPrec@5 99.000 (95.810)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 6.9413 (7.8079)\tPrec@1 62.000 (61.935)\tPrec@5 98.000 (95.774)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.2295 (7.8488)\tPrec@1 65.000 (61.634)\tPrec@5 94.000 (95.366)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.7538 (7.8075)\tPrec@1 62.000 (61.588)\tPrec@5 95.000 (95.412)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.5191 (7.8498)\tPrec@1 68.000 (61.230)\tPrec@5 94.000 (95.311)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.1634 (7.8460)\tPrec@1 65.000 (61.197)\tPrec@5 98.000 (95.465)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 7.5556 (7.8307)\tPrec@1 63.000 (61.222)\tPrec@5 97.000 (95.654)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.8933 (7.9053)\tPrec@1 58.000 (60.846)\tPrec@5 99.000 (95.648)\n",
      "val Results: Prec@1 60.850 Prec@5 95.650 Loss 7.91228\n",
      "val Class Accuracy: [0.978,0.945,0.739,0.719,0.713,0.425,0.341,0.568,0.612,0.045]\n",
      "Best Prec@1: 64.050\n",
      "\n",
      "Epoch: [76][0/97], lr: 0.01000\tTime 0.423 (0.423)\tData 0.272 (0.272)\tLoss 2.1630 (2.1630)\tPrec@1 90.625 (90.625)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [76][10/97], lr: 0.01000\tTime 0.301 (0.312)\tData 0.000 (0.040)\tLoss 2.1265 (1.9655)\tPrec@1 85.938 (88.778)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [76][20/97], lr: 0.01000\tTime 0.294 (0.304)\tData 0.000 (0.029)\tLoss 2.0583 (2.0317)\tPrec@1 86.719 (87.984)\tPrec@5 99.219 (99.256)\n",
      "Epoch: [76][30/97], lr: 0.01000\tTime 0.299 (0.302)\tData 0.000 (0.025)\tLoss 2.0423 (2.0354)\tPrec@1 87.500 (88.054)\tPrec@5 98.438 (99.168)\n",
      "Epoch: [76][40/97], lr: 0.01000\tTime 0.293 (0.301)\tData 0.000 (0.023)\tLoss 2.3709 (2.1066)\tPrec@1 83.594 (87.424)\tPrec@5 99.219 (99.123)\n",
      "Epoch: [76][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.022)\tLoss 2.4605 (2.1055)\tPrec@1 82.812 (87.316)\tPrec@5 100.000 (99.173)\n",
      "Epoch: [76][60/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 2.6091 (2.1429)\tPrec@1 82.812 (87.026)\tPrec@5 99.219 (99.180)\n",
      "Epoch: [76][70/97], lr: 0.01000\tTime 0.293 (0.299)\tData 0.000 (0.021)\tLoss 1.8768 (2.1336)\tPrec@1 86.719 (87.027)\tPrec@5 99.219 (99.186)\n",
      "Epoch: [76][80/97], lr: 0.01000\tTime 0.293 (0.299)\tData 0.000 (0.020)\tLoss 1.8610 (2.1322)\tPrec@1 88.281 (87.008)\tPrec@5 100.000 (99.199)\n",
      "Epoch: [76][90/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 2.4577 (2.1153)\tPrec@1 84.375 (87.062)\tPrec@5 98.438 (99.202)\n",
      "Epoch: [76][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 2.2946 (2.1139)\tPrec@1 86.441 (87.079)\tPrec@5 99.153 (99.218)\n",
      "Test: [0/100]\tTime 0.243 (0.243)\tLoss 9.3317 (9.3317)\tPrec@1 54.000 (54.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 7.1947 (8.9114)\tPrec@1 61.000 (55.455)\tPrec@5 97.000 (96.091)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 8.1665 (8.8887)\tPrec@1 56.000 (55.333)\tPrec@5 97.000 (95.381)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 8.5183 (8.9670)\tPrec@1 59.000 (55.258)\tPrec@5 95.000 (95.548)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.2204 (9.0160)\tPrec@1 52.000 (55.220)\tPrec@5 93.000 (95.317)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.4052 (8.9353)\tPrec@1 60.000 (55.529)\tPrec@5 93.000 (95.294)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.9293 (8.9576)\tPrec@1 59.000 (55.426)\tPrec@5 94.000 (95.066)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.4533 (8.9346)\tPrec@1 59.000 (55.690)\tPrec@5 97.000 (95.183)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.5933 (8.8963)\tPrec@1 58.000 (55.790)\tPrec@5 94.000 (95.173)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.0047 (8.9238)\tPrec@1 53.000 (55.637)\tPrec@5 98.000 (95.154)\n",
      "val Results: Prec@1 55.630 Prec@5 95.100 Loss 8.93935\n",
      "val Class Accuracy: [0.978,0.991,0.723,0.780,0.500,0.175,0.690,0.365,0.318,0.043]\n",
      "Best Prec@1: 64.050\n",
      "\n",
      "Epoch: [77][0/97], lr: 0.01000\tTime 0.412 (0.412)\tData 0.242 (0.242)\tLoss 1.7173 (1.7173)\tPrec@1 90.625 (90.625)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [77][10/97], lr: 0.01000\tTime 0.298 (0.312)\tData 0.000 (0.037)\tLoss 2.1474 (1.8593)\tPrec@1 83.594 (88.281)\tPrec@5 99.219 (99.361)\n",
      "Epoch: [77][20/97], lr: 0.01000\tTime 0.299 (0.306)\tData 0.000 (0.027)\tLoss 2.2962 (1.9337)\tPrec@1 85.156 (88.318)\tPrec@5 99.219 (99.368)\n",
      "Epoch: [77][30/97], lr: 0.01000\tTime 0.298 (0.304)\tData 0.000 (0.024)\tLoss 1.7692 (1.9909)\tPrec@1 90.625 (88.155)\tPrec@5 99.219 (99.194)\n",
      "Epoch: [77][40/97], lr: 0.01000\tTime 0.294 (0.303)\tData 0.000 (0.023)\tLoss 1.7161 (2.0449)\tPrec@1 89.844 (87.691)\tPrec@5 100.000 (99.295)\n",
      "Epoch: [77][50/97], lr: 0.01000\tTime 0.294 (0.302)\tData 0.000 (0.022)\tLoss 2.5712 (2.0722)\tPrec@1 88.281 (87.592)\tPrec@5 97.656 (99.234)\n",
      "Epoch: [77][60/97], lr: 0.01000\tTime 0.297 (0.302)\tData 0.000 (0.021)\tLoss 2.1489 (2.0888)\tPrec@1 89.062 (87.398)\tPrec@5 100.000 (99.232)\n",
      "Epoch: [77][70/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.020)\tLoss 2.3516 (2.0961)\tPrec@1 85.938 (87.236)\tPrec@5 98.438 (99.230)\n",
      "Epoch: [77][80/97], lr: 0.01000\tTime 0.294 (0.301)\tData 0.000 (0.020)\tLoss 2.3469 (2.1048)\tPrec@1 85.938 (87.182)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [77][90/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.020)\tLoss 2.2388 (2.1180)\tPrec@1 86.719 (87.105)\tPrec@5 100.000 (99.236)\n",
      "Epoch: [77][96/97], lr: 0.01000\tTime 0.291 (0.301)\tData 0.000 (0.020)\tLoss 2.8245 (2.1201)\tPrec@1 80.508 (87.079)\tPrec@5 99.153 (99.218)\n",
      "Test: [0/100]\tTime 0.242 (0.242)\tLoss 10.5567 (10.5567)\tPrec@1 46.000 (46.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 7.9960 (9.5606)\tPrec@1 61.000 (52.182)\tPrec@5 97.000 (95.000)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 7.6959 (9.5270)\tPrec@1 59.000 (52.190)\tPrec@5 92.000 (94.810)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 9.2596 (9.4797)\tPrec@1 52.000 (52.355)\tPrec@5 93.000 (94.387)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.2983 (9.4979)\tPrec@1 52.000 (52.171)\tPrec@5 90.000 (94.268)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.9690 (9.4380)\tPrec@1 57.000 (52.412)\tPrec@5 89.000 (94.431)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 7.8552 (9.4926)\tPrec@1 64.000 (52.049)\tPrec@5 95.000 (94.410)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.4681 (9.4560)\tPrec@1 52.000 (52.254)\tPrec@5 96.000 (94.592)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.8604 (9.4147)\tPrec@1 56.000 (52.321)\tPrec@5 94.000 (94.679)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 8.9863 (9.4972)\tPrec@1 56.000 (51.956)\tPrec@5 96.000 (94.769)\n",
      "val Results: Prec@1 51.880 Prec@5 94.620 Loss 9.52354\n",
      "val Class Accuracy: [0.968,0.997,0.881,0.434,0.596,0.507,0.224,0.268,0.236,0.077]\n",
      "Best Prec@1: 64.050\n",
      "\n",
      "Epoch: [78][0/97], lr: 0.01000\tTime 0.346 (0.346)\tData 0.201 (0.201)\tLoss 1.4291 (1.4291)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [78][10/97], lr: 0.01000\tTime 0.294 (0.305)\tData 0.000 (0.033)\tLoss 2.2145 (2.0066)\tPrec@1 85.938 (87.997)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [78][20/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.026)\tLoss 1.7648 (2.0317)\tPrec@1 89.062 (87.649)\tPrec@5 98.438 (99.182)\n",
      "Epoch: [78][30/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.023)\tLoss 2.7776 (2.0624)\tPrec@1 78.906 (87.273)\tPrec@5 99.219 (99.093)\n",
      "Epoch: [78][40/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.022)\tLoss 3.1381 (2.0600)\tPrec@1 82.031 (87.405)\tPrec@5 98.438 (99.123)\n",
      "Epoch: [78][50/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 1.4609 (2.0564)\tPrec@1 93.750 (87.485)\tPrec@5 99.219 (99.050)\n",
      "Epoch: [78][60/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.020)\tLoss 2.1067 (2.0744)\tPrec@1 87.500 (87.398)\tPrec@5 99.219 (99.065)\n",
      "Epoch: [78][70/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 1.7323 (2.0604)\tPrec@1 91.406 (87.511)\tPrec@5 99.219 (99.142)\n",
      "Epoch: [78][80/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.019)\tLoss 2.1132 (2.0651)\tPrec@1 87.500 (87.384)\tPrec@5 99.219 (99.151)\n",
      "Epoch: [78][90/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.019)\tLoss 2.0721 (2.0564)\tPrec@1 86.719 (87.448)\tPrec@5 100.000 (99.133)\n",
      "Epoch: [78][96/97], lr: 0.01000\tTime 0.290 (0.298)\tData 0.000 (0.020)\tLoss 2.1541 (2.0587)\tPrec@1 88.136 (87.401)\tPrec@5 99.153 (99.129)\n",
      "Test: [0/100]\tTime 0.285 (0.285)\tLoss 7.7155 (7.7155)\tPrec@1 65.000 (65.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.072 (0.092)\tLoss 6.1057 (7.2549)\tPrec@1 73.000 (65.182)\tPrec@5 96.000 (97.182)\n",
      "Test: [20/100]\tTime 0.072 (0.082)\tLoss 5.2170 (7.1499)\tPrec@1 76.000 (65.571)\tPrec@5 98.000 (96.857)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 6.4444 (7.1566)\tPrec@1 72.000 (65.419)\tPrec@5 93.000 (96.839)\n",
      "Test: [40/100]\tTime 0.072 (0.078)\tLoss 6.9806 (7.1912)\tPrec@1 70.000 (65.220)\tPrec@5 92.000 (96.707)\n",
      "Test: [50/100]\tTime 0.072 (0.077)\tLoss 6.5302 (7.1302)\tPrec@1 69.000 (65.412)\tPrec@5 98.000 (96.745)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 5.8765 (7.1308)\tPrec@1 71.000 (65.656)\tPrec@5 98.000 (96.754)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 6.3599 (7.1000)\tPrec@1 71.000 (65.986)\tPrec@5 98.000 (96.732)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.9238 (7.0719)\tPrec@1 69.000 (66.099)\tPrec@5 96.000 (96.802)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.8826 (7.1141)\tPrec@1 69.000 (65.835)\tPrec@5 98.000 (96.879)\n",
      "val Results: Prec@1 65.650 Prec@5 96.830 Loss 7.14507\n",
      "val Class Accuracy: [0.931,0.994,0.732,0.700,0.706,0.690,0.609,0.576,0.412,0.215]\n",
      "Best Prec@1: 65.650\n",
      "\n",
      "Epoch: [79][0/97], lr: 0.01000\tTime 0.394 (0.394)\tData 0.234 (0.234)\tLoss 1.9294 (1.9294)\tPrec@1 87.500 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [79][10/97], lr: 0.01000\tTime 0.295 (0.310)\tData 0.000 (0.036)\tLoss 1.7667 (2.1401)\tPrec@1 89.844 (86.222)\tPrec@5 100.000 (99.148)\n",
      "Epoch: [79][20/97], lr: 0.01000\tTime 0.296 (0.304)\tData 0.000 (0.027)\tLoss 1.2423 (2.0877)\tPrec@1 93.750 (86.682)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [79][30/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.024)\tLoss 1.7135 (2.0832)\tPrec@1 88.281 (86.845)\tPrec@5 100.000 (99.269)\n",
      "Epoch: [79][40/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.022)\tLoss 1.9166 (2.0849)\tPrec@1 87.500 (86.814)\tPrec@5 99.219 (99.295)\n",
      "Epoch: [79][50/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 1.8891 (2.0585)\tPrec@1 89.062 (87.178)\tPrec@5 100.000 (99.341)\n",
      "Epoch: [79][60/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 1.9093 (2.0381)\tPrec@1 86.719 (87.333)\tPrec@5 100.000 (99.360)\n",
      "Epoch: [79][70/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 1.9906 (2.0433)\tPrec@1 87.500 (87.313)\tPrec@5 99.219 (99.373)\n",
      "Epoch: [79][80/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 2.3282 (2.0546)\tPrec@1 86.719 (87.201)\tPrec@5 97.656 (99.306)\n",
      "Epoch: [79][90/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 1.7279 (2.0942)\tPrec@1 89.844 (86.959)\tPrec@5 100.000 (99.305)\n",
      "Epoch: [79][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 2.2176 (2.1140)\tPrec@1 85.593 (86.837)\tPrec@5 100.000 (99.299)\n",
      "Test: [0/100]\tTime 0.245 (0.245)\tLoss 7.0064 (7.0064)\tPrec@1 65.000 (65.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 5.8455 (6.7374)\tPrec@1 72.000 (67.182)\tPrec@5 92.000 (96.364)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 6.1543 (6.7518)\tPrec@1 67.000 (67.048)\tPrec@5 98.000 (96.524)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 6.6938 (6.7369)\tPrec@1 70.000 (67.419)\tPrec@5 97.000 (96.484)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.8565 (6.7517)\tPrec@1 74.000 (67.390)\tPrec@5 96.000 (96.341)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.4508 (6.7008)\tPrec@1 68.000 (67.686)\tPrec@5 96.000 (96.353)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 6.1433 (6.7349)\tPrec@1 69.000 (67.475)\tPrec@5 97.000 (96.377)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 5.8371 (6.7197)\tPrec@1 74.000 (67.634)\tPrec@5 99.000 (96.437)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.8065 (6.7001)\tPrec@1 69.000 (67.765)\tPrec@5 97.000 (96.519)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 7.1023 (6.7404)\tPrec@1 65.000 (67.659)\tPrec@5 97.000 (96.440)\n",
      "val Results: Prec@1 67.690 Prec@5 96.400 Loss 6.74901\n",
      "val Class Accuracy: [0.856,0.990,0.683,0.815,0.838,0.553,0.553,0.659,0.600,0.222]\n",
      "Best Prec@1: 67.690\n",
      "\n",
      "Epoch: [80][0/97], lr: 0.01000\tTime 0.371 (0.371)\tData 0.230 (0.230)\tLoss 2.2004 (2.2004)\tPrec@1 85.156 (85.156)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [80][10/97], lr: 0.01000\tTime 0.295 (0.306)\tData 0.000 (0.036)\tLoss 2.0194 (2.2011)\tPrec@1 89.062 (87.429)\tPrec@5 99.219 (99.361)\n",
      "Epoch: [80][20/97], lr: 0.01000\tTime 0.294 (0.302)\tData 0.000 (0.027)\tLoss 1.7864 (2.0176)\tPrec@1 89.844 (88.021)\tPrec@5 99.219 (99.256)\n",
      "Epoch: [80][30/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.024)\tLoss 2.2896 (1.9492)\tPrec@1 86.719 (88.458)\tPrec@5 100.000 (99.269)\n",
      "Epoch: [80][40/97], lr: 0.01000\tTime 0.293 (0.300)\tData 0.000 (0.022)\tLoss 1.9041 (1.9797)\tPrec@1 89.844 (88.224)\tPrec@5 100.000 (99.181)\n",
      "Epoch: [80][50/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 2.4767 (2.0201)\tPrec@1 82.812 (87.944)\tPrec@5 98.438 (99.173)\n",
      "Epoch: [80][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 1.5746 (2.0431)\tPrec@1 90.625 (87.731)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [80][70/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.020)\tLoss 1.4632 (2.0603)\tPrec@1 91.406 (87.566)\tPrec@5 100.000 (99.263)\n",
      "Epoch: [80][80/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 2.1977 (2.0899)\tPrec@1 88.281 (87.394)\tPrec@5 99.219 (99.238)\n",
      "Epoch: [80][90/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.019)\tLoss 1.8304 (2.0848)\tPrec@1 89.844 (87.431)\tPrec@5 100.000 (99.245)\n",
      "Epoch: [80][96/97], lr: 0.01000\tTime 0.289 (0.298)\tData 0.000 (0.020)\tLoss 2.4762 (2.0835)\tPrec@1 82.203 (87.417)\tPrec@5 97.458 (99.218)\n",
      "Test: [0/100]\tTime 0.245 (0.245)\tLoss 7.6762 (7.6762)\tPrec@1 59.000 (59.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 6.1634 (7.5945)\tPrec@1 66.000 (62.818)\tPrec@5 97.000 (96.182)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.9669 (7.4721)\tPrec@1 65.000 (63.238)\tPrec@5 95.000 (95.905)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 6.6511 (7.4439)\tPrec@1 67.000 (63.323)\tPrec@5 96.000 (96.065)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 7.3730 (7.5435)\tPrec@1 64.000 (63.049)\tPrec@5 97.000 (95.976)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.1225 (7.5424)\tPrec@1 68.000 (63.118)\tPrec@5 97.000 (96.255)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 6.6732 (7.5456)\tPrec@1 64.000 (62.885)\tPrec@5 98.000 (96.246)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.6872 (7.5278)\tPrec@1 60.000 (62.972)\tPrec@5 96.000 (96.296)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.1310 (7.4797)\tPrec@1 65.000 (63.148)\tPrec@5 97.000 (96.420)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 7.6760 (7.5285)\tPrec@1 60.000 (62.835)\tPrec@5 100.000 (96.462)\n",
      "val Results: Prec@1 62.690 Prec@5 96.420 Loss 7.55975\n",
      "val Class Accuracy: [0.953,0.983,0.688,0.590,0.754,0.456,0.741,0.554,0.497,0.053]\n",
      "Best Prec@1: 67.690\n",
      "\n",
      "Epoch: [81][0/97], lr: 0.01000\tTime 0.402 (0.402)\tData 0.241 (0.241)\tLoss 2.3026 (2.3026)\tPrec@1 87.500 (87.500)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [81][10/97], lr: 0.01000\tTime 0.295 (0.310)\tData 0.000 (0.037)\tLoss 1.7534 (1.9230)\tPrec@1 89.844 (88.849)\tPrec@5 98.438 (99.148)\n",
      "Epoch: [81][20/97], lr: 0.01000\tTime 0.289 (0.304)\tData 0.000 (0.028)\tLoss 2.5885 (1.9801)\tPrec@1 85.156 (87.760)\tPrec@5 98.438 (99.182)\n",
      "Epoch: [81][30/97], lr: 0.01000\tTime 0.293 (0.302)\tData 0.000 (0.024)\tLoss 1.9625 (1.9186)\tPrec@1 89.062 (88.382)\tPrec@5 99.219 (99.294)\n",
      "Epoch: [81][40/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.023)\tLoss 2.3432 (1.9073)\tPrec@1 87.500 (88.186)\tPrec@5 96.875 (99.276)\n",
      "Epoch: [81][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.022)\tLoss 2.4718 (1.9740)\tPrec@1 85.156 (87.822)\tPrec@5 100.000 (99.280)\n",
      "Epoch: [81][60/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 1.8820 (1.9807)\tPrec@1 90.625 (87.833)\tPrec@5 100.000 (99.321)\n",
      "Epoch: [81][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.9687 (2.0086)\tPrec@1 81.250 (87.687)\tPrec@5 98.438 (99.296)\n",
      "Epoch: [81][80/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 2.3442 (2.0336)\tPrec@1 85.938 (87.500)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [81][90/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.9600 (2.0514)\tPrec@1 92.188 (87.466)\tPrec@5 97.656 (99.287)\n",
      "Epoch: [81][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 1.7536 (2.0536)\tPrec@1 88.983 (87.482)\tPrec@5 100.000 (99.291)\n",
      "Test: [0/100]\tTime 0.249 (0.249)\tLoss 8.3227 (8.3227)\tPrec@1 64.000 (64.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 6.2704 (7.6337)\tPrec@1 70.000 (63.818)\tPrec@5 97.000 (94.909)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.5308 (7.5694)\tPrec@1 70.000 (64.714)\tPrec@5 92.000 (94.429)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 6.7996 (7.5357)\tPrec@1 69.000 (64.774)\tPrec@5 96.000 (94.452)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.2763 (7.5637)\tPrec@1 66.000 (64.878)\tPrec@5 92.000 (94.293)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.8322 (7.4923)\tPrec@1 68.000 (65.176)\tPrec@5 97.000 (94.275)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 6.6873 (7.4324)\tPrec@1 69.000 (65.311)\tPrec@5 92.000 (94.344)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 6.8625 (7.4114)\tPrec@1 70.000 (65.268)\tPrec@5 96.000 (94.239)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.3512 (7.3506)\tPrec@1 72.000 (65.543)\tPrec@5 95.000 (94.296)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.2384 (7.3976)\tPrec@1 66.000 (65.242)\tPrec@5 94.000 (94.308)\n",
      "val Results: Prec@1 65.280 Prec@5 94.230 Loss 7.41037\n",
      "val Class Accuracy: [0.959,0.965,0.733,0.769,0.784,0.538,0.713,0.733,0.153,0.181]\n",
      "Best Prec@1: 67.690\n",
      "\n",
      "Epoch: [82][0/97], lr: 0.01000\tTime 0.474 (0.474)\tData 0.309 (0.309)\tLoss 2.1875 (2.1875)\tPrec@1 88.281 (88.281)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [82][10/97], lr: 0.01000\tTime 0.295 (0.315)\tData 0.000 (0.043)\tLoss 1.8001 (1.8759)\tPrec@1 88.281 (88.139)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [82][20/97], lr: 0.01000\tTime 0.292 (0.307)\tData 0.000 (0.031)\tLoss 1.5556 (1.8575)\tPrec@1 89.062 (88.095)\tPrec@5 100.000 (99.368)\n",
      "Epoch: [82][30/97], lr: 0.01000\tTime 0.296 (0.303)\tData 0.000 (0.026)\tLoss 1.8405 (1.8704)\tPrec@1 89.062 (88.180)\tPrec@5 99.219 (99.320)\n",
      "Epoch: [82][40/97], lr: 0.01000\tTime 0.297 (0.302)\tData 0.000 (0.024)\tLoss 2.5076 (1.9580)\tPrec@1 82.031 (87.481)\tPrec@5 99.219 (99.257)\n",
      "Epoch: [82][50/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.023)\tLoss 1.8769 (1.9445)\tPrec@1 89.062 (87.822)\tPrec@5 97.656 (99.188)\n",
      "Epoch: [82][60/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.022)\tLoss 2.1758 (1.9748)\tPrec@1 86.719 (87.692)\tPrec@5 100.000 (99.168)\n",
      "Epoch: [82][70/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 2.5579 (1.9866)\tPrec@1 84.375 (87.764)\tPrec@5 99.219 (99.175)\n",
      "Epoch: [82][80/97], lr: 0.01000\tTime 0.298 (0.300)\tData 0.000 (0.021)\tLoss 2.0315 (2.0366)\tPrec@1 87.500 (87.394)\tPrec@5 100.000 (99.151)\n",
      "Epoch: [82][90/97], lr: 0.01000\tTime 0.292 (0.300)\tData 0.000 (0.020)\tLoss 2.2625 (2.0398)\tPrec@1 85.156 (87.328)\tPrec@5 99.219 (99.176)\n",
      "Epoch: [82][96/97], lr: 0.01000\tTime 0.287 (0.299)\tData 0.000 (0.021)\tLoss 2.0303 (2.0475)\tPrec@1 88.136 (87.337)\tPrec@5 100.000 (99.202)\n",
      "Test: [0/100]\tTime 0.258 (0.258)\tLoss 8.9883 (8.9883)\tPrec@1 59.000 (59.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 6.8838 (8.0118)\tPrec@1 66.000 (60.727)\tPrec@5 93.000 (96.091)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 6.7709 (7.9989)\tPrec@1 64.000 (60.286)\tPrec@5 95.000 (96.095)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 7.3915 (7.9099)\tPrec@1 62.000 (61.065)\tPrec@5 96.000 (96.129)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.5660 (7.8681)\tPrec@1 66.000 (61.244)\tPrec@5 95.000 (96.049)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.2627 (7.8204)\tPrec@1 66.000 (61.529)\tPrec@5 98.000 (96.314)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7077 (7.8566)\tPrec@1 65.000 (61.197)\tPrec@5 96.000 (96.426)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.6851 (7.8546)\tPrec@1 62.000 (61.239)\tPrec@5 97.000 (96.437)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 7.5862 (7.8376)\tPrec@1 62.000 (61.160)\tPrec@5 97.000 (96.556)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 7.4723 (7.9047)\tPrec@1 63.000 (60.791)\tPrec@5 98.000 (96.527)\n",
      "val Results: Prec@1 60.700 Prec@5 96.510 Loss 7.92643\n",
      "val Class Accuracy: [0.970,0.937,0.857,0.474,0.663,0.621,0.498,0.498,0.424,0.128]\n",
      "Best Prec@1: 67.690\n",
      "\n",
      "Epoch: [83][0/97], lr: 0.01000\tTime 0.347 (0.347)\tData 0.194 (0.194)\tLoss 1.6633 (1.6633)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [83][10/97], lr: 0.01000\tTime 0.297 (0.306)\tData 0.000 (0.033)\tLoss 1.5084 (2.1109)\tPrec@1 90.625 (87.784)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [83][20/97], lr: 0.01000\tTime 0.297 (0.303)\tData 0.000 (0.025)\tLoss 2.0453 (2.0964)\tPrec@1 87.500 (87.277)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [83][30/97], lr: 0.01000\tTime 0.298 (0.302)\tData 0.000 (0.023)\tLoss 2.7394 (2.0502)\tPrec@1 82.031 (87.450)\tPrec@5 97.656 (99.219)\n",
      "Epoch: [83][40/97], lr: 0.01000\tTime 0.299 (0.302)\tData 0.000 (0.022)\tLoss 2.1410 (2.0026)\tPrec@1 85.938 (87.748)\tPrec@5 97.656 (99.143)\n",
      "Epoch: [83][50/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 2.1990 (2.0370)\tPrec@1 87.500 (87.561)\tPrec@5 98.438 (99.173)\n",
      "Epoch: [83][60/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.020)\tLoss 2.1550 (2.0561)\tPrec@1 85.156 (87.244)\tPrec@5 99.219 (99.168)\n",
      "Epoch: [83][70/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.020)\tLoss 1.7015 (2.0550)\tPrec@1 88.281 (87.258)\tPrec@5 100.000 (99.186)\n",
      "Epoch: [83][80/97], lr: 0.01000\tTime 0.299 (0.300)\tData 0.000 (0.019)\tLoss 2.4529 (2.0540)\tPrec@1 85.156 (87.346)\tPrec@5 98.438 (99.248)\n",
      "Epoch: [83][90/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.019)\tLoss 2.2863 (2.0789)\tPrec@1 85.938 (87.234)\tPrec@5 100.000 (99.253)\n",
      "Epoch: [83][96/97], lr: 0.01000\tTime 0.294 (0.300)\tData 0.000 (0.020)\tLoss 2.2171 (2.0852)\tPrec@1 84.746 (87.176)\tPrec@5 99.153 (99.258)\n",
      "Test: [0/100]\tTime 0.247 (0.247)\tLoss 10.2696 (10.2696)\tPrec@1 51.000 (51.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.072 (0.088)\tLoss 8.3088 (9.6314)\tPrec@1 58.000 (53.909)\tPrec@5 90.000 (95.000)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 9.4114 (9.6783)\tPrec@1 54.000 (52.857)\tPrec@5 91.000 (94.429)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.8074 (9.6955)\tPrec@1 52.000 (52.677)\tPrec@5 95.000 (94.645)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 10.2419 (9.6521)\tPrec@1 52.000 (53.195)\tPrec@5 91.000 (94.512)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.6146 (9.6107)\tPrec@1 51.000 (53.314)\tPrec@5 96.000 (94.647)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 7.5734 (9.6202)\tPrec@1 65.000 (53.049)\tPrec@5 95.000 (94.623)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.8750 (9.5985)\tPrec@1 56.000 (53.183)\tPrec@5 93.000 (94.521)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.9919 (9.5661)\tPrec@1 58.000 (53.148)\tPrec@5 94.000 (94.617)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 9.3833 (9.6042)\tPrec@1 54.000 (52.912)\tPrec@5 98.000 (94.538)\n",
      "val Results: Prec@1 52.900 Prec@5 94.460 Loss 9.62287\n",
      "val Class Accuracy: [0.954,0.981,0.878,0.741,0.639,0.132,0.356,0.243,0.309,0.057]\n",
      "Best Prec@1: 67.690\n",
      "\n",
      "Epoch: [84][0/97], lr: 0.01000\tTime 0.427 (0.427)\tData 0.280 (0.280)\tLoss 2.7517 (2.7517)\tPrec@1 83.594 (83.594)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [84][10/97], lr: 0.01000\tTime 0.295 (0.311)\tData 0.000 (0.040)\tLoss 2.4254 (2.0075)\tPrec@1 83.594 (87.642)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [84][20/97], lr: 0.01000\tTime 0.295 (0.305)\tData 0.000 (0.029)\tLoss 2.0232 (2.0183)\tPrec@1 85.938 (87.351)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [84][30/97], lr: 0.01000\tTime 0.295 (0.303)\tData 0.000 (0.026)\tLoss 1.9901 (2.0959)\tPrec@1 85.938 (86.971)\tPrec@5 100.000 (99.370)\n",
      "Epoch: [84][40/97], lr: 0.01000\tTime 0.289 (0.301)\tData 0.000 (0.024)\tLoss 1.8193 (2.0902)\tPrec@1 89.062 (87.062)\tPrec@5 100.000 (99.390)\n",
      "Epoch: [84][50/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.022)\tLoss 1.5680 (2.0714)\tPrec@1 92.188 (87.331)\tPrec@5 100.000 (99.357)\n",
      "Epoch: [84][60/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 2.5905 (2.0673)\tPrec@1 84.375 (87.449)\tPrec@5 100.000 (99.347)\n",
      "Epoch: [84][70/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 1.9524 (2.0849)\tPrec@1 87.500 (87.335)\tPrec@5 100.000 (99.351)\n",
      "Epoch: [84][80/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.6519 (2.0790)\tPrec@1 90.625 (87.346)\tPrec@5 98.438 (99.334)\n",
      "Epoch: [84][90/97], lr: 0.01000\tTime 0.293 (0.299)\tData 0.000 (0.020)\tLoss 2.2851 (2.0795)\tPrec@1 86.719 (87.397)\tPrec@5 99.219 (99.330)\n",
      "Epoch: [84][96/97], lr: 0.01000\tTime 0.292 (0.299)\tData 0.000 (0.021)\tLoss 1.9509 (2.0600)\tPrec@1 85.593 (87.466)\tPrec@5 98.305 (99.315)\n",
      "Test: [0/100]\tTime 0.263 (0.263)\tLoss 10.3338 (10.3338)\tPrec@1 52.000 (52.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.072 (0.090)\tLoss 8.5855 (9.4660)\tPrec@1 57.000 (54.636)\tPrec@5 87.000 (91.455)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 7.5287 (9.2917)\tPrec@1 60.000 (54.905)\tPrec@5 93.000 (91.810)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 8.0901 (9.1430)\tPrec@1 62.000 (55.839)\tPrec@5 93.000 (91.839)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 8.6995 (9.1307)\tPrec@1 57.000 (56.024)\tPrec@5 90.000 (91.878)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.6144 (9.0994)\tPrec@1 60.000 (56.039)\tPrec@5 92.000 (91.961)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.5469 (9.0853)\tPrec@1 70.000 (56.131)\tPrec@5 96.000 (91.852)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.4405 (9.0479)\tPrec@1 59.000 (56.451)\tPrec@5 95.000 (91.803)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 8.5205 (8.9770)\tPrec@1 59.000 (56.765)\tPrec@5 94.000 (92.012)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.5736 (9.0482)\tPrec@1 59.000 (56.451)\tPrec@5 98.000 (91.802)\n",
      "val Results: Prec@1 56.560 Prec@5 91.720 Loss 9.04546\n",
      "val Class Accuracy: [0.965,0.987,0.763,0.745,0.534,0.607,0.068,0.642,0.284,0.061]\n",
      "Best Prec@1: 67.690\n",
      "\n",
      "Epoch: [85][0/97], lr: 0.01000\tTime 0.404 (0.404)\tData 0.249 (0.249)\tLoss 2.4168 (2.4168)\tPrec@1 85.156 (85.156)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [85][10/97], lr: 0.01000\tTime 0.296 (0.309)\tData 0.000 (0.037)\tLoss 1.6862 (2.3154)\tPrec@1 89.062 (86.293)\tPrec@5 99.219 (99.361)\n",
      "Epoch: [85][20/97], lr: 0.01000\tTime 0.295 (0.303)\tData 0.000 (0.028)\tLoss 1.7656 (2.1188)\tPrec@1 89.062 (87.388)\tPrec@5 98.438 (99.368)\n",
      "Epoch: [85][30/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.024)\tLoss 1.7370 (2.1427)\tPrec@1 87.500 (86.769)\tPrec@5 100.000 (99.320)\n",
      "Epoch: [85][40/97], lr: 0.01000\tTime 0.289 (0.300)\tData 0.000 (0.023)\tLoss 2.1992 (2.0733)\tPrec@1 85.938 (87.462)\tPrec@5 99.219 (99.295)\n",
      "Epoch: [85][50/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.022)\tLoss 1.6827 (2.0656)\tPrec@1 85.938 (87.423)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [85][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 1.7130 (2.0757)\tPrec@1 89.062 (87.269)\tPrec@5 100.000 (99.129)\n",
      "Epoch: [85][70/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 1.8640 (2.0594)\tPrec@1 87.500 (87.258)\tPrec@5 99.219 (99.164)\n",
      "Epoch: [85][80/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 1.8308 (2.0729)\tPrec@1 89.062 (87.143)\tPrec@5 100.000 (99.209)\n",
      "Epoch: [85][90/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 2.3389 (2.0612)\tPrec@1 85.938 (87.217)\tPrec@5 99.219 (99.236)\n",
      "Epoch: [85][96/97], lr: 0.01000\tTime 0.288 (0.299)\tData 0.000 (0.020)\tLoss 2.1873 (2.0565)\tPrec@1 86.441 (87.240)\tPrec@5 99.153 (99.258)\n",
      "Test: [0/100]\tTime 0.230 (0.230)\tLoss 8.4632 (8.4632)\tPrec@1 57.000 (57.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 5.4925 (7.4665)\tPrec@1 74.000 (63.000)\tPrec@5 96.000 (95.818)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 6.7579 (7.3529)\tPrec@1 66.000 (63.571)\tPrec@5 94.000 (95.667)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 6.7170 (7.3765)\tPrec@1 66.000 (63.613)\tPrec@5 98.000 (95.452)\n",
      "Test: [40/100]\tTime 0.073 (0.076)\tLoss 7.2562 (7.4775)\tPrec@1 64.000 (63.098)\tPrec@5 94.000 (95.220)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.8672 (7.4316)\tPrec@1 70.000 (63.471)\tPrec@5 96.000 (95.255)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 6.3136 (7.4294)\tPrec@1 69.000 (63.328)\tPrec@5 95.000 (95.082)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.9379 (7.4098)\tPrec@1 59.000 (63.563)\tPrec@5 94.000 (95.070)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 7.1338 (7.3738)\tPrec@1 67.000 (63.864)\tPrec@5 94.000 (95.136)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 6.5751 (7.3833)\tPrec@1 70.000 (63.890)\tPrec@5 98.000 (95.176)\n",
      "val Results: Prec@1 63.750 Prec@5 95.160 Loss 7.42357\n",
      "val Class Accuracy: [0.909,0.984,0.810,0.524,0.830,0.576,0.819,0.205,0.683,0.035]\n",
      "Best Prec@1: 67.690\n",
      "\n",
      "Epoch: [86][0/97], lr: 0.01000\tTime 0.388 (0.388)\tData 0.243 (0.243)\tLoss 2.0395 (2.0395)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [86][10/97], lr: 0.01000\tTime 0.296 (0.308)\tData 0.000 (0.037)\tLoss 2.3091 (2.0792)\tPrec@1 85.938 (87.003)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [86][20/97], lr: 0.01000\tTime 0.295 (0.303)\tData 0.000 (0.028)\tLoss 1.9434 (2.0465)\tPrec@1 89.062 (87.537)\tPrec@5 99.219 (99.442)\n",
      "Epoch: [86][30/97], lr: 0.01000\tTime 0.297 (0.302)\tData 0.000 (0.024)\tLoss 2.4941 (2.0533)\tPrec@1 82.031 (87.298)\tPrec@5 99.219 (99.521)\n",
      "Epoch: [86][40/97], lr: 0.01000\tTime 0.291 (0.301)\tData 0.000 (0.023)\tLoss 2.9149 (2.0851)\tPrec@1 81.250 (87.348)\tPrec@5 99.219 (99.428)\n",
      "Epoch: [86][50/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.022)\tLoss 1.5693 (2.0836)\tPrec@1 88.281 (87.393)\tPrec@5 100.000 (99.387)\n",
      "Epoch: [86][60/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 2.2115 (2.1146)\tPrec@1 85.938 (87.026)\tPrec@5 98.438 (99.308)\n",
      "Epoch: [86][70/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 2.5773 (2.1328)\tPrec@1 82.812 (86.829)\tPrec@5 99.219 (99.263)\n",
      "Epoch: [86][80/97], lr: 0.01000\tTime 0.298 (0.300)\tData 0.000 (0.020)\tLoss 2.2521 (2.1171)\tPrec@1 88.281 (86.902)\tPrec@5 99.219 (99.277)\n",
      "Epoch: [86][90/97], lr: 0.01000\tTime 0.300 (0.300)\tData 0.000 (0.020)\tLoss 2.1993 (2.0958)\tPrec@1 86.719 (87.096)\tPrec@5 100.000 (99.245)\n",
      "Epoch: [86][96/97], lr: 0.01000\tTime 0.291 (0.300)\tData 0.000 (0.020)\tLoss 2.3608 (2.0997)\tPrec@1 86.441 (87.071)\tPrec@5 100.000 (99.250)\n",
      "Test: [0/100]\tTime 0.232 (0.232)\tLoss 11.3766 (11.3766)\tPrec@1 42.000 (42.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.072 (0.087)\tLoss 8.4779 (10.4355)\tPrec@1 60.000 (49.545)\tPrec@5 90.000 (94.182)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 8.8905 (10.3765)\tPrec@1 57.000 (49.333)\tPrec@5 98.000 (94.905)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 9.4318 (10.3060)\tPrec@1 54.000 (49.484)\tPrec@5 96.000 (95.032)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 11.7376 (10.3505)\tPrec@1 41.000 (49.293)\tPrec@5 94.000 (95.098)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 10.1061 (10.3168)\tPrec@1 50.000 (49.451)\tPrec@5 98.000 (95.216)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 8.9183 (10.3541)\tPrec@1 57.000 (49.115)\tPrec@5 97.000 (95.410)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 9.7042 (10.3539)\tPrec@1 53.000 (49.042)\tPrec@5 96.000 (95.437)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 9.2682 (10.2966)\tPrec@1 53.000 (49.309)\tPrec@5 95.000 (95.531)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 9.5407 (10.3620)\tPrec@1 51.000 (49.000)\tPrec@5 97.000 (95.363)\n",
      "val Results: Prec@1 48.980 Prec@5 95.340 Loss 10.38574\n",
      "val Class Accuracy: [0.901,0.958,0.962,0.567,0.401,0.255,0.317,0.286,0.162,0.089]\n",
      "Best Prec@1: 67.690\n",
      "\n",
      "Epoch: [87][0/97], lr: 0.01000\tTime 0.379 (0.379)\tData 0.219 (0.219)\tLoss 1.4687 (1.4687)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [87][10/97], lr: 0.01000\tTime 0.296 (0.306)\tData 0.000 (0.035)\tLoss 2.0274 (1.9332)\tPrec@1 89.062 (88.423)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [87][20/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.027)\tLoss 1.8771 (1.8539)\tPrec@1 89.062 (88.914)\tPrec@5 99.219 (99.479)\n",
      "Epoch: [87][30/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.024)\tLoss 2.5454 (1.9258)\tPrec@1 82.812 (88.332)\tPrec@5 99.219 (99.546)\n",
      "Epoch: [87][40/97], lr: 0.01000\tTime 0.286 (0.299)\tData 0.000 (0.022)\tLoss 2.3668 (1.9581)\tPrec@1 87.500 (88.091)\tPrec@5 97.656 (99.428)\n",
      "Epoch: [87][50/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.021)\tLoss 1.7916 (1.9942)\tPrec@1 88.281 (87.837)\tPrec@5 99.219 (99.341)\n",
      "Epoch: [87][60/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.2078 (1.9852)\tPrec@1 87.500 (87.859)\tPrec@5 99.219 (99.360)\n",
      "Epoch: [87][70/97], lr: 0.01000\tTime 0.296 (0.298)\tData 0.000 (0.020)\tLoss 2.6019 (2.0012)\tPrec@1 84.375 (87.753)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [87][80/97], lr: 0.01000\tTime 0.302 (0.299)\tData 0.000 (0.020)\tLoss 1.6084 (1.9917)\tPrec@1 91.406 (87.818)\tPrec@5 100.000 (99.383)\n",
      "Epoch: [87][90/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.019)\tLoss 2.2563 (1.9930)\tPrec@1 87.500 (87.792)\tPrec@5 99.219 (99.373)\n",
      "Epoch: [87][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 2.8228 (2.0134)\tPrec@1 82.203 (87.700)\tPrec@5 99.153 (99.323)\n",
      "Test: [0/100]\tTime 0.247 (0.247)\tLoss 9.2879 (9.2879)\tPrec@1 56.000 (56.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.072 (0.088)\tLoss 6.0693 (8.5323)\tPrec@1 68.000 (58.455)\tPrec@5 98.000 (95.545)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 7.8901 (8.1942)\tPrec@1 59.000 (60.810)\tPrec@5 95.000 (95.333)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 7.8298 (8.2946)\tPrec@1 62.000 (60.452)\tPrec@5 95.000 (95.387)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 8.7680 (8.4033)\tPrec@1 57.000 (60.171)\tPrec@5 93.000 (95.024)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.0193 (8.3638)\tPrec@1 61.000 (60.235)\tPrec@5 92.000 (95.157)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 7.5417 (8.3782)\tPrec@1 63.000 (59.967)\tPrec@5 97.000 (95.131)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 8.1435 (8.3280)\tPrec@1 61.000 (60.338)\tPrec@5 94.000 (95.056)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 7.7056 (8.3021)\tPrec@1 61.000 (60.444)\tPrec@5 92.000 (95.074)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 8.1548 (8.3392)\tPrec@1 60.000 (60.143)\tPrec@5 97.000 (95.154)\n",
      "val Results: Prec@1 60.150 Prec@5 95.110 Loss 8.36505\n",
      "val Class Accuracy: [0.883,0.975,0.747,0.488,0.946,0.388,0.655,0.144,0.557,0.232]\n",
      "Best Prec@1: 67.690\n",
      "\n",
      "Epoch: [88][0/97], lr: 0.01000\tTime 0.413 (0.413)\tData 0.255 (0.255)\tLoss 1.5513 (1.5513)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [88][10/97], lr: 0.01000\tTime 0.297 (0.313)\tData 0.000 (0.038)\tLoss 1.7481 (1.9868)\tPrec@1 89.844 (87.784)\tPrec@5 99.219 (99.148)\n",
      "Epoch: [88][20/97], lr: 0.01000\tTime 0.296 (0.306)\tData 0.000 (0.028)\tLoss 2.1629 (2.0080)\tPrec@1 88.281 (87.760)\tPrec@5 98.438 (99.144)\n",
      "Epoch: [88][30/97], lr: 0.01000\tTime 0.297 (0.304)\tData 0.000 (0.025)\tLoss 2.2143 (2.0168)\tPrec@1 85.938 (87.651)\tPrec@5 99.219 (99.143)\n",
      "Epoch: [88][40/97], lr: 0.01000\tTime 0.300 (0.303)\tData 0.000 (0.023)\tLoss 1.8650 (2.0071)\tPrec@1 86.719 (87.767)\tPrec@5 100.000 (99.123)\n",
      "Epoch: [88][50/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.022)\tLoss 1.1023 (1.9916)\tPrec@1 95.312 (87.837)\tPrec@5 100.000 (99.157)\n",
      "Epoch: [88][60/97], lr: 0.01000\tTime 0.297 (0.302)\tData 0.000 (0.021)\tLoss 2.6772 (2.0040)\tPrec@1 79.688 (87.846)\tPrec@5 100.000 (99.193)\n",
      "Epoch: [88][70/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 2.1970 (2.0026)\tPrec@1 86.719 (87.896)\tPrec@5 99.219 (99.230)\n",
      "Epoch: [88][80/97], lr: 0.01000\tTime 0.306 (0.301)\tData 0.000 (0.020)\tLoss 2.0241 (2.0213)\tPrec@1 88.281 (87.722)\tPrec@5 100.000 (99.267)\n",
      "Epoch: [88][90/97], lr: 0.01000\tTime 0.303 (0.301)\tData 0.000 (0.020)\tLoss 1.6713 (2.0263)\tPrec@1 89.062 (87.629)\tPrec@5 99.219 (99.253)\n",
      "Epoch: [88][96/97], lr: 0.01000\tTime 0.290 (0.301)\tData 0.000 (0.020)\tLoss 2.6249 (2.0257)\tPrec@1 83.898 (87.603)\tPrec@5 100.000 (99.291)\n",
      "Test: [0/100]\tTime 0.275 (0.275)\tLoss 7.8902 (7.8902)\tPrec@1 61.000 (61.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.072 (0.091)\tLoss 6.7277 (7.7903)\tPrec@1 70.000 (61.545)\tPrec@5 91.000 (95.909)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.3477 (7.4923)\tPrec@1 67.000 (62.762)\tPrec@5 96.000 (95.810)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 6.5363 (7.4438)\tPrec@1 66.000 (63.290)\tPrec@5 98.000 (96.000)\n",
      "Test: [40/100]\tTime 0.072 (0.078)\tLoss 6.9516 (7.4443)\tPrec@1 66.000 (63.610)\tPrec@5 94.000 (95.902)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.2511 (7.4188)\tPrec@1 65.000 (63.667)\tPrec@5 95.000 (96.000)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.9875 (7.4734)\tPrec@1 63.000 (63.180)\tPrec@5 97.000 (96.082)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 6.4388 (7.4348)\tPrec@1 71.000 (63.380)\tPrec@5 97.000 (96.225)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 7.2894 (7.4142)\tPrec@1 65.000 (63.531)\tPrec@5 98.000 (96.358)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 6.9109 (7.4340)\tPrec@1 67.000 (63.505)\tPrec@5 98.000 (96.352)\n",
      "val Results: Prec@1 63.480 Prec@5 96.370 Loss 7.44977\n",
      "val Class Accuracy: [0.946,0.986,0.824,0.622,0.637,0.488,0.423,0.659,0.701,0.062]\n",
      "Best Prec@1: 67.690\n",
      "\n",
      "Epoch: [89][0/97], lr: 0.01000\tTime 0.396 (0.396)\tData 0.239 (0.239)\tLoss 1.4580 (1.4580)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [89][10/97], lr: 0.01000\tTime 0.296 (0.308)\tData 0.000 (0.037)\tLoss 2.9942 (1.9016)\tPrec@1 79.688 (88.636)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [89][20/97], lr: 0.01000\tTime 0.295 (0.303)\tData 0.000 (0.028)\tLoss 3.1021 (1.9023)\tPrec@1 81.250 (88.616)\tPrec@5 98.438 (99.107)\n",
      "Epoch: [89][30/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.024)\tLoss 2.1181 (2.0725)\tPrec@1 83.594 (87.374)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [89][40/97], lr: 0.01000\tTime 0.300 (0.301)\tData 0.000 (0.023)\tLoss 2.1820 (2.0634)\tPrec@1 85.156 (87.500)\tPrec@5 99.219 (99.028)\n",
      "Epoch: [89][50/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 1.7746 (2.0431)\tPrec@1 88.281 (87.638)\tPrec@5 100.000 (99.127)\n",
      "Epoch: [89][60/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 1.8202 (2.0343)\tPrec@1 88.281 (87.615)\tPrec@5 100.000 (99.168)\n",
      "Epoch: [89][70/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 1.9364 (2.0393)\tPrec@1 89.062 (87.544)\tPrec@5 99.219 (99.241)\n",
      "Epoch: [89][80/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 1.4597 (2.0112)\tPrec@1 92.188 (87.731)\tPrec@5 99.219 (99.257)\n",
      "Epoch: [89][90/97], lr: 0.01000\tTime 0.306 (0.299)\tData 0.000 (0.020)\tLoss 2.7800 (2.0334)\tPrec@1 82.812 (87.594)\tPrec@5 98.438 (99.167)\n",
      "Epoch: [89][96/97], lr: 0.01000\tTime 0.288 (0.299)\tData 0.000 (0.020)\tLoss 2.0012 (2.0198)\tPrec@1 88.983 (87.716)\tPrec@5 98.305 (99.170)\n",
      "Test: [0/100]\tTime 0.262 (0.262)\tLoss 6.7930 (6.7930)\tPrec@1 70.000 (70.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 4.8082 (7.1133)\tPrec@1 83.000 (66.455)\tPrec@5 96.000 (96.545)\n",
      "Test: [20/100]\tTime 0.072 (0.082)\tLoss 5.8096 (6.9924)\tPrec@1 74.000 (67.095)\tPrec@5 97.000 (96.857)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 5.7141 (6.9869)\tPrec@1 74.000 (66.774)\tPrec@5 97.000 (96.839)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 7.0091 (7.0090)\tPrec@1 66.000 (66.951)\tPrec@5 97.000 (96.878)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.5678 (6.9819)\tPrec@1 68.000 (66.784)\tPrec@5 98.000 (96.961)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 6.4435 (7.0127)\tPrec@1 66.000 (66.426)\tPrec@5 100.000 (97.131)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 6.3404 (6.9801)\tPrec@1 68.000 (66.507)\tPrec@5 100.000 (97.099)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.8986 (6.9639)\tPrec@1 67.000 (66.494)\tPrec@5 96.000 (97.074)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.2200 (7.0185)\tPrec@1 66.000 (66.297)\tPrec@5 99.000 (97.143)\n",
      "val Results: Prec@1 66.200 Prec@5 97.040 Loss 7.05258\n",
      "val Class Accuracy: [0.951,0.961,0.753,0.746,0.824,0.476,0.615,0.644,0.534,0.116]\n",
      "Best Prec@1: 67.690\n",
      "\n",
      "Epoch: [90][0/97], lr: 0.01000\tTime 0.408 (0.408)\tData 0.250 (0.250)\tLoss 1.7635 (1.7635)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [90][10/97], lr: 0.01000\tTime 0.295 (0.310)\tData 0.000 (0.038)\tLoss 1.6569 (2.0079)\tPrec@1 92.969 (87.713)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [90][20/97], lr: 0.01000\tTime 0.295 (0.303)\tData 0.000 (0.028)\tLoss 2.0152 (2.1067)\tPrec@1 89.062 (87.202)\tPrec@5 99.219 (99.293)\n",
      "Epoch: [90][30/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.025)\tLoss 2.8172 (2.1153)\tPrec@1 82.812 (87.097)\tPrec@5 100.000 (99.345)\n",
      "Epoch: [90][40/97], lr: 0.01000\tTime 0.293 (0.301)\tData 0.000 (0.023)\tLoss 2.5943 (2.0892)\tPrec@1 84.375 (87.176)\tPrec@5 99.219 (99.333)\n",
      "Epoch: [90][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.022)\tLoss 2.8995 (2.0692)\tPrec@1 82.812 (87.377)\tPrec@5 98.438 (99.311)\n",
      "Epoch: [90][60/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 2.6166 (2.0371)\tPrec@1 81.250 (87.500)\tPrec@5 100.000 (99.372)\n",
      "Epoch: [90][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.3268 (2.0326)\tPrec@1 82.812 (87.588)\tPrec@5 97.656 (99.340)\n",
      "Epoch: [90][80/97], lr: 0.01000\tTime 0.299 (0.299)\tData 0.000 (0.020)\tLoss 2.4795 (2.0274)\tPrec@1 82.812 (87.596)\tPrec@5 100.000 (99.344)\n",
      "Epoch: [90][90/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 1.7198 (2.0384)\tPrec@1 89.062 (87.509)\tPrec@5 99.219 (99.339)\n",
      "Epoch: [90][96/97], lr: 0.01000\tTime 0.288 (0.299)\tData 0.000 (0.020)\tLoss 2.2870 (2.0589)\tPrec@1 86.441 (87.401)\tPrec@5 97.458 (99.299)\n",
      "Test: [0/100]\tTime 0.265 (0.265)\tLoss 8.7588 (8.7588)\tPrec@1 57.000 (57.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.072 (0.090)\tLoss 5.8629 (7.8704)\tPrec@1 74.000 (61.636)\tPrec@5 95.000 (94.909)\n",
      "Test: [20/100]\tTime 0.072 (0.082)\tLoss 6.5333 (7.8056)\tPrec@1 68.000 (61.810)\tPrec@5 94.000 (95.143)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 7.0936 (7.8200)\tPrec@1 62.000 (61.226)\tPrec@5 96.000 (94.613)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 8.3567 (7.8959)\tPrec@1 57.000 (61.171)\tPrec@5 93.000 (94.585)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 7.3664 (7.8441)\tPrec@1 64.000 (61.451)\tPrec@5 93.000 (94.588)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 6.0489 (7.8054)\tPrec@1 69.000 (61.607)\tPrec@5 96.000 (94.525)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.0294 (7.7762)\tPrec@1 60.000 (61.845)\tPrec@5 97.000 (94.535)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.8319 (7.7668)\tPrec@1 60.000 (61.802)\tPrec@5 91.000 (94.654)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 8.3142 (7.8213)\tPrec@1 61.000 (61.440)\tPrec@5 98.000 (94.725)\n",
      "val Results: Prec@1 61.280 Prec@5 94.670 Loss 7.85428\n",
      "val Class Accuracy: [0.983,0.938,0.573,0.549,0.621,0.757,0.849,0.436,0.339,0.083]\n",
      "Best Prec@1: 67.690\n",
      "\n",
      "Epoch: [91][0/97], lr: 0.01000\tTime 0.402 (0.402)\tData 0.238 (0.238)\tLoss 3.1854 (3.1854)\tPrec@1 82.031 (82.031)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [91][10/97], lr: 0.01000\tTime 0.297 (0.312)\tData 0.000 (0.036)\tLoss 1.5075 (1.9716)\tPrec@1 91.406 (87.855)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [91][20/97], lr: 0.01000\tTime 0.298 (0.306)\tData 0.000 (0.027)\tLoss 2.3734 (1.9950)\tPrec@1 86.719 (87.537)\tPrec@5 98.438 (99.293)\n",
      "Epoch: [91][30/97], lr: 0.01000\tTime 0.295 (0.305)\tData 0.000 (0.024)\tLoss 1.7423 (1.9659)\tPrec@1 87.500 (87.954)\tPrec@5 100.000 (99.345)\n",
      "Epoch: [91][40/97], lr: 0.01000\tTime 0.296 (0.303)\tData 0.000 (0.022)\tLoss 2.2906 (1.9272)\tPrec@1 84.375 (88.300)\tPrec@5 97.656 (99.314)\n",
      "Epoch: [91][50/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.021)\tLoss 2.2594 (1.9311)\tPrec@1 84.375 (88.266)\tPrec@5 99.219 (99.295)\n",
      "Epoch: [91][60/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 2.3385 (2.0362)\tPrec@1 86.719 (87.615)\tPrec@5 99.219 (99.244)\n",
      "Epoch: [91][70/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.020)\tLoss 1.8119 (2.0173)\tPrec@1 85.156 (87.676)\tPrec@5 100.000 (99.252)\n",
      "Epoch: [91][80/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.020)\tLoss 1.8509 (2.0252)\tPrec@1 88.281 (87.625)\tPrec@5 99.219 (99.296)\n",
      "Epoch: [91][90/97], lr: 0.01000\tTime 0.302 (0.300)\tData 0.000 (0.020)\tLoss 2.3322 (2.0166)\tPrec@1 89.062 (87.783)\tPrec@5 98.438 (99.322)\n",
      "Epoch: [91][96/97], lr: 0.01000\tTime 0.288 (0.300)\tData 0.000 (0.020)\tLoss 2.0902 (2.0164)\tPrec@1 88.136 (87.756)\tPrec@5 98.305 (99.315)\n",
      "Test: [0/100]\tTime 0.273 (0.273)\tLoss 8.6815 (8.6815)\tPrec@1 59.000 (59.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 7.0904 (8.0494)\tPrec@1 64.000 (61.273)\tPrec@5 96.000 (97.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.0728 (7.8952)\tPrec@1 72.000 (62.381)\tPrec@5 98.000 (97.048)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.8832 (7.9191)\tPrec@1 63.000 (62.419)\tPrec@5 97.000 (96.710)\n",
      "Test: [40/100]\tTime 0.072 (0.078)\tLoss 7.8474 (7.9278)\tPrec@1 64.000 (62.634)\tPrec@5 96.000 (96.585)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.7473 (7.8796)\tPrec@1 68.000 (62.647)\tPrec@5 96.000 (96.627)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 6.3539 (7.8444)\tPrec@1 68.000 (62.770)\tPrec@5 100.000 (96.590)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 8.2158 (7.8033)\tPrec@1 60.000 (63.155)\tPrec@5 98.000 (96.563)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.2178 (7.7620)\tPrec@1 62.000 (63.420)\tPrec@5 96.000 (96.605)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.2933 (7.8251)\tPrec@1 64.000 (63.044)\tPrec@5 98.000 (96.604)\n",
      "val Results: Prec@1 62.910 Prec@5 96.610 Loss 7.85079\n",
      "val Class Accuracy: [0.962,0.986,0.726,0.309,0.749,0.865,0.701,0.370,0.442,0.181]\n",
      "Best Prec@1: 67.690\n",
      "\n",
      "Epoch: [92][0/97], lr: 0.01000\tTime 0.424 (0.424)\tData 0.274 (0.274)\tLoss 1.8274 (1.8274)\tPrec@1 86.719 (86.719)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [92][10/97], lr: 0.01000\tTime 0.295 (0.312)\tData 0.000 (0.040)\tLoss 2.1819 (2.0073)\tPrec@1 87.500 (87.784)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [92][20/97], lr: 0.01000\tTime 0.295 (0.305)\tData 0.000 (0.029)\tLoss 2.5332 (2.0920)\tPrec@1 82.812 (87.165)\tPrec@5 100.000 (99.033)\n",
      "Epoch: [92][30/97], lr: 0.01000\tTime 0.296 (0.303)\tData 0.000 (0.025)\tLoss 1.9610 (1.9861)\tPrec@1 90.625 (88.105)\tPrec@5 100.000 (99.143)\n",
      "Epoch: [92][40/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.023)\tLoss 2.2434 (2.0030)\tPrec@1 85.156 (88.014)\tPrec@5 99.219 (99.257)\n",
      "Epoch: [92][50/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.022)\tLoss 1.5395 (2.0059)\tPrec@1 89.062 (87.960)\tPrec@5 100.000 (99.203)\n",
      "Epoch: [92][60/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 2.0468 (2.0411)\tPrec@1 87.500 (87.782)\tPrec@5 99.219 (99.155)\n",
      "Epoch: [92][70/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 1.4888 (2.0154)\tPrec@1 90.625 (87.951)\tPrec@5 99.219 (99.131)\n",
      "Epoch: [92][80/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 1.9134 (2.0434)\tPrec@1 89.062 (87.741)\tPrec@5 98.438 (99.074)\n",
      "Epoch: [92][90/97], lr: 0.01000\tTime 0.294 (0.300)\tData 0.000 (0.020)\tLoss 2.0151 (2.0376)\tPrec@1 88.281 (87.843)\tPrec@5 99.219 (99.073)\n",
      "Epoch: [92][96/97], lr: 0.01000\tTime 0.288 (0.299)\tData 0.000 (0.021)\tLoss 1.6597 (2.0376)\tPrec@1 88.983 (87.828)\tPrec@5 98.305 (99.073)\n",
      "Test: [0/100]\tTime 0.254 (0.254)\tLoss 9.6476 (9.6476)\tPrec@1 58.000 (58.000)\tPrec@5 90.000 (90.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.0248 (9.2150)\tPrec@1 67.000 (57.364)\tPrec@5 91.000 (86.636)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.8121 (8.9800)\tPrec@1 61.000 (58.238)\tPrec@5 91.000 (87.143)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 8.9647 (8.9593)\tPrec@1 57.000 (58.484)\tPrec@5 87.000 (86.968)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.1037 (9.0585)\tPrec@1 60.000 (58.073)\tPrec@5 89.000 (87.171)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 8.6669 (8.9682)\tPrec@1 61.000 (58.255)\tPrec@5 89.000 (87.431)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.0950 (8.9474)\tPrec@1 68.000 (58.262)\tPrec@5 93.000 (87.836)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 8.5400 (8.9458)\tPrec@1 60.000 (58.324)\tPrec@5 91.000 (87.831)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 8.7558 (8.9090)\tPrec@1 65.000 (58.617)\tPrec@5 88.000 (87.938)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.8035 (8.9650)\tPrec@1 62.000 (58.407)\tPrec@5 86.000 (87.890)\n",
      "val Results: Prec@1 58.370 Prec@5 87.850 Loss 8.99397\n",
      "val Class Accuracy: [0.830,0.898,0.779,0.850,0.746,0.585,0.698,0.201,0.219,0.031]\n",
      "Best Prec@1: 67.690\n",
      "\n",
      "Epoch: [93][0/97], lr: 0.01000\tTime 0.406 (0.406)\tData 0.241 (0.241)\tLoss 2.2106 (2.2106)\tPrec@1 85.938 (85.938)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [93][10/97], lr: 0.01000\tTime 0.297 (0.309)\tData 0.000 (0.037)\tLoss 1.8676 (2.1111)\tPrec@1 88.281 (87.358)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [93][20/97], lr: 0.01000\tTime 0.297 (0.304)\tData 0.000 (0.027)\tLoss 2.1796 (2.0107)\tPrec@1 88.281 (88.207)\tPrec@5 99.219 (99.516)\n",
      "Epoch: [93][30/97], lr: 0.01000\tTime 0.297 (0.303)\tData 0.000 (0.024)\tLoss 1.8398 (1.9804)\tPrec@1 89.844 (88.357)\tPrec@5 99.219 (99.496)\n",
      "Epoch: [93][40/97], lr: 0.01000\tTime 0.298 (0.302)\tData 0.000 (0.023)\tLoss 1.6573 (1.9459)\tPrec@1 89.062 (88.472)\tPrec@5 99.219 (99.447)\n",
      "Epoch: [93][50/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.021)\tLoss 2.0707 (1.9744)\tPrec@1 86.719 (88.220)\tPrec@5 100.000 (99.433)\n",
      "Epoch: [93][60/97], lr: 0.01000\tTime 0.301 (0.301)\tData 0.000 (0.021)\tLoss 2.3659 (2.0227)\tPrec@1 84.375 (87.769)\tPrec@5 98.438 (99.372)\n",
      "Epoch: [93][70/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.020)\tLoss 1.6920 (2.0177)\tPrec@1 90.625 (87.808)\tPrec@5 100.000 (99.384)\n",
      "Epoch: [93][80/97], lr: 0.01000\tTime 0.305 (0.301)\tData 0.000 (0.020)\tLoss 2.1933 (2.0196)\tPrec@1 86.719 (87.799)\tPrec@5 100.000 (99.431)\n",
      "Epoch: [93][90/97], lr: 0.01000\tTime 0.307 (0.301)\tData 0.000 (0.020)\tLoss 2.4827 (2.0190)\tPrec@1 84.375 (87.715)\tPrec@5 100.000 (99.416)\n",
      "Epoch: [93][96/97], lr: 0.01000\tTime 0.288 (0.300)\tData 0.000 (0.020)\tLoss 1.5373 (2.0045)\tPrec@1 90.678 (87.804)\tPrec@5 100.000 (99.420)\n",
      "Test: [0/100]\tTime 0.264 (0.264)\tLoss 9.4279 (9.4279)\tPrec@1 60.000 (60.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 7.9853 (8.9727)\tPrec@1 62.000 (55.818)\tPrec@5 93.000 (96.364)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.7748 (8.8629)\tPrec@1 58.000 (56.095)\tPrec@5 95.000 (95.286)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.5892 (8.7640)\tPrec@1 55.000 (56.645)\tPrec@5 96.000 (95.419)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.5427 (8.7104)\tPrec@1 55.000 (57.122)\tPrec@5 92.000 (95.463)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.8020 (8.6542)\tPrec@1 60.000 (57.373)\tPrec@5 94.000 (95.490)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.3356 (8.6733)\tPrec@1 60.000 (56.967)\tPrec@5 95.000 (95.508)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.4124 (8.6833)\tPrec@1 56.000 (56.845)\tPrec@5 98.000 (95.521)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.2932 (8.6690)\tPrec@1 60.000 (56.914)\tPrec@5 95.000 (95.580)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.7065 (8.7343)\tPrec@1 61.000 (56.593)\tPrec@5 97.000 (95.538)\n",
      "val Results: Prec@1 56.580 Prec@5 95.540 Loss 8.74680\n",
      "val Class Accuracy: [0.983,0.981,0.838,0.322,0.594,0.364,0.417,0.551,0.318,0.290]\n",
      "Best Prec@1: 67.690\n",
      "\n",
      "Epoch: [94][0/97], lr: 0.01000\tTime 0.354 (0.354)\tData 0.216 (0.216)\tLoss 1.6172 (1.6172)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [94][10/97], lr: 0.01000\tTime 0.293 (0.306)\tData 0.000 (0.035)\tLoss 1.7382 (1.9618)\tPrec@1 88.281 (88.352)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [94][20/97], lr: 0.01000\tTime 0.300 (0.302)\tData 0.000 (0.026)\tLoss 1.8133 (1.9904)\tPrec@1 86.719 (87.946)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [94][30/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.023)\tLoss 1.3680 (1.9906)\tPrec@1 90.625 (88.080)\tPrec@5 99.219 (99.370)\n",
      "Epoch: [94][40/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.022)\tLoss 1.9138 (2.0046)\tPrec@1 89.062 (87.919)\tPrec@5 99.219 (99.409)\n",
      "Epoch: [94][50/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 2.0121 (2.0011)\tPrec@1 86.719 (87.975)\tPrec@5 99.219 (99.418)\n",
      "Epoch: [94][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 1.3576 (2.0070)\tPrec@1 92.188 (88.012)\tPrec@5 100.000 (99.385)\n",
      "Epoch: [94][70/97], lr: 0.01000\tTime 0.318 (0.299)\tData 0.000 (0.020)\tLoss 1.7527 (1.9779)\tPrec@1 88.281 (88.204)\tPrec@5 99.219 (99.406)\n",
      "Epoch: [94][80/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 1.7371 (1.9912)\tPrec@1 90.625 (88.079)\tPrec@5 99.219 (99.363)\n",
      "Epoch: [94][90/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.019)\tLoss 1.1598 (2.0148)\tPrec@1 93.750 (87.964)\tPrec@5 99.219 (99.348)\n",
      "Epoch: [94][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 1.8516 (2.0079)\tPrec@1 88.136 (87.941)\tPrec@5 100.000 (99.371)\n",
      "Test: [0/100]\tTime 0.264 (0.264)\tLoss 8.9265 (8.9265)\tPrec@1 58.000 (58.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.8525 (8.3693)\tPrec@1 69.000 (60.000)\tPrec@5 93.000 (95.182)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.2462 (8.1426)\tPrec@1 69.000 (60.714)\tPrec@5 97.000 (94.810)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.6098 (8.2313)\tPrec@1 54.000 (60.097)\tPrec@5 95.000 (95.323)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.9629 (8.2585)\tPrec@1 63.000 (60.073)\tPrec@5 95.000 (95.268)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.9473 (8.2251)\tPrec@1 65.000 (60.176)\tPrec@5 96.000 (95.549)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.8952 (8.2570)\tPrec@1 67.000 (60.049)\tPrec@5 95.000 (95.689)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.9206 (8.2358)\tPrec@1 61.000 (60.282)\tPrec@5 98.000 (95.690)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.3011 (8.2116)\tPrec@1 57.000 (60.309)\tPrec@5 94.000 (95.691)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.5788 (8.2614)\tPrec@1 63.000 (60.000)\tPrec@5 97.000 (95.725)\n",
      "val Results: Prec@1 59.910 Prec@5 95.720 Loss 8.29345\n",
      "val Class Accuracy: [0.908,0.992,0.867,0.436,0.616,0.797,0.514,0.347,0.501,0.013]\n",
      "Best Prec@1: 67.690\n",
      "\n",
      "Epoch: [95][0/97], lr: 0.01000\tTime 0.418 (0.418)\tData 0.256 (0.256)\tLoss 1.6959 (1.6959)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [95][10/97], lr: 0.01000\tTime 0.296 (0.311)\tData 0.000 (0.038)\tLoss 1.9742 (1.8040)\tPrec@1 88.281 (88.707)\tPrec@5 99.219 (99.716)\n",
      "Epoch: [95][20/97], lr: 0.01000\tTime 0.295 (0.304)\tData 0.000 (0.028)\tLoss 1.4785 (1.8551)\tPrec@1 89.844 (88.839)\tPrec@5 99.219 (99.554)\n",
      "Epoch: [95][30/97], lr: 0.01000\tTime 0.297 (0.302)\tData 0.000 (0.025)\tLoss 1.9313 (1.9155)\tPrec@1 89.844 (88.080)\tPrec@5 98.438 (99.471)\n",
      "Epoch: [95][40/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.023)\tLoss 2.0441 (1.9155)\tPrec@1 88.281 (88.338)\tPrec@5 100.000 (99.447)\n",
      "Epoch: [95][50/97], lr: 0.01000\tTime 0.294 (0.301)\tData 0.000 (0.022)\tLoss 1.0494 (1.9408)\tPrec@1 92.969 (88.067)\tPrec@5 100.000 (99.449)\n",
      "Epoch: [95][60/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 2.2989 (1.9068)\tPrec@1 83.594 (88.281)\tPrec@5 99.219 (99.462)\n",
      "Epoch: [95][70/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 1.7071 (1.9332)\tPrec@1 91.406 (88.083)\tPrec@5 100.000 (99.439)\n",
      "Epoch: [95][80/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 1.7678 (1.9581)\tPrec@1 89.844 (88.021)\tPrec@5 100.000 (99.402)\n",
      "Epoch: [95][90/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.9190 (1.9378)\tPrec@1 88.281 (88.170)\tPrec@5 99.219 (99.425)\n",
      "Epoch: [95][96/97], lr: 0.01000\tTime 0.290 (0.299)\tData 0.000 (0.020)\tLoss 1.7755 (1.9305)\tPrec@1 87.288 (88.231)\tPrec@5 100.000 (99.436)\n",
      "Test: [0/100]\tTime 0.271 (0.271)\tLoss 6.9416 (6.9416)\tPrec@1 70.000 (70.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 5.3095 (6.6946)\tPrec@1 71.000 (67.636)\tPrec@5 96.000 (96.909)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.5483 (6.4576)\tPrec@1 76.000 (69.476)\tPrec@5 97.000 (96.810)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.0183 (6.5221)\tPrec@1 72.000 (69.452)\tPrec@5 97.000 (96.484)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 6.4761 (6.6042)\tPrec@1 70.000 (69.122)\tPrec@5 97.000 (96.415)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 6.5986 (6.5739)\tPrec@1 67.000 (69.000)\tPrec@5 97.000 (96.608)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 5.1352 (6.5933)\tPrec@1 77.000 (68.787)\tPrec@5 99.000 (96.656)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 5.5736 (6.5910)\tPrec@1 73.000 (68.718)\tPrec@5 98.000 (96.789)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.5048 (6.5503)\tPrec@1 70.000 (68.877)\tPrec@5 96.000 (96.877)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 5.7830 (6.5949)\tPrec@1 73.000 (68.626)\tPrec@5 100.000 (96.835)\n",
      "val Results: Prec@1 68.430 Prec@5 96.820 Loss 6.61871\n",
      "val Class Accuracy: [0.947,0.961,0.793,0.706,0.778,0.688,0.641,0.560,0.617,0.152]\n",
      "Best Prec@1: 68.430\n",
      "\n",
      "Epoch: [96][0/97], lr: 0.01000\tTime 0.379 (0.379)\tData 0.222 (0.222)\tLoss 1.6473 (1.6473)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [96][10/97], lr: 0.01000\tTime 0.297 (0.308)\tData 0.000 (0.035)\tLoss 1.6256 (1.8068)\tPrec@1 89.062 (88.494)\tPrec@5 99.219 (99.503)\n",
      "Epoch: [96][20/97], lr: 0.01000\tTime 0.297 (0.304)\tData 0.000 (0.027)\tLoss 1.9513 (1.7865)\tPrec@1 86.719 (89.100)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [96][30/97], lr: 0.01000\tTime 0.298 (0.302)\tData 0.000 (0.024)\tLoss 1.5591 (1.8824)\tPrec@1 89.844 (88.659)\tPrec@5 99.219 (99.446)\n",
      "Epoch: [96][40/97], lr: 0.01000\tTime 0.293 (0.301)\tData 0.000 (0.022)\tLoss 1.7410 (1.9318)\tPrec@1 89.062 (88.434)\tPrec@5 99.219 (99.371)\n",
      "Epoch: [96][50/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.021)\tLoss 1.7182 (1.9194)\tPrec@1 89.062 (88.480)\tPrec@5 98.438 (99.357)\n",
      "Epoch: [96][60/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.021)\tLoss 2.1932 (1.9120)\tPrec@1 82.031 (88.371)\tPrec@5 100.000 (99.398)\n",
      "Epoch: [96][70/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 1.9024 (1.9268)\tPrec@1 88.281 (88.237)\tPrec@5 98.438 (99.362)\n",
      "Epoch: [96][80/97], lr: 0.01000\tTime 0.301 (0.300)\tData 0.000 (0.020)\tLoss 2.2496 (1.9615)\tPrec@1 85.156 (88.002)\tPrec@5 96.875 (99.277)\n",
      "Epoch: [96][90/97], lr: 0.01000\tTime 0.304 (0.300)\tData 0.000 (0.019)\tLoss 1.8380 (1.9544)\tPrec@1 89.844 (88.075)\tPrec@5 99.219 (99.287)\n",
      "Epoch: [96][96/97], lr: 0.01000\tTime 0.291 (0.300)\tData 0.000 (0.020)\tLoss 1.4540 (1.9589)\tPrec@1 93.220 (88.062)\tPrec@5 99.153 (99.283)\n",
      "Test: [0/100]\tTime 0.270 (0.270)\tLoss 9.9516 (9.9516)\tPrec@1 54.000 (54.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 7.7203 (9.2373)\tPrec@1 65.000 (57.545)\tPrec@5 95.000 (94.091)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 8.1765 (9.1955)\tPrec@1 60.000 (57.000)\tPrec@5 94.000 (94.333)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.3071 (9.1076)\tPrec@1 62.000 (57.419)\tPrec@5 93.000 (94.290)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.7307 (9.0961)\tPrec@1 54.000 (57.415)\tPrec@5 96.000 (94.317)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.4275 (9.0388)\tPrec@1 61.000 (57.510)\tPrec@5 95.000 (94.490)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 7.6487 (9.0331)\tPrec@1 63.000 (57.377)\tPrec@5 94.000 (94.475)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.5011 (9.0463)\tPrec@1 61.000 (57.183)\tPrec@5 98.000 (94.521)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.4621 (9.0160)\tPrec@1 63.000 (57.346)\tPrec@5 92.000 (94.605)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.8724 (9.1048)\tPrec@1 60.000 (56.879)\tPrec@5 94.000 (94.495)\n",
      "val Results: Prec@1 56.740 Prec@5 94.320 Loss 9.15470\n",
      "val Class Accuracy: [0.951,0.971,0.904,0.603,0.642,0.397,0.565,0.525,0.104,0.012]\n",
      "Best Prec@1: 68.430\n",
      "\n",
      "Epoch: [97][0/97], lr: 0.01000\tTime 0.349 (0.349)\tData 0.208 (0.208)\tLoss 1.6474 (1.6474)\tPrec@1 89.844 (89.844)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [97][10/97], lr: 0.01000\tTime 0.294 (0.304)\tData 0.000 (0.034)\tLoss 1.8550 (1.8854)\tPrec@1 88.281 (88.849)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [97][20/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.026)\tLoss 2.2319 (1.8559)\tPrec@1 85.938 (88.728)\tPrec@5 99.219 (99.368)\n",
      "Epoch: [97][30/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.023)\tLoss 1.9717 (1.9140)\tPrec@1 84.375 (88.231)\tPrec@5 98.438 (99.320)\n",
      "Epoch: [97][40/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.022)\tLoss 2.0648 (1.9284)\tPrec@1 86.719 (87.995)\tPrec@5 99.219 (99.409)\n",
      "Epoch: [97][50/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 1.7312 (1.9235)\tPrec@1 92.969 (88.113)\tPrec@5 99.219 (99.357)\n",
      "Epoch: [97][60/97], lr: 0.01000\tTime 0.300 (0.299)\tData 0.000 (0.020)\tLoss 2.2097 (1.9258)\tPrec@1 89.062 (88.102)\tPrec@5 100.000 (99.360)\n",
      "Epoch: [97][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.9740 (1.9744)\tPrec@1 87.500 (87.764)\tPrec@5 99.219 (99.340)\n",
      "Epoch: [97][80/97], lr: 0.01000\tTime 0.302 (0.299)\tData 0.000 (0.020)\tLoss 2.0685 (1.9912)\tPrec@1 88.281 (87.703)\tPrec@5 99.219 (99.354)\n",
      "Epoch: [97][90/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.019)\tLoss 2.3921 (1.9965)\tPrec@1 85.938 (87.680)\tPrec@5 96.875 (99.313)\n",
      "Epoch: [97][96/97], lr: 0.01000\tTime 0.289 (0.298)\tData 0.000 (0.020)\tLoss 1.9152 (1.9943)\tPrec@1 87.288 (87.716)\tPrec@5 100.000 (99.315)\n",
      "Test: [0/100]\tTime 0.257 (0.257)\tLoss 6.5503 (6.5503)\tPrec@1 72.000 (72.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.072 (0.089)\tLoss 5.4392 (6.4415)\tPrec@1 70.000 (69.727)\tPrec@5 97.000 (96.909)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 4.8122 (6.1863)\tPrec@1 76.000 (70.619)\tPrec@5 95.000 (96.714)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 5.6643 (6.2539)\tPrec@1 75.000 (70.710)\tPrec@5 94.000 (96.000)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 5.8339 (6.3187)\tPrec@1 73.000 (70.561)\tPrec@5 97.000 (95.951)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 5.9140 (6.3105)\tPrec@1 74.000 (70.392)\tPrec@5 96.000 (96.196)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.2738 (6.3386)\tPrec@1 66.000 (69.967)\tPrec@5 95.000 (96.262)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 5.4072 (6.3059)\tPrec@1 76.000 (70.070)\tPrec@5 96.000 (96.324)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.8515 (6.2824)\tPrec@1 75.000 (70.235)\tPrec@5 95.000 (96.370)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.7271 (6.3154)\tPrec@1 73.000 (69.967)\tPrec@5 99.000 (96.440)\n",
      "val Results: Prec@1 69.940 Prec@5 96.430 Loss 6.34638\n",
      "val Class Accuracy: [0.930,0.969,0.742,0.681,0.764,0.618,0.834,0.649,0.550,0.257]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [98][0/97], lr: 0.01000\tTime 0.384 (0.384)\tData 0.241 (0.241)\tLoss 1.8690 (1.8690)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [98][10/97], lr: 0.01000\tTime 0.291 (0.307)\tData 0.000 (0.037)\tLoss 2.0798 (1.8716)\tPrec@1 89.844 (88.352)\tPrec@5 99.219 (99.006)\n",
      "Epoch: [98][20/97], lr: 0.01000\tTime 0.299 (0.303)\tData 0.000 (0.028)\tLoss 2.0865 (1.8560)\tPrec@1 88.281 (88.542)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [98][30/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.024)\tLoss 1.9018 (1.9582)\tPrec@1 85.938 (88.130)\tPrec@5 100.000 (99.244)\n",
      "Epoch: [98][40/97], lr: 0.01000\tTime 0.286 (0.300)\tData 0.000 (0.023)\tLoss 1.9228 (1.9683)\tPrec@1 87.500 (87.957)\tPrec@5 100.000 (99.390)\n",
      "Epoch: [98][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 2.0554 (1.9769)\tPrec@1 89.062 (88.006)\tPrec@5 99.219 (99.387)\n",
      "Epoch: [98][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 1.4423 (1.9602)\tPrec@1 91.406 (88.012)\tPrec@5 99.219 (99.424)\n",
      "Epoch: [98][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.4213 (1.9722)\tPrec@1 85.156 (87.852)\tPrec@5 99.219 (99.417)\n",
      "Epoch: [98][80/97], lr: 0.01000\tTime 0.298 (0.299)\tData 0.000 (0.020)\tLoss 2.2957 (1.9888)\tPrec@1 88.281 (87.799)\tPrec@5 98.438 (99.373)\n",
      "Epoch: [98][90/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 1.9652 (1.9901)\tPrec@1 89.062 (87.826)\tPrec@5 99.219 (99.365)\n",
      "Epoch: [98][96/97], lr: 0.01000\tTime 0.288 (0.299)\tData 0.000 (0.020)\tLoss 2.5784 (1.9887)\tPrec@1 85.593 (87.925)\tPrec@5 99.153 (99.371)\n",
      "Test: [0/100]\tTime 0.259 (0.259)\tLoss 8.9926 (8.9926)\tPrec@1 58.000 (58.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 7.3119 (8.1209)\tPrec@1 63.000 (63.000)\tPrec@5 91.000 (94.545)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 6.5322 (8.0288)\tPrec@1 67.000 (62.714)\tPrec@5 98.000 (94.619)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 6.7654 (7.9037)\tPrec@1 69.000 (63.258)\tPrec@5 97.000 (94.903)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.3522 (7.8778)\tPrec@1 57.000 (63.488)\tPrec@5 93.000 (94.927)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.3263 (7.8095)\tPrec@1 65.000 (63.588)\tPrec@5 95.000 (94.941)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.4422 (7.8274)\tPrec@1 71.000 (63.475)\tPrec@5 94.000 (94.869)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.2124 (7.8410)\tPrec@1 66.000 (63.479)\tPrec@5 94.000 (94.761)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 7.3122 (7.8126)\tPrec@1 65.000 (63.531)\tPrec@5 96.000 (94.914)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.3842 (7.8936)\tPrec@1 67.000 (63.033)\tPrec@5 99.000 (94.879)\n",
      "val Results: Prec@1 62.980 Prec@5 94.810 Loss 7.91483\n",
      "val Class Accuracy: [0.918,0.976,0.857,0.687,0.731,0.652,0.258,0.592,0.323,0.304]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [99][0/97], lr: 0.01000\tTime 0.350 (0.350)\tData 0.209 (0.209)\tLoss 0.9583 (0.9583)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [99][10/97], lr: 0.01000\tTime 0.297 (0.305)\tData 0.000 (0.034)\tLoss 1.4502 (1.8178)\tPrec@1 92.188 (88.352)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [99][20/97], lr: 0.01000\tTime 0.302 (0.302)\tData 0.000 (0.026)\tLoss 1.7289 (2.0488)\tPrec@1 90.625 (87.240)\tPrec@5 98.438 (99.144)\n",
      "Epoch: [99][30/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.023)\tLoss 1.5197 (2.0168)\tPrec@1 89.844 (87.324)\tPrec@5 99.219 (99.194)\n",
      "Epoch: [99][40/97], lr: 0.01000\tTime 0.305 (0.301)\tData 0.000 (0.022)\tLoss 2.4181 (1.9951)\tPrec@1 83.594 (87.500)\tPrec@5 99.219 (99.314)\n",
      "Epoch: [99][50/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.021)\tLoss 1.9763 (2.0214)\tPrec@1 88.281 (87.423)\tPrec@5 99.219 (99.295)\n",
      "Epoch: [99][60/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.020)\tLoss 2.5547 (2.0426)\tPrec@1 82.812 (87.257)\tPrec@5 96.875 (99.257)\n",
      "Epoch: [99][70/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.020)\tLoss 2.8120 (2.0528)\tPrec@1 82.812 (87.269)\tPrec@5 100.000 (99.274)\n",
      "Epoch: [99][80/97], lr: 0.01000\tTime 0.305 (0.300)\tData 0.000 (0.020)\tLoss 1.7540 (2.0134)\tPrec@1 90.625 (87.529)\tPrec@5 98.438 (99.315)\n",
      "Epoch: [99][90/97], lr: 0.01000\tTime 0.305 (0.300)\tData 0.000 (0.019)\tLoss 2.3942 (2.0297)\tPrec@1 85.156 (87.423)\tPrec@5 100.000 (99.305)\n",
      "Epoch: [99][96/97], lr: 0.01000\tTime 0.290 (0.300)\tData 0.000 (0.020)\tLoss 2.2115 (2.0161)\tPrec@1 88.136 (87.538)\tPrec@5 98.305 (99.323)\n",
      "Test: [0/100]\tTime 0.269 (0.269)\tLoss 8.0484 (8.0484)\tPrec@1 61.000 (61.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 6.1560 (7.3821)\tPrec@1 69.000 (65.273)\tPrec@5 97.000 (95.273)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.6013 (7.3071)\tPrec@1 66.000 (65.476)\tPrec@5 97.000 (95.143)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.2973 (7.2535)\tPrec@1 68.000 (65.774)\tPrec@5 96.000 (94.677)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 6.8513 (7.2955)\tPrec@1 70.000 (65.829)\tPrec@5 94.000 (94.537)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.0318 (7.2478)\tPrec@1 68.000 (66.000)\tPrec@5 91.000 (94.667)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.6895 (7.3014)\tPrec@1 65.000 (65.492)\tPrec@5 96.000 (94.721)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 6.5364 (7.3058)\tPrec@1 68.000 (65.507)\tPrec@5 97.000 (94.676)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.0074 (7.2966)\tPrec@1 61.000 (65.531)\tPrec@5 91.000 (94.741)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.1889 (7.3238)\tPrec@1 70.000 (65.451)\tPrec@5 95.000 (94.604)\n",
      "val Results: Prec@1 65.330 Prec@5 94.560 Loss 7.36910\n",
      "val Class Accuracy: [0.959,0.936,0.849,0.658,0.787,0.612,0.650,0.491,0.587,0.004]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [100][0/97], lr: 0.01000\tTime 0.376 (0.376)\tData 0.214 (0.214)\tLoss 2.1848 (2.1848)\tPrec@1 85.156 (85.156)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [100][10/97], lr: 0.01000\tTime 0.293 (0.305)\tData 0.000 (0.034)\tLoss 2.2680 (2.0874)\tPrec@1 85.156 (86.719)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [100][20/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.026)\tLoss 1.8436 (2.0565)\tPrec@1 86.719 (87.202)\tPrec@5 99.219 (99.293)\n",
      "Epoch: [100][30/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.023)\tLoss 2.3212 (1.9752)\tPrec@1 84.375 (87.727)\tPrec@5 97.656 (99.269)\n",
      "Epoch: [100][40/97], lr: 0.01000\tTime 0.298 (0.300)\tData 0.000 (0.022)\tLoss 1.9626 (1.9349)\tPrec@1 87.500 (88.053)\tPrec@5 99.219 (99.371)\n",
      "Epoch: [100][50/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 1.5370 (1.9036)\tPrec@1 90.625 (88.312)\tPrec@5 100.000 (99.387)\n",
      "Epoch: [100][60/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.020)\tLoss 1.9349 (1.9012)\tPrec@1 86.719 (88.358)\tPrec@5 100.000 (99.360)\n",
      "Epoch: [100][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.8600 (1.8975)\tPrec@1 85.156 (88.380)\tPrec@5 97.656 (99.329)\n",
      "Epoch: [100][80/97], lr: 0.01000\tTime 0.298 (0.299)\tData 0.000 (0.019)\tLoss 1.2633 (1.8947)\tPrec@1 92.969 (88.358)\tPrec@5 99.219 (99.325)\n",
      "Epoch: [100][90/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.019)\tLoss 1.5982 (1.9160)\tPrec@1 91.406 (88.204)\tPrec@5 100.000 (99.348)\n",
      "Epoch: [100][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 1.5491 (1.9098)\tPrec@1 90.678 (88.256)\tPrec@5 100.000 (99.379)\n",
      "Test: [0/100]\tTime 0.265 (0.265)\tLoss 8.1247 (8.1247)\tPrec@1 61.000 (61.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.6984 (8.2581)\tPrec@1 68.000 (60.727)\tPrec@5 95.000 (95.182)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.0736 (8.0440)\tPrec@1 64.000 (61.619)\tPrec@5 94.000 (94.905)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 7.7456 (8.1002)\tPrec@1 64.000 (61.903)\tPrec@5 95.000 (95.000)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 7.8195 (8.1443)\tPrec@1 67.000 (61.927)\tPrec@5 95.000 (94.951)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.3377 (8.1404)\tPrec@1 67.000 (61.843)\tPrec@5 94.000 (95.059)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 8.0268 (8.1870)\tPrec@1 58.000 (61.475)\tPrec@5 93.000 (94.852)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 7.7169 (8.1824)\tPrec@1 65.000 (61.366)\tPrec@5 99.000 (94.887)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 8.2574 (8.1684)\tPrec@1 62.000 (61.481)\tPrec@5 95.000 (95.012)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.4119 (8.1371)\tPrec@1 61.000 (61.747)\tPrec@5 95.000 (95.033)\n",
      "val Results: Prec@1 61.700 Prec@5 94.940 Loss 8.16149\n",
      "val Class Accuracy: [0.943,0.987,0.701,0.785,0.764,0.218,0.729,0.387,0.634,0.022]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [101][0/97], lr: 0.01000\tTime 0.372 (0.372)\tData 0.234 (0.234)\tLoss 1.7031 (1.7031)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [101][10/97], lr: 0.01000\tTime 0.293 (0.307)\tData 0.000 (0.037)\tLoss 0.7993 (1.7813)\tPrec@1 95.312 (89.062)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [101][20/97], lr: 0.01000\tTime 0.307 (0.303)\tData 0.000 (0.027)\tLoss 1.7254 (1.8566)\tPrec@1 88.281 (88.393)\tPrec@5 99.219 (99.479)\n",
      "Epoch: [101][30/97], lr: 0.01000\tTime 0.297 (0.302)\tData 0.000 (0.024)\tLoss 2.2339 (1.9117)\tPrec@1 86.719 (88.206)\tPrec@5 99.219 (99.269)\n",
      "Epoch: [101][40/97], lr: 0.01000\tTime 0.302 (0.301)\tData 0.000 (0.022)\tLoss 2.1473 (1.9113)\tPrec@1 85.938 (88.262)\tPrec@5 99.219 (99.238)\n",
      "Epoch: [101][50/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.021)\tLoss 2.7671 (1.9762)\tPrec@1 81.250 (87.730)\tPrec@5 100.000 (99.234)\n",
      "Epoch: [101][60/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 1.1989 (1.9941)\tPrec@1 92.969 (87.628)\tPrec@5 100.000 (99.193)\n",
      "Epoch: [101][70/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 2.1688 (1.9877)\tPrec@1 85.938 (87.632)\tPrec@5 96.875 (99.164)\n",
      "Epoch: [101][80/97], lr: 0.01000\tTime 0.300 (0.300)\tData 0.000 (0.020)\tLoss 1.8517 (1.9786)\tPrec@1 87.500 (87.693)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [101][90/97], lr: 0.01000\tTime 0.301 (0.299)\tData 0.000 (0.020)\tLoss 2.3818 (2.0002)\tPrec@1 85.156 (87.672)\tPrec@5 99.219 (99.184)\n",
      "Epoch: [101][96/97], lr: 0.01000\tTime 0.288 (0.299)\tData 0.000 (0.020)\tLoss 1.8844 (2.0078)\tPrec@1 86.441 (87.619)\tPrec@5 100.000 (99.194)\n",
      "Test: [0/100]\tTime 0.260 (0.260)\tLoss 8.0518 (8.0518)\tPrec@1 64.000 (64.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.072 (0.089)\tLoss 5.9235 (7.3533)\tPrec@1 72.000 (64.636)\tPrec@5 98.000 (97.182)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.5468 (7.3003)\tPrec@1 73.000 (64.381)\tPrec@5 98.000 (97.095)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 7.3522 (7.3533)\tPrec@1 63.000 (64.194)\tPrec@5 97.000 (96.710)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 6.7473 (7.3597)\tPrec@1 67.000 (64.366)\tPrec@5 97.000 (96.341)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.6528 (7.2906)\tPrec@1 67.000 (64.529)\tPrec@5 97.000 (96.529)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.8602 (7.3347)\tPrec@1 63.000 (64.328)\tPrec@5 98.000 (96.443)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 6.1584 (7.3172)\tPrec@1 72.000 (64.451)\tPrec@5 100.000 (96.437)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.4417 (7.2589)\tPrec@1 65.000 (64.877)\tPrec@5 94.000 (96.469)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.7841 (7.2836)\tPrec@1 66.000 (64.769)\tPrec@5 100.000 (96.462)\n",
      "val Results: Prec@1 64.640 Prec@5 96.440 Loss 7.31239\n",
      "val Class Accuracy: [0.969,0.972,0.663,0.704,0.665,0.638,0.748,0.415,0.487,0.203]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [102][0/97], lr: 0.01000\tTime 0.382 (0.382)\tData 0.213 (0.213)\tLoss 1.5690 (1.5690)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [102][10/97], lr: 0.01000\tTime 0.297 (0.307)\tData 0.000 (0.034)\tLoss 2.3167 (2.2012)\tPrec@1 82.031 (85.795)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [102][20/97], lr: 0.01000\tTime 0.303 (0.307)\tData 0.000 (0.026)\tLoss 2.6263 (2.0648)\tPrec@1 83.594 (86.868)\tPrec@5 99.219 (99.479)\n",
      "Epoch: [102][30/97], lr: 0.01000\tTime 0.297 (0.307)\tData 0.000 (0.023)\tLoss 1.7134 (2.1003)\tPrec@1 89.844 (87.097)\tPrec@5 100.000 (99.370)\n",
      "Epoch: [102][40/97], lr: 0.01000\tTime 0.292 (0.305)\tData 0.000 (0.022)\tLoss 1.3534 (2.0912)\tPrec@1 92.188 (87.100)\tPrec@5 100.000 (99.314)\n",
      "Epoch: [102][50/97], lr: 0.01000\tTime 0.296 (0.304)\tData 0.000 (0.021)\tLoss 2.5266 (2.0735)\tPrec@1 85.156 (87.255)\tPrec@5 98.438 (99.311)\n",
      "Epoch: [102][60/97], lr: 0.01000\tTime 0.295 (0.303)\tData 0.000 (0.020)\tLoss 2.0832 (2.0361)\tPrec@1 85.938 (87.410)\tPrec@5 98.438 (99.283)\n",
      "Epoch: [102][70/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.020)\tLoss 1.2550 (1.9924)\tPrec@1 92.969 (87.753)\tPrec@5 99.219 (99.285)\n",
      "Epoch: [102][80/97], lr: 0.01000\tTime 0.305 (0.302)\tData 0.000 (0.019)\tLoss 1.7732 (1.9848)\tPrec@1 88.281 (87.712)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [102][90/97], lr: 0.01000\tTime 0.302 (0.302)\tData 0.000 (0.019)\tLoss 1.7457 (1.9851)\tPrec@1 90.625 (87.680)\tPrec@5 100.000 (99.313)\n",
      "Epoch: [102][96/97], lr: 0.01000\tTime 0.290 (0.301)\tData 0.000 (0.020)\tLoss 1.4390 (1.9836)\tPrec@1 92.373 (87.732)\tPrec@5 99.153 (99.315)\n",
      "Test: [0/100]\tTime 0.259 (0.259)\tLoss 8.6315 (8.6315)\tPrec@1 63.000 (63.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.6821 (8.0388)\tPrec@1 70.000 (61.636)\tPrec@5 97.000 (96.455)\n",
      "Test: [20/100]\tTime 0.072 (0.082)\tLoss 5.9772 (7.7994)\tPrec@1 70.000 (63.190)\tPrec@5 97.000 (95.667)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.7623 (7.8578)\tPrec@1 63.000 (62.613)\tPrec@5 97.000 (95.903)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.8638 (7.8730)\tPrec@1 61.000 (62.293)\tPrec@5 92.000 (95.634)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.8842 (7.7775)\tPrec@1 67.000 (62.941)\tPrec@5 97.000 (95.627)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 7.0307 (7.7786)\tPrec@1 66.000 (62.852)\tPrec@5 94.000 (95.525)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.1445 (7.7748)\tPrec@1 68.000 (62.944)\tPrec@5 98.000 (95.577)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.2419 (7.7411)\tPrec@1 65.000 (63.074)\tPrec@5 97.000 (95.617)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.8591 (7.7832)\tPrec@1 63.000 (62.901)\tPrec@5 96.000 (95.637)\n",
      "val Results: Prec@1 62.710 Prec@5 95.470 Loss 7.82754\n",
      "val Class Accuracy: [0.851,0.997,0.685,0.659,0.787,0.705,0.671,0.469,0.366,0.081]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [103][0/97], lr: 0.01000\tTime 0.386 (0.386)\tData 0.239 (0.239)\tLoss 2.6732 (2.6732)\tPrec@1 84.375 (84.375)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [103][10/97], lr: 0.01000\tTime 0.295 (0.307)\tData 0.000 (0.037)\tLoss 1.5961 (1.7278)\tPrec@1 90.625 (89.986)\tPrec@5 98.438 (99.503)\n",
      "Epoch: [103][20/97], lr: 0.01000\tTime 0.295 (0.303)\tData 0.000 (0.028)\tLoss 1.8368 (1.9034)\tPrec@1 88.281 (88.616)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [103][30/97], lr: 0.01000\tTime 0.304 (0.303)\tData 0.000 (0.024)\tLoss 1.7497 (1.9464)\tPrec@1 90.625 (88.130)\tPrec@5 100.000 (99.572)\n",
      "Epoch: [103][40/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.022)\tLoss 2.8176 (1.9853)\tPrec@1 83.594 (87.881)\tPrec@5 100.000 (99.505)\n",
      "Epoch: [103][50/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 2.2419 (1.9942)\tPrec@1 87.500 (87.822)\tPrec@5 100.000 (99.464)\n",
      "Epoch: [103][60/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 2.4405 (1.9681)\tPrec@1 85.938 (87.948)\tPrec@5 96.875 (99.411)\n",
      "Epoch: [103][70/97], lr: 0.01000\tTime 0.294 (0.300)\tData 0.000 (0.020)\tLoss 1.8467 (1.9610)\tPrec@1 88.281 (88.006)\tPrec@5 100.000 (99.384)\n",
      "Epoch: [103][80/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 1.4673 (1.9743)\tPrec@1 91.406 (88.002)\tPrec@5 99.219 (99.373)\n",
      "Epoch: [103][90/97], lr: 0.01000\tTime 0.294 (0.300)\tData 0.000 (0.020)\tLoss 2.8981 (1.9657)\tPrec@1 80.469 (87.981)\tPrec@5 99.219 (99.382)\n",
      "Epoch: [103][96/97], lr: 0.01000\tTime 0.288 (0.299)\tData 0.000 (0.020)\tLoss 1.7113 (1.9507)\tPrec@1 88.983 (88.086)\tPrec@5 100.000 (99.363)\n",
      "Test: [0/100]\tTime 0.270 (0.270)\tLoss 9.2631 (9.2631)\tPrec@1 54.000 (54.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 6.5501 (8.6313)\tPrec@1 69.000 (59.364)\tPrec@5 97.000 (96.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.1985 (8.5571)\tPrec@1 63.000 (58.762)\tPrec@5 98.000 (96.143)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 7.9572 (8.5233)\tPrec@1 57.000 (58.839)\tPrec@5 95.000 (95.903)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 8.0184 (8.4981)\tPrec@1 66.000 (59.390)\tPrec@5 96.000 (95.805)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.2740 (8.4401)\tPrec@1 67.000 (59.784)\tPrec@5 96.000 (95.686)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.6434 (8.4012)\tPrec@1 66.000 (59.787)\tPrec@5 95.000 (95.639)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.3343 (8.3677)\tPrec@1 60.000 (60.113)\tPrec@5 94.000 (95.577)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.4484 (8.3510)\tPrec@1 63.000 (60.160)\tPrec@5 94.000 (95.642)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.5225 (8.3991)\tPrec@1 59.000 (59.967)\tPrec@5 98.000 (95.626)\n",
      "val Results: Prec@1 59.890 Prec@5 95.600 Loss 8.43236\n",
      "val Class Accuracy: [0.968,0.984,0.677,0.411,0.639,0.855,0.809,0.423,0.182,0.041]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [104][0/97], lr: 0.01000\tTime 0.363 (0.363)\tData 0.203 (0.203)\tLoss 1.6567 (1.6567)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [104][10/97], lr: 0.01000\tTime 0.297 (0.305)\tData 0.000 (0.033)\tLoss 1.5681 (1.7328)\tPrec@1 92.188 (89.560)\tPrec@5 98.438 (99.361)\n",
      "Epoch: [104][20/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.026)\tLoss 1.6797 (1.8062)\tPrec@1 89.844 (89.174)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [104][30/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.023)\tLoss 1.8461 (1.8373)\tPrec@1 89.844 (88.962)\tPrec@5 100.000 (99.521)\n",
      "Epoch: [104][40/97], lr: 0.01000\tTime 0.292 (0.299)\tData 0.000 (0.022)\tLoss 2.2104 (1.9058)\tPrec@1 88.281 (88.586)\tPrec@5 97.656 (99.447)\n",
      "Epoch: [104][50/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 1.5340 (1.9324)\tPrec@1 90.625 (88.343)\tPrec@5 98.438 (99.418)\n",
      "Epoch: [104][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 2.0216 (1.9259)\tPrec@1 86.719 (88.345)\tPrec@5 100.000 (99.449)\n",
      "Epoch: [104][70/97], lr: 0.01000\tTime 0.298 (0.299)\tData 0.000 (0.020)\tLoss 1.7564 (1.9177)\tPrec@1 90.625 (88.446)\tPrec@5 98.438 (99.417)\n",
      "Epoch: [104][80/97], lr: 0.01000\tTime 0.300 (0.299)\tData 0.000 (0.019)\tLoss 1.9260 (1.9451)\tPrec@1 85.156 (88.175)\tPrec@5 100.000 (99.441)\n",
      "Epoch: [104][90/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.019)\tLoss 1.4764 (1.9742)\tPrec@1 91.406 (88.067)\tPrec@5 100.000 (99.408)\n",
      "Epoch: [104][96/97], lr: 0.01000\tTime 0.290 (0.299)\tData 0.000 (0.020)\tLoss 1.3259 (1.9683)\tPrec@1 92.373 (88.135)\tPrec@5 99.153 (99.395)\n",
      "Test: [0/100]\tTime 0.255 (0.255)\tLoss 11.4658 (11.4658)\tPrec@1 46.000 (46.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 8.3454 (10.7784)\tPrec@1 62.000 (49.273)\tPrec@5 91.000 (94.364)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 9.3015 (10.6964)\tPrec@1 53.000 (48.952)\tPrec@5 96.000 (94.476)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 10.6556 (10.6219)\tPrec@1 47.000 (49.419)\tPrec@5 95.000 (94.484)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 11.6321 (10.6235)\tPrec@1 43.000 (49.122)\tPrec@5 92.000 (94.317)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 10.9683 (10.5917)\tPrec@1 45.000 (49.196)\tPrec@5 93.000 (94.176)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 8.9849 (10.5918)\tPrec@1 59.000 (49.148)\tPrec@5 94.000 (94.164)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 10.4418 (10.5731)\tPrec@1 50.000 (49.296)\tPrec@5 95.000 (94.113)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.4724 (10.4928)\tPrec@1 56.000 (49.630)\tPrec@5 95.000 (94.284)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.1770 (10.5773)\tPrec@1 61.000 (49.308)\tPrec@5 96.000 (94.242)\n",
      "val Results: Prec@1 49.270 Prec@5 94.100 Loss 10.60063\n",
      "val Class Accuracy: [0.955,0.981,0.927,0.459,0.325,0.364,0.233,0.385,0.188,0.110]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [105][0/97], lr: 0.01000\tTime 0.363 (0.363)\tData 0.226 (0.226)\tLoss 2.3218 (2.3218)\tPrec@1 82.031 (82.031)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [105][10/97], lr: 0.01000\tTime 0.295 (0.306)\tData 0.000 (0.036)\tLoss 2.3388 (1.9038)\tPrec@1 85.156 (88.565)\tPrec@5 98.438 (99.361)\n",
      "Epoch: [105][20/97], lr: 0.01000\tTime 0.298 (0.302)\tData 0.000 (0.027)\tLoss 2.5723 (1.9671)\tPrec@1 82.812 (87.946)\tPrec@5 99.219 (99.330)\n",
      "Epoch: [105][30/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.024)\tLoss 1.5264 (1.9454)\tPrec@1 91.406 (88.130)\tPrec@5 100.000 (99.294)\n",
      "Epoch: [105][40/97], lr: 0.01000\tTime 0.303 (0.300)\tData 0.000 (0.022)\tLoss 2.4100 (1.9904)\tPrec@1 85.938 (87.976)\tPrec@5 98.438 (99.257)\n",
      "Epoch: [105][50/97], lr: 0.01000\tTime 0.298 (0.300)\tData 0.000 (0.021)\tLoss 1.3758 (1.9763)\tPrec@1 91.406 (87.990)\tPrec@5 100.000 (99.234)\n",
      "Epoch: [105][60/97], lr: 0.01000\tTime 0.298 (0.300)\tData 0.000 (0.021)\tLoss 1.5349 (1.9786)\tPrec@1 89.844 (87.884)\tPrec@5 100.000 (99.270)\n",
      "Epoch: [105][70/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 2.5379 (2.0315)\tPrec@1 86.719 (87.687)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [105][80/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 2.1827 (2.0437)\tPrec@1 85.156 (87.596)\tPrec@5 100.000 (99.257)\n",
      "Epoch: [105][90/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.019)\tLoss 1.9233 (2.0407)\tPrec@1 87.500 (87.680)\tPrec@5 100.000 (99.305)\n",
      "Epoch: [105][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 1.1895 (2.0215)\tPrec@1 93.220 (87.764)\tPrec@5 99.153 (99.283)\n",
      "Test: [0/100]\tTime 0.278 (0.278)\tLoss 8.3660 (8.3660)\tPrec@1 59.000 (59.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 6.4405 (7.7515)\tPrec@1 72.000 (62.818)\tPrec@5 97.000 (97.545)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.7055 (7.6228)\tPrec@1 70.000 (63.476)\tPrec@5 96.000 (97.095)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 6.5933 (7.5572)\tPrec@1 69.000 (64.290)\tPrec@5 94.000 (96.806)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 6.3550 (7.5508)\tPrec@1 72.000 (64.415)\tPrec@5 95.000 (96.707)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.5738 (7.4684)\tPrec@1 72.000 (64.824)\tPrec@5 98.000 (96.980)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.1989 (7.4149)\tPrec@1 69.000 (65.016)\tPrec@5 98.000 (96.902)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 6.8782 (7.4059)\tPrec@1 69.000 (65.028)\tPrec@5 99.000 (96.887)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.7437 (7.3682)\tPrec@1 67.000 (65.111)\tPrec@5 97.000 (96.951)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.2609 (7.4441)\tPrec@1 63.000 (64.725)\tPrec@5 96.000 (96.890)\n",
      "val Results: Prec@1 64.570 Prec@5 96.850 Loss 7.48507\n",
      "val Class Accuracy: [0.965,0.991,0.552,0.751,0.788,0.702,0.590,0.691,0.228,0.199]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [106][0/97], lr: 0.01000\tTime 0.365 (0.365)\tData 0.209 (0.209)\tLoss 1.7531 (1.7531)\tPrec@1 88.281 (88.281)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [106][10/97], lr: 0.01000\tTime 0.297 (0.308)\tData 0.000 (0.034)\tLoss 1.3615 (1.7671)\tPrec@1 92.969 (89.773)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [106][20/97], lr: 0.01000\tTime 0.299 (0.304)\tData 0.000 (0.026)\tLoss 1.4874 (1.8747)\tPrec@1 90.625 (88.839)\tPrec@5 99.219 (99.591)\n",
      "Epoch: [106][30/97], lr: 0.01000\tTime 0.297 (0.303)\tData 0.000 (0.023)\tLoss 1.4531 (1.8384)\tPrec@1 89.844 (88.987)\tPrec@5 99.219 (99.546)\n",
      "Epoch: [106][40/97], lr: 0.01000\tTime 0.291 (0.302)\tData 0.000 (0.022)\tLoss 2.2626 (1.8359)\tPrec@1 87.500 (89.139)\tPrec@5 100.000 (99.524)\n",
      "Epoch: [106][50/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.021)\tLoss 2.1117 (1.8411)\tPrec@1 86.719 (89.093)\tPrec@5 99.219 (99.525)\n",
      "Epoch: [106][60/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.020)\tLoss 1.6909 (1.8782)\tPrec@1 86.719 (88.729)\tPrec@5 99.219 (99.449)\n",
      "Epoch: [106][70/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.020)\tLoss 1.3616 (1.8833)\tPrec@1 91.406 (88.666)\tPrec@5 99.219 (99.439)\n",
      "Epoch: [106][80/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.020)\tLoss 2.3733 (1.9132)\tPrec@1 86.719 (88.397)\tPrec@5 100.000 (99.489)\n",
      "Epoch: [106][90/97], lr: 0.01000\tTime 0.305 (0.300)\tData 0.000 (0.019)\tLoss 1.3371 (1.9197)\tPrec@1 92.969 (88.350)\tPrec@5 99.219 (99.459)\n",
      "Epoch: [106][96/97], lr: 0.01000\tTime 0.290 (0.300)\tData 0.000 (0.020)\tLoss 1.3959 (1.9168)\tPrec@1 91.525 (88.312)\tPrec@5 100.000 (99.444)\n",
      "Test: [0/100]\tTime 0.244 (0.244)\tLoss 9.4125 (9.4125)\tPrec@1 54.000 (54.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 7.7877 (8.5441)\tPrec@1 58.000 (56.909)\tPrec@5 89.000 (94.182)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.5580 (8.5849)\tPrec@1 69.000 (57.000)\tPrec@5 96.000 (94.333)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.9614 (8.5074)\tPrec@1 58.000 (57.839)\tPrec@5 91.000 (94.323)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.7179 (8.4809)\tPrec@1 62.000 (58.195)\tPrec@5 96.000 (94.049)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.5746 (8.4459)\tPrec@1 64.000 (58.529)\tPrec@5 94.000 (94.098)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 7.8274 (8.4399)\tPrec@1 61.000 (58.508)\tPrec@5 95.000 (93.951)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.7433 (8.4365)\tPrec@1 63.000 (58.535)\tPrec@5 96.000 (94.000)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.5138 (8.3799)\tPrec@1 61.000 (58.790)\tPrec@5 95.000 (94.136)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 8.5117 (8.4612)\tPrec@1 58.000 (58.286)\tPrec@5 98.000 (94.187)\n",
      "val Results: Prec@1 58.330 Prec@5 94.260 Loss 8.47499\n",
      "val Class Accuracy: [0.975,0.973,0.711,0.328,0.646,0.669,0.471,0.753,0.169,0.138]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [107][0/97], lr: 0.01000\tTime 0.380 (0.380)\tData 0.224 (0.224)\tLoss 1.7828 (1.7828)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [107][10/97], lr: 0.01000\tTime 0.300 (0.309)\tData 0.000 (0.036)\tLoss 2.2189 (1.9962)\tPrec@1 87.500 (88.281)\tPrec@5 98.438 (99.361)\n",
      "Epoch: [107][20/97], lr: 0.01000\tTime 0.296 (0.304)\tData 0.000 (0.027)\tLoss 1.5153 (1.9083)\tPrec@1 91.406 (88.542)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [107][30/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.024)\tLoss 2.2168 (1.8309)\tPrec@1 84.375 (89.037)\tPrec@5 100.000 (99.521)\n",
      "Epoch: [107][40/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.022)\tLoss 1.6419 (1.7757)\tPrec@1 91.406 (89.310)\tPrec@5 98.438 (99.486)\n",
      "Epoch: [107][50/97], lr: 0.01000\tTime 0.295 (0.305)\tData 0.000 (0.025)\tLoss 1.9752 (1.7985)\tPrec@1 88.281 (89.277)\tPrec@5 100.000 (99.433)\n",
      "Epoch: [107][60/97], lr: 0.01000\tTime 0.296 (0.304)\tData 0.000 (0.024)\tLoss 2.0621 (1.8345)\tPrec@1 87.500 (88.947)\tPrec@5 100.000 (99.436)\n",
      "Epoch: [107][70/97], lr: 0.01000\tTime 0.296 (0.303)\tData 0.000 (0.023)\tLoss 2.2520 (1.8657)\tPrec@1 85.938 (88.732)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [107][80/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.022)\tLoss 2.7981 (1.8743)\tPrec@1 84.375 (88.667)\tPrec@5 98.438 (99.412)\n",
      "Epoch: [107][90/97], lr: 0.01000\tTime 0.297 (0.302)\tData 0.000 (0.022)\tLoss 1.5760 (1.9068)\tPrec@1 92.188 (88.513)\tPrec@5 100.000 (99.373)\n",
      "Epoch: [107][96/97], lr: 0.01000\tTime 0.294 (0.302)\tData 0.000 (0.022)\tLoss 2.9632 (1.9107)\tPrec@1 83.898 (88.514)\tPrec@5 98.305 (99.363)\n",
      "Test: [0/100]\tTime 0.288 (0.288)\tLoss 8.5453 (8.5453)\tPrec@1 58.000 (58.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 6.7838 (7.9509)\tPrec@1 64.000 (62.091)\tPrec@5 97.000 (96.545)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 7.2129 (7.8276)\tPrec@1 63.000 (62.476)\tPrec@5 98.000 (96.333)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 7.2305 (7.7527)\tPrec@1 65.000 (62.774)\tPrec@5 98.000 (96.581)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 7.4742 (7.7790)\tPrec@1 67.000 (62.805)\tPrec@5 98.000 (96.488)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.2150 (7.7724)\tPrec@1 64.000 (62.765)\tPrec@5 96.000 (96.725)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.1898 (7.7790)\tPrec@1 63.000 (62.721)\tPrec@5 97.000 (96.738)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 7.0176 (7.7577)\tPrec@1 68.000 (62.746)\tPrec@5 98.000 (96.859)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 6.9350 (7.7123)\tPrec@1 68.000 (63.012)\tPrec@5 96.000 (96.852)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.4483 (7.7745)\tPrec@1 65.000 (62.725)\tPrec@5 99.000 (96.791)\n",
      "val Results: Prec@1 62.570 Prec@5 96.770 Loss 7.81111\n",
      "val Class Accuracy: [0.927,0.971,0.703,0.420,0.957,0.543,0.525,0.460,0.506,0.245]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [108][0/97], lr: 0.01000\tTime 0.395 (0.395)\tData 0.229 (0.229)\tLoss 2.0575 (2.0575)\tPrec@1 88.281 (88.281)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [108][10/97], lr: 0.01000\tTime 0.302 (0.312)\tData 0.000 (0.036)\tLoss 1.6811 (1.7631)\tPrec@1 88.281 (89.347)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [108][20/97], lr: 0.01000\tTime 0.295 (0.306)\tData 0.000 (0.027)\tLoss 1.7051 (1.8003)\tPrec@1 92.969 (89.435)\tPrec@5 99.219 (99.405)\n",
      "Epoch: [108][30/97], lr: 0.01000\tTime 0.296 (0.303)\tData 0.000 (0.024)\tLoss 1.9572 (1.8031)\tPrec@1 89.062 (89.340)\tPrec@5 100.000 (99.420)\n",
      "Epoch: [108][40/97], lr: 0.01000\tTime 0.287 (0.302)\tData 0.000 (0.022)\tLoss 2.4424 (1.8098)\tPrec@1 84.375 (89.215)\tPrec@5 98.438 (99.466)\n",
      "Epoch: [108][50/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 1.8373 (1.8200)\tPrec@1 90.625 (89.154)\tPrec@5 99.219 (99.494)\n",
      "Epoch: [108][60/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.020)\tLoss 2.6345 (1.8796)\tPrec@1 85.156 (88.819)\tPrec@5 99.219 (99.424)\n",
      "Epoch: [108][70/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 1.9485 (1.8835)\tPrec@1 90.625 (88.743)\tPrec@5 99.219 (99.450)\n",
      "Epoch: [108][80/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 2.5636 (1.9007)\tPrec@1 84.375 (88.648)\tPrec@5 99.219 (99.392)\n",
      "Epoch: [108][90/97], lr: 0.01000\tTime 0.294 (0.300)\tData 0.000 (0.019)\tLoss 2.0111 (1.9092)\tPrec@1 87.500 (88.599)\tPrec@5 99.219 (99.382)\n",
      "Epoch: [108][96/97], lr: 0.01000\tTime 0.290 (0.299)\tData 0.000 (0.020)\tLoss 2.4877 (1.9151)\tPrec@1 84.746 (88.562)\tPrec@5 99.153 (99.363)\n",
      "Test: [0/100]\tTime 0.247 (0.247)\tLoss 7.4622 (7.4622)\tPrec@1 63.000 (63.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 5.3911 (7.2567)\tPrec@1 74.000 (64.818)\tPrec@5 95.000 (96.091)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.8264 (7.2471)\tPrec@1 70.000 (64.857)\tPrec@5 95.000 (95.619)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.3611 (7.2683)\tPrec@1 63.000 (65.129)\tPrec@5 96.000 (95.452)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.6964 (7.2827)\tPrec@1 64.000 (65.317)\tPrec@5 93.000 (95.341)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.5271 (7.3344)\tPrec@1 65.000 (64.804)\tPrec@5 95.000 (95.431)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 6.3297 (7.3349)\tPrec@1 67.000 (64.721)\tPrec@5 96.000 (95.459)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.1820 (7.2971)\tPrec@1 66.000 (64.958)\tPrec@5 98.000 (95.535)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.1654 (7.2869)\tPrec@1 59.000 (65.074)\tPrec@5 94.000 (95.568)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.6535 (7.3465)\tPrec@1 67.000 (64.703)\tPrec@5 96.000 (95.571)\n",
      "val Results: Prec@1 64.760 Prec@5 95.550 Loss 7.36635\n",
      "val Class Accuracy: [0.925,0.982,0.836,0.409,0.507,0.805,0.625,0.476,0.646,0.265]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [109][0/97], lr: 0.01000\tTime 0.404 (0.404)\tData 0.237 (0.237)\tLoss 1.9977 (1.9977)\tPrec@1 87.500 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [109][10/97], lr: 0.01000\tTime 0.297 (0.309)\tData 0.000 (0.036)\tLoss 1.7127 (1.9547)\tPrec@1 89.062 (88.423)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [109][20/97], lr: 0.01000\tTime 0.296 (0.304)\tData 0.000 (0.027)\tLoss 1.7891 (1.8268)\tPrec@1 90.625 (89.286)\tPrec@5 100.000 (99.628)\n",
      "Epoch: [109][30/97], lr: 0.01000\tTime 0.297 (0.302)\tData 0.000 (0.024)\tLoss 1.8976 (1.8190)\tPrec@1 88.281 (89.113)\tPrec@5 100.000 (99.647)\n",
      "Epoch: [109][40/97], lr: 0.01000\tTime 0.290 (0.301)\tData 0.000 (0.022)\tLoss 2.2103 (1.8635)\tPrec@1 85.938 (88.662)\tPrec@5 96.875 (99.466)\n",
      "Epoch: [109][50/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.021)\tLoss 1.7083 (1.8465)\tPrec@1 91.406 (88.833)\tPrec@5 100.000 (99.510)\n",
      "Epoch: [109][60/97], lr: 0.01000\tTime 0.300 (0.301)\tData 0.000 (0.021)\tLoss 2.6730 (1.8902)\tPrec@1 81.250 (88.550)\tPrec@5 99.219 (99.462)\n",
      "Epoch: [109][70/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 1.8535 (1.8984)\tPrec@1 88.281 (88.468)\tPrec@5 100.000 (99.472)\n",
      "Epoch: [109][80/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.020)\tLoss 1.7188 (1.9069)\tPrec@1 89.844 (88.465)\tPrec@5 100.000 (99.489)\n",
      "Epoch: [109][90/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.020)\tLoss 2.5105 (1.9230)\tPrec@1 85.938 (88.401)\tPrec@5 99.219 (99.493)\n",
      "Epoch: [109][96/97], lr: 0.01000\tTime 0.288 (0.300)\tData 0.000 (0.020)\tLoss 2.3524 (1.9262)\tPrec@1 84.746 (88.344)\tPrec@5 100.000 (99.500)\n",
      "Test: [0/100]\tTime 0.240 (0.240)\tLoss 8.9581 (8.9581)\tPrec@1 60.000 (60.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 7.3260 (8.2545)\tPrec@1 63.000 (60.000)\tPrec@5 89.000 (92.909)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.8120 (8.1362)\tPrec@1 66.000 (59.571)\tPrec@5 92.000 (93.190)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 7.5886 (7.9974)\tPrec@1 59.000 (60.065)\tPrec@5 94.000 (93.226)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.9640 (7.9874)\tPrec@1 64.000 (60.537)\tPrec@5 96.000 (93.463)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.6718 (7.9900)\tPrec@1 61.000 (60.569)\tPrec@5 93.000 (93.686)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 7.4427 (8.0455)\tPrec@1 65.000 (60.410)\tPrec@5 93.000 (93.443)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.6761 (7.9859)\tPrec@1 64.000 (60.901)\tPrec@5 94.000 (93.563)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.4013 (7.9532)\tPrec@1 57.000 (60.914)\tPrec@5 93.000 (93.630)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 7.1965 (8.0120)\tPrec@1 65.000 (60.637)\tPrec@5 96.000 (93.549)\n",
      "val Results: Prec@1 60.610 Prec@5 93.530 Loss 8.03284\n",
      "val Class Accuracy: [0.963,0.991,0.669,0.598,0.615,0.647,0.200,0.593,0.517,0.268]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [110][0/97], lr: 0.01000\tTime 0.406 (0.406)\tData 0.244 (0.244)\tLoss 1.5704 (1.5704)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [110][10/97], lr: 0.01000\tTime 0.295 (0.309)\tData 0.000 (0.037)\tLoss 1.8586 (1.7781)\tPrec@1 87.500 (88.210)\tPrec@5 99.219 (99.503)\n",
      "Epoch: [110][20/97], lr: 0.01000\tTime 0.295 (0.303)\tData 0.000 (0.028)\tLoss 1.6893 (1.7997)\tPrec@1 89.844 (88.467)\tPrec@5 99.219 (99.554)\n",
      "Epoch: [110][30/97], lr: 0.01000\tTime 0.294 (0.301)\tData 0.000 (0.024)\tLoss 1.4978 (1.8794)\tPrec@1 90.625 (88.180)\tPrec@5 100.000 (99.420)\n",
      "Epoch: [110][40/97], lr: 0.01000\tTime 0.292 (0.301)\tData 0.000 (0.023)\tLoss 2.1345 (1.8908)\tPrec@1 87.500 (88.338)\tPrec@5 98.438 (99.390)\n",
      "Epoch: [110][50/97], lr: 0.01000\tTime 0.294 (0.300)\tData 0.000 (0.022)\tLoss 1.9866 (1.9048)\tPrec@1 87.500 (88.235)\tPrec@5 99.219 (99.403)\n",
      "Epoch: [110][60/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 2.1006 (1.9236)\tPrec@1 85.938 (88.153)\tPrec@5 100.000 (99.424)\n",
      "Epoch: [110][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.8766 (1.9228)\tPrec@1 87.500 (88.259)\tPrec@5 100.000 (99.439)\n",
      "Epoch: [110][80/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 1.7231 (1.9109)\tPrec@1 89.844 (88.339)\tPrec@5 98.438 (99.392)\n",
      "Epoch: [110][90/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.0837 (1.9026)\tPrec@1 87.500 (88.393)\tPrec@5 99.219 (99.373)\n",
      "Epoch: [110][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 1.9913 (1.8873)\tPrec@1 87.288 (88.489)\tPrec@5 100.000 (99.404)\n",
      "Test: [0/100]\tTime 0.250 (0.250)\tLoss 7.9391 (7.9391)\tPrec@1 66.000 (66.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 5.9433 (7.5858)\tPrec@1 71.000 (64.727)\tPrec@5 98.000 (97.909)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 6.0519 (7.3456)\tPrec@1 69.000 (65.524)\tPrec@5 97.000 (97.381)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 6.6482 (7.2636)\tPrec@1 67.000 (66.065)\tPrec@5 97.000 (97.290)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.8148 (7.2568)\tPrec@1 74.000 (66.415)\tPrec@5 97.000 (97.244)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 6.0891 (7.1650)\tPrec@1 73.000 (66.961)\tPrec@5 99.000 (97.529)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 5.8321 (7.1774)\tPrec@1 73.000 (66.803)\tPrec@5 99.000 (97.377)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 6.5958 (7.1447)\tPrec@1 68.000 (66.873)\tPrec@5 99.000 (97.394)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 7.6210 (7.1259)\tPrec@1 63.000 (66.914)\tPrec@5 99.000 (97.556)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.7160 (7.1779)\tPrec@1 71.000 (66.648)\tPrec@5 100.000 (97.571)\n",
      "val Results: Prec@1 66.680 Prec@5 97.580 Loss 7.18276\n",
      "val Class Accuracy: [0.921,0.995,0.667,0.789,0.716,0.669,0.717,0.581,0.548,0.065]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [111][0/97], lr: 0.01000\tTime 0.380 (0.380)\tData 0.222 (0.222)\tLoss 1.5612 (1.5612)\tPrec@1 88.281 (88.281)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [111][10/97], lr: 0.01000\tTime 0.295 (0.307)\tData 0.000 (0.035)\tLoss 1.7165 (1.6927)\tPrec@1 90.625 (89.844)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [111][20/97], lr: 0.01000\tTime 0.294 (0.302)\tData 0.000 (0.027)\tLoss 1.0120 (1.7098)\tPrec@1 93.750 (89.658)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [111][30/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.024)\tLoss 1.9235 (1.7896)\tPrec@1 90.625 (89.340)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [111][40/97], lr: 0.01000\tTime 0.299 (0.301)\tData 0.000 (0.022)\tLoss 1.8422 (1.8016)\tPrec@1 88.281 (89.196)\tPrec@5 98.438 (99.371)\n",
      "Epoch: [111][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 1.8868 (1.8068)\tPrec@1 87.500 (89.124)\tPrec@5 100.000 (99.403)\n",
      "Epoch: [111][60/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 1.4743 (1.7900)\tPrec@1 90.625 (89.178)\tPrec@5 98.438 (99.347)\n",
      "Epoch: [111][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.0224 (1.7980)\tPrec@1 95.312 (89.338)\tPrec@5 99.219 (99.340)\n",
      "Epoch: [111][80/97], lr: 0.01000\tTime 0.292 (0.299)\tData 0.000 (0.020)\tLoss 1.7465 (1.8107)\tPrec@1 89.062 (89.207)\tPrec@5 100.000 (99.392)\n",
      "Epoch: [111][90/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.019)\tLoss 1.0306 (1.8302)\tPrec@1 94.531 (89.020)\tPrec@5 100.000 (99.399)\n",
      "Epoch: [111][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 1.3688 (1.8253)\tPrec@1 92.373 (89.054)\tPrec@5 100.000 (99.395)\n",
      "Test: [0/100]\tTime 0.264 (0.264)\tLoss 10.0074 (10.0074)\tPrec@1 51.000 (51.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.072 (0.090)\tLoss 6.4066 (8.5729)\tPrec@1 68.000 (60.000)\tPrec@5 96.000 (93.727)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 7.0305 (8.3091)\tPrec@1 63.000 (60.952)\tPrec@5 97.000 (93.952)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 7.7442 (8.2185)\tPrec@1 65.000 (61.548)\tPrec@5 95.000 (93.742)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 7.6282 (8.2200)\tPrec@1 64.000 (61.683)\tPrec@5 93.000 (93.390)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.6918 (8.1931)\tPrec@1 65.000 (61.686)\tPrec@5 93.000 (93.647)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.3062 (8.2051)\tPrec@1 70.000 (61.639)\tPrec@5 94.000 (93.721)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 6.9120 (8.1821)\tPrec@1 67.000 (61.789)\tPrec@5 98.000 (93.803)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 8.1073 (8.1377)\tPrec@1 64.000 (62.000)\tPrec@5 88.000 (93.778)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.5844 (8.1906)\tPrec@1 64.000 (61.692)\tPrec@5 94.000 (93.604)\n",
      "val Results: Prec@1 61.800 Prec@5 93.460 Loss 8.19828\n",
      "val Class Accuracy: [0.897,0.958,0.918,0.700,0.662,0.540,0.528,0.384,0.566,0.027]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [112][0/97], lr: 0.01000\tTime 0.381 (0.381)\tData 0.215 (0.215)\tLoss 1.6859 (1.6859)\tPrec@1 90.625 (90.625)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [112][10/97], lr: 0.01000\tTime 0.298 (0.308)\tData 0.000 (0.034)\tLoss 1.7255 (1.8521)\tPrec@1 90.625 (88.991)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [112][20/97], lr: 0.01000\tTime 0.296 (0.304)\tData 0.000 (0.026)\tLoss 2.7879 (1.8777)\tPrec@1 82.812 (88.430)\tPrec@5 99.219 (99.293)\n",
      "Epoch: [112][30/97], lr: 0.01000\tTime 0.298 (0.303)\tData 0.000 (0.023)\tLoss 2.4374 (1.8547)\tPrec@1 85.938 (88.735)\tPrec@5 100.000 (99.370)\n",
      "Epoch: [112][40/97], lr: 0.01000\tTime 0.299 (0.302)\tData 0.000 (0.022)\tLoss 1.8410 (1.8737)\tPrec@1 90.625 (88.548)\tPrec@5 100.000 (99.352)\n",
      "Epoch: [112][50/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 2.1508 (1.8848)\tPrec@1 85.156 (88.434)\tPrec@5 98.438 (99.372)\n",
      "Epoch: [112][60/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.020)\tLoss 2.8834 (1.8843)\tPrec@1 79.688 (88.332)\tPrec@5 100.000 (99.398)\n",
      "Epoch: [112][70/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 1.5120 (1.9089)\tPrec@1 91.406 (88.171)\tPrec@5 99.219 (99.406)\n",
      "Epoch: [112][80/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 1.5564 (1.9078)\tPrec@1 89.062 (88.146)\tPrec@5 100.000 (99.421)\n",
      "Epoch: [112][90/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.019)\tLoss 2.5098 (1.9341)\tPrec@1 85.938 (87.981)\tPrec@5 99.219 (99.416)\n",
      "Epoch: [112][96/97], lr: 0.01000\tTime 0.290 (0.300)\tData 0.000 (0.020)\tLoss 1.6653 (1.9363)\tPrec@1 90.678 (87.998)\tPrec@5 100.000 (99.436)\n",
      "Test: [0/100]\tTime 0.268 (0.268)\tLoss 8.5930 (8.5930)\tPrec@1 63.000 (63.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.2608 (8.2849)\tPrec@1 71.000 (61.364)\tPrec@5 95.000 (95.636)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.8810 (8.2170)\tPrec@1 60.000 (61.333)\tPrec@5 95.000 (95.667)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.5514 (8.1915)\tPrec@1 63.000 (61.226)\tPrec@5 96.000 (95.903)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 8.7904 (8.2662)\tPrec@1 59.000 (60.927)\tPrec@5 94.000 (95.634)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.7385 (8.1939)\tPrec@1 65.000 (61.353)\tPrec@5 97.000 (95.804)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 6.8627 (8.2144)\tPrec@1 66.000 (60.951)\tPrec@5 93.000 (95.852)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.3057 (8.1805)\tPrec@1 67.000 (61.113)\tPrec@5 96.000 (95.944)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.4658 (8.1544)\tPrec@1 69.000 (61.099)\tPrec@5 94.000 (96.086)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.3493 (8.1698)\tPrec@1 65.000 (60.978)\tPrec@5 99.000 (96.209)\n",
      "val Results: Prec@1 60.990 Prec@5 96.240 Loss 8.18679\n",
      "val Class Accuracy: [0.893,0.977,0.920,0.620,0.753,0.302,0.761,0.283,0.306,0.284]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [113][0/97], lr: 0.01000\tTime 0.363 (0.363)\tData 0.199 (0.199)\tLoss 2.2091 (2.2091)\tPrec@1 86.719 (86.719)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [113][10/97], lr: 0.01000\tTime 0.295 (0.305)\tData 0.000 (0.033)\tLoss 1.5479 (1.9015)\tPrec@1 88.281 (88.707)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [113][20/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.025)\tLoss 1.8879 (1.8216)\tPrec@1 87.500 (88.765)\tPrec@5 98.438 (99.293)\n",
      "Epoch: [113][30/97], lr: 0.01000\tTime 0.300 (0.300)\tData 0.000 (0.023)\tLoss 1.6494 (1.8083)\tPrec@1 91.406 (88.936)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [113][40/97], lr: 0.01000\tTime 0.294 (0.300)\tData 0.000 (0.021)\tLoss 2.2546 (1.8170)\tPrec@1 85.938 (88.853)\tPrec@5 100.000 (99.295)\n",
      "Epoch: [113][50/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 1.6986 (1.8396)\tPrec@1 91.406 (88.833)\tPrec@5 100.000 (99.249)\n",
      "Epoch: [113][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 2.3029 (1.8813)\tPrec@1 85.156 (88.550)\tPrec@5 98.438 (99.257)\n",
      "Epoch: [113][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.3643 (1.9324)\tPrec@1 84.375 (88.226)\tPrec@5 98.438 (99.197)\n",
      "Epoch: [113][80/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.019)\tLoss 2.1377 (1.9339)\tPrec@1 86.719 (88.243)\tPrec@5 96.875 (99.180)\n",
      "Epoch: [113][90/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.019)\tLoss 1.8659 (1.9287)\tPrec@1 87.500 (88.324)\tPrec@5 99.219 (99.210)\n",
      "Epoch: [113][96/97], lr: 0.01000\tTime 0.288 (0.298)\tData 0.000 (0.020)\tLoss 2.2935 (1.9269)\tPrec@1 85.593 (88.336)\tPrec@5 99.153 (99.226)\n",
      "Test: [0/100]\tTime 0.226 (0.226)\tLoss 5.8911 (5.8911)\tPrec@1 75.000 (75.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 5.7622 (6.5817)\tPrec@1 74.000 (69.091)\tPrec@5 95.000 (96.455)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 5.1374 (6.4293)\tPrec@1 73.000 (69.905)\tPrec@5 97.000 (96.571)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 6.0304 (6.4660)\tPrec@1 72.000 (69.645)\tPrec@5 98.000 (96.484)\n",
      "Test: [40/100]\tTime 0.073 (0.076)\tLoss 5.7038 (6.4932)\tPrec@1 72.000 (69.512)\tPrec@5 97.000 (96.488)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 5.9777 (6.4626)\tPrec@1 71.000 (69.549)\tPrec@5 99.000 (96.627)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 5.8756 (6.4364)\tPrec@1 69.000 (69.689)\tPrec@5 96.000 (96.721)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 5.9890 (6.4347)\tPrec@1 72.000 (69.592)\tPrec@5 97.000 (96.592)\n",
      "Test: [80/100]\tTime 0.073 (0.074)\tLoss 6.1296 (6.4065)\tPrec@1 72.000 (69.815)\tPrec@5 97.000 (96.691)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 6.3262 (6.4509)\tPrec@1 71.000 (69.527)\tPrec@5 97.000 (96.725)\n",
      "val Results: Prec@1 69.490 Prec@5 96.800 Loss 6.46677\n",
      "val Class Accuracy: [0.915,0.971,0.822,0.400,0.757,0.797,0.747,0.689,0.517,0.334]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [114][0/97], lr: 0.01000\tTime 0.381 (0.381)\tData 0.227 (0.227)\tLoss 1.7624 (1.7624)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [114][10/97], lr: 0.01000\tTime 0.295 (0.307)\tData 0.000 (0.036)\tLoss 2.9751 (1.7806)\tPrec@1 82.031 (89.986)\tPrec@5 98.438 (99.361)\n",
      "Epoch: [114][20/97], lr: 0.01000\tTime 0.294 (0.302)\tData 0.000 (0.027)\tLoss 1.7416 (1.7796)\tPrec@1 88.281 (89.769)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [114][30/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.024)\tLoss 1.8919 (1.7468)\tPrec@1 89.062 (89.693)\tPrec@5 99.219 (99.521)\n",
      "Epoch: [114][40/97], lr: 0.01000\tTime 0.294 (0.300)\tData 0.000 (0.022)\tLoss 1.4548 (1.7582)\tPrec@1 89.062 (89.596)\tPrec@5 98.438 (99.486)\n",
      "Epoch: [114][50/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 1.4585 (1.7573)\tPrec@1 92.188 (89.507)\tPrec@5 99.219 (99.494)\n",
      "Epoch: [114][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 2.4620 (1.7654)\tPrec@1 85.156 (89.370)\tPrec@5 97.656 (99.462)\n",
      "Epoch: [114][70/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 2.6636 (1.8017)\tPrec@1 84.375 (89.018)\tPrec@5 97.656 (99.428)\n",
      "Epoch: [114][80/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 1.6433 (1.7740)\tPrec@1 91.406 (89.217)\tPrec@5 99.219 (99.450)\n",
      "Epoch: [114][90/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.019)\tLoss 1.8192 (1.7887)\tPrec@1 87.500 (89.080)\tPrec@5 100.000 (99.468)\n",
      "Epoch: [114][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 1.5085 (1.8130)\tPrec@1 94.068 (88.941)\tPrec@5 99.153 (99.452)\n",
      "Test: [0/100]\tTime 0.243 (0.243)\tLoss 8.4447 (8.4447)\tPrec@1 62.000 (62.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 6.5505 (8.0186)\tPrec@1 70.000 (63.091)\tPrec@5 95.000 (95.364)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 7.2159 (7.9082)\tPrec@1 63.000 (63.429)\tPrec@5 96.000 (95.476)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 7.6219 (7.8906)\tPrec@1 63.000 (62.903)\tPrec@5 95.000 (95.774)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.8136 (7.9025)\tPrec@1 60.000 (62.854)\tPrec@5 96.000 (95.683)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 8.0945 (7.9405)\tPrec@1 59.000 (62.314)\tPrec@5 96.000 (95.824)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 7.0185 (7.9849)\tPrec@1 65.000 (61.984)\tPrec@5 97.000 (95.852)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 6.8664 (7.9539)\tPrec@1 67.000 (62.141)\tPrec@5 96.000 (95.746)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 7.1349 (7.8918)\tPrec@1 67.000 (62.494)\tPrec@5 96.000 (95.753)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 7.4381 (7.9482)\tPrec@1 64.000 (62.330)\tPrec@5 98.000 (95.824)\n",
      "val Results: Prec@1 62.370 Prec@5 95.740 Loss 7.95173\n",
      "val Class Accuracy: [0.837,0.951,0.954,0.379,0.625,0.498,0.651,0.603,0.338,0.401]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [115][0/97], lr: 0.01000\tTime 0.336 (0.336)\tData 0.196 (0.196)\tLoss 1.5651 (1.5651)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [115][10/97], lr: 0.01000\tTime 0.297 (0.304)\tData 0.000 (0.033)\tLoss 1.4200 (1.6751)\tPrec@1 91.406 (89.347)\tPrec@5 99.219 (99.716)\n",
      "Epoch: [115][20/97], lr: 0.01000\tTime 0.294 (0.301)\tData 0.000 (0.026)\tLoss 1.6082 (1.7362)\tPrec@1 89.062 (89.100)\tPrec@5 99.219 (99.591)\n",
      "Epoch: [115][30/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.023)\tLoss 2.2675 (1.7805)\tPrec@1 84.375 (88.962)\tPrec@5 100.000 (99.546)\n",
      "Epoch: [115][40/97], lr: 0.01000\tTime 0.284 (0.299)\tData 0.000 (0.022)\tLoss 1.2634 (1.8324)\tPrec@1 93.750 (88.720)\tPrec@5 100.000 (99.486)\n",
      "Epoch: [115][50/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 0.9307 (1.8436)\tPrec@1 94.531 (88.572)\tPrec@5 100.000 (99.510)\n",
      "Epoch: [115][60/97], lr: 0.01000\tTime 0.296 (0.298)\tData 0.000 (0.020)\tLoss 1.9246 (1.8354)\tPrec@1 87.500 (88.678)\tPrec@5 98.438 (99.501)\n",
      "Epoch: [115][70/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.020)\tLoss 1.6222 (1.8723)\tPrec@1 92.188 (88.501)\tPrec@5 99.219 (99.461)\n",
      "Epoch: [115][80/97], lr: 0.01000\tTime 0.296 (0.298)\tData 0.000 (0.019)\tLoss 1.8086 (1.8818)\tPrec@1 86.719 (88.426)\tPrec@5 100.000 (99.431)\n",
      "Epoch: [115][90/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.019)\tLoss 1.4533 (1.8643)\tPrec@1 91.406 (88.547)\tPrec@5 100.000 (99.425)\n",
      "Epoch: [115][96/97], lr: 0.01000\tTime 0.289 (0.298)\tData 0.000 (0.020)\tLoss 2.0642 (1.8745)\tPrec@1 87.288 (88.481)\tPrec@5 100.000 (99.444)\n",
      "Test: [0/100]\tTime 0.266 (0.266)\tLoss 10.4106 (10.4106)\tPrec@1 56.000 (56.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.072 (0.090)\tLoss 7.3523 (9.1234)\tPrec@1 62.000 (58.091)\tPrec@5 95.000 (92.182)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 8.2025 (8.8998)\tPrec@1 61.000 (59.524)\tPrec@5 96.000 (92.333)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 8.2583 (8.9462)\tPrec@1 60.000 (59.032)\tPrec@5 94.000 (91.968)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.8422 (9.0507)\tPrec@1 55.000 (58.732)\tPrec@5 88.000 (91.512)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 8.9557 (8.9883)\tPrec@1 63.000 (58.804)\tPrec@5 93.000 (91.627)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.8101 (9.0327)\tPrec@1 63.000 (58.279)\tPrec@5 94.000 (91.721)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.5828 (9.0236)\tPrec@1 65.000 (58.366)\tPrec@5 94.000 (91.803)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.3994 (9.0275)\tPrec@1 58.000 (58.309)\tPrec@5 87.000 (91.840)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.3570 (9.0287)\tPrec@1 62.000 (58.462)\tPrec@5 95.000 (91.824)\n",
      "val Results: Prec@1 58.400 Prec@5 91.840 Loss 9.03560\n",
      "val Class Accuracy: [0.859,0.946,0.838,0.864,0.676,0.289,0.606,0.256,0.496,0.010]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [116][0/97], lr: 0.01000\tTime 0.359 (0.359)\tData 0.224 (0.224)\tLoss 1.4383 (1.4383)\tPrec@1 91.406 (91.406)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [116][10/97], lr: 0.01000\tTime 0.295 (0.304)\tData 0.000 (0.036)\tLoss 2.9874 (1.7039)\tPrec@1 81.250 (89.844)\tPrec@5 98.438 (99.503)\n",
      "Epoch: [116][20/97], lr: 0.01000\tTime 0.302 (0.301)\tData 0.000 (0.027)\tLoss 2.0464 (1.7660)\tPrec@1 86.719 (89.100)\tPrec@5 99.219 (99.442)\n",
      "Epoch: [116][30/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.024)\tLoss 1.7169 (1.8391)\tPrec@1 89.844 (88.659)\tPrec@5 99.219 (99.496)\n",
      "Epoch: [116][40/97], lr: 0.01000\tTime 0.299 (0.299)\tData 0.000 (0.022)\tLoss 1.8753 (1.8450)\tPrec@1 89.844 (88.796)\tPrec@5 99.219 (99.466)\n",
      "Epoch: [116][50/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 2.2808 (1.8833)\tPrec@1 87.500 (88.496)\tPrec@5 98.438 (99.433)\n",
      "Epoch: [116][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 1.9940 (1.8850)\tPrec@1 87.500 (88.435)\tPrec@5 100.000 (99.411)\n",
      "Epoch: [116][70/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.020)\tLoss 2.4237 (1.8670)\tPrec@1 85.156 (88.545)\tPrec@5 99.219 (99.450)\n",
      "Epoch: [116][80/97], lr: 0.01000\tTime 0.297 (0.298)\tData 0.000 (0.020)\tLoss 2.0534 (1.8328)\tPrec@1 88.281 (88.764)\tPrec@5 98.438 (99.460)\n",
      "Epoch: [116][90/97], lr: 0.01000\tTime 0.297 (0.298)\tData 0.000 (0.019)\tLoss 1.5522 (1.8490)\tPrec@1 92.188 (88.711)\tPrec@5 99.219 (99.408)\n",
      "Epoch: [116][96/97], lr: 0.01000\tTime 0.289 (0.298)\tData 0.000 (0.020)\tLoss 1.7342 (1.8536)\tPrec@1 89.831 (88.683)\tPrec@5 99.153 (99.379)\n",
      "Test: [0/100]\tTime 0.243 (0.243)\tLoss 8.9160 (8.9160)\tPrec@1 59.000 (59.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 7.1937 (8.8256)\tPrec@1 64.000 (58.273)\tPrec@5 96.000 (94.273)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.5676 (8.7256)\tPrec@1 66.000 (58.571)\tPrec@5 98.000 (93.857)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 8.6497 (8.7162)\tPrec@1 58.000 (58.613)\tPrec@5 91.000 (93.774)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 8.6968 (8.7296)\tPrec@1 58.000 (58.756)\tPrec@5 93.000 (93.780)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.8661 (8.6328)\tPrec@1 61.000 (59.020)\tPrec@5 95.000 (93.961)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 6.1414 (8.5960)\tPrec@1 69.000 (59.180)\tPrec@5 96.000 (93.770)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.1721 (8.5918)\tPrec@1 65.000 (59.423)\tPrec@5 95.000 (93.577)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 8.4445 (8.5435)\tPrec@1 57.000 (59.580)\tPrec@5 92.000 (93.704)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 7.9928 (8.6145)\tPrec@1 64.000 (59.253)\tPrec@5 95.000 (93.593)\n",
      "val Results: Prec@1 59.270 Prec@5 93.530 Loss 8.62701\n",
      "val Class Accuracy: [0.974,0.981,0.771,0.773,0.511,0.562,0.454,0.517,0.163,0.221]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [117][0/97], lr: 0.01000\tTime 0.386 (0.386)\tData 0.230 (0.230)\tLoss 1.6833 (1.6833)\tPrec@1 89.062 (89.062)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [117][10/97], lr: 0.01000\tTime 0.295 (0.306)\tData 0.000 (0.036)\tLoss 2.2947 (1.8867)\tPrec@1 85.938 (88.849)\tPrec@5 100.000 (99.148)\n",
      "Epoch: [117][20/97], lr: 0.01000\tTime 0.294 (0.302)\tData 0.000 (0.027)\tLoss 1.5002 (1.9130)\tPrec@1 89.844 (88.393)\tPrec@5 98.438 (99.182)\n",
      "Epoch: [117][30/97], lr: 0.01000\tTime 0.294 (0.301)\tData 0.000 (0.024)\tLoss 1.4847 (1.8568)\tPrec@1 92.969 (88.785)\tPrec@5 100.000 (99.244)\n",
      "Epoch: [117][40/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.022)\tLoss 1.4278 (1.8538)\tPrec@1 90.625 (88.758)\tPrec@5 100.000 (99.276)\n",
      "Epoch: [117][50/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.021)\tLoss 2.3239 (1.8458)\tPrec@1 86.719 (88.802)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [117][60/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 2.4211 (1.8524)\tPrec@1 85.156 (88.653)\tPrec@5 99.219 (99.334)\n",
      "Epoch: [117][70/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 1.9207 (1.8534)\tPrec@1 88.281 (88.677)\tPrec@5 99.219 (99.340)\n",
      "Epoch: [117][80/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 0.8314 (1.8463)\tPrec@1 96.875 (88.744)\tPrec@5 100.000 (99.344)\n",
      "Epoch: [117][90/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.2570 (1.8640)\tPrec@1 85.156 (88.693)\tPrec@5 98.438 (99.339)\n",
      "Epoch: [117][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 2.1762 (1.8646)\tPrec@1 87.288 (88.699)\tPrec@5 99.153 (99.315)\n",
      "Test: [0/100]\tTime 0.260 (0.260)\tLoss 8.7820 (8.7820)\tPrec@1 56.000 (56.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 7.2064 (8.2978)\tPrec@1 64.000 (60.455)\tPrec@5 94.000 (95.818)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 6.0427 (8.0780)\tPrec@1 69.000 (61.714)\tPrec@5 97.000 (96.000)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 7.4406 (8.0295)\tPrec@1 62.000 (61.742)\tPrec@5 95.000 (95.839)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.1491 (8.0252)\tPrec@1 68.000 (62.024)\tPrec@5 97.000 (95.683)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.6477 (7.9586)\tPrec@1 64.000 (62.176)\tPrec@5 96.000 (95.843)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.1945 (7.9608)\tPrec@1 66.000 (62.049)\tPrec@5 96.000 (95.803)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.2885 (7.9041)\tPrec@1 65.000 (62.408)\tPrec@5 100.000 (95.775)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 8.0332 (7.8581)\tPrec@1 65.000 (62.704)\tPrec@5 96.000 (95.914)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.6119 (7.9670)\tPrec@1 60.000 (62.110)\tPrec@5 98.000 (95.868)\n",
      "val Results: Prec@1 62.010 Prec@5 95.850 Loss 8.00114\n",
      "val Class Accuracy: [0.964,0.949,0.635,0.725,0.681,0.765,0.291,0.584,0.529,0.078]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [118][0/97], lr: 0.01000\tTime 0.361 (0.361)\tData 0.222 (0.222)\tLoss 1.9711 (1.9711)\tPrec@1 85.938 (85.938)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [118][10/97], lr: 0.01000\tTime 0.296 (0.306)\tData 0.000 (0.035)\tLoss 1.6356 (2.0309)\tPrec@1 90.625 (87.287)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [118][20/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.027)\tLoss 2.5472 (1.9340)\tPrec@1 87.500 (87.909)\tPrec@5 100.000 (99.368)\n",
      "Epoch: [118][30/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.024)\tLoss 2.6229 (1.9639)\tPrec@1 81.250 (87.601)\tPrec@5 98.438 (99.370)\n",
      "Epoch: [118][40/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.022)\tLoss 1.8820 (1.9305)\tPrec@1 89.062 (87.957)\tPrec@5 100.000 (99.352)\n",
      "Epoch: [118][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 1.8884 (1.9315)\tPrec@1 88.281 (88.082)\tPrec@5 99.219 (99.372)\n",
      "Epoch: [118][60/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.020)\tLoss 1.7765 (1.8946)\tPrec@1 90.625 (88.217)\tPrec@5 100.000 (99.436)\n",
      "Epoch: [118][70/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.020)\tLoss 1.8197 (1.8899)\tPrec@1 88.281 (88.413)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [118][80/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 1.3906 (1.9193)\tPrec@1 92.969 (88.281)\tPrec@5 100.000 (99.431)\n",
      "Epoch: [118][90/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.019)\tLoss 2.9176 (1.9328)\tPrec@1 83.594 (88.161)\tPrec@5 100.000 (99.433)\n",
      "Epoch: [118][96/97], lr: 0.01000\tTime 0.290 (0.299)\tData 0.000 (0.020)\tLoss 1.9655 (1.9265)\tPrec@1 86.441 (88.223)\tPrec@5 100.000 (99.436)\n",
      "Test: [0/100]\tTime 0.270 (0.270)\tLoss 7.9327 (7.9327)\tPrec@1 63.000 (63.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.4022 (7.6477)\tPrec@1 71.000 (63.727)\tPrec@5 96.000 (95.000)\n",
      "Test: [20/100]\tTime 0.072 (0.082)\tLoss 5.1275 (7.5127)\tPrec@1 75.000 (64.476)\tPrec@5 97.000 (95.190)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 7.4915 (7.5824)\tPrec@1 64.000 (64.097)\tPrec@5 95.000 (94.871)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 7.3230 (7.5909)\tPrec@1 65.000 (64.244)\tPrec@5 94.000 (94.732)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 7.7263 (7.5264)\tPrec@1 65.000 (64.706)\tPrec@5 93.000 (94.843)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.0113 (7.5735)\tPrec@1 65.000 (64.213)\tPrec@5 95.000 (94.754)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 7.1442 (7.5070)\tPrec@1 67.000 (64.606)\tPrec@5 94.000 (94.761)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.3162 (7.4754)\tPrec@1 66.000 (64.877)\tPrec@5 92.000 (94.790)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.1201 (7.5191)\tPrec@1 68.000 (64.516)\tPrec@5 97.000 (94.934)\n",
      "val Results: Prec@1 64.700 Prec@5 94.850 Loss 7.52975\n",
      "val Class Accuracy: [0.879,0.982,0.776,0.580,0.526,0.849,0.431,0.406,0.614,0.427]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [119][0/97], lr: 0.01000\tTime 0.373 (0.373)\tData 0.227 (0.227)\tLoss 1.3056 (1.3056)\tPrec@1 92.188 (92.188)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [119][10/97], lr: 0.01000\tTime 0.298 (0.307)\tData 0.000 (0.035)\tLoss 1.3224 (1.9138)\tPrec@1 92.188 (88.281)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [119][20/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.027)\tLoss 2.2790 (1.9475)\tPrec@1 87.500 (88.281)\tPrec@5 97.656 (99.293)\n",
      "Epoch: [119][30/97], lr: 0.01000\tTime 0.297 (0.302)\tData 0.000 (0.024)\tLoss 1.4429 (1.9394)\tPrec@1 91.406 (88.180)\tPrec@5 100.000 (99.194)\n",
      "Epoch: [119][40/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.022)\tLoss 1.4678 (1.8713)\tPrec@1 92.188 (88.605)\tPrec@5 99.219 (99.295)\n",
      "Epoch: [119][50/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.021)\tLoss 2.1083 (1.8502)\tPrec@1 85.938 (88.680)\tPrec@5 100.000 (99.387)\n",
      "Epoch: [119][60/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.021)\tLoss 2.1438 (1.8768)\tPrec@1 88.281 (88.499)\tPrec@5 99.219 (99.411)\n",
      "Epoch: [119][70/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.020)\tLoss 1.9162 (1.8917)\tPrec@1 88.281 (88.424)\tPrec@5 99.219 (99.384)\n",
      "Epoch: [119][80/97], lr: 0.01000\tTime 0.300 (0.300)\tData 0.000 (0.020)\tLoss 1.8820 (1.9048)\tPrec@1 89.844 (88.281)\tPrec@5 100.000 (99.392)\n",
      "Epoch: [119][90/97], lr: 0.01000\tTime 0.299 (0.300)\tData 0.000 (0.019)\tLoss 1.3470 (1.9066)\tPrec@1 89.844 (88.264)\tPrec@5 100.000 (99.382)\n",
      "Epoch: [119][96/97], lr: 0.01000\tTime 0.290 (0.300)\tData 0.000 (0.020)\tLoss 1.7457 (1.9039)\tPrec@1 88.983 (88.312)\tPrec@5 100.000 (99.412)\n",
      "Test: [0/100]\tTime 0.271 (0.271)\tLoss 8.2106 (8.2106)\tPrec@1 63.000 (63.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 5.6282 (7.4584)\tPrec@1 74.000 (65.091)\tPrec@5 94.000 (94.818)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.3534 (7.2882)\tPrec@1 68.000 (65.524)\tPrec@5 94.000 (94.857)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.9985 (7.3083)\tPrec@1 63.000 (65.548)\tPrec@5 95.000 (94.774)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 7.0588 (7.2950)\tPrec@1 70.000 (65.780)\tPrec@5 96.000 (94.927)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 6.9501 (7.3088)\tPrec@1 67.000 (65.588)\tPrec@5 94.000 (95.000)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 5.8945 (7.2894)\tPrec@1 72.000 (65.656)\tPrec@5 93.000 (95.082)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 6.3958 (7.2565)\tPrec@1 73.000 (65.789)\tPrec@5 97.000 (95.042)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.8189 (7.2251)\tPrec@1 66.000 (65.852)\tPrec@5 97.000 (95.185)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.0286 (7.2790)\tPrec@1 71.000 (65.659)\tPrec@5 98.000 (95.198)\n",
      "val Results: Prec@1 65.690 Prec@5 95.130 Loss 7.29591\n",
      "val Class Accuracy: [0.903,0.962,0.861,0.439,0.696,0.747,0.653,0.563,0.325,0.420]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [120][0/97], lr: 0.01000\tTime 0.398 (0.398)\tData 0.241 (0.241)\tLoss 1.9459 (1.9459)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [120][10/97], lr: 0.01000\tTime 0.296 (0.308)\tData 0.000 (0.037)\tLoss 1.2538 (1.6916)\tPrec@1 92.188 (88.849)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [120][20/97], lr: 0.01000\tTime 0.299 (0.303)\tData 0.000 (0.028)\tLoss 1.6149 (1.8090)\tPrec@1 89.062 (88.653)\tPrec@5 96.875 (99.256)\n",
      "Epoch: [120][30/97], lr: 0.01000\tTime 0.298 (0.302)\tData 0.000 (0.024)\tLoss 1.9639 (1.8597)\tPrec@1 87.500 (88.382)\tPrec@5 100.000 (99.370)\n",
      "Epoch: [120][40/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.023)\tLoss 1.6895 (1.8933)\tPrec@1 89.062 (88.243)\tPrec@5 100.000 (99.409)\n",
      "Epoch: [120][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.022)\tLoss 2.1886 (1.9080)\tPrec@1 87.500 (88.251)\tPrec@5 99.219 (99.433)\n",
      "Epoch: [120][60/97], lr: 0.01000\tTime 0.301 (0.300)\tData 0.000 (0.021)\tLoss 2.3257 (1.9372)\tPrec@1 85.938 (88.051)\tPrec@5 99.219 (99.411)\n",
      "Epoch: [120][70/97], lr: 0.01000\tTime 0.306 (0.300)\tData 0.000 (0.020)\tLoss 2.5489 (1.9245)\tPrec@1 84.375 (88.149)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [120][80/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.6302 (1.9065)\tPrec@1 89.844 (88.339)\tPrec@5 98.438 (99.383)\n",
      "Epoch: [120][90/97], lr: 0.01000\tTime 0.293 (0.299)\tData 0.000 (0.020)\tLoss 1.9525 (1.8902)\tPrec@1 88.281 (88.384)\tPrec@5 99.219 (99.390)\n",
      "Epoch: [120][96/97], lr: 0.01000\tTime 0.290 (0.299)\tData 0.000 (0.020)\tLoss 2.2893 (1.8782)\tPrec@1 87.288 (88.481)\tPrec@5 99.153 (99.412)\n",
      "Test: [0/100]\tTime 0.231 (0.231)\tLoss 8.4990 (8.4990)\tPrec@1 63.000 (63.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.072 (0.087)\tLoss 6.4508 (7.4868)\tPrec@1 71.000 (66.000)\tPrec@5 92.000 (95.455)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 5.8530 (7.3826)\tPrec@1 70.000 (65.381)\tPrec@5 95.000 (95.333)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 6.6980 (7.3604)\tPrec@1 65.000 (65.161)\tPrec@5 95.000 (95.645)\n",
      "Test: [40/100]\tTime 0.073 (0.076)\tLoss 6.4200 (7.3363)\tPrec@1 74.000 (65.463)\tPrec@5 98.000 (95.927)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 5.9228 (7.2917)\tPrec@1 77.000 (65.804)\tPrec@5 91.000 (96.000)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 6.1640 (7.2742)\tPrec@1 68.000 (65.623)\tPrec@5 94.000 (95.934)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 6.8828 (7.2808)\tPrec@1 64.000 (65.408)\tPrec@5 98.000 (96.070)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.3265 (7.2329)\tPrec@1 66.000 (65.654)\tPrec@5 94.000 (96.099)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 7.0121 (7.2891)\tPrec@1 70.000 (65.429)\tPrec@5 97.000 (96.077)\n",
      "val Results: Prec@1 65.270 Prec@5 96.050 Loss 7.30600\n",
      "val Class Accuracy: [0.976,0.957,0.652,0.412,0.643,0.775,0.662,0.645,0.361,0.444]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [121][0/97], lr: 0.01000\tTime 0.390 (0.390)\tData 0.226 (0.226)\tLoss 1.1120 (1.1120)\tPrec@1 94.531 (94.531)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [121][10/97], lr: 0.01000\tTime 0.297 (0.308)\tData 0.000 (0.035)\tLoss 2.3966 (1.6796)\tPrec@1 87.500 (90.341)\tPrec@5 97.656 (99.574)\n",
      "Epoch: [121][20/97], lr: 0.01000\tTime 0.296 (0.303)\tData 0.000 (0.027)\tLoss 2.0319 (1.6751)\tPrec@1 87.500 (90.476)\tPrec@5 99.219 (99.591)\n",
      "Epoch: [121][30/97], lr: 0.01000\tTime 0.297 (0.303)\tData 0.000 (0.024)\tLoss 0.9020 (1.6822)\tPrec@1 96.875 (90.222)\tPrec@5 100.000 (99.622)\n",
      "Epoch: [121][40/97], lr: 0.01000\tTime 0.296 (0.303)\tData 0.000 (0.022)\tLoss 1.7938 (1.7073)\tPrec@1 88.281 (90.034)\tPrec@5 97.656 (99.505)\n",
      "Epoch: [121][50/97], lr: 0.01000\tTime 0.297 (0.302)\tData 0.000 (0.021)\tLoss 1.8326 (1.7523)\tPrec@1 86.719 (89.568)\tPrec@5 100.000 (99.510)\n",
      "Epoch: [121][60/97], lr: 0.01000\tTime 0.297 (0.302)\tData 0.000 (0.021)\tLoss 2.1189 (1.8109)\tPrec@1 86.719 (89.075)\tPrec@5 97.656 (99.411)\n",
      "Epoch: [121][70/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.020)\tLoss 2.2667 (1.8396)\tPrec@1 84.375 (88.875)\tPrec@5 98.438 (99.384)\n",
      "Epoch: [121][80/97], lr: 0.01000\tTime 0.299 (0.301)\tData 0.000 (0.020)\tLoss 1.8362 (1.8644)\tPrec@1 86.719 (88.764)\tPrec@5 99.219 (99.363)\n",
      "Epoch: [121][90/97], lr: 0.01000\tTime 0.303 (0.301)\tData 0.000 (0.019)\tLoss 1.6981 (1.8938)\tPrec@1 90.625 (88.590)\tPrec@5 100.000 (99.313)\n",
      "Epoch: [121][96/97], lr: 0.01000\tTime 0.293 (0.301)\tData 0.000 (0.020)\tLoss 2.3227 (1.9009)\tPrec@1 84.746 (88.530)\tPrec@5 100.000 (99.258)\n",
      "Test: [0/100]\tTime 0.246 (0.246)\tLoss 8.8382 (8.8382)\tPrec@1 60.000 (60.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.072 (0.088)\tLoss 5.6712 (8.1239)\tPrec@1 74.000 (62.182)\tPrec@5 95.000 (93.727)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.5364 (7.9174)\tPrec@1 66.000 (62.524)\tPrec@5 97.000 (94.429)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.1260 (7.9158)\tPrec@1 65.000 (62.032)\tPrec@5 93.000 (94.258)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.8533 (8.0158)\tPrec@1 65.000 (61.927)\tPrec@5 94.000 (93.976)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.4778 (7.9759)\tPrec@1 65.000 (62.157)\tPrec@5 93.000 (93.902)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 6.5815 (7.9987)\tPrec@1 68.000 (62.033)\tPrec@5 94.000 (93.787)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 7.6011 (7.9659)\tPrec@1 65.000 (62.310)\tPrec@5 94.000 (93.845)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 7.9475 (7.9489)\tPrec@1 62.000 (62.407)\tPrec@5 93.000 (93.914)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 7.2312 (7.9614)\tPrec@1 69.000 (62.308)\tPrec@5 97.000 (94.066)\n",
      "val Results: Prec@1 62.320 Prec@5 94.030 Loss 7.97304\n",
      "val Class Accuracy: [0.910,0.981,0.874,0.796,0.661,0.585,0.593,0.156,0.563,0.113]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [122][0/97], lr: 0.01000\tTime 0.345 (0.345)\tData 0.195 (0.195)\tLoss 0.9474 (0.9474)\tPrec@1 94.531 (94.531)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [122][10/97], lr: 0.01000\tTime 0.302 (0.305)\tData 0.000 (0.033)\tLoss 2.1097 (1.8336)\tPrec@1 86.719 (88.849)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [122][20/97], lr: 0.01000\tTime 0.301 (0.304)\tData 0.000 (0.025)\tLoss 1.4805 (1.7960)\tPrec@1 92.969 (89.211)\tPrec@5 99.219 (99.405)\n",
      "Epoch: [122][30/97], lr: 0.01000\tTime 0.306 (0.304)\tData 0.000 (0.023)\tLoss 1.4565 (1.7662)\tPrec@1 89.844 (89.138)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [122][40/97], lr: 0.01000\tTime 0.299 (0.303)\tData 0.000 (0.021)\tLoss 1.9000 (1.7943)\tPrec@1 87.500 (88.986)\tPrec@5 97.656 (99.333)\n",
      "Epoch: [122][50/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.020)\tLoss 1.6828 (1.8299)\tPrec@1 90.625 (88.802)\tPrec@5 97.656 (99.326)\n",
      "Epoch: [122][60/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.020)\tLoss 1.5793 (1.8408)\tPrec@1 90.625 (88.717)\tPrec@5 97.656 (99.334)\n",
      "Epoch: [122][70/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.020)\tLoss 1.9654 (1.8074)\tPrec@1 88.281 (88.996)\tPrec@5 99.219 (99.340)\n",
      "Epoch: [122][80/97], lr: 0.01000\tTime 0.299 (0.301)\tData 0.000 (0.019)\tLoss 1.7846 (1.8375)\tPrec@1 87.500 (88.764)\tPrec@5 100.000 (99.354)\n",
      "Epoch: [122][90/97], lr: 0.01000\tTime 0.312 (0.301)\tData 0.000 (0.019)\tLoss 1.8918 (1.8585)\tPrec@1 89.062 (88.625)\tPrec@5 99.219 (99.348)\n",
      "Epoch: [122][96/97], lr: 0.01000\tTime 0.290 (0.300)\tData 0.000 (0.020)\tLoss 2.9007 (1.8872)\tPrec@1 83.051 (88.417)\tPrec@5 96.610 (99.315)\n",
      "Test: [0/100]\tTime 0.249 (0.249)\tLoss 8.3100 (8.3100)\tPrec@1 62.000 (62.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.1160 (7.9030)\tPrec@1 65.000 (62.091)\tPrec@5 88.000 (94.000)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.0197 (7.8747)\tPrec@1 69.000 (61.952)\tPrec@5 92.000 (94.333)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 7.4083 (7.9002)\tPrec@1 61.000 (61.677)\tPrec@5 97.000 (94.194)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.2432 (7.8876)\tPrec@1 64.000 (61.829)\tPrec@5 97.000 (94.512)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 7.7511 (7.9057)\tPrec@1 62.000 (61.608)\tPrec@5 95.000 (94.765)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 6.7802 (7.8840)\tPrec@1 63.000 (61.852)\tPrec@5 97.000 (94.803)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 6.2808 (7.8387)\tPrec@1 67.000 (61.986)\tPrec@5 100.000 (94.915)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.3977 (7.7692)\tPrec@1 63.000 (62.444)\tPrec@5 95.000 (95.049)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 7.4488 (7.8326)\tPrec@1 64.000 (62.176)\tPrec@5 99.000 (95.011)\n",
      "val Results: Prec@1 62.130 Prec@5 94.960 Loss 7.84755\n",
      "val Class Accuracy: [0.949,0.993,0.845,0.581,0.518,0.651,0.434,0.683,0.446,0.113]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [123][0/97], lr: 0.01000\tTime 0.346 (0.346)\tData 0.206 (0.206)\tLoss 1.6272 (1.6272)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [123][10/97], lr: 0.01000\tTime 0.295 (0.304)\tData 0.000 (0.034)\tLoss 1.8838 (1.7783)\tPrec@1 89.062 (88.707)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [123][20/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.026)\tLoss 1.5754 (1.7927)\tPrec@1 90.625 (88.765)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [123][30/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.023)\tLoss 2.2565 (1.9110)\tPrec@1 85.938 (88.054)\tPrec@5 99.219 (99.446)\n",
      "Epoch: [123][40/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.022)\tLoss 1.7549 (1.9452)\tPrec@1 89.062 (87.957)\tPrec@5 99.219 (99.428)\n",
      "Epoch: [123][50/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.021)\tLoss 2.3334 (1.9445)\tPrec@1 84.375 (87.944)\tPrec@5 100.000 (99.433)\n",
      "Epoch: [123][60/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.020)\tLoss 2.6700 (1.9321)\tPrec@1 82.812 (88.064)\tPrec@5 98.438 (99.398)\n",
      "Epoch: [123][70/97], lr: 0.01000\tTime 0.295 (0.298)\tData 0.000 (0.020)\tLoss 1.3161 (1.9080)\tPrec@1 91.406 (88.182)\tPrec@5 100.000 (99.373)\n",
      "Epoch: [123][80/97], lr: 0.01000\tTime 0.296 (0.298)\tData 0.000 (0.019)\tLoss 1.4628 (1.8823)\tPrec@1 93.750 (88.455)\tPrec@5 100.000 (99.392)\n",
      "Epoch: [123][90/97], lr: 0.01000\tTime 0.294 (0.298)\tData 0.000 (0.019)\tLoss 2.1938 (1.8613)\tPrec@1 86.719 (88.565)\tPrec@5 100.000 (99.373)\n",
      "Epoch: [123][96/97], lr: 0.01000\tTime 0.289 (0.298)\tData 0.000 (0.020)\tLoss 2.4316 (1.8818)\tPrec@1 86.441 (88.506)\tPrec@5 100.000 (99.355)\n",
      "Test: [0/100]\tTime 0.235 (0.235)\tLoss 7.0714 (7.0714)\tPrec@1 66.000 (66.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 6.3119 (7.0883)\tPrec@1 70.000 (66.636)\tPrec@5 89.000 (95.545)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 5.1800 (6.8456)\tPrec@1 76.000 (68.095)\tPrec@5 98.000 (96.000)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 5.7912 (6.8948)\tPrec@1 72.000 (67.645)\tPrec@5 99.000 (96.258)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 6.6279 (6.9191)\tPrec@1 68.000 (67.707)\tPrec@5 97.000 (96.415)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.3182 (6.8807)\tPrec@1 65.000 (67.647)\tPrec@5 95.000 (96.569)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 5.7849 (6.9463)\tPrec@1 71.000 (67.213)\tPrec@5 98.000 (96.541)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 5.8812 (6.9048)\tPrec@1 70.000 (67.408)\tPrec@5 97.000 (96.521)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.5185 (6.8581)\tPrec@1 71.000 (67.778)\tPrec@5 97.000 (96.642)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 6.8322 (6.9216)\tPrec@1 70.000 (67.560)\tPrec@5 96.000 (96.593)\n",
      "val Results: Prec@1 67.660 Prec@5 96.560 Loss 6.92890\n",
      "val Class Accuracy: [0.919,0.964,0.787,0.768,0.712,0.567,0.412,0.641,0.630,0.366]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [124][0/97], lr: 0.01000\tTime 0.356 (0.356)\tData 0.202 (0.202)\tLoss 2.1792 (2.1792)\tPrec@1 84.375 (84.375)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [124][10/97], lr: 0.01000\tTime 0.296 (0.304)\tData 0.000 (0.033)\tLoss 1.5835 (1.8310)\tPrec@1 89.844 (88.565)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [124][20/97], lr: 0.01000\tTime 0.294 (0.301)\tData 0.000 (0.026)\tLoss 1.8419 (1.8364)\tPrec@1 89.062 (88.430)\tPrec@5 100.000 (99.591)\n",
      "Epoch: [124][30/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.023)\tLoss 1.7191 (1.8236)\tPrec@1 88.281 (88.735)\tPrec@5 100.000 (99.622)\n",
      "Epoch: [124][40/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.022)\tLoss 2.1105 (1.8726)\tPrec@1 86.719 (88.281)\tPrec@5 100.000 (99.466)\n",
      "Epoch: [124][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 2.2078 (1.9027)\tPrec@1 84.375 (88.128)\tPrec@5 98.438 (99.418)\n",
      "Epoch: [124][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 1.6529 (1.9215)\tPrec@1 91.406 (88.038)\tPrec@5 99.219 (99.411)\n",
      "Epoch: [124][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.8769 (1.9269)\tPrec@1 87.500 (88.006)\tPrec@5 100.000 (99.362)\n",
      "Epoch: [124][80/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.019)\tLoss 2.0910 (1.9004)\tPrec@1 89.062 (88.243)\tPrec@5 98.438 (99.325)\n",
      "Epoch: [124][90/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.019)\tLoss 1.9507 (1.8889)\tPrec@1 89.062 (88.316)\tPrec@5 99.219 (99.322)\n",
      "Epoch: [124][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 1.9709 (1.8917)\tPrec@1 88.983 (88.328)\tPrec@5 100.000 (99.339)\n",
      "Test: [0/100]\tTime 0.234 (0.234)\tLoss 7.5237 (7.5237)\tPrec@1 64.000 (64.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 5.5576 (7.6019)\tPrec@1 75.000 (65.000)\tPrec@5 96.000 (94.455)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 6.1671 (7.4814)\tPrec@1 67.000 (64.714)\tPrec@5 94.000 (94.143)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 6.6422 (7.4939)\tPrec@1 67.000 (64.968)\tPrec@5 95.000 (93.839)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 7.5825 (7.5165)\tPrec@1 65.000 (65.049)\tPrec@5 92.000 (94.000)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 6.7810 (7.4708)\tPrec@1 69.000 (65.039)\tPrec@5 93.000 (93.941)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 5.6879 (7.4256)\tPrec@1 74.000 (65.098)\tPrec@5 96.000 (94.131)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 7.2526 (7.4132)\tPrec@1 65.000 (65.239)\tPrec@5 96.000 (94.099)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 7.2081 (7.3735)\tPrec@1 68.000 (65.370)\tPrec@5 93.000 (94.160)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 6.2833 (7.4304)\tPrec@1 74.000 (65.143)\tPrec@5 97.000 (94.220)\n",
      "val Results: Prec@1 64.930 Prec@5 94.120 Loss 7.47251\n",
      "val Class Accuracy: [0.977,0.939,0.775,0.592,0.697,0.601,0.870,0.586,0.290,0.166]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [125][0/97], lr: 0.01000\tTime 0.351 (0.351)\tData 0.211 (0.211)\tLoss 0.7382 (0.7382)\tPrec@1 95.312 (95.312)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [125][10/97], lr: 0.01000\tTime 0.297 (0.306)\tData 0.000 (0.035)\tLoss 1.9526 (1.5128)\tPrec@1 89.844 (91.122)\tPrec@5 97.656 (99.432)\n",
      "Epoch: [125][20/97], lr: 0.01000\tTime 0.297 (0.303)\tData 0.000 (0.026)\tLoss 2.5709 (1.7022)\tPrec@1 83.594 (89.881)\tPrec@5 98.438 (99.368)\n",
      "Epoch: [125][30/97], lr: 0.01000\tTime 0.298 (0.302)\tData 0.000 (0.023)\tLoss 2.5971 (1.7736)\tPrec@1 82.812 (89.289)\tPrec@5 100.000 (99.420)\n",
      "Epoch: [125][40/97], lr: 0.01000\tTime 0.299 (0.301)\tData 0.000 (0.022)\tLoss 1.9810 (1.8039)\tPrec@1 89.062 (89.177)\tPrec@5 100.000 (99.466)\n",
      "Epoch: [125][50/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 1.5774 (1.8529)\tPrec@1 89.844 (88.817)\tPrec@5 100.000 (99.357)\n",
      "Epoch: [125][60/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.020)\tLoss 1.6847 (1.8744)\tPrec@1 89.062 (88.627)\tPrec@5 99.219 (99.385)\n",
      "Epoch: [125][70/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 1.7371 (1.8488)\tPrec@1 89.062 (88.919)\tPrec@5 100.000 (99.417)\n",
      "Epoch: [125][80/97], lr: 0.01000\tTime 0.302 (0.300)\tData 0.000 (0.020)\tLoss 2.3499 (1.8638)\tPrec@1 84.375 (88.725)\tPrec@5 99.219 (99.344)\n",
      "Epoch: [125][90/97], lr: 0.01000\tTime 0.303 (0.300)\tData 0.000 (0.019)\tLoss 1.7085 (1.8624)\tPrec@1 90.625 (88.745)\tPrec@5 100.000 (99.339)\n",
      "Epoch: [125][96/97], lr: 0.01000\tTime 0.291 (0.300)\tData 0.000 (0.020)\tLoss 2.1859 (1.8670)\tPrec@1 84.746 (88.691)\tPrec@5 100.000 (99.347)\n",
      "Test: [0/100]\tTime 0.236 (0.236)\tLoss 8.2256 (8.2256)\tPrec@1 62.000 (62.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 6.0296 (7.9497)\tPrec@1 70.000 (62.091)\tPrec@5 95.000 (92.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.0897 (7.8053)\tPrec@1 69.000 (63.238)\tPrec@5 97.000 (92.524)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.7165 (7.8637)\tPrec@1 61.000 (62.968)\tPrec@5 92.000 (92.516)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.4680 (7.8602)\tPrec@1 64.000 (63.244)\tPrec@5 96.000 (92.512)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.7435 (7.7797)\tPrec@1 64.000 (63.431)\tPrec@5 92.000 (92.549)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.0511 (7.7384)\tPrec@1 71.000 (63.541)\tPrec@5 95.000 (92.590)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.6054 (7.7358)\tPrec@1 65.000 (63.634)\tPrec@5 97.000 (92.592)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.3531 (7.7100)\tPrec@1 59.000 (63.753)\tPrec@5 91.000 (92.716)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.5825 (7.7604)\tPrec@1 58.000 (63.429)\tPrec@5 92.000 (92.626)\n",
      "val Results: Prec@1 63.350 Prec@5 92.590 Loss 7.79245\n",
      "val Class Accuracy: [0.948,0.963,0.685,0.784,0.627,0.689,0.710,0.552,0.302,0.075]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [126][0/97], lr: 0.01000\tTime 0.385 (0.385)\tData 0.243 (0.243)\tLoss 2.9197 (2.9197)\tPrec@1 79.688 (79.688)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [126][10/97], lr: 0.01000\tTime 0.295 (0.307)\tData 0.000 (0.037)\tLoss 1.7278 (1.9512)\tPrec@1 88.281 (87.358)\tPrec@5 99.219 (99.361)\n",
      "Epoch: [126][20/97], lr: 0.01000\tTime 0.298 (0.303)\tData 0.000 (0.028)\tLoss 1.2409 (1.8552)\tPrec@1 93.750 (88.281)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [126][30/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.024)\tLoss 1.6947 (1.8004)\tPrec@1 89.844 (88.684)\tPrec@5 99.219 (99.496)\n",
      "Epoch: [126][40/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.023)\tLoss 1.9670 (1.7998)\tPrec@1 87.500 (88.700)\tPrec@5 98.438 (99.486)\n",
      "Epoch: [126][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.022)\tLoss 1.6603 (1.8278)\tPrec@1 92.188 (88.572)\tPrec@5 97.656 (99.357)\n",
      "Epoch: [126][60/97], lr: 0.01000\tTime 0.299 (0.300)\tData 0.000 (0.021)\tLoss 1.8052 (1.8019)\tPrec@1 87.500 (88.845)\tPrec@5 99.219 (99.398)\n",
      "Epoch: [126][70/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 2.2448 (1.8395)\tPrec@1 85.156 (88.633)\tPrec@5 99.219 (99.384)\n",
      "Epoch: [126][80/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.9761 (1.8568)\tPrec@1 86.719 (88.455)\tPrec@5 100.000 (99.431)\n",
      "Epoch: [126][90/97], lr: 0.01000\tTime 0.291 (0.299)\tData 0.000 (0.020)\tLoss 1.9518 (1.8540)\tPrec@1 86.719 (88.530)\tPrec@5 99.219 (99.416)\n",
      "Epoch: [126][96/97], lr: 0.01000\tTime 0.288 (0.299)\tData 0.000 (0.020)\tLoss 1.7867 (1.8558)\tPrec@1 89.831 (88.586)\tPrec@5 100.000 (99.412)\n",
      "Test: [0/100]\tTime 0.260 (0.260)\tLoss 10.2048 (10.2048)\tPrec@1 55.000 (55.000)\tPrec@5 89.000 (89.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 7.4614 (9.3892)\tPrec@1 70.000 (58.455)\tPrec@5 88.000 (87.818)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.0446 (9.2134)\tPrec@1 66.000 (58.571)\tPrec@5 94.000 (88.810)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 8.7202 (9.1982)\tPrec@1 56.000 (58.258)\tPrec@5 87.000 (88.452)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.1601 (9.1922)\tPrec@1 56.000 (58.463)\tPrec@5 90.000 (88.341)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.8687 (9.1142)\tPrec@1 59.000 (58.627)\tPrec@5 93.000 (88.608)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.9396 (9.0552)\tPrec@1 69.000 (58.770)\tPrec@5 92.000 (88.770)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.7581 (8.9816)\tPrec@1 64.000 (59.254)\tPrec@5 91.000 (88.704)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.9977 (8.9538)\tPrec@1 62.000 (59.333)\tPrec@5 88.000 (88.852)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.9427 (9.0088)\tPrec@1 62.000 (59.132)\tPrec@5 88.000 (88.659)\n",
      "val Results: Prec@1 59.260 Prec@5 88.610 Loss 9.01545\n",
      "val Class Accuracy: [0.907,0.949,0.780,0.858,0.507,0.631,0.447,0.597,0.248,0.002]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [127][0/97], lr: 0.01000\tTime 0.394 (0.394)\tData 0.243 (0.243)\tLoss 1.1518 (1.1518)\tPrec@1 92.969 (92.969)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [127][10/97], lr: 0.01000\tTime 0.295 (0.308)\tData 0.000 (0.037)\tLoss 1.7973 (1.7552)\tPrec@1 90.625 (89.134)\tPrec@5 98.438 (99.361)\n",
      "Epoch: [127][20/97], lr: 0.01000\tTime 0.300 (0.304)\tData 0.000 (0.028)\tLoss 1.5461 (1.8866)\tPrec@1 91.406 (88.207)\tPrec@5 99.219 (99.368)\n",
      "Epoch: [127][30/97], lr: 0.01000\tTime 0.294 (0.302)\tData 0.000 (0.024)\tLoss 2.2479 (1.9556)\tPrec@1 87.500 (87.500)\tPrec@5 98.438 (99.244)\n",
      "Epoch: [127][40/97], lr: 0.01000\tTime 0.299 (0.302)\tData 0.000 (0.023)\tLoss 1.7382 (1.9107)\tPrec@1 88.281 (87.900)\tPrec@5 99.219 (99.371)\n",
      "Epoch: [127][50/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 1.4555 (1.8932)\tPrec@1 91.406 (88.128)\tPrec@5 100.000 (99.295)\n",
      "Epoch: [127][60/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 1.4717 (1.8607)\tPrec@1 90.625 (88.435)\tPrec@5 100.000 (99.398)\n",
      "Epoch: [127][70/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 2.1828 (1.8789)\tPrec@1 84.375 (88.270)\tPrec@5 99.219 (99.384)\n",
      "Epoch: [127][80/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 1.3932 (1.8811)\tPrec@1 89.062 (88.262)\tPrec@5 100.000 (99.373)\n",
      "Epoch: [127][90/97], lr: 0.01000\tTime 0.301 (0.300)\tData 0.000 (0.020)\tLoss 2.1983 (1.8905)\tPrec@1 86.719 (88.195)\tPrec@5 99.219 (99.399)\n",
      "Epoch: [127][96/97], lr: 0.01000\tTime 0.289 (0.300)\tData 0.000 (0.020)\tLoss 2.4002 (1.8923)\tPrec@1 83.051 (88.151)\tPrec@5 100.000 (99.412)\n",
      "Test: [0/100]\tTime 0.277 (0.277)\tLoss 8.8735 (8.8735)\tPrec@1 61.000 (61.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 6.5387 (8.5584)\tPrec@1 69.000 (59.545)\tPrec@5 96.000 (94.636)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.6893 (8.3969)\tPrec@1 66.000 (59.476)\tPrec@5 96.000 (94.619)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.3530 (8.4050)\tPrec@1 61.000 (59.065)\tPrec@5 95.000 (94.645)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 8.4314 (8.3427)\tPrec@1 62.000 (59.683)\tPrec@5 92.000 (94.707)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 8.2191 (8.3252)\tPrec@1 59.000 (59.667)\tPrec@5 92.000 (94.745)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.2186 (8.3821)\tPrec@1 67.000 (59.525)\tPrec@5 94.000 (94.590)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.8370 (8.3596)\tPrec@1 61.000 (59.592)\tPrec@5 96.000 (94.789)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.5358 (8.3216)\tPrec@1 67.000 (59.790)\tPrec@5 93.000 (94.877)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.6420 (8.3649)\tPrec@1 64.000 (59.593)\tPrec@5 97.000 (94.956)\n",
      "val Results: Prec@1 59.350 Prec@5 94.870 Loss 8.40034\n",
      "val Class Accuracy: [0.933,0.994,0.872,0.485,0.689,0.397,0.534,0.399,0.490,0.142]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [128][0/97], lr: 0.01000\tTime 0.385 (0.385)\tData 0.244 (0.244)\tLoss 1.0034 (1.0034)\tPrec@1 94.531 (94.531)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [128][10/97], lr: 0.01000\tTime 0.297 (0.309)\tData 0.000 (0.038)\tLoss 1.6399 (1.6766)\tPrec@1 90.625 (89.418)\tPrec@5 98.438 (99.574)\n",
      "Epoch: [128][20/97], lr: 0.01000\tTime 0.296 (0.303)\tData 0.000 (0.028)\tLoss 2.1103 (1.8056)\tPrec@1 88.281 (88.951)\tPrec@5 100.000 (99.554)\n",
      "Epoch: [128][30/97], lr: 0.01000\tTime 0.297 (0.303)\tData 0.000 (0.025)\tLoss 1.8046 (1.8057)\tPrec@1 90.625 (88.962)\tPrec@5 98.438 (99.546)\n",
      "Epoch: [128][40/97], lr: 0.01000\tTime 0.304 (0.303)\tData 0.000 (0.023)\tLoss 1.3312 (1.8046)\tPrec@1 92.969 (88.948)\tPrec@5 100.000 (99.524)\n",
      "Epoch: [128][50/97], lr: 0.01000\tTime 0.300 (0.303)\tData 0.000 (0.022)\tLoss 1.6640 (1.8308)\tPrec@1 89.844 (88.817)\tPrec@5 98.438 (99.449)\n",
      "Epoch: [128][60/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.021)\tLoss 2.1311 (1.8529)\tPrec@1 85.938 (88.640)\tPrec@5 100.000 (99.436)\n",
      "Epoch: [128][70/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.020)\tLoss 2.3643 (1.8448)\tPrec@1 83.594 (88.578)\tPrec@5 99.219 (99.428)\n",
      "Epoch: [128][80/97], lr: 0.01000\tTime 0.299 (0.301)\tData 0.000 (0.020)\tLoss 1.7028 (1.8268)\tPrec@1 87.500 (88.628)\tPrec@5 99.219 (99.450)\n",
      "Epoch: [128][90/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.020)\tLoss 2.0075 (1.8326)\tPrec@1 87.500 (88.633)\tPrec@5 100.000 (99.451)\n",
      "Epoch: [128][96/97], lr: 0.01000\tTime 0.290 (0.301)\tData 0.000 (0.020)\tLoss 0.9690 (1.8390)\tPrec@1 93.220 (88.626)\tPrec@5 100.000 (99.460)\n",
      "Test: [0/100]\tTime 0.257 (0.257)\tLoss 7.7292 (7.7292)\tPrec@1 68.000 (68.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 5.5814 (7.5160)\tPrec@1 77.000 (67.364)\tPrec@5 95.000 (94.091)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.7216 (7.3715)\tPrec@1 71.000 (67.619)\tPrec@5 97.000 (94.762)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.7904 (7.3660)\tPrec@1 68.000 (67.548)\tPrec@5 96.000 (94.484)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 6.1897 (7.3761)\tPrec@1 73.000 (67.415)\tPrec@5 95.000 (94.171)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.7108 (7.3516)\tPrec@1 74.000 (67.392)\tPrec@5 97.000 (94.490)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 5.6353 (7.3001)\tPrec@1 73.000 (67.311)\tPrec@5 98.000 (94.508)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 6.2220 (7.2468)\tPrec@1 69.000 (67.465)\tPrec@5 99.000 (94.493)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.5381 (7.2127)\tPrec@1 68.000 (67.642)\tPrec@5 90.000 (94.593)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 7.5339 (7.2549)\tPrec@1 66.000 (67.484)\tPrec@5 99.000 (94.571)\n",
      "val Results: Prec@1 67.350 Prec@5 94.560 Loss 7.28872\n",
      "val Class Accuracy: [0.934,0.991,0.751,0.757,0.716,0.709,0.675,0.661,0.520,0.021]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [129][0/97], lr: 0.01000\tTime 0.354 (0.354)\tData 0.202 (0.202)\tLoss 1.6578 (1.6578)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [129][10/97], lr: 0.01000\tTime 0.295 (0.304)\tData 0.000 (0.034)\tLoss 1.7718 (1.6927)\tPrec@1 86.719 (89.631)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [129][20/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.026)\tLoss 1.7048 (1.7524)\tPrec@1 90.625 (89.435)\tPrec@5 100.000 (99.591)\n",
      "Epoch: [129][30/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.023)\tLoss 1.5566 (1.7407)\tPrec@1 90.625 (89.441)\tPrec@5 99.219 (99.521)\n",
      "Epoch: [129][40/97], lr: 0.01000\tTime 0.289 (0.300)\tData 0.000 (0.022)\tLoss 0.9317 (1.7052)\tPrec@1 93.750 (89.596)\tPrec@5 100.000 (99.543)\n",
      "Epoch: [129][50/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 2.3328 (1.7436)\tPrec@1 85.938 (89.583)\tPrec@5 96.875 (99.464)\n",
      "Epoch: [129][60/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.020)\tLoss 1.5297 (1.7897)\tPrec@1 89.062 (89.229)\tPrec@5 99.219 (99.462)\n",
      "Epoch: [129][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.8148 (1.7848)\tPrec@1 86.719 (89.305)\tPrec@5 100.000 (99.461)\n",
      "Epoch: [129][80/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.019)\tLoss 1.9926 (1.8230)\tPrec@1 86.719 (88.985)\tPrec@5 100.000 (99.460)\n",
      "Epoch: [129][90/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.019)\tLoss 2.0029 (1.8483)\tPrec@1 88.281 (88.831)\tPrec@5 99.219 (99.451)\n",
      "Epoch: [129][96/97], lr: 0.01000\tTime 0.289 (0.298)\tData 0.000 (0.020)\tLoss 2.1038 (1.8630)\tPrec@1 86.441 (88.691)\tPrec@5 98.305 (99.428)\n",
      "Test: [0/100]\tTime 0.248 (0.248)\tLoss 9.2415 (9.2415)\tPrec@1 56.000 (56.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.0127 (8.3819)\tPrec@1 70.000 (61.364)\tPrec@5 89.000 (93.545)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 6.1204 (8.2961)\tPrec@1 70.000 (61.381)\tPrec@5 97.000 (93.571)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.6949 (8.3043)\tPrec@1 65.000 (61.161)\tPrec@5 94.000 (93.742)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.4252 (8.2971)\tPrec@1 66.000 (61.268)\tPrec@5 94.000 (93.951)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 7.9423 (8.1943)\tPrec@1 63.000 (61.804)\tPrec@5 96.000 (94.059)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 7.0098 (8.1247)\tPrec@1 66.000 (62.049)\tPrec@5 94.000 (94.098)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 7.0506 (8.0612)\tPrec@1 68.000 (62.451)\tPrec@5 96.000 (94.169)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.6618 (8.0122)\tPrec@1 65.000 (62.840)\tPrec@5 97.000 (94.309)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.3192 (8.0729)\tPrec@1 63.000 (62.637)\tPrec@5 95.000 (94.275)\n",
      "val Results: Prec@1 62.650 Prec@5 94.260 Loss 8.08627\n",
      "val Class Accuracy: [0.943,0.983,0.636,0.801,0.723,0.661,0.540,0.641,0.234,0.103]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [130][0/97], lr: 0.01000\tTime 0.391 (0.391)\tData 0.235 (0.235)\tLoss 1.8856 (1.8856)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [130][10/97], lr: 0.01000\tTime 0.294 (0.308)\tData 0.000 (0.037)\tLoss 1.4290 (1.8683)\tPrec@1 91.406 (88.991)\tPrec@5 98.438 (99.503)\n",
      "Epoch: [130][20/97], lr: 0.01000\tTime 0.295 (0.303)\tData 0.000 (0.027)\tLoss 1.4816 (1.8800)\tPrec@1 90.625 (88.802)\tPrec@5 99.219 (99.479)\n",
      "Epoch: [130][30/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.024)\tLoss 2.1343 (1.7818)\tPrec@1 87.500 (89.516)\tPrec@5 99.219 (99.546)\n",
      "Epoch: [130][40/97], lr: 0.01000\tTime 0.299 (0.301)\tData 0.000 (0.022)\tLoss 1.3579 (1.7609)\tPrec@1 92.188 (89.558)\tPrec@5 98.438 (99.466)\n",
      "Epoch: [130][50/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 2.2886 (1.7903)\tPrec@1 85.938 (89.292)\tPrec@5 100.000 (99.494)\n",
      "Epoch: [130][60/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 1.8547 (1.8125)\tPrec@1 90.625 (89.203)\tPrec@5 99.219 (99.513)\n",
      "Epoch: [130][70/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 1.7039 (1.8229)\tPrec@1 88.281 (88.908)\tPrec@5 100.000 (99.494)\n",
      "Epoch: [130][80/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 2.4729 (1.8304)\tPrec@1 84.375 (88.889)\tPrec@5 100.000 (99.518)\n",
      "Epoch: [130][90/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.5244 (1.8216)\tPrec@1 89.062 (88.899)\tPrec@5 100.000 (99.519)\n",
      "Epoch: [130][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 1.7496 (1.8182)\tPrec@1 88.983 (88.917)\tPrec@5 100.000 (99.524)\n",
      "Test: [0/100]\tTime 0.246 (0.246)\tLoss 9.3047 (9.3047)\tPrec@1 59.000 (59.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 7.2644 (8.0041)\tPrec@1 65.000 (61.818)\tPrec@5 89.000 (92.727)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 5.9645 (7.8989)\tPrec@1 68.000 (61.762)\tPrec@5 95.000 (92.905)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.1008 (7.8410)\tPrec@1 68.000 (62.516)\tPrec@5 92.000 (93.129)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 6.6597 (7.7411)\tPrec@1 70.000 (63.390)\tPrec@5 94.000 (93.463)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.6686 (7.7054)\tPrec@1 72.000 (63.549)\tPrec@5 96.000 (93.588)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 7.2614 (7.6678)\tPrec@1 67.000 (63.705)\tPrec@5 93.000 (93.770)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 6.9442 (7.6642)\tPrec@1 68.000 (63.817)\tPrec@5 95.000 (93.746)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.8361 (7.5916)\tPrec@1 69.000 (64.136)\tPrec@5 94.000 (93.852)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.4751 (7.6494)\tPrec@1 64.000 (63.967)\tPrec@5 96.000 (93.692)\n",
      "val Results: Prec@1 64.000 Prec@5 93.670 Loss 7.65409\n",
      "val Class Accuracy: [0.935,0.992,0.782,0.609,0.693,0.622,0.527,0.823,0.233,0.184]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [131][0/97], lr: 0.01000\tTime 0.340 (0.340)\tData 0.200 (0.200)\tLoss 1.9187 (1.9187)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [131][10/97], lr: 0.01000\tTime 0.297 (0.305)\tData 0.000 (0.033)\tLoss 2.2842 (1.9361)\tPrec@1 86.719 (88.423)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [131][20/97], lr: 0.01000\tTime 0.297 (0.303)\tData 0.000 (0.026)\tLoss 1.9862 (1.8091)\tPrec@1 87.500 (89.286)\tPrec@5 97.656 (99.479)\n",
      "Epoch: [131][30/97], lr: 0.01000\tTime 0.297 (0.302)\tData 0.000 (0.023)\tLoss 1.5104 (1.8240)\tPrec@1 90.625 (88.886)\tPrec@5 100.000 (99.496)\n",
      "Epoch: [131][40/97], lr: 0.01000\tTime 0.299 (0.301)\tData 0.000 (0.022)\tLoss 2.4636 (1.8230)\tPrec@1 84.375 (88.948)\tPrec@5 100.000 (99.505)\n",
      "Epoch: [131][50/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 2.2271 (1.8356)\tPrec@1 87.500 (88.971)\tPrec@5 98.438 (99.449)\n",
      "Epoch: [131][60/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.020)\tLoss 1.5071 (1.8177)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.411)\n",
      "Epoch: [131][70/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 2.3876 (1.8403)\tPrec@1 86.719 (88.875)\tPrec@5 99.219 (99.461)\n",
      "Epoch: [131][80/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.020)\tLoss 1.6688 (1.8356)\tPrec@1 88.281 (88.889)\tPrec@5 100.000 (99.441)\n",
      "Epoch: [131][90/97], lr: 0.01000\tTime 0.301 (0.300)\tData 0.000 (0.019)\tLoss 2.2404 (1.8445)\tPrec@1 84.375 (88.796)\tPrec@5 99.219 (99.442)\n",
      "Epoch: [131][96/97], lr: 0.01000\tTime 0.291 (0.300)\tData 0.000 (0.020)\tLoss 2.3121 (1.8401)\tPrec@1 85.593 (88.868)\tPrec@5 100.000 (99.460)\n",
      "Test: [0/100]\tTime 0.238 (0.238)\tLoss 8.5010 (8.5010)\tPrec@1 63.000 (63.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 6.8917 (7.6654)\tPrec@1 66.000 (63.727)\tPrec@5 87.000 (91.727)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.9373 (7.4640)\tPrec@1 74.000 (64.857)\tPrec@5 96.000 (93.048)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 6.5987 (7.3411)\tPrec@1 71.000 (65.935)\tPrec@5 96.000 (93.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.0627 (7.3578)\tPrec@1 68.000 (65.927)\tPrec@5 92.000 (93.293)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.9580 (7.3797)\tPrec@1 67.000 (65.608)\tPrec@5 95.000 (93.255)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 5.6508 (7.3336)\tPrec@1 73.000 (65.639)\tPrec@5 95.000 (93.361)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 6.3874 (7.3114)\tPrec@1 72.000 (65.746)\tPrec@5 94.000 (93.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.6635 (7.2602)\tPrec@1 63.000 (65.988)\tPrec@5 93.000 (93.506)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.2097 (7.3219)\tPrec@1 69.000 (65.769)\tPrec@5 96.000 (93.396)\n",
      "val Results: Prec@1 65.780 Prec@5 93.460 Loss 7.32806\n",
      "val Class Accuracy: [0.942,0.942,0.885,0.620,0.739,0.485,0.462,0.731,0.351,0.421]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [132][0/97], lr: 0.01000\tTime 0.342 (0.342)\tData 0.183 (0.183)\tLoss 1.7912 (1.7912)\tPrec@1 90.625 (90.625)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [132][10/97], lr: 0.01000\tTime 0.295 (0.305)\tData 0.000 (0.032)\tLoss 1.7144 (1.7023)\tPrec@1 90.625 (90.341)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [132][20/97], lr: 0.01000\tTime 0.297 (0.302)\tData 0.000 (0.025)\tLoss 2.2201 (1.8094)\tPrec@1 86.719 (89.323)\tPrec@5 98.438 (99.442)\n",
      "Epoch: [132][30/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.022)\tLoss 2.0844 (1.8247)\tPrec@1 86.719 (89.264)\tPrec@5 99.219 (99.546)\n",
      "Epoch: [132][40/97], lr: 0.01000\tTime 0.287 (0.300)\tData 0.000 (0.021)\tLoss 2.0281 (1.8529)\tPrec@1 89.062 (88.948)\tPrec@5 100.000 (99.581)\n",
      "Epoch: [132][50/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 1.6299 (1.8251)\tPrec@1 89.844 (89.108)\tPrec@5 100.000 (99.510)\n",
      "Epoch: [132][60/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.020)\tLoss 1.7670 (1.8448)\tPrec@1 88.281 (88.922)\tPrec@5 100.000 (99.501)\n",
      "Epoch: [132][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.019)\tLoss 2.0423 (1.8579)\tPrec@1 85.938 (88.831)\tPrec@5 99.219 (99.472)\n",
      "Epoch: [132][80/97], lr: 0.01000\tTime 0.298 (0.299)\tData 0.000 (0.019)\tLoss 2.0482 (1.8710)\tPrec@1 86.719 (88.667)\tPrec@5 100.000 (99.489)\n",
      "Epoch: [132][90/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.019)\tLoss 1.5596 (1.8873)\tPrec@1 89.844 (88.496)\tPrec@5 100.000 (99.493)\n",
      "Epoch: [132][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 1.7380 (1.8814)\tPrec@1 90.678 (88.562)\tPrec@5 100.000 (99.508)\n",
      "Test: [0/100]\tTime 0.269 (0.269)\tLoss 9.9505 (9.9505)\tPrec@1 56.000 (56.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.9494 (8.5074)\tPrec@1 70.000 (61.818)\tPrec@5 94.000 (92.909)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.4907 (8.4146)\tPrec@1 73.000 (61.333)\tPrec@5 97.000 (93.667)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.2859 (8.4851)\tPrec@1 57.000 (60.903)\tPrec@5 96.000 (93.484)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.9363 (8.5172)\tPrec@1 62.000 (60.732)\tPrec@5 95.000 (93.463)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.2463 (8.5109)\tPrec@1 63.000 (60.667)\tPrec@5 91.000 (93.412)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.4124 (8.4763)\tPrec@1 68.000 (60.623)\tPrec@5 95.000 (93.525)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.1128 (8.4043)\tPrec@1 62.000 (61.141)\tPrec@5 98.000 (93.676)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.5540 (8.3596)\tPrec@1 60.000 (61.346)\tPrec@5 90.000 (93.667)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 8.6267 (8.4154)\tPrec@1 61.000 (61.022)\tPrec@5 98.000 (93.703)\n",
      "val Results: Prec@1 61.130 Prec@5 93.700 Loss 8.43272\n",
      "val Class Accuracy: [0.953,0.980,0.779,0.660,0.472,0.788,0.580,0.622,0.265,0.014]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [133][0/97], lr: 0.01000\tTime 0.341 (0.341)\tData 0.205 (0.205)\tLoss 1.9936 (1.9936)\tPrec@1 85.156 (85.156)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [133][10/97], lr: 0.01000\tTime 0.295 (0.303)\tData 0.000 (0.034)\tLoss 1.8591 (1.6164)\tPrec@1 89.062 (90.128)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [133][20/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.026)\tLoss 2.2920 (1.6985)\tPrec@1 89.062 (90.030)\tPrec@5 98.438 (99.330)\n",
      "Epoch: [133][30/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.023)\tLoss 1.7352 (1.7219)\tPrec@1 89.062 (89.617)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [133][40/97], lr: 0.01000\tTime 0.294 (0.300)\tData 0.000 (0.022)\tLoss 1.6397 (1.7544)\tPrec@1 89.062 (89.386)\tPrec@5 98.438 (99.371)\n",
      "Epoch: [133][50/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 1.7816 (1.7989)\tPrec@1 89.062 (89.185)\tPrec@5 100.000 (99.418)\n",
      "Epoch: [133][60/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 2.8068 (1.8245)\tPrec@1 84.375 (89.011)\tPrec@5 96.875 (99.436)\n",
      "Epoch: [133][70/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 1.7691 (1.8244)\tPrec@1 90.625 (89.029)\tPrec@5 98.438 (99.472)\n",
      "Epoch: [133][80/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 1.8524 (1.8305)\tPrec@1 89.844 (88.995)\tPrec@5 98.438 (99.431)\n",
      "Epoch: [133][90/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.019)\tLoss 1.7218 (1.8052)\tPrec@1 89.844 (89.174)\tPrec@5 99.219 (99.408)\n",
      "Epoch: [133][96/97], lr: 0.01000\tTime 0.286 (0.299)\tData 0.000 (0.020)\tLoss 2.0880 (1.8184)\tPrec@1 86.441 (89.126)\tPrec@5 99.153 (99.412)\n",
      "Test: [0/100]\tTime 0.265 (0.265)\tLoss 7.9495 (7.9495)\tPrec@1 61.000 (61.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 5.4228 (7.5188)\tPrec@1 75.000 (65.727)\tPrec@5 97.000 (96.727)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.6367 (7.1673)\tPrec@1 68.000 (67.571)\tPrec@5 99.000 (96.190)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 6.8119 (7.2378)\tPrec@1 67.000 (67.000)\tPrec@5 97.000 (96.161)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 6.9626 (7.2875)\tPrec@1 70.000 (66.927)\tPrec@5 95.000 (96.000)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.1644 (7.2242)\tPrec@1 72.000 (67.098)\tPrec@5 93.000 (95.902)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 5.6731 (7.2297)\tPrec@1 73.000 (67.082)\tPrec@5 96.000 (95.951)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 6.8254 (7.2073)\tPrec@1 68.000 (67.183)\tPrec@5 96.000 (95.930)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 7.0103 (7.1771)\tPrec@1 68.000 (67.173)\tPrec@5 94.000 (95.975)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 6.3608 (7.2167)\tPrec@1 70.000 (66.989)\tPrec@5 99.000 (96.077)\n",
      "val Results: Prec@1 66.970 Prec@5 96.010 Loss 7.23579\n",
      "val Class Accuracy: [0.942,0.978,0.672,0.795,0.821,0.621,0.748,0.395,0.531,0.194]\n",
      "Best Prec@1: 69.940\n",
      "\n",
      "Epoch: [134][0/97], lr: 0.01000\tTime 0.347 (0.347)\tData 0.192 (0.192)\tLoss 2.3473 (2.3473)\tPrec@1 86.719 (86.719)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [134][10/97], lr: 0.01000\tTime 0.301 (0.305)\tData 0.000 (0.033)\tLoss 1.2199 (1.8265)\tPrec@1 93.750 (89.062)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [134][20/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.025)\tLoss 2.0046 (1.7285)\tPrec@1 88.281 (89.732)\tPrec@5 97.656 (99.330)\n",
      "Epoch: [134][30/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.023)\tLoss 2.2699 (1.7654)\tPrec@1 85.938 (89.163)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [134][40/97], lr: 0.01000\tTime 0.301 (0.300)\tData 0.000 (0.021)\tLoss 1.2749 (1.7815)\tPrec@1 92.188 (89.158)\tPrec@5 100.000 (99.276)\n",
      "Epoch: [134][50/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 1.6980 (1.7952)\tPrec@1 89.844 (89.124)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [134][60/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.9893 (1.8194)\tPrec@1 87.500 (88.934)\tPrec@5 99.219 (99.244)\n",
      "Epoch: [134][70/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 1.6567 (1.8108)\tPrec@1 89.844 (89.018)\tPrec@5 100.000 (99.241)\n",
      "Epoch: [134][80/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.019)\tLoss 2.0212 (1.8112)\tPrec@1 85.938 (88.937)\tPrec@5 100.000 (99.228)\n",
      "Epoch: [134][90/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.019)\tLoss 1.7536 (1.8066)\tPrec@1 91.406 (88.994)\tPrec@5 99.219 (99.279)\n",
      "Epoch: [134][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 1.3267 (1.8102)\tPrec@1 92.373 (88.981)\tPrec@5 100.000 (99.315)\n",
      "Test: [0/100]\tTime 0.238 (0.238)\tLoss 6.0392 (6.0392)\tPrec@1 67.000 (67.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 4.9901 (6.4732)\tPrec@1 78.000 (68.727)\tPrec@5 96.000 (96.273)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 4.8346 (6.2571)\tPrec@1 73.000 (69.810)\tPrec@5 97.000 (96.524)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 5.7319 (6.2440)\tPrec@1 73.000 (70.194)\tPrec@5 94.000 (96.065)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.9201 (6.3236)\tPrec@1 68.000 (70.293)\tPrec@5 96.000 (95.780)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.5961 (6.3859)\tPrec@1 70.000 (70.137)\tPrec@5 94.000 (95.725)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 5.2515 (6.3801)\tPrec@1 76.000 (70.197)\tPrec@5 97.000 (95.836)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 5.6384 (6.3385)\tPrec@1 75.000 (70.437)\tPrec@5 98.000 (95.831)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.6600 (6.3213)\tPrec@1 70.000 (70.481)\tPrec@5 93.000 (95.926)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 4.8404 (6.3366)\tPrec@1 79.000 (70.462)\tPrec@5 99.000 (96.000)\n",
      "val Results: Prec@1 70.350 Prec@5 95.990 Loss 6.35596\n",
      "val Class Accuracy: [0.888,0.957,0.879,0.700,0.647,0.581,0.817,0.674,0.694,0.198]\n",
      "Best Prec@1: 70.350\n",
      "\n",
      "Epoch: [135][0/97], lr: 0.01000\tTime 0.356 (0.356)\tData 0.200 (0.200)\tLoss 1.7440 (1.7440)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [135][10/97], lr: 0.01000\tTime 0.297 (0.305)\tData 0.000 (0.034)\tLoss 2.1902 (1.6651)\tPrec@1 87.500 (89.631)\tPrec@5 100.000 (99.858)\n",
      "Epoch: [135][20/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.026)\tLoss 1.8101 (1.6822)\tPrec@1 88.281 (89.621)\tPrec@5 97.656 (99.702)\n",
      "Epoch: [135][30/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.023)\tLoss 1.9502 (1.7076)\tPrec@1 87.500 (89.592)\tPrec@5 100.000 (99.622)\n",
      "Epoch: [135][40/97], lr: 0.01000\tTime 0.300 (0.301)\tData 0.000 (0.022)\tLoss 1.7034 (1.6784)\tPrec@1 89.062 (89.748)\tPrec@5 100.000 (99.600)\n",
      "Epoch: [135][50/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.021)\tLoss 2.7995 (1.7082)\tPrec@1 82.031 (89.645)\tPrec@5 99.219 (99.525)\n",
      "Epoch: [135][60/97], lr: 0.01000\tTime 0.299 (0.301)\tData 0.000 (0.020)\tLoss 2.2913 (1.7200)\tPrec@1 85.938 (89.600)\tPrec@5 98.438 (99.539)\n",
      "Epoch: [135][70/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 2.5563 (1.7123)\tPrec@1 85.156 (89.690)\tPrec@5 99.219 (99.560)\n",
      "Epoch: [135][80/97], lr: 0.01000\tTime 0.299 (0.301)\tData 0.000 (0.020)\tLoss 2.1551 (1.7092)\tPrec@1 84.375 (89.728)\tPrec@5 100.000 (99.556)\n",
      "Epoch: [135][90/97], lr: 0.01000\tTime 0.303 (0.300)\tData 0.000 (0.019)\tLoss 1.9964 (1.7525)\tPrec@1 86.719 (89.457)\tPrec@5 99.219 (99.536)\n",
      "Epoch: [135][96/97], lr: 0.01000\tTime 0.290 (0.300)\tData 0.000 (0.020)\tLoss 1.8913 (1.7524)\tPrec@1 87.288 (89.481)\tPrec@5 98.305 (99.516)\n",
      "Test: [0/100]\tTime 0.249 (0.249)\tLoss 9.4403 (9.4403)\tPrec@1 58.000 (58.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 6.8352 (8.5636)\tPrec@1 68.000 (61.364)\tPrec@5 95.000 (94.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.6041 (8.4059)\tPrec@1 67.000 (60.952)\tPrec@5 99.000 (94.762)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 8.4034 (8.4002)\tPrec@1 59.000 (60.871)\tPrec@5 94.000 (94.677)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.1571 (8.3890)\tPrec@1 56.000 (60.780)\tPrec@5 93.000 (94.659)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.6044 (8.2902)\tPrec@1 64.000 (61.314)\tPrec@5 94.000 (94.784)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.1099 (8.3141)\tPrec@1 70.000 (61.180)\tPrec@5 95.000 (94.738)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.7690 (8.2786)\tPrec@1 63.000 (61.282)\tPrec@5 97.000 (94.732)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.4658 (8.2239)\tPrec@1 59.000 (61.531)\tPrec@5 93.000 (94.877)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 8.1128 (8.3064)\tPrec@1 60.000 (61.055)\tPrec@5 97.000 (94.835)\n",
      "val Results: Prec@1 61.020 Prec@5 94.800 Loss 8.32473\n",
      "val Class Accuracy: [0.943,0.991,0.828,0.447,0.615,0.842,0.420,0.354,0.354,0.308]\n",
      "Best Prec@1: 70.350\n",
      "\n",
      "Epoch: [136][0/97], lr: 0.01000\tTime 0.389 (0.389)\tData 0.233 (0.233)\tLoss 1.8189 (1.8189)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [136][10/97], lr: 0.01000\tTime 0.295 (0.307)\tData 0.000 (0.037)\tLoss 1.7216 (1.6921)\tPrec@1 89.844 (90.625)\tPrec@5 99.219 (99.716)\n",
      "Epoch: [136][20/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.027)\tLoss 1.5790 (1.7142)\tPrec@1 91.406 (89.993)\tPrec@5 100.000 (99.702)\n",
      "Epoch: [136][30/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.024)\tLoss 2.9270 (1.8234)\tPrec@1 82.812 (89.415)\tPrec@5 98.438 (99.672)\n",
      "Epoch: [136][40/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.022)\tLoss 2.0947 (1.8635)\tPrec@1 86.719 (88.986)\tPrec@5 99.219 (99.524)\n",
      "Epoch: [136][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 2.0263 (1.8760)\tPrec@1 88.281 (88.894)\tPrec@5 99.219 (99.510)\n",
      "Epoch: [136][60/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 1.8036 (1.8314)\tPrec@1 86.719 (89.037)\tPrec@5 100.000 (99.526)\n",
      "Epoch: [136][70/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 2.0358 (1.8297)\tPrec@1 88.281 (88.974)\tPrec@5 99.219 (99.494)\n",
      "Epoch: [136][80/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 1.7172 (1.8284)\tPrec@1 91.406 (89.034)\tPrec@5 99.219 (99.518)\n",
      "Epoch: [136][90/97], lr: 0.01000\tTime 0.299 (0.299)\tData 0.000 (0.020)\tLoss 1.2713 (1.8252)\tPrec@1 92.188 (88.968)\tPrec@5 100.000 (99.502)\n",
      "Epoch: [136][96/97], lr: 0.01000\tTime 0.290 (0.299)\tData 0.000 (0.020)\tLoss 2.2872 (1.8174)\tPrec@1 85.593 (88.989)\tPrec@5 99.153 (99.492)\n",
      "Test: [0/100]\tTime 0.306 (0.306)\tLoss 11.4047 (11.4047)\tPrec@1 48.000 (48.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.073 (0.094)\tLoss 8.0650 (10.5216)\tPrec@1 66.000 (53.636)\tPrec@5 93.000 (87.455)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 7.8759 (10.3960)\tPrec@1 63.000 (54.571)\tPrec@5 93.000 (87.476)\n",
      "Test: [30/100]\tTime 0.072 (0.080)\tLoss 10.2153 (10.3721)\tPrec@1 48.000 (54.548)\tPrec@5 91.000 (87.258)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 11.4629 (10.4858)\tPrec@1 51.000 (54.268)\tPrec@5 83.000 (86.805)\n",
      "Test: [50/100]\tTime 0.072 (0.077)\tLoss 9.9041 (10.4383)\tPrec@1 58.000 (54.412)\tPrec@5 90.000 (87.078)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.0592 (10.3920)\tPrec@1 63.000 (54.426)\tPrec@5 89.000 (87.246)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 9.8093 (10.3608)\tPrec@1 58.000 (54.606)\tPrec@5 91.000 (87.282)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 10.4666 (10.3201)\tPrec@1 51.000 (54.716)\tPrec@5 84.000 (87.284)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.4569 (10.3593)\tPrec@1 63.000 (54.626)\tPrec@5 88.000 (87.198)\n",
      "val Results: Prec@1 54.490 Prec@5 87.030 Loss 10.39123\n",
      "val Class Accuracy: [0.947,0.976,0.891,0.701,0.440,0.453,0.603,0.262,0.175,0.001]\n",
      "Best Prec@1: 70.350\n",
      "\n",
      "Epoch: [137][0/97], lr: 0.01000\tTime 0.385 (0.385)\tData 0.229 (0.229)\tLoss 2.0439 (2.0439)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [137][10/97], lr: 0.01000\tTime 0.295 (0.307)\tData 0.000 (0.036)\tLoss 2.0515 (1.8728)\tPrec@1 89.062 (88.920)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [137][20/97], lr: 0.01000\tTime 0.302 (0.303)\tData 0.000 (0.027)\tLoss 1.0776 (1.8831)\tPrec@1 92.969 (88.393)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [137][30/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.024)\tLoss 2.2078 (1.7847)\tPrec@1 89.062 (89.138)\tPrec@5 98.438 (99.546)\n",
      "Epoch: [137][40/97], lr: 0.01000\tTime 0.291 (0.301)\tData 0.000 (0.022)\tLoss 1.3391 (1.7984)\tPrec@1 91.406 (89.062)\tPrec@5 100.000 (99.486)\n",
      "Epoch: [137][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 1.7622 (1.8240)\tPrec@1 89.062 (88.986)\tPrec@5 99.219 (99.464)\n",
      "Epoch: [137][60/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 2.0083 (1.8390)\tPrec@1 86.719 (88.858)\tPrec@5 99.219 (99.360)\n",
      "Epoch: [137][70/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 2.2254 (1.8692)\tPrec@1 85.938 (88.600)\tPrec@5 99.219 (99.362)\n",
      "Epoch: [137][80/97], lr: 0.01000\tTime 0.302 (0.300)\tData 0.000 (0.020)\tLoss 1.5670 (1.8422)\tPrec@1 88.281 (88.764)\tPrec@5 100.000 (99.383)\n",
      "Epoch: [137][90/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.019)\tLoss 1.2411 (1.8458)\tPrec@1 94.531 (88.711)\tPrec@5 99.219 (99.365)\n",
      "Epoch: [137][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 1.8139 (1.8496)\tPrec@1 91.525 (88.675)\tPrec@5 98.305 (99.355)\n",
      "Test: [0/100]\tTime 0.236 (0.236)\tLoss 9.8102 (9.8102)\tPrec@1 53.000 (53.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 6.5730 (8.4672)\tPrec@1 68.000 (61.182)\tPrec@5 96.000 (94.364)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 6.9789 (8.2719)\tPrec@1 65.000 (62.000)\tPrec@5 96.000 (94.524)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 8.3266 (8.2770)\tPrec@1 64.000 (62.097)\tPrec@5 94.000 (94.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.8863 (8.3449)\tPrec@1 60.000 (61.780)\tPrec@5 91.000 (94.171)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.7344 (8.2673)\tPrec@1 66.000 (62.216)\tPrec@5 93.000 (94.235)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 7.3074 (8.2911)\tPrec@1 65.000 (62.197)\tPrec@5 95.000 (94.049)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.0238 (8.2573)\tPrec@1 63.000 (62.352)\tPrec@5 97.000 (94.099)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.9686 (8.2063)\tPrec@1 64.000 (62.568)\tPrec@5 91.000 (94.074)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 7.8977 (8.2998)\tPrec@1 65.000 (62.055)\tPrec@5 95.000 (94.055)\n",
      "val Results: Prec@1 61.950 Prec@5 93.990 Loss 8.33000\n",
      "val Class Accuracy: [0.906,0.994,0.886,0.432,0.866,0.618,0.579,0.244,0.430,0.240]\n",
      "Best Prec@1: 70.350\n",
      "\n",
      "Epoch: [138][0/97], lr: 0.01000\tTime 0.378 (0.378)\tData 0.224 (0.224)\tLoss 1.6569 (1.6569)\tPrec@1 92.188 (92.188)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [138][10/97], lr: 0.01000\tTime 0.294 (0.308)\tData 0.000 (0.036)\tLoss 1.9023 (1.7823)\tPrec@1 86.719 (89.915)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [138][20/97], lr: 0.01000\tTime 0.296 (0.304)\tData 0.000 (0.027)\tLoss 2.0581 (1.8296)\tPrec@1 86.719 (89.323)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [138][30/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.024)\tLoss 1.1850 (1.8068)\tPrec@1 92.969 (89.264)\tPrec@5 100.000 (99.496)\n",
      "Epoch: [138][40/97], lr: 0.01000\tTime 0.298 (0.302)\tData 0.000 (0.022)\tLoss 1.7306 (1.8015)\tPrec@1 89.062 (89.253)\tPrec@5 100.000 (99.505)\n",
      "Epoch: [138][50/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 1.9063 (1.8135)\tPrec@1 89.844 (89.292)\tPrec@5 99.219 (99.387)\n",
      "Epoch: [138][60/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 1.5512 (1.7986)\tPrec@1 90.625 (89.395)\tPrec@5 99.219 (99.436)\n",
      "Epoch: [138][70/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 1.6072 (1.7933)\tPrec@1 89.844 (89.371)\tPrec@5 99.219 (99.428)\n",
      "Epoch: [138][80/97], lr: 0.01000\tTime 0.299 (0.300)\tData 0.000 (0.020)\tLoss 1.7556 (1.8257)\tPrec@1 88.281 (89.159)\tPrec@5 99.219 (99.392)\n",
      "Epoch: [138][90/97], lr: 0.01000\tTime 0.301 (0.300)\tData 0.000 (0.020)\tLoss 1.4852 (1.8222)\tPrec@1 89.844 (89.131)\tPrec@5 99.219 (99.408)\n",
      "Epoch: [138][96/97], lr: 0.01000\tTime 0.289 (0.300)\tData 0.000 (0.020)\tLoss 1.4927 (1.8162)\tPrec@1 90.678 (89.199)\tPrec@5 100.000 (99.420)\n",
      "Test: [0/100]\tTime 0.278 (0.278)\tLoss 8.1713 (8.1713)\tPrec@1 63.000 (63.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 6.2477 (7.6642)\tPrec@1 70.000 (63.727)\tPrec@5 98.000 (96.364)\n",
      "Test: [20/100]\tTime 0.072 (0.082)\tLoss 6.0560 (7.4276)\tPrec@1 68.000 (64.714)\tPrec@5 97.000 (96.476)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 6.7388 (7.4389)\tPrec@1 69.000 (64.613)\tPrec@5 97.000 (96.484)\n",
      "Test: [40/100]\tTime 0.072 (0.078)\tLoss 7.2644 (7.4595)\tPrec@1 66.000 (64.902)\tPrec@5 99.000 (96.488)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.4719 (7.4522)\tPrec@1 64.000 (64.706)\tPrec@5 95.000 (96.549)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.1850 (7.4635)\tPrec@1 71.000 (64.639)\tPrec@5 96.000 (96.475)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 6.9646 (7.4240)\tPrec@1 68.000 (64.930)\tPrec@5 98.000 (96.592)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.2855 (7.3949)\tPrec@1 66.000 (65.000)\tPrec@5 93.000 (96.679)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.4668 (7.4578)\tPrec@1 70.000 (64.670)\tPrec@5 99.000 (96.615)\n",
      "val Results: Prec@1 64.570 Prec@5 96.560 Loss 7.48014\n",
      "val Class Accuracy: [0.961,0.987,0.853,0.454,0.776,0.657,0.589,0.608,0.526,0.046]\n",
      "Best Prec@1: 70.350\n",
      "\n",
      "Epoch: [139][0/97], lr: 0.01000\tTime 0.355 (0.355)\tData 0.203 (0.203)\tLoss 2.4929 (2.4929)\tPrec@1 82.812 (82.812)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [139][10/97], lr: 0.01000\tTime 0.295 (0.305)\tData 0.000 (0.034)\tLoss 1.9248 (1.8098)\tPrec@1 88.281 (88.565)\tPrec@5 99.219 (99.503)\n",
      "Epoch: [139][20/97], lr: 0.01000\tTime 0.294 (0.301)\tData 0.000 (0.026)\tLoss 1.0722 (1.7123)\tPrec@1 92.969 (89.174)\tPrec@5 99.219 (99.479)\n",
      "Epoch: [139][30/97], lr: 0.01000\tTime 0.300 (0.301)\tData 0.000 (0.023)\tLoss 1.8191 (1.7183)\tPrec@1 87.500 (89.239)\tPrec@5 99.219 (99.471)\n",
      "Epoch: [139][40/97], lr: 0.01000\tTime 0.286 (0.300)\tData 0.000 (0.022)\tLoss 1.9057 (1.7564)\tPrec@1 88.281 (89.062)\tPrec@5 100.000 (99.524)\n",
      "Epoch: [139][50/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 2.0164 (1.7896)\tPrec@1 88.281 (88.909)\tPrec@5 99.219 (99.433)\n",
      "Epoch: [139][60/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 1.3366 (1.7939)\tPrec@1 91.406 (88.883)\tPrec@5 100.000 (99.424)\n",
      "Epoch: [139][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.7939 (1.7784)\tPrec@1 88.281 (88.985)\tPrec@5 99.219 (99.450)\n",
      "Epoch: [139][80/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.019)\tLoss 2.1629 (1.7807)\tPrec@1 85.938 (88.956)\tPrec@5 100.000 (99.460)\n",
      "Epoch: [139][90/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.019)\tLoss 2.1528 (1.8031)\tPrec@1 85.938 (88.831)\tPrec@5 100.000 (99.485)\n",
      "Epoch: [139][96/97], lr: 0.01000\tTime 0.292 (0.299)\tData 0.000 (0.020)\tLoss 1.2567 (1.7979)\tPrec@1 90.678 (88.884)\tPrec@5 100.000 (99.484)\n",
      "Test: [0/100]\tTime 0.267 (0.267)\tLoss 6.6550 (6.6550)\tPrec@1 70.000 (70.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 5.2340 (6.3216)\tPrec@1 76.000 (70.636)\tPrec@5 95.000 (96.000)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.2193 (6.3029)\tPrec@1 74.000 (70.333)\tPrec@5 97.000 (96.095)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 5.8460 (6.2992)\tPrec@1 70.000 (70.742)\tPrec@5 99.000 (96.032)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.7884 (6.2369)\tPrec@1 74.000 (71.049)\tPrec@5 96.000 (96.171)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 5.8262 (6.2383)\tPrec@1 74.000 (71.059)\tPrec@5 97.000 (96.255)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 5.6312 (6.2199)\tPrec@1 76.000 (71.066)\tPrec@5 96.000 (96.246)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 5.2628 (6.2066)\tPrec@1 74.000 (71.099)\tPrec@5 99.000 (96.310)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.0640 (6.1522)\tPrec@1 71.000 (71.235)\tPrec@5 94.000 (96.407)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.8321 (6.2202)\tPrec@1 76.000 (71.011)\tPrec@5 99.000 (96.440)\n",
      "val Results: Prec@1 70.990 Prec@5 96.440 Loss 6.24861\n",
      "val Class Accuracy: [0.972,0.958,0.820,0.687,0.746,0.593,0.629,0.728,0.464,0.502]\n",
      "Best Prec@1: 70.990\n",
      "\n",
      "Epoch: [140][0/97], lr: 0.01000\tTime 0.362 (0.362)\tData 0.224 (0.224)\tLoss 1.6764 (1.6764)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [140][10/97], lr: 0.01000\tTime 0.296 (0.307)\tData 0.000 (0.036)\tLoss 2.0749 (1.8608)\tPrec@1 88.281 (88.494)\tPrec@5 99.219 (99.361)\n",
      "Epoch: [140][20/97], lr: 0.01000\tTime 0.296 (0.303)\tData 0.000 (0.027)\tLoss 1.2142 (1.6911)\tPrec@1 92.188 (89.583)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [140][30/97], lr: 0.01000\tTime 0.301 (0.302)\tData 0.000 (0.024)\tLoss 1.9204 (1.7133)\tPrec@1 88.281 (89.441)\tPrec@5 100.000 (99.471)\n",
      "Epoch: [140][40/97], lr: 0.01000\tTime 0.301 (0.301)\tData 0.000 (0.022)\tLoss 1.3484 (1.7283)\tPrec@1 89.844 (89.386)\tPrec@5 100.000 (99.486)\n",
      "Epoch: [140][50/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 2.3325 (1.7738)\tPrec@1 85.938 (89.017)\tPrec@5 98.438 (99.449)\n",
      "Epoch: [140][60/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.021)\tLoss 1.7583 (1.8102)\tPrec@1 89.062 (88.691)\tPrec@5 99.219 (99.436)\n",
      "Epoch: [140][70/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 1.7840 (1.8136)\tPrec@1 86.719 (88.688)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [140][80/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.020)\tLoss 1.5432 (1.8114)\tPrec@1 92.188 (88.812)\tPrec@5 99.219 (99.441)\n",
      "Epoch: [140][90/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.020)\tLoss 1.6445 (1.7829)\tPrec@1 91.406 (89.037)\tPrec@5 100.000 (99.451)\n",
      "Epoch: [140][96/97], lr: 0.01000\tTime 0.291 (0.300)\tData 0.000 (0.020)\tLoss 2.0786 (1.7920)\tPrec@1 88.983 (89.005)\tPrec@5 99.153 (99.452)\n",
      "Test: [0/100]\tTime 0.268 (0.268)\tLoss 9.7622 (9.7622)\tPrec@1 49.000 (49.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 7.1862 (8.7637)\tPrec@1 66.000 (59.000)\tPrec@5 94.000 (95.545)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.4093 (8.8866)\tPrec@1 65.000 (58.476)\tPrec@5 96.000 (95.095)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.9890 (8.8336)\tPrec@1 59.000 (58.871)\tPrec@5 97.000 (95.226)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.2166 (8.8287)\tPrec@1 56.000 (58.927)\tPrec@5 93.000 (95.122)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.4832 (8.7780)\tPrec@1 62.000 (59.118)\tPrec@5 94.000 (95.137)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.7065 (8.7492)\tPrec@1 63.000 (59.066)\tPrec@5 94.000 (95.115)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.3230 (8.7238)\tPrec@1 64.000 (59.225)\tPrec@5 97.000 (95.085)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.9839 (8.6731)\tPrec@1 60.000 (59.543)\tPrec@5 94.000 (95.099)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.1191 (8.7346)\tPrec@1 63.000 (59.308)\tPrec@5 96.000 (95.143)\n",
      "val Results: Prec@1 59.150 Prec@5 95.170 Loss 8.77191\n",
      "val Class Accuracy: [0.982,0.982,0.786,0.705,0.564,0.509,0.580,0.518,0.140,0.149]\n",
      "Best Prec@1: 70.990\n",
      "\n",
      "Epoch: [141][0/97], lr: 0.01000\tTime 0.366 (0.366)\tData 0.209 (0.209)\tLoss 1.4454 (1.4454)\tPrec@1 90.625 (90.625)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [141][10/97], lr: 0.01000\tTime 0.296 (0.305)\tData 0.000 (0.034)\tLoss 1.1728 (1.5108)\tPrec@1 92.969 (90.909)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [141][20/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.026)\tLoss 1.2764 (1.5926)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (99.554)\n",
      "Epoch: [141][30/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.023)\tLoss 1.6549 (1.6579)\tPrec@1 90.625 (89.793)\tPrec@5 99.219 (99.420)\n",
      "Epoch: [141][40/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.022)\tLoss 1.5305 (1.6587)\tPrec@1 92.969 (89.882)\tPrec@5 99.219 (99.390)\n",
      "Epoch: [141][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 1.4967 (1.6536)\tPrec@1 92.188 (89.951)\tPrec@5 99.219 (99.387)\n",
      "Epoch: [141][60/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.7770 (1.7265)\tPrec@1 86.719 (89.421)\tPrec@5 99.219 (99.321)\n",
      "Epoch: [141][70/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.2292 (1.7644)\tPrec@1 85.156 (89.184)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [141][80/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.020)\tLoss 2.1745 (1.7842)\tPrec@1 87.500 (89.111)\tPrec@5 99.219 (99.334)\n",
      "Epoch: [141][90/97], lr: 0.01000\tTime 0.298 (0.299)\tData 0.000 (0.019)\tLoss 1.7416 (1.7821)\tPrec@1 89.062 (89.105)\tPrec@5 100.000 (99.330)\n",
      "Epoch: [141][96/97], lr: 0.01000\tTime 0.286 (0.299)\tData 0.000 (0.020)\tLoss 2.2456 (1.7831)\tPrec@1 86.441 (89.094)\tPrec@5 100.000 (99.331)\n",
      "Test: [0/100]\tTime 0.266 (0.266)\tLoss 9.4335 (9.4335)\tPrec@1 57.000 (57.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.3492 (8.4390)\tPrec@1 70.000 (60.909)\tPrec@5 98.000 (93.727)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.8382 (8.3934)\tPrec@1 67.000 (61.000)\tPrec@5 97.000 (94.190)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 7.9773 (8.3160)\tPrec@1 59.000 (61.258)\tPrec@5 94.000 (93.968)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 8.6860 (8.3900)\tPrec@1 58.000 (60.780)\tPrec@5 92.000 (93.805)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.8854 (8.3557)\tPrec@1 66.000 (60.941)\tPrec@5 90.000 (93.784)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.6859 (8.3707)\tPrec@1 70.000 (60.639)\tPrec@5 93.000 (93.754)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.8612 (8.3445)\tPrec@1 64.000 (60.718)\tPrec@5 98.000 (93.944)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.6121 (8.3167)\tPrec@1 62.000 (60.951)\tPrec@5 90.000 (93.951)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.5054 (8.3511)\tPrec@1 57.000 (60.846)\tPrec@5 97.000 (93.890)\n",
      "val Results: Prec@1 60.720 Prec@5 93.890 Loss 8.39498\n",
      "val Class Accuracy: [0.985,0.983,0.675,0.700,0.861,0.509,0.555,0.276,0.438,0.090]\n",
      "Best Prec@1: 70.990\n",
      "\n",
      "Epoch: [142][0/97], lr: 0.01000\tTime 0.398 (0.398)\tData 0.240 (0.240)\tLoss 1.5341 (1.5341)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [142][10/97], lr: 0.01000\tTime 0.301 (0.309)\tData 0.000 (0.037)\tLoss 1.6968 (1.8156)\tPrec@1 89.062 (88.849)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [142][20/97], lr: 0.01000\tTime 0.295 (0.303)\tData 0.000 (0.028)\tLoss 1.1233 (1.6516)\tPrec@1 94.531 (90.216)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [142][30/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.024)\tLoss 1.3637 (1.6404)\tPrec@1 90.625 (89.894)\tPrec@5 100.000 (99.521)\n",
      "Epoch: [142][40/97], lr: 0.01000\tTime 0.290 (0.301)\tData 0.000 (0.023)\tLoss 1.7522 (1.6875)\tPrec@1 89.062 (89.482)\tPrec@5 100.000 (99.543)\n",
      "Epoch: [142][50/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.022)\tLoss 1.0706 (1.7379)\tPrec@1 93.750 (89.277)\tPrec@5 99.219 (99.449)\n",
      "Epoch: [142][60/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 1.9971 (1.7703)\tPrec@1 87.500 (89.216)\tPrec@5 100.000 (99.436)\n",
      "Epoch: [142][70/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 1.2112 (1.7620)\tPrec@1 92.969 (89.272)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [142][80/97], lr: 0.01000\tTime 0.301 (0.300)\tData 0.000 (0.020)\tLoss 2.1616 (1.7576)\tPrec@1 86.719 (89.342)\tPrec@5 100.000 (99.392)\n",
      "Epoch: [142][90/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.2970 (1.7978)\tPrec@1 85.938 (89.062)\tPrec@5 97.656 (99.390)\n",
      "Epoch: [142][96/97], lr: 0.01000\tTime 0.288 (0.299)\tData 0.000 (0.020)\tLoss 1.3356 (1.7905)\tPrec@1 91.525 (89.078)\tPrec@5 99.153 (99.379)\n",
      "Test: [0/100]\tTime 0.262 (0.262)\tLoss 9.5252 (9.5252)\tPrec@1 55.000 (55.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.072 (0.090)\tLoss 6.3935 (8.1717)\tPrec@1 71.000 (61.909)\tPrec@5 96.000 (95.818)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.4693 (8.0188)\tPrec@1 66.000 (62.095)\tPrec@5 97.000 (95.571)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 8.1532 (8.0650)\tPrec@1 61.000 (61.710)\tPrec@5 95.000 (95.355)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 8.0682 (8.0563)\tPrec@1 63.000 (62.122)\tPrec@5 91.000 (95.146)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 7.6922 (8.0021)\tPrec@1 64.000 (62.392)\tPrec@5 93.000 (95.196)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 6.2411 (7.9953)\tPrec@1 70.000 (62.328)\tPrec@5 94.000 (95.180)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.6438 (7.9384)\tPrec@1 66.000 (62.676)\tPrec@5 96.000 (95.197)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.9958 (7.8945)\tPrec@1 61.000 (62.963)\tPrec@5 93.000 (95.198)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 7.6608 (7.9666)\tPrec@1 61.000 (62.516)\tPrec@5 97.000 (95.143)\n",
      "val Results: Prec@1 62.500 Prec@5 95.140 Loss 7.99994\n",
      "val Class Accuracy: [0.935,0.997,0.797,0.675,0.847,0.644,0.546,0.349,0.315,0.145]\n",
      "Best Prec@1: 70.990\n",
      "\n",
      "Epoch: [143][0/97], lr: 0.01000\tTime 0.381 (0.381)\tData 0.221 (0.221)\tLoss 1.5528 (1.5528)\tPrec@1 88.281 (88.281)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [143][10/97], lr: 0.01000\tTime 0.295 (0.307)\tData 0.000 (0.035)\tLoss 1.4931 (1.6086)\tPrec@1 89.062 (89.915)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [143][20/97], lr: 0.01000\tTime 0.297 (0.303)\tData 0.000 (0.027)\tLoss 1.7265 (1.6665)\tPrec@1 87.500 (89.695)\tPrec@5 98.438 (99.442)\n",
      "Epoch: [143][30/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.024)\tLoss 2.3650 (1.7074)\tPrec@1 85.156 (89.365)\tPrec@5 98.438 (99.370)\n",
      "Epoch: [143][40/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.022)\tLoss 1.3070 (1.7701)\tPrec@1 92.188 (89.196)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [143][50/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.021)\tLoss 1.5490 (1.7639)\tPrec@1 89.844 (89.231)\tPrec@5 100.000 (99.341)\n",
      "Epoch: [143][60/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 1.9102 (1.7726)\tPrec@1 87.500 (89.178)\tPrec@5 100.000 (99.385)\n",
      "Epoch: [143][70/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 1.8040 (1.7511)\tPrec@1 88.281 (89.393)\tPrec@5 99.219 (99.417)\n",
      "Epoch: [143][80/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 2.4054 (1.7394)\tPrec@1 84.375 (89.371)\tPrec@5 99.219 (99.450)\n",
      "Epoch: [143][90/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.019)\tLoss 1.3339 (1.7232)\tPrec@1 94.531 (89.492)\tPrec@5 99.219 (99.476)\n",
      "Epoch: [143][96/97], lr: 0.01000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 1.8278 (1.7179)\tPrec@1 90.678 (89.529)\tPrec@5 99.153 (99.500)\n",
      "Test: [0/100]\tTime 0.256 (0.256)\tLoss 8.6326 (8.6326)\tPrec@1 60.000 (60.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.072 (0.089)\tLoss 6.2350 (7.8300)\tPrec@1 70.000 (63.273)\tPrec@5 97.000 (96.182)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 6.2857 (7.7435)\tPrec@1 69.000 (63.952)\tPrec@5 97.000 (96.095)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 7.3192 (7.7014)\tPrec@1 65.000 (64.194)\tPrec@5 92.000 (95.710)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 7.8779 (7.6986)\tPrec@1 63.000 (64.195)\tPrec@5 94.000 (95.780)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 6.5783 (7.6295)\tPrec@1 69.000 (64.431)\tPrec@5 97.000 (95.863)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 6.5762 (7.5931)\tPrec@1 68.000 (64.459)\tPrec@5 94.000 (95.787)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.6912 (7.5770)\tPrec@1 68.000 (64.521)\tPrec@5 95.000 (95.634)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.9290 (7.5352)\tPrec@1 60.000 (64.753)\tPrec@5 95.000 (95.679)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.5458 (7.5948)\tPrec@1 64.000 (64.418)\tPrec@5 95.000 (95.725)\n",
      "val Results: Prec@1 64.360 Prec@5 95.690 Loss 7.61343\n",
      "val Class Accuracy: [0.990,0.952,0.588,0.685,0.641,0.740,0.836,0.553,0.141,0.310]\n",
      "Best Prec@1: 70.990\n",
      "\n",
      "Epoch: [144][0/97], lr: 0.01000\tTime 0.365 (0.365)\tData 0.204 (0.204)\tLoss 1.4875 (1.4875)\tPrec@1 90.625 (90.625)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [144][10/97], lr: 0.01000\tTime 0.296 (0.308)\tData 0.000 (0.034)\tLoss 1.8194 (1.8565)\tPrec@1 89.844 (88.636)\tPrec@5 99.219 (99.574)\n",
      "Epoch: [144][20/97], lr: 0.01000\tTime 0.298 (0.305)\tData 0.000 (0.026)\tLoss 1.6839 (1.8268)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [144][30/97], lr: 0.01000\tTime 0.297 (0.303)\tData 0.000 (0.023)\tLoss 1.3321 (1.7890)\tPrec@1 92.969 (89.441)\tPrec@5 100.000 (99.471)\n",
      "Epoch: [144][40/97], lr: 0.01000\tTime 0.297 (0.302)\tData 0.000 (0.022)\tLoss 2.0062 (1.7981)\tPrec@1 89.062 (89.272)\tPrec@5 100.000 (99.524)\n",
      "Epoch: [144][50/97], lr: 0.01000\tTime 0.292 (0.302)\tData 0.000 (0.021)\tLoss 1.3798 (1.7545)\tPrec@1 92.969 (89.491)\tPrec@5 99.219 (99.525)\n",
      "Epoch: [144][60/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.020)\tLoss 2.7020 (1.7457)\tPrec@1 84.375 (89.524)\tPrec@5 99.219 (99.513)\n",
      "Epoch: [144][70/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.020)\tLoss 1.4546 (1.7537)\tPrec@1 92.969 (89.514)\tPrec@5 100.000 (99.549)\n",
      "Epoch: [144][80/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.020)\tLoss 1.5442 (1.7471)\tPrec@1 90.625 (89.487)\tPrec@5 99.219 (99.566)\n",
      "Epoch: [144][90/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.019)\tLoss 2.0209 (1.7716)\tPrec@1 88.281 (89.320)\tPrec@5 99.219 (99.562)\n",
      "Epoch: [144][96/97], lr: 0.01000\tTime 0.291 (0.300)\tData 0.000 (0.020)\tLoss 1.9002 (1.7953)\tPrec@1 88.983 (89.183)\tPrec@5 100.000 (99.524)\n",
      "Test: [0/100]\tTime 0.239 (0.239)\tLoss 7.8842 (7.8842)\tPrec@1 68.000 (68.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 5.3473 (7.1616)\tPrec@1 76.000 (67.545)\tPrec@5 98.000 (96.091)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.8694 (6.8821)\tPrec@1 70.000 (68.524)\tPrec@5 96.000 (96.000)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 6.1358 (6.9328)\tPrec@1 72.000 (68.065)\tPrec@5 98.000 (95.484)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 6.1411 (6.9910)\tPrec@1 74.000 (67.854)\tPrec@5 94.000 (95.244)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 6.5340 (6.9116)\tPrec@1 70.000 (68.216)\tPrec@5 98.000 (95.431)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 6.3069 (6.9088)\tPrec@1 69.000 (68.000)\tPrec@5 93.000 (95.492)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 6.5326 (6.9014)\tPrec@1 72.000 (68.028)\tPrec@5 98.000 (95.577)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 7.0391 (6.8595)\tPrec@1 70.000 (68.136)\tPrec@5 93.000 (95.667)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 6.9386 (6.9078)\tPrec@1 67.000 (67.824)\tPrec@5 98.000 (95.648)\n",
      "val Results: Prec@1 67.680 Prec@5 95.570 Loss 6.93628\n",
      "val Class Accuracy: [0.888,0.960,0.693,0.726,0.936,0.633,0.597,0.441,0.532,0.362]\n",
      "Best Prec@1: 70.990\n",
      "\n",
      "Epoch: [145][0/97], lr: 0.01000\tTime 0.367 (0.367)\tData 0.212 (0.212)\tLoss 1.4670 (1.4670)\tPrec@1 92.969 (92.969)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [145][10/97], lr: 0.01000\tTime 0.295 (0.307)\tData 0.000 (0.034)\tLoss 1.5834 (1.8096)\tPrec@1 89.844 (89.276)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [145][20/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.026)\tLoss 2.8377 (1.7937)\tPrec@1 84.375 (89.025)\tPrec@5 98.438 (99.479)\n",
      "Epoch: [145][30/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.023)\tLoss 1.0846 (1.7676)\tPrec@1 92.969 (89.088)\tPrec@5 100.000 (99.471)\n",
      "Epoch: [145][40/97], lr: 0.01000\tTime 0.308 (0.301)\tData 0.000 (0.022)\tLoss 1.6783 (1.8025)\tPrec@1 89.062 (88.796)\tPrec@5 100.000 (99.409)\n",
      "Epoch: [145][50/97], lr: 0.01000\tTime 0.293 (0.300)\tData 0.000 (0.021)\tLoss 1.6820 (1.7980)\tPrec@1 89.062 (88.848)\tPrec@5 100.000 (99.464)\n",
      "Epoch: [145][60/97], lr: 0.01000\tTime 0.298 (0.300)\tData 0.000 (0.020)\tLoss 1.7877 (1.8015)\tPrec@1 89.844 (88.909)\tPrec@5 99.219 (99.501)\n",
      "Epoch: [145][70/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.020)\tLoss 1.5799 (1.8019)\tPrec@1 91.406 (88.930)\tPrec@5 100.000 (99.472)\n",
      "Epoch: [145][80/97], lr: 0.01000\tTime 0.297 (0.299)\tData 0.000 (0.020)\tLoss 1.9347 (1.7865)\tPrec@1 89.062 (88.976)\tPrec@5 99.219 (99.470)\n",
      "Epoch: [145][90/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.019)\tLoss 1.2752 (1.7751)\tPrec@1 93.750 (89.028)\tPrec@5 100.000 (99.468)\n",
      "Epoch: [145][96/97], lr: 0.01000\tTime 0.290 (0.299)\tData 0.000 (0.020)\tLoss 1.3492 (1.7777)\tPrec@1 90.678 (89.030)\tPrec@5 100.000 (99.468)\n",
      "Test: [0/100]\tTime 0.253 (0.253)\tLoss 6.8092 (6.8092)\tPrec@1 69.000 (69.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.072 (0.089)\tLoss 5.1277 (7.2373)\tPrec@1 78.000 (66.909)\tPrec@5 95.000 (95.636)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.4460 (7.2277)\tPrec@1 69.000 (66.571)\tPrec@5 97.000 (95.429)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 7.3575 (7.2708)\tPrec@1 64.000 (66.097)\tPrec@5 96.000 (95.000)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.1993 (7.3309)\tPrec@1 66.000 (66.049)\tPrec@5 94.000 (94.951)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.5364 (7.2679)\tPrec@1 67.000 (66.314)\tPrec@5 97.000 (95.078)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 6.7625 (7.2852)\tPrec@1 69.000 (65.902)\tPrec@5 95.000 (95.213)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 6.7065 (7.2188)\tPrec@1 69.000 (66.310)\tPrec@5 96.000 (95.282)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.0910 (7.1982)\tPrec@1 67.000 (66.309)\tPrec@5 90.000 (95.296)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.7324 (7.2298)\tPrec@1 67.000 (66.220)\tPrec@5 97.000 (95.253)\n",
      "val Results: Prec@1 66.250 Prec@5 95.290 Loss 7.24862\n",
      "val Class Accuracy: [0.860,0.970,0.851,0.802,0.605,0.476,0.846,0.448,0.632,0.135]\n",
      "Best Prec@1: 70.990\n",
      "\n",
      "Epoch: [146][0/97], lr: 0.01000\tTime 0.382 (0.382)\tData 0.234 (0.234)\tLoss 2.2839 (2.2839)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [146][10/97], lr: 0.01000\tTime 0.298 (0.310)\tData 0.000 (0.037)\tLoss 1.6418 (1.8278)\tPrec@1 89.062 (88.210)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [146][20/97], lr: 0.01000\tTime 0.296 (0.305)\tData 0.000 (0.027)\tLoss 1.9883 (1.8705)\tPrec@1 89.062 (88.207)\tPrec@5 99.219 (99.628)\n",
      "Epoch: [146][30/97], lr: 0.01000\tTime 0.296 (0.303)\tData 0.000 (0.024)\tLoss 1.9252 (1.8471)\tPrec@1 89.062 (88.508)\tPrec@5 100.000 (99.647)\n",
      "Epoch: [146][40/97], lr: 0.01000\tTime 0.300 (0.302)\tData 0.000 (0.023)\tLoss 1.7417 (1.8726)\tPrec@1 86.719 (88.453)\tPrec@5 100.000 (99.505)\n",
      "Epoch: [146][50/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.021)\tLoss 1.2822 (1.8761)\tPrec@1 92.188 (88.480)\tPrec@5 100.000 (99.571)\n",
      "Epoch: [146][60/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.021)\tLoss 1.3866 (1.8367)\tPrec@1 92.188 (88.665)\tPrec@5 98.438 (99.552)\n",
      "Epoch: [146][70/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.020)\tLoss 2.4852 (1.8606)\tPrec@1 85.938 (88.622)\tPrec@5 99.219 (99.549)\n",
      "Epoch: [146][80/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.020)\tLoss 2.7734 (1.8748)\tPrec@1 82.812 (88.532)\tPrec@5 95.312 (99.498)\n",
      "Epoch: [146][90/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.020)\tLoss 1.4485 (1.8770)\tPrec@1 90.625 (88.582)\tPrec@5 100.000 (99.476)\n",
      "Epoch: [146][96/97], lr: 0.01000\tTime 0.291 (0.300)\tData 0.000 (0.020)\tLoss 2.1466 (1.8888)\tPrec@1 88.136 (88.497)\tPrec@5 99.153 (99.460)\n",
      "Test: [0/100]\tTime 0.261 (0.261)\tLoss 9.4066 (9.4066)\tPrec@1 55.000 (55.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.072 (0.090)\tLoss 6.4978 (8.1937)\tPrec@1 69.000 (60.818)\tPrec@5 98.000 (95.273)\n",
      "Test: [20/100]\tTime 0.072 (0.082)\tLoss 7.3816 (8.1168)\tPrec@1 64.000 (61.095)\tPrec@5 95.000 (95.238)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.1021 (7.9869)\tPrec@1 63.000 (61.710)\tPrec@5 96.000 (95.290)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.8329 (8.0220)\tPrec@1 59.000 (61.463)\tPrec@5 96.000 (95.244)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.6626 (7.9564)\tPrec@1 66.000 (61.686)\tPrec@5 94.000 (95.255)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.6026 (7.9651)\tPrec@1 71.000 (61.623)\tPrec@5 95.000 (95.311)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.1576 (7.9277)\tPrec@1 65.000 (61.859)\tPrec@5 98.000 (95.366)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.8329 (7.8852)\tPrec@1 64.000 (62.074)\tPrec@5 93.000 (95.481)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.5047 (7.9136)\tPrec@1 64.000 (61.846)\tPrec@5 97.000 (95.440)\n",
      "val Results: Prec@1 61.830 Prec@5 95.370 Loss 7.94999\n",
      "val Class Accuracy: [0.919,0.982,0.906,0.658,0.721,0.433,0.664,0.411,0.433,0.056]\n",
      "Best Prec@1: 70.990\n",
      "\n",
      "Epoch: [147][0/97], lr: 0.01000\tTime 0.428 (0.428)\tData 0.261 (0.261)\tLoss 1.8485 (1.8485)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [147][10/97], lr: 0.01000\tTime 0.299 (0.314)\tData 0.000 (0.038)\tLoss 1.7066 (1.7701)\tPrec@1 90.625 (89.489)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [147][20/97], lr: 0.01000\tTime 0.294 (0.307)\tData 0.000 (0.028)\tLoss 1.3891 (1.7408)\tPrec@1 92.188 (89.546)\tPrec@5 98.438 (99.442)\n",
      "Epoch: [147][30/97], lr: 0.01000\tTime 0.296 (0.304)\tData 0.000 (0.025)\tLoss 2.6063 (1.8196)\tPrec@1 86.719 (89.214)\tPrec@5 98.438 (99.320)\n",
      "Epoch: [147][40/97], lr: 0.01000\tTime 0.297 (0.303)\tData 0.000 (0.023)\tLoss 2.0788 (1.7786)\tPrec@1 85.938 (89.310)\tPrec@5 99.219 (99.371)\n",
      "Epoch: [147][50/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.022)\tLoss 1.3490 (1.7595)\tPrec@1 91.406 (89.308)\tPrec@5 99.219 (99.372)\n",
      "Epoch: [147][60/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.021)\tLoss 1.5831 (1.7905)\tPrec@1 90.625 (89.165)\tPrec@5 99.219 (99.436)\n",
      "Epoch: [147][70/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.020)\tLoss 1.7115 (1.7897)\tPrec@1 89.844 (89.140)\tPrec@5 98.438 (99.461)\n",
      "Epoch: [147][80/97], lr: 0.01000\tTime 0.303 (0.301)\tData 0.000 (0.020)\tLoss 1.0916 (1.7681)\tPrec@1 92.969 (89.236)\tPrec@5 99.219 (99.450)\n",
      "Epoch: [147][90/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 2.0625 (1.7537)\tPrec@1 88.281 (89.320)\tPrec@5 97.656 (99.433)\n",
      "Epoch: [147][96/97], lr: 0.01000\tTime 0.289 (0.300)\tData 0.000 (0.020)\tLoss 1.6643 (1.7571)\tPrec@1 88.136 (89.279)\tPrec@5 99.153 (99.420)\n",
      "Test: [0/100]\tTime 0.251 (0.251)\tLoss 5.6799 (5.6799)\tPrec@1 71.000 (71.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.072 (0.089)\tLoss 5.1836 (6.2116)\tPrec@1 77.000 (70.636)\tPrec@5 98.000 (96.818)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 5.6504 (6.2312)\tPrec@1 68.000 (70.905)\tPrec@5 98.000 (97.143)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 5.4000 (6.1301)\tPrec@1 75.000 (71.516)\tPrec@5 96.000 (97.000)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.7752 (6.1460)\tPrec@1 72.000 (71.463)\tPrec@5 97.000 (96.878)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 6.0137 (6.1448)\tPrec@1 72.000 (71.412)\tPrec@5 98.000 (97.000)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 5.4550 (6.1447)\tPrec@1 76.000 (71.246)\tPrec@5 97.000 (97.049)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 5.8700 (6.1277)\tPrec@1 68.000 (71.239)\tPrec@5 98.000 (97.127)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.2390 (6.0998)\tPrec@1 71.000 (71.309)\tPrec@5 97.000 (97.086)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 6.3303 (6.1553)\tPrec@1 69.000 (70.989)\tPrec@5 95.000 (97.099)\n",
      "val Results: Prec@1 70.840 Prec@5 97.060 Loss 6.19498\n",
      "val Class Accuracy: [0.957,0.943,0.792,0.545,0.693,0.671,0.893,0.734,0.438,0.418]\n",
      "Best Prec@1: 70.990\n",
      "\n",
      "Epoch: [148][0/97], lr: 0.01000\tTime 0.390 (0.390)\tData 0.240 (0.240)\tLoss 1.3314 (1.3314)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [148][10/97], lr: 0.01000\tTime 0.297 (0.309)\tData 0.000 (0.037)\tLoss 1.3542 (1.7188)\tPrec@1 92.969 (89.489)\tPrec@5 100.000 (99.716)\n",
      "Epoch: [148][20/97], lr: 0.01000\tTime 0.295 (0.304)\tData 0.000 (0.028)\tLoss 1.7618 (1.6866)\tPrec@1 88.281 (89.918)\tPrec@5 98.438 (99.591)\n",
      "Epoch: [148][30/97], lr: 0.01000\tTime 0.296 (0.303)\tData 0.000 (0.024)\tLoss 1.6409 (1.6579)\tPrec@1 89.062 (89.919)\tPrec@5 100.000 (99.546)\n",
      "Epoch: [148][40/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.023)\tLoss 2.2193 (1.7117)\tPrec@1 85.938 (89.539)\tPrec@5 97.656 (99.486)\n",
      "Epoch: [148][50/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.022)\tLoss 1.8941 (1.7525)\tPrec@1 89.062 (89.262)\tPrec@5 99.219 (99.433)\n",
      "Epoch: [148][60/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 1.7006 (1.7512)\tPrec@1 88.281 (89.229)\tPrec@5 99.219 (99.436)\n",
      "Epoch: [148][70/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.020)\tLoss 1.7159 (1.7634)\tPrec@1 91.406 (89.261)\tPrec@5 99.219 (99.439)\n",
      "Epoch: [148][80/97], lr: 0.01000\tTime 0.299 (0.301)\tData 0.000 (0.020)\tLoss 2.1963 (1.7595)\tPrec@1 85.938 (89.342)\tPrec@5 99.219 (99.470)\n",
      "Epoch: [148][90/97], lr: 0.01000\tTime 0.305 (0.301)\tData 0.000 (0.020)\tLoss 1.5756 (1.7771)\tPrec@1 92.969 (89.277)\tPrec@5 99.219 (99.442)\n",
      "Epoch: [148][96/97], lr: 0.01000\tTime 0.290 (0.300)\tData 0.000 (0.020)\tLoss 1.8886 (1.7775)\tPrec@1 89.831 (89.320)\tPrec@5 99.153 (99.452)\n",
      "Test: [0/100]\tTime 0.248 (0.248)\tLoss 7.8509 (7.8509)\tPrec@1 64.000 (64.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 5.5934 (7.2163)\tPrec@1 73.000 (66.273)\tPrec@5 97.000 (95.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.6878 (7.2143)\tPrec@1 67.000 (66.667)\tPrec@5 97.000 (95.190)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 6.8624 (7.2109)\tPrec@1 66.000 (66.516)\tPrec@5 95.000 (95.194)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.4261 (7.1923)\tPrec@1 66.000 (66.829)\tPrec@5 92.000 (94.927)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.1563 (7.1626)\tPrec@1 74.000 (67.059)\tPrec@5 91.000 (94.784)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 6.0012 (7.1664)\tPrec@1 73.000 (67.000)\tPrec@5 94.000 (94.574)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 6.8254 (7.1613)\tPrec@1 67.000 (67.070)\tPrec@5 97.000 (94.690)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.9280 (7.1268)\tPrec@1 71.000 (67.321)\tPrec@5 95.000 (94.765)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 6.9114 (7.1844)\tPrec@1 69.000 (67.055)\tPrec@5 97.000 (94.780)\n",
      "val Results: Prec@1 66.990 Prec@5 94.730 Loss 7.20894\n",
      "val Class Accuracy: [0.960,0.989,0.792,0.758,0.713,0.528,0.827,0.493,0.321,0.318]\n",
      "Best Prec@1: 70.990\n",
      "\n",
      "Epoch: [149][0/97], lr: 0.01000\tTime 0.363 (0.363)\tData 0.210 (0.210)\tLoss 1.5776 (1.5776)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [149][10/97], lr: 0.01000\tTime 0.293 (0.307)\tData 0.000 (0.034)\tLoss 1.9211 (1.6276)\tPrec@1 89.844 (89.915)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [149][20/97], lr: 0.01000\tTime 0.295 (0.303)\tData 0.000 (0.026)\tLoss 1.6080 (1.6748)\tPrec@1 88.281 (89.807)\tPrec@5 100.000 (99.591)\n",
      "Epoch: [149][30/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.023)\tLoss 1.9545 (1.6427)\tPrec@1 85.938 (89.970)\tPrec@5 99.219 (99.471)\n",
      "Epoch: [149][40/97], lr: 0.01000\tTime 0.297 (0.301)\tData 0.000 (0.022)\tLoss 2.1680 (1.6865)\tPrec@1 85.938 (89.634)\tPrec@5 99.219 (99.524)\n",
      "Epoch: [149][50/97], lr: 0.01000\tTime 0.295 (0.301)\tData 0.000 (0.021)\tLoss 2.4196 (1.7036)\tPrec@1 85.938 (89.491)\tPrec@5 98.438 (99.479)\n",
      "Epoch: [149][60/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 2.1141 (1.7416)\tPrec@1 86.719 (89.216)\tPrec@5 100.000 (99.449)\n",
      "Epoch: [149][70/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 2.2852 (1.7596)\tPrec@1 87.500 (89.195)\tPrec@5 100.000 (99.472)\n",
      "Epoch: [149][80/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 1.5990 (1.7581)\tPrec@1 91.406 (89.226)\tPrec@5 98.438 (99.441)\n",
      "Epoch: [149][90/97], lr: 0.01000\tTime 0.294 (0.299)\tData 0.000 (0.019)\tLoss 2.0021 (1.7693)\tPrec@1 91.406 (89.260)\tPrec@5 98.438 (99.416)\n",
      "Epoch: [149][96/97], lr: 0.01000\tTime 0.291 (0.299)\tData 0.000 (0.020)\tLoss 2.2954 (1.7829)\tPrec@1 86.441 (89.158)\tPrec@5 98.305 (99.412)\n",
      "Test: [0/100]\tTime 0.258 (0.258)\tLoss 11.2072 (11.2072)\tPrec@1 50.000 (50.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 7.3727 (9.8879)\tPrec@1 66.000 (55.273)\tPrec@5 97.000 (95.273)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 8.4947 (9.8156)\tPrec@1 60.000 (54.952)\tPrec@5 97.000 (95.143)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.7657 (9.7964)\tPrec@1 56.000 (54.677)\tPrec@5 96.000 (94.935)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 10.5393 (9.8208)\tPrec@1 52.000 (54.756)\tPrec@5 94.000 (94.683)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 9.4336 (9.7758)\tPrec@1 57.000 (54.941)\tPrec@5 96.000 (94.804)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 8.8834 (9.7476)\tPrec@1 61.000 (54.869)\tPrec@5 94.000 (94.803)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.4775 (9.7533)\tPrec@1 54.000 (54.887)\tPrec@5 95.000 (94.577)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.3602 (9.7167)\tPrec@1 55.000 (54.963)\tPrec@5 90.000 (94.605)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.1028 (9.7583)\tPrec@1 61.000 (54.813)\tPrec@5 96.000 (94.626)\n",
      "val Results: Prec@1 54.730 Prec@5 94.550 Loss 9.80063\n",
      "val Class Accuracy: [0.939,0.973,0.919,0.627,0.656,0.186,0.627,0.354,0.143,0.049]\n",
      "Best Prec@1: 70.990\n",
      "\n",
      "Epoch: [150][0/97], lr: 0.01000\tTime 0.396 (0.396)\tData 0.247 (0.247)\tLoss 2.5698 (2.5698)\tPrec@1 81.250 (81.250)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [150][10/97], lr: 0.01000\tTime 0.296 (0.308)\tData 0.000 (0.037)\tLoss 1.9876 (1.8395)\tPrec@1 87.500 (88.778)\tPrec@5 100.000 (98.793)\n",
      "Epoch: [150][20/97], lr: 0.01000\tTime 0.293 (0.303)\tData 0.000 (0.028)\tLoss 1.5545 (1.8282)\tPrec@1 92.188 (88.914)\tPrec@5 100.000 (99.107)\n",
      "Epoch: [150][30/97], lr: 0.01000\tTime 0.297 (0.302)\tData 0.000 (0.024)\tLoss 1.1054 (1.8352)\tPrec@1 94.531 (89.037)\tPrec@5 100.000 (99.244)\n",
      "Epoch: [150][40/97], lr: 0.01000\tTime 0.288 (0.301)\tData 0.000 (0.023)\tLoss 2.2039 (1.7707)\tPrec@1 85.938 (89.272)\tPrec@5 99.219 (99.314)\n",
      "Epoch: [150][50/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 1.9672 (1.7425)\tPrec@1 87.500 (89.323)\tPrec@5 100.000 (99.387)\n",
      "Epoch: [150][60/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 1.7897 (1.7723)\tPrec@1 89.844 (89.165)\tPrec@5 100.000 (99.424)\n",
      "Epoch: [150][70/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 1.1312 (1.7781)\tPrec@1 91.406 (89.151)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [150][80/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 2.1380 (1.7821)\tPrec@1 86.719 (89.082)\tPrec@5 99.219 (99.392)\n",
      "Epoch: [150][90/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.6073 (1.7769)\tPrec@1 89.062 (89.131)\tPrec@5 99.219 (99.408)\n",
      "Epoch: [150][96/97], lr: 0.01000\tTime 0.288 (0.299)\tData 0.000 (0.020)\tLoss 1.3156 (1.7598)\tPrec@1 92.373 (89.239)\tPrec@5 100.000 (99.412)\n",
      "Test: [0/100]\tTime 0.263 (0.263)\tLoss 10.1325 (10.1325)\tPrec@1 53.000 (53.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.9889 (9.3024)\tPrec@1 66.000 (57.364)\tPrec@5 96.000 (93.727)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.2276 (9.2999)\tPrec@1 65.000 (56.952)\tPrec@5 96.000 (93.476)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.1745 (9.1543)\tPrec@1 61.000 (57.774)\tPrec@5 94.000 (93.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.4987 (9.2256)\tPrec@1 55.000 (57.854)\tPrec@5 95.000 (93.488)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 9.0159 (9.1548)\tPrec@1 60.000 (58.294)\tPrec@5 96.000 (93.725)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.4669 (9.1388)\tPrec@1 66.000 (58.311)\tPrec@5 94.000 (93.852)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 9.2622 (9.1232)\tPrec@1 55.000 (58.380)\tPrec@5 95.000 (93.859)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.8057 (9.0638)\tPrec@1 64.000 (58.691)\tPrec@5 89.000 (94.111)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.0526 (9.1256)\tPrec@1 65.000 (58.352)\tPrec@5 93.000 (93.989)\n",
      "val Results: Prec@1 58.190 Prec@5 93.950 Loss 9.16621\n",
      "val Class Accuracy: [0.970,0.930,0.871,0.582,0.475,0.623,0.609,0.450,0.212,0.097]\n",
      "Best Prec@1: 70.990\n",
      "\n",
      "Epoch: [151][0/97], lr: 0.01000\tTime 0.351 (0.351)\tData 0.196 (0.196)\tLoss 1.2096 (1.2096)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [151][10/97], lr: 0.01000\tTime 0.296 (0.306)\tData 0.000 (0.033)\tLoss 1.8779 (1.5895)\tPrec@1 89.062 (90.625)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [151][20/97], lr: 0.01000\tTime 0.297 (0.303)\tData 0.000 (0.026)\tLoss 1.6956 (1.5784)\tPrec@1 90.625 (90.625)\tPrec@5 98.438 (99.479)\n",
      "Epoch: [151][30/97], lr: 0.01000\tTime 0.297 (0.302)\tData 0.000 (0.023)\tLoss 2.0649 (1.6804)\tPrec@1 87.500 (89.718)\tPrec@5 99.219 (99.446)\n",
      "Epoch: [151][40/97], lr: 0.01000\tTime 0.300 (0.301)\tData 0.000 (0.022)\tLoss 1.9193 (1.7113)\tPrec@1 86.719 (89.444)\tPrec@5 99.219 (99.447)\n",
      "Epoch: [151][50/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 2.0957 (1.7340)\tPrec@1 88.281 (89.323)\tPrec@5 100.000 (99.418)\n",
      "Epoch: [151][60/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 1.7319 (1.7211)\tPrec@1 90.625 (89.524)\tPrec@5 99.219 (99.462)\n",
      "Epoch: [151][70/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.020)\tLoss 2.5082 (1.7353)\tPrec@1 83.594 (89.404)\tPrec@5 100.000 (99.472)\n",
      "Epoch: [151][80/97], lr: 0.01000\tTime 0.298 (0.300)\tData 0.000 (0.019)\tLoss 2.0882 (1.7568)\tPrec@1 87.500 (89.294)\tPrec@5 100.000 (99.489)\n",
      "Epoch: [151][90/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.019)\tLoss 2.6351 (1.7739)\tPrec@1 83.594 (89.234)\tPrec@5 99.219 (99.485)\n",
      "Epoch: [151][96/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 2.0896 (1.7782)\tPrec@1 88.136 (89.199)\tPrec@5 100.000 (99.500)\n",
      "Test: [0/100]\tTime 0.264 (0.264)\tLoss 9.6508 (9.6508)\tPrec@1 54.000 (54.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 8.1466 (8.9393)\tPrec@1 63.000 (58.545)\tPrec@5 94.000 (94.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.1397 (8.7714)\tPrec@1 65.000 (58.524)\tPrec@5 97.000 (94.476)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.6323 (8.7868)\tPrec@1 57.000 (58.484)\tPrec@5 93.000 (94.194)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 9.3500 (8.7895)\tPrec@1 55.000 (58.756)\tPrec@5 93.000 (93.951)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.4109 (8.7465)\tPrec@1 64.000 (58.882)\tPrec@5 91.000 (94.059)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 7.5907 (8.7953)\tPrec@1 64.000 (58.459)\tPrec@5 96.000 (94.131)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.2669 (8.7775)\tPrec@1 59.000 (58.493)\tPrec@5 98.000 (94.296)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.3585 (8.7836)\tPrec@1 57.000 (58.481)\tPrec@5 87.000 (94.407)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.8106 (8.8296)\tPrec@1 63.000 (58.209)\tPrec@5 98.000 (94.440)\n",
      "val Results: Prec@1 58.240 Prec@5 94.420 Loss 8.83922\n",
      "val Class Accuracy: [0.975,0.990,0.757,0.716,0.656,0.274,0.349,0.400,0.540,0.167]\n",
      "Best Prec@1: 70.990\n",
      "\n",
      "Epoch: [152][0/97], lr: 0.01000\tTime 0.369 (0.369)\tData 0.213 (0.213)\tLoss 1.8165 (1.8165)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [152][10/97], lr: 0.01000\tTime 0.296 (0.306)\tData 0.000 (0.035)\tLoss 1.6909 (1.7659)\tPrec@1 91.406 (89.062)\tPrec@5 98.438 (99.645)\n",
      "Epoch: [152][20/97], lr: 0.01000\tTime 0.301 (0.302)\tData 0.000 (0.026)\tLoss 1.7246 (1.8541)\tPrec@1 89.844 (88.728)\tPrec@5 99.219 (99.368)\n",
      "Epoch: [152][30/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.023)\tLoss 0.9733 (1.8219)\tPrec@1 95.312 (88.760)\tPrec@5 100.000 (99.496)\n",
      "Epoch: [152][40/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.022)\tLoss 1.5650 (1.7771)\tPrec@1 92.188 (89.158)\tPrec@5 99.219 (99.466)\n",
      "Epoch: [152][50/97], lr: 0.01000\tTime 0.294 (0.300)\tData 0.000 (0.021)\tLoss 2.0832 (1.7752)\tPrec@1 87.500 (89.246)\tPrec@5 99.219 (99.510)\n",
      "Epoch: [152][60/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 1.0347 (1.7521)\tPrec@1 94.531 (89.306)\tPrec@5 100.000 (99.526)\n",
      "Epoch: [152][70/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 1.9616 (1.7800)\tPrec@1 89.062 (89.162)\tPrec@5 99.219 (99.538)\n",
      "Epoch: [152][80/97], lr: 0.01000\tTime 0.303 (0.300)\tData 0.000 (0.020)\tLoss 1.6217 (1.7803)\tPrec@1 87.500 (89.217)\tPrec@5 98.438 (99.537)\n",
      "Epoch: [152][90/97], lr: 0.01000\tTime 0.304 (0.300)\tData 0.000 (0.019)\tLoss 1.9322 (1.7948)\tPrec@1 90.625 (89.200)\tPrec@5 99.219 (99.519)\n",
      "Epoch: [152][96/97], lr: 0.01000\tTime 0.290 (0.300)\tData 0.000 (0.020)\tLoss 2.0816 (1.8119)\tPrec@1 87.288 (89.070)\tPrec@5 100.000 (99.541)\n",
      "Test: [0/100]\tTime 0.259 (0.259)\tLoss 8.5210 (8.5210)\tPrec@1 62.000 (62.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 7.6019 (7.9879)\tPrec@1 60.000 (63.182)\tPrec@5 90.000 (94.091)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.1106 (7.7767)\tPrec@1 71.000 (64.286)\tPrec@5 95.000 (94.476)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.5188 (7.7182)\tPrec@1 64.000 (64.129)\tPrec@5 94.000 (94.613)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 7.8867 (7.7992)\tPrec@1 65.000 (63.415)\tPrec@5 95.000 (94.634)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.6158 (7.7337)\tPrec@1 67.000 (63.863)\tPrec@5 95.000 (94.922)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 6.3061 (7.7464)\tPrec@1 69.000 (63.721)\tPrec@5 95.000 (94.951)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.3593 (7.7024)\tPrec@1 72.000 (64.169)\tPrec@5 94.000 (95.014)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.1810 (7.6441)\tPrec@1 62.000 (64.432)\tPrec@5 94.000 (95.160)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.7616 (7.7125)\tPrec@1 72.000 (63.934)\tPrec@5 97.000 (95.165)\n",
      "val Results: Prec@1 63.860 Prec@5 95.140 Loss 7.73120\n",
      "val Class Accuracy: [0.894,0.937,0.881,0.705,0.652,0.745,0.312,0.414,0.476,0.370]\n",
      "Best Prec@1: 70.990\n",
      "\n",
      "Epoch: [153][0/97], lr: 0.01000\tTime 0.370 (0.370)\tData 0.225 (0.225)\tLoss 1.1994 (1.1994)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [153][10/97], lr: 0.01000\tTime 0.305 (0.309)\tData 0.000 (0.035)\tLoss 1.7039 (1.6313)\tPrec@1 90.625 (90.199)\tPrec@5 99.219 (99.645)\n",
      "Epoch: [153][20/97], lr: 0.01000\tTime 0.298 (0.305)\tData 0.000 (0.027)\tLoss 2.0861 (1.7461)\tPrec@1 85.938 (89.062)\tPrec@5 99.219 (99.554)\n",
      "Epoch: [153][30/97], lr: 0.01000\tTime 0.308 (0.307)\tData 0.000 (0.024)\tLoss 1.6550 (1.7200)\tPrec@1 89.062 (89.390)\tPrec@5 99.219 (99.622)\n",
      "Epoch: [153][40/97], lr: 0.01000\tTime 0.310 (0.310)\tData 0.000 (0.022)\tLoss 1.2692 (1.7101)\tPrec@1 94.531 (89.520)\tPrec@5 100.000 (99.562)\n",
      "Epoch: [153][50/97], lr: 0.01000\tTime 0.297 (0.309)\tData 0.000 (0.021)\tLoss 1.7607 (1.7098)\tPrec@1 89.062 (89.568)\tPrec@5 100.000 (99.586)\n",
      "Epoch: [153][60/97], lr: 0.01000\tTime 0.301 (0.307)\tData 0.000 (0.020)\tLoss 1.7943 (1.7042)\tPrec@1 89.844 (89.600)\tPrec@5 98.438 (99.552)\n",
      "Epoch: [153][70/97], lr: 0.01000\tTime 0.296 (0.306)\tData 0.000 (0.020)\tLoss 2.1903 (1.7193)\tPrec@1 85.156 (89.569)\tPrec@5 99.219 (99.505)\n",
      "Epoch: [153][80/97], lr: 0.01000\tTime 0.297 (0.305)\tData 0.000 (0.020)\tLoss 1.8621 (1.7532)\tPrec@1 86.719 (89.390)\tPrec@5 100.000 (99.460)\n",
      "Epoch: [153][90/97], lr: 0.01000\tTime 0.295 (0.304)\tData 0.000 (0.019)\tLoss 2.0385 (1.7662)\tPrec@1 86.719 (89.226)\tPrec@5 100.000 (99.459)\n",
      "Epoch: [153][96/97], lr: 0.01000\tTime 0.290 (0.304)\tData 0.000 (0.020)\tLoss 2.0564 (1.7740)\tPrec@1 84.746 (89.126)\tPrec@5 99.153 (99.468)\n",
      "Test: [0/100]\tTime 0.251 (0.251)\tLoss 9.7125 (9.7125)\tPrec@1 56.000 (56.000)\tPrec@5 86.000 (86.000)\n",
      "Test: [10/100]\tTime 0.072 (0.089)\tLoss 7.4474 (8.7298)\tPrec@1 65.000 (59.909)\tPrec@5 89.000 (89.364)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 7.3526 (8.8251)\tPrec@1 63.000 (59.381)\tPrec@5 93.000 (89.333)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 8.6406 (8.7969)\tPrec@1 58.000 (59.516)\tPrec@5 91.000 (89.323)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.7565 (8.7594)\tPrec@1 61.000 (60.000)\tPrec@5 88.000 (89.488)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 8.0540 (8.6629)\tPrec@1 65.000 (60.510)\tPrec@5 92.000 (89.824)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.5934 (8.5732)\tPrec@1 63.000 (60.705)\tPrec@5 94.000 (90.016)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 8.2458 (8.5649)\tPrec@1 59.000 (60.746)\tPrec@5 91.000 (89.901)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.9663 (8.5020)\tPrec@1 60.000 (61.123)\tPrec@5 86.000 (90.037)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.3537 (8.5826)\tPrec@1 57.000 (60.835)\tPrec@5 87.000 (89.835)\n",
      "val Results: Prec@1 60.610 Prec@5 89.770 Loss 8.63231\n",
      "val Class Accuracy: [0.970,0.971,0.764,0.773,0.741,0.533,0.604,0.479,0.055,0.171]\n",
      "Best Prec@1: 70.990\n",
      "\n",
      "Epoch: [154][0/97], lr: 0.01000\tTime 0.382 (0.382)\tData 0.225 (0.225)\tLoss 1.7923 (1.7923)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [154][10/97], lr: 0.01000\tTime 0.294 (0.305)\tData 0.000 (0.036)\tLoss 1.1217 (1.6853)\tPrec@1 92.188 (89.631)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [154][20/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.027)\tLoss 2.5269 (1.7434)\tPrec@1 85.156 (89.062)\tPrec@5 97.656 (99.293)\n",
      "Epoch: [154][30/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.024)\tLoss 2.2075 (1.7828)\tPrec@1 85.938 (88.911)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [154][40/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.022)\tLoss 1.5963 (1.7324)\tPrec@1 89.844 (89.234)\tPrec@5 99.219 (99.390)\n",
      "Epoch: [154][50/97], lr: 0.01000\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 1.6291 (1.7360)\tPrec@1 88.281 (89.308)\tPrec@5 98.438 (99.418)\n",
      "Epoch: [154][60/97], lr: 0.01000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 1.0586 (1.7123)\tPrec@1 92.969 (89.460)\tPrec@5 100.000 (99.475)\n",
      "Epoch: [154][70/97], lr: 0.01000\tTime 0.299 (0.299)\tData 0.000 (0.020)\tLoss 1.8528 (1.6977)\tPrec@1 89.844 (89.635)\tPrec@5 97.656 (99.450)\n",
      "Epoch: [154][80/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 1.0297 (1.7237)\tPrec@1 95.312 (89.477)\tPrec@5 100.000 (99.450)\n",
      "Epoch: [154][90/97], lr: 0.01000\tTime 0.296 (0.300)\tData 0.000 (0.019)\tLoss 1.9089 (1.7373)\tPrec@1 89.062 (89.423)\tPrec@5 99.219 (99.476)\n",
      "Epoch: [154][96/97], lr: 0.01000\tTime 0.289 (0.300)\tData 0.000 (0.020)\tLoss 1.0320 (1.7305)\tPrec@1 95.763 (89.481)\tPrec@5 100.000 (99.484)\n",
      "Test: [0/100]\tTime 0.292 (0.292)\tLoss 6.8224 (6.8224)\tPrec@1 67.000 (67.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.072 (0.093)\tLoss 6.2275 (7.2652)\tPrec@1 69.000 (67.000)\tPrec@5 97.000 (95.545)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 5.5470 (7.0122)\tPrec@1 72.000 (67.571)\tPrec@5 98.000 (95.762)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.3469 (7.0222)\tPrec@1 72.000 (67.516)\tPrec@5 92.000 (95.355)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 7.1049 (7.0824)\tPrec@1 66.000 (67.390)\tPrec@5 96.000 (95.293)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.8524 (7.0207)\tPrec@1 66.000 (67.569)\tPrec@5 97.000 (95.333)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 5.3723 (7.0125)\tPrec@1 75.000 (67.574)\tPrec@5 94.000 (95.262)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 6.6302 (6.9882)\tPrec@1 71.000 (67.761)\tPrec@5 96.000 (95.197)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 7.4066 (6.9676)\tPrec@1 65.000 (67.765)\tPrec@5 92.000 (95.198)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.5495 (7.0394)\tPrec@1 64.000 (67.396)\tPrec@5 96.000 (95.198)\n",
      "val Results: Prec@1 67.370 Prec@5 95.160 Loss 7.06064\n",
      "val Class Accuracy: [0.962,0.947,0.700,0.861,0.749,0.591,0.619,0.650,0.302,0.356]\n",
      "Best Prec@1: 70.990\n",
      "\n",
      "Epoch: [155][0/97], lr: 0.01000\tTime 0.386 (0.386)\tData 0.226 (0.226)\tLoss 1.6228 (1.6228)\tPrec@1 90.625 (90.625)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [155][10/97], lr: 0.01000\tTime 0.296 (0.309)\tData 0.000 (0.036)\tLoss 1.9690 (1.5442)\tPrec@1 88.281 (91.122)\tPrec@5 99.219 (99.645)\n",
      "Epoch: [155][20/97], lr: 0.01000\tTime 0.297 (0.305)\tData 0.000 (0.027)\tLoss 1.1242 (1.5801)\tPrec@1 95.312 (91.109)\tPrec@5 99.219 (99.665)\n",
      "Epoch: [155][30/97], lr: 0.01000\tTime 0.296 (0.302)\tData 0.000 (0.024)\tLoss 2.1739 (1.5856)\tPrec@1 86.719 (90.776)\tPrec@5 98.438 (99.572)\n",
      "Epoch: [155][40/97], lr: 0.01000\tTime 0.297 (0.302)\tData 0.000 (0.022)\tLoss 1.0566 (1.6010)\tPrec@1 93.750 (90.739)\tPrec@5 100.000 (99.657)\n",
      "Epoch: [155][50/97], lr: 0.01000\tTime 0.299 (0.302)\tData 0.000 (0.021)\tLoss 2.5548 (1.6403)\tPrec@1 84.375 (90.441)\tPrec@5 98.438 (99.571)\n",
      "Epoch: [155][60/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.021)\tLoss 1.6877 (1.6478)\tPrec@1 88.281 (90.305)\tPrec@5 99.219 (99.565)\n",
      "Epoch: [155][70/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.020)\tLoss 1.9954 (1.6672)\tPrec@1 89.844 (90.163)\tPrec@5 100.000 (99.549)\n",
      "Epoch: [155][80/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.020)\tLoss 2.1600 (1.7079)\tPrec@1 89.062 (89.873)\tPrec@5 100.000 (99.576)\n",
      "Epoch: [155][90/97], lr: 0.01000\tTime 0.296 (0.301)\tData 0.000 (0.020)\tLoss 1.8021 (1.7283)\tPrec@1 89.844 (89.698)\tPrec@5 100.000 (99.562)\n",
      "Epoch: [155][96/97], lr: 0.01000\tTime 0.291 (0.300)\tData 0.000 (0.020)\tLoss 1.9539 (1.7401)\tPrec@1 88.136 (89.626)\tPrec@5 99.153 (99.541)\n",
      "Test: [0/100]\tTime 0.246 (0.246)\tLoss 9.1303 (9.1303)\tPrec@1 59.000 (59.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 6.6466 (8.4813)\tPrec@1 68.000 (61.364)\tPrec@5 91.000 (91.818)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.3202 (8.3083)\tPrec@1 71.000 (62.333)\tPrec@5 96.000 (92.048)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 7.4206 (8.2406)\tPrec@1 63.000 (62.516)\tPrec@5 91.000 (91.871)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.2382 (8.2838)\tPrec@1 61.000 (62.488)\tPrec@5 91.000 (92.000)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 8.0151 (8.2248)\tPrec@1 60.000 (62.647)\tPrec@5 93.000 (92.078)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 6.2809 (8.1863)\tPrec@1 72.000 (62.705)\tPrec@5 95.000 (92.164)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 7.2786 (8.1369)\tPrec@1 68.000 (63.000)\tPrec@5 96.000 (92.113)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 7.6464 (8.0768)\tPrec@1 65.000 (63.235)\tPrec@5 92.000 (92.272)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.2336 (8.1599)\tPrec@1 59.000 (62.824)\tPrec@5 97.000 (92.121)\n",
      "val Results: Prec@1 62.750 Prec@5 92.030 Loss 8.18603\n",
      "val Class Accuracy: [0.921,0.967,0.743,0.847,0.820,0.644,0.446,0.523,0.218,0.146]\n",
      "Best Prec@1: 70.990\n",
      "\n",
      "Epoch: [156][0/97], lr: 0.01000\tTime 0.421 (0.421)\tData 0.262 (0.262)\tLoss 1.7892 (1.7892)\tPrec@1 90.625 (90.625)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [156][10/97], lr: 0.01000\tTime 0.295 (0.311)\tData 0.000 (0.039)\tLoss 1.6523 (1.7068)\tPrec@1 90.625 (89.986)\tPrec@5 98.438 (99.290)\n",
      "Epoch: [156][20/97], lr: 0.01000\tTime 0.295 (0.305)\tData 0.000 (0.029)\tLoss 1.5725 (1.6871)\tPrec@1 91.406 (90.067)\tPrec@5 100.000 (99.368)\n",
      "Epoch: [156][30/97], lr: 0.01000\tTime 0.305 (0.303)\tData 0.000 (0.025)\tLoss 2.0386 (1.6919)\tPrec@1 86.719 (89.894)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [156][40/97], lr: 0.01000\tTime 0.307 (0.306)\tData 0.000 (0.023)\tLoss 1.6508 (1.6830)\tPrec@1 89.062 (90.015)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [156][50/97], lr: 0.01000\tTime 0.320 (0.310)\tData 0.000 (0.022)\tLoss 1.6435 (1.6990)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [156][60/97], lr: 0.01000\tTime 0.302 (0.311)\tData 0.000 (0.021)\tLoss 1.9092 (1.7050)\tPrec@1 89.844 (89.831)\tPrec@5 99.219 (99.424)\n",
      "Epoch: [156][70/97], lr: 0.01000\tTime 0.302 (0.312)\tData 0.000 (0.020)\tLoss 2.2791 (1.7338)\tPrec@1 86.719 (89.679)\tPrec@5 99.219 (99.439)\n",
      "Epoch: [156][80/97], lr: 0.01000\tTime 0.303 (0.311)\tData 0.000 (0.020)\tLoss 1.8765 (1.7657)\tPrec@1 89.844 (89.487)\tPrec@5 99.219 (99.402)\n",
      "Epoch: [156][90/97], lr: 0.01000\tTime 0.303 (0.310)\tData 0.000 (0.019)\tLoss 2.2315 (1.7522)\tPrec@1 85.156 (89.535)\tPrec@5 99.219 (99.425)\n",
      "Epoch: [156][96/97], lr: 0.01000\tTime 0.295 (0.310)\tData 0.000 (0.020)\tLoss 1.8444 (1.7560)\tPrec@1 88.136 (89.497)\tPrec@5 100.000 (99.436)\n",
      "Test: [0/100]\tTime 0.286 (0.286)\tLoss 6.5736 (6.5736)\tPrec@1 69.000 (69.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 4.8796 (6.0900)\tPrec@1 78.000 (71.545)\tPrec@5 96.000 (97.000)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 4.7709 (5.7807)\tPrec@1 77.000 (72.857)\tPrec@5 97.000 (96.762)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 5.9516 (5.8308)\tPrec@1 71.000 (72.935)\tPrec@5 97.000 (96.839)\n",
      "Test: [40/100]\tTime 0.072 (0.078)\tLoss 5.6261 (5.8549)\tPrec@1 74.000 (72.902)\tPrec@5 95.000 (96.732)\n",
      "Test: [50/100]\tTime 0.072 (0.077)\tLoss 5.3132 (5.7467)\tPrec@1 80.000 (73.588)\tPrec@5 95.000 (96.902)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 5.8990 (5.7661)\tPrec@1 75.000 (73.443)\tPrec@5 97.000 (97.033)\n",
      "Test: [70/100]\tTime 0.072 (0.076)\tLoss 5.5879 (5.7538)\tPrec@1 73.000 (73.507)\tPrec@5 99.000 (97.113)\n",
      "Test: [80/100]\tTime 0.074 (0.075)\tLoss 6.0204 (5.7279)\tPrec@1 73.000 (73.432)\tPrec@5 95.000 (97.160)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.6803 (5.8006)\tPrec@1 73.000 (73.121)\tPrec@5 99.000 (97.088)\n",
      "val Results: Prec@1 73.030 Prec@5 97.040 Loss 5.82450\n",
      "val Class Accuracy: [0.903,0.939,0.648,0.780,0.884,0.695,0.716,0.580,0.472,0.686]\n",
      "Best Prec@1: 73.030\n",
      "\n",
      "Epoch: [157][0/97], lr: 0.01000\tTime 0.487 (0.487)\tData 0.318 (0.318)\tLoss 1.6867 (1.6867)\tPrec@1 88.281 (88.281)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [157][10/97], lr: 0.01000\tTime 0.306 (0.323)\tData 0.000 (0.043)\tLoss 1.0695 (1.5133)\tPrec@1 91.406 (90.767)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [157][20/97], lr: 0.01000\tTime 0.301 (0.313)\tData 0.000 (0.030)\tLoss 1.9759 (1.5598)\tPrec@1 88.281 (90.662)\tPrec@5 98.438 (99.516)\n",
      "Epoch: [157][30/97], lr: 0.01000\tTime 0.300 (0.311)\tData 0.000 (0.026)\tLoss 2.3621 (1.6108)\tPrec@1 86.719 (90.373)\tPrec@5 98.438 (99.395)\n",
      "Epoch: [157][40/97], lr: 0.01000\tTime 0.300 (0.309)\tData 0.000 (0.024)\tLoss 2.2211 (1.6511)\tPrec@1 85.156 (89.996)\tPrec@5 100.000 (99.466)\n",
      "Epoch: [157][50/97], lr: 0.01000\tTime 0.300 (0.307)\tData 0.000 (0.022)\tLoss 1.5411 (1.6630)\tPrec@1 91.406 (89.905)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [157][60/97], lr: 0.01000\tTime 0.298 (0.306)\tData 0.000 (0.021)\tLoss 2.1734 (1.7354)\tPrec@1 82.812 (89.408)\tPrec@5 98.438 (99.424)\n",
      "Epoch: [157][70/97], lr: 0.01000\tTime 0.296 (0.306)\tData 0.000 (0.021)\tLoss 1.4155 (1.7391)\tPrec@1 91.406 (89.481)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [157][80/97], lr: 0.01000\tTime 0.296 (0.305)\tData 0.000 (0.020)\tLoss 1.7705 (1.7462)\tPrec@1 90.625 (89.497)\tPrec@5 99.219 (99.460)\n",
      "Epoch: [157][90/97], lr: 0.01000\tTime 0.294 (0.304)\tData 0.000 (0.020)\tLoss 1.8886 (1.7351)\tPrec@1 89.844 (89.526)\tPrec@5 100.000 (99.485)\n",
      "Epoch: [157][96/97], lr: 0.01000\tTime 0.291 (0.304)\tData 0.000 (0.021)\tLoss 2.1628 (1.7375)\tPrec@1 83.051 (89.457)\tPrec@5 99.153 (99.476)\n",
      "Test: [0/100]\tTime 0.248 (0.248)\tLoss 8.6107 (8.6107)\tPrec@1 61.000 (61.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.072 (0.088)\tLoss 7.2095 (8.3657)\tPrec@1 66.000 (60.182)\tPrec@5 93.000 (93.909)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 6.6319 (8.3900)\tPrec@1 70.000 (60.048)\tPrec@5 96.000 (94.048)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 8.1289 (8.3855)\tPrec@1 61.000 (60.129)\tPrec@5 90.000 (94.032)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 9.4660 (8.3955)\tPrec@1 52.000 (60.293)\tPrec@5 91.000 (93.951)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.5029 (8.2932)\tPrec@1 63.000 (60.765)\tPrec@5 92.000 (94.039)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.9591 (8.3232)\tPrec@1 64.000 (60.443)\tPrec@5 96.000 (93.885)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.6720 (8.2903)\tPrec@1 55.000 (60.493)\tPrec@5 97.000 (94.070)\n",
      "Test: [80/100]\tTime 0.074 (0.075)\tLoss 9.4904 (8.2539)\tPrec@1 54.000 (60.494)\tPrec@5 91.000 (94.062)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.7685 (8.2982)\tPrec@1 66.000 (60.341)\tPrec@5 94.000 (94.088)\n",
      "val Results: Prec@1 60.170 Prec@5 94.010 Loss 8.33582\n",
      "val Class Accuracy: [0.969,0.989,0.668,0.698,0.469,0.570,0.631,0.276,0.344,0.403]\n",
      "Best Prec@1: 73.030\n",
      "\n",
      "Epoch: [158][0/97], lr: 0.01000\tTime 0.476 (0.476)\tData 0.291 (0.291)\tLoss 1.1747 (1.1747)\tPrec@1 92.969 (92.969)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [158][10/97], lr: 0.01000\tTime 0.294 (0.325)\tData 0.000 (0.040)\tLoss 1.8229 (1.6438)\tPrec@1 90.625 (89.489)\tPrec@5 99.219 (99.574)\n",
      "Epoch: [158][20/97], lr: 0.01000\tTime 0.300 (0.316)\tData 0.000 (0.029)\tLoss 2.0920 (1.6839)\tPrec@1 86.719 (89.360)\tPrec@5 100.000 (99.628)\n",
      "Epoch: [158][30/97], lr: 0.01000\tTime 0.305 (0.314)\tData 0.000 (0.025)\tLoss 1.2809 (1.6611)\tPrec@1 92.188 (89.743)\tPrec@5 99.219 (99.572)\n",
      "Epoch: [158][40/97], lr: 0.01000\tTime 0.299 (0.311)\tData 0.000 (0.023)\tLoss 1.5798 (1.7023)\tPrec@1 91.406 (89.482)\tPrec@5 100.000 (99.562)\n",
      "Epoch: [158][50/97], lr: 0.01000\tTime 0.296 (0.309)\tData 0.000 (0.022)\tLoss 1.9778 (1.7011)\tPrec@1 89.062 (89.522)\tPrec@5 100.000 (99.494)\n",
      "Epoch: [158][60/97], lr: 0.01000\tTime 0.300 (0.307)\tData 0.000 (0.021)\tLoss 1.4143 (1.6910)\tPrec@1 91.406 (89.728)\tPrec@5 99.219 (99.513)\n",
      "Epoch: [158][70/97], lr: 0.01000\tTime 0.303 (0.306)\tData 0.000 (0.021)\tLoss 1.6462 (1.6710)\tPrec@1 88.281 (89.888)\tPrec@5 99.219 (99.527)\n",
      "Epoch: [158][80/97], lr: 0.01000\tTime 0.301 (0.306)\tData 0.000 (0.020)\tLoss 2.5248 (1.7045)\tPrec@1 84.375 (89.680)\tPrec@5 99.219 (99.527)\n",
      "Epoch: [158][90/97], lr: 0.01000\tTime 0.298 (0.306)\tData 0.000 (0.020)\tLoss 1.8253 (1.7517)\tPrec@1 90.625 (89.406)\tPrec@5 100.000 (99.536)\n",
      "Epoch: [158][96/97], lr: 0.01000\tTime 0.295 (0.306)\tData 0.000 (0.020)\tLoss 1.7979 (1.7784)\tPrec@1 89.831 (89.247)\tPrec@5 99.153 (99.500)\n",
      "Test: [0/100]\tTime 0.279 (0.279)\tLoss 11.4548 (11.4548)\tPrec@1 44.000 (44.000)\tPrec@5 89.000 (89.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 7.4005 (10.1659)\tPrec@1 66.000 (52.545)\tPrec@5 95.000 (92.818)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 8.5190 (10.0798)\tPrec@1 59.000 (52.857)\tPrec@5 93.000 (92.333)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 10.2729 (10.1089)\tPrec@1 51.000 (52.677)\tPrec@5 89.000 (92.387)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 9.9097 (10.0905)\tPrec@1 58.000 (52.927)\tPrec@5 89.000 (92.537)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 8.8836 (9.9732)\tPrec@1 60.000 (53.588)\tPrec@5 92.000 (92.412)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 9.1048 (9.9264)\tPrec@1 61.000 (53.770)\tPrec@5 90.000 (92.066)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 10.1374 (9.9039)\tPrec@1 52.000 (53.887)\tPrec@5 92.000 (92.056)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.6859 (9.8746)\tPrec@1 52.000 (54.000)\tPrec@5 90.000 (92.086)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.8812 (9.9353)\tPrec@1 51.000 (53.780)\tPrec@5 93.000 (92.044)\n",
      "val Results: Prec@1 53.850 Prec@5 92.010 Loss 9.94400\n",
      "val Class Accuracy: [0.911,0.993,0.533,0.233,0.501,0.646,0.929,0.322,0.125,0.192]\n",
      "Best Prec@1: 73.030\n",
      "\n",
      "Epoch: [159][0/97], lr: 0.01000\tTime 0.337 (0.337)\tData 0.197 (0.197)\tLoss 2.2311 (2.2311)\tPrec@1 85.938 (85.938)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [159][10/97], lr: 0.01000\tTime 0.296 (0.303)\tData 0.000 (0.033)\tLoss 1.8582 (1.9239)\tPrec@1 90.625 (88.849)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [159][20/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.026)\tLoss 1.1360 (1.7315)\tPrec@1 92.969 (89.844)\tPrec@5 100.000 (99.368)\n",
      "Epoch: [159][30/97], lr: 0.01000\tTime 0.295 (0.300)\tData 0.000 (0.023)\tLoss 2.7647 (1.8039)\tPrec@1 83.594 (89.315)\tPrec@5 97.656 (99.320)\n",
      "Epoch: [159][40/97], lr: 0.01000\tTime 0.300 (0.300)\tData 0.000 (0.021)\tLoss 1.2693 (1.7877)\tPrec@1 91.406 (89.291)\tPrec@5 99.219 (99.352)\n",
      "Epoch: [159][50/97], lr: 0.01000\tTime 0.297 (0.300)\tData 0.000 (0.021)\tLoss 1.8633 (1.7782)\tPrec@1 90.625 (89.354)\tPrec@5 99.219 (99.418)\n",
      "Epoch: [159][60/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.020)\tLoss 1.9359 (1.7461)\tPrec@1 88.281 (89.524)\tPrec@5 100.000 (99.475)\n",
      "Epoch: [159][70/97], lr: 0.01000\tTime 0.298 (0.301)\tData 0.000 (0.020)\tLoss 2.3605 (1.7786)\tPrec@1 85.156 (89.283)\tPrec@5 99.219 (99.461)\n",
      "Epoch: [159][80/97], lr: 0.01000\tTime 0.301 (0.302)\tData 0.000 (0.019)\tLoss 2.1892 (1.7921)\tPrec@1 86.719 (89.159)\tPrec@5 98.438 (99.421)\n",
      "Epoch: [159][90/97], lr: 0.01000\tTime 0.295 (0.302)\tData 0.000 (0.019)\tLoss 1.5161 (1.7908)\tPrec@1 89.844 (89.217)\tPrec@5 100.000 (99.399)\n",
      "Epoch: [159][96/97], lr: 0.01000\tTime 0.291 (0.302)\tData 0.000 (0.020)\tLoss 2.0578 (1.7952)\tPrec@1 85.593 (89.191)\tPrec@5 99.153 (99.404)\n",
      "Test: [0/100]\tTime 0.266 (0.266)\tLoss 8.6987 (8.6987)\tPrec@1 59.000 (59.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.072 (0.090)\tLoss 7.9184 (8.8267)\tPrec@1 62.000 (58.545)\tPrec@5 92.000 (95.727)\n",
      "Test: [20/100]\tTime 0.072 (0.082)\tLoss 6.6443 (8.7323)\tPrec@1 66.000 (58.905)\tPrec@5 98.000 (95.952)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.0641 (8.7155)\tPrec@1 59.000 (58.903)\tPrec@5 97.000 (95.903)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 8.6173 (8.7017)\tPrec@1 58.000 (59.293)\tPrec@5 96.000 (95.805)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 8.8083 (8.6360)\tPrec@1 59.000 (59.745)\tPrec@5 96.000 (95.608)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 6.7682 (8.5845)\tPrec@1 68.000 (59.918)\tPrec@5 95.000 (95.525)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 8.7954 (8.5482)\tPrec@1 57.000 (60.127)\tPrec@5 97.000 (95.577)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 8.3162 (8.5237)\tPrec@1 61.000 (60.346)\tPrec@5 98.000 (95.679)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.3516 (8.5809)\tPrec@1 61.000 (60.088)\tPrec@5 99.000 (95.659)\n",
      "val Results: Prec@1 60.070 Prec@5 95.600 Loss 8.59638\n",
      "val Class Accuracy: [0.901,1.000,0.793,0.637,0.556,0.728,0.547,0.492,0.249,0.104]\n",
      "Best Prec@1: 73.030\n",
      "\n",
      "Epoch: [160][0/97], lr: 0.00010\tTime 0.402 (0.402)\tData 0.243 (0.243)\tLoss 4.1629 (4.1629)\tPrec@1 92.188 (92.188)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [160][10/97], lr: 0.00010\tTime 0.302 (0.311)\tData 0.000 (0.037)\tLoss 4.8781 (5.4697)\tPrec@1 90.625 (89.844)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [160][20/97], lr: 0.00010\tTime 0.297 (0.306)\tData 0.000 (0.028)\tLoss 3.1656 (5.2179)\tPrec@1 87.500 (90.141)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [160][30/97], lr: 0.00010\tTime 0.300 (0.304)\tData 0.000 (0.024)\tLoss 2.3892 (4.5400)\tPrec@1 93.750 (90.398)\tPrec@5 99.219 (99.521)\n",
      "Epoch: [160][40/97], lr: 0.00010\tTime 0.299 (0.303)\tData 0.000 (0.023)\tLoss 4.1088 (4.4643)\tPrec@1 90.625 (90.377)\tPrec@5 100.000 (99.524)\n",
      "Epoch: [160][50/97], lr: 0.00010\tTime 0.298 (0.302)\tData 0.000 (0.022)\tLoss 4.1218 (4.3175)\tPrec@1 91.406 (90.365)\tPrec@5 100.000 (99.540)\n",
      "Epoch: [160][60/97], lr: 0.00010\tTime 0.304 (0.302)\tData 0.000 (0.021)\tLoss 5.4060 (4.2479)\tPrec@1 92.969 (90.126)\tPrec@5 100.000 (99.565)\n",
      "Epoch: [160][70/97], lr: 0.00010\tTime 0.296 (0.302)\tData 0.000 (0.020)\tLoss 2.0224 (4.2317)\tPrec@1 89.844 (89.877)\tPrec@5 100.000 (99.527)\n",
      "Epoch: [160][80/97], lr: 0.00010\tTime 0.295 (0.302)\tData 0.000 (0.020)\tLoss 2.6229 (4.1664)\tPrec@1 93.750 (89.969)\tPrec@5 100.000 (99.537)\n",
      "Epoch: [160][90/97], lr: 0.00010\tTime 0.297 (0.302)\tData 0.000 (0.020)\tLoss 7.5411 (4.1394)\tPrec@1 85.938 (89.852)\tPrec@5 99.219 (99.511)\n",
      "Epoch: [160][96/97], lr: 0.00010\tTime 0.290 (0.301)\tData 0.000 (0.020)\tLoss 2.1684 (4.0661)\tPrec@1 91.525 (89.876)\tPrec@5 100.000 (99.532)\n",
      "Test: [0/100]\tTime 0.256 (0.256)\tLoss 3.5724 (3.5724)\tPrec@1 84.000 (84.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 5.1433 (6.8051)\tPrec@1 83.000 (79.000)\tPrec@5 97.000 (98.727)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.8415 (6.6824)\tPrec@1 77.000 (78.429)\tPrec@5 99.000 (98.524)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.1032 (6.7915)\tPrec@1 79.000 (78.323)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.1352 (6.8627)\tPrec@1 81.000 (78.512)\tPrec@5 99.000 (98.122)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.9031 (6.8992)\tPrec@1 76.000 (78.608)\tPrec@5 98.000 (98.255)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7526 (6.9288)\tPrec@1 81.000 (78.344)\tPrec@5 99.000 (98.279)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 5.2611 (6.9156)\tPrec@1 78.000 (78.423)\tPrec@5 97.000 (98.282)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.4640 (6.8514)\tPrec@1 79.000 (78.469)\tPrec@5 99.000 (98.420)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.2379 (6.8875)\tPrec@1 80.000 (78.341)\tPrec@5 100.000 (98.440)\n",
      "val Results: Prec@1 78.310 Prec@5 98.440 Loss 6.90423\n",
      "val Class Accuracy: [0.948,0.958,0.784,0.678,0.776,0.725,0.821,0.676,0.696,0.769]\n",
      "Best Prec@1: 78.310\n",
      "\n",
      "Epoch: [161][0/97], lr: 0.00010\tTime 0.395 (0.395)\tData 0.247 (0.247)\tLoss 1.8535 (1.8535)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [161][10/97], lr: 0.00010\tTime 0.295 (0.309)\tData 0.000 (0.038)\tLoss 2.2168 (2.9995)\tPrec@1 89.062 (88.636)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [161][20/97], lr: 0.00010\tTime 0.294 (0.304)\tData 0.000 (0.028)\tLoss 3.8345 (3.3245)\tPrec@1 89.844 (88.542)\tPrec@5 100.000 (99.702)\n",
      "Epoch: [161][30/97], lr: 0.00010\tTime 0.294 (0.302)\tData 0.000 (0.025)\tLoss 3.6336 (3.2991)\tPrec@1 84.375 (88.760)\tPrec@5 96.875 (99.572)\n",
      "Epoch: [161][40/97], lr: 0.00010\tTime 0.294 (0.301)\tData 0.000 (0.023)\tLoss 5.0128 (3.3700)\tPrec@1 88.281 (88.624)\tPrec@5 99.219 (99.543)\n",
      "Epoch: [161][50/97], lr: 0.00010\tTime 0.295 (0.300)\tData 0.000 (0.022)\tLoss 4.4067 (3.3173)\tPrec@1 90.625 (88.450)\tPrec@5 100.000 (99.510)\n",
      "Epoch: [161][60/97], lr: 0.00010\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 2.8350 (3.2562)\tPrec@1 86.719 (88.512)\tPrec@5 99.219 (99.513)\n",
      "Epoch: [161][70/97], lr: 0.00010\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 6.3079 (3.1448)\tPrec@1 82.031 (88.732)\tPrec@5 96.875 (99.483)\n",
      "Epoch: [161][80/97], lr: 0.00010\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 5.5948 (3.1747)\tPrec@1 87.500 (88.792)\tPrec@5 100.000 (99.489)\n",
      "Epoch: [161][90/97], lr: 0.00010\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 5.3848 (3.2756)\tPrec@1 89.062 (88.633)\tPrec@5 100.000 (99.511)\n",
      "Epoch: [161][96/97], lr: 0.00010\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 2.4887 (3.2774)\tPrec@1 83.898 (88.546)\tPrec@5 99.153 (99.500)\n",
      "Test: [0/100]\tTime 0.273 (0.273)\tLoss 2.9079 (2.9079)\tPrec@1 84.000 (84.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.072 (0.091)\tLoss 4.0639 (5.7648)\tPrec@1 84.000 (80.545)\tPrec@5 98.000 (98.818)\n",
      "Test: [20/100]\tTime 0.072 (0.082)\tLoss 5.2514 (5.6763)\tPrec@1 78.000 (80.238)\tPrec@5 100.000 (98.571)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 4.8658 (5.7516)\tPrec@1 80.000 (80.032)\tPrec@5 97.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 4.0027 (5.8483)\tPrec@1 84.000 (80.049)\tPrec@5 99.000 (98.171)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.0369 (5.8677)\tPrec@1 78.000 (80.196)\tPrec@5 98.000 (98.333)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.2840 (5.8933)\tPrec@1 82.000 (79.967)\tPrec@5 100.000 (98.410)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.8468 (5.8747)\tPrec@1 80.000 (79.972)\tPrec@5 99.000 (98.437)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 5.5358 (5.8176)\tPrec@1 83.000 (80.086)\tPrec@5 99.000 (98.531)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.2058 (5.8445)\tPrec@1 84.000 (79.912)\tPrec@5 100.000 (98.549)\n",
      "val Results: Prec@1 79.840 Prec@5 98.560 Loss 5.85148\n",
      "val Class Accuracy: [0.937,0.940,0.804,0.680,0.801,0.735,0.828,0.710,0.734,0.815]\n",
      "Best Prec@1: 79.840\n",
      "\n",
      "Epoch: [162][0/97], lr: 0.00010\tTime 0.403 (0.403)\tData 0.239 (0.239)\tLoss 6.2005 (6.2005)\tPrec@1 92.969 (92.969)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [162][10/97], lr: 0.00010\tTime 0.297 (0.311)\tData 0.000 (0.037)\tLoss 1.4167 (2.9321)\tPrec@1 87.500 (88.210)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [162][20/97], lr: 0.00010\tTime 0.296 (0.306)\tData 0.000 (0.028)\tLoss 4.7346 (3.0150)\tPrec@1 86.719 (87.537)\tPrec@5 99.219 (99.405)\n",
      "Epoch: [162][30/97], lr: 0.00010\tTime 0.297 (0.304)\tData 0.000 (0.024)\tLoss 2.5486 (3.1826)\tPrec@1 89.844 (87.399)\tPrec@5 99.219 (99.370)\n",
      "Epoch: [162][40/97], lr: 0.00010\tTime 0.297 (0.303)\tData 0.000 (0.023)\tLoss 2.9404 (3.1809)\tPrec@1 90.625 (87.614)\tPrec@5 100.000 (99.371)\n",
      "Epoch: [162][50/97], lr: 0.00010\tTime 0.296 (0.302)\tData 0.000 (0.022)\tLoss 1.4014 (3.1425)\tPrec@1 89.844 (87.990)\tPrec@5 100.000 (99.433)\n",
      "Epoch: [162][60/97], lr: 0.00010\tTime 0.298 (0.301)\tData 0.000 (0.021)\tLoss 1.8437 (3.0548)\tPrec@1 88.281 (87.948)\tPrec@5 99.219 (99.360)\n",
      "Epoch: [162][70/97], lr: 0.00010\tTime 0.296 (0.301)\tData 0.000 (0.020)\tLoss 2.8681 (3.0177)\tPrec@1 89.062 (88.094)\tPrec@5 100.000 (99.417)\n",
      "Epoch: [162][80/97], lr: 0.00010\tTime 0.299 (0.301)\tData 0.000 (0.020)\tLoss 3.0338 (3.0746)\tPrec@1 88.281 (88.069)\tPrec@5 98.438 (99.431)\n",
      "Epoch: [162][90/97], lr: 0.00010\tTime 0.298 (0.301)\tData 0.000 (0.020)\tLoss 3.1902 (3.0376)\tPrec@1 84.375 (88.084)\tPrec@5 100.000 (99.433)\n",
      "Epoch: [162][96/97], lr: 0.00010\tTime 0.290 (0.301)\tData 0.000 (0.020)\tLoss 2.2125 (2.9975)\tPrec@1 86.441 (88.038)\tPrec@5 98.305 (99.436)\n",
      "Test: [0/100]\tTime 0.245 (0.245)\tLoss 2.7435 (2.7435)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 4.2088 (5.8183)\tPrec@1 82.000 (81.818)\tPrec@5 99.000 (99.182)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 5.1284 (5.6909)\tPrec@1 79.000 (81.333)\tPrec@5 99.000 (98.857)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.2047 (5.7508)\tPrec@1 83.000 (81.226)\tPrec@5 97.000 (98.516)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 4.0664 (5.8506)\tPrec@1 84.000 (81.073)\tPrec@5 99.000 (98.366)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.4105 (5.8928)\tPrec@1 78.000 (81.059)\tPrec@5 99.000 (98.529)\n",
      "Test: [60/100]\tTime 0.074 (0.075)\tLoss 7.0119 (5.8892)\tPrec@1 82.000 (81.016)\tPrec@5 99.000 (98.574)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.6820 (5.8601)\tPrec@1 79.000 (80.958)\tPrec@5 99.000 (98.606)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 5.5211 (5.8080)\tPrec@1 85.000 (81.049)\tPrec@5 99.000 (98.704)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.0180 (5.8336)\tPrec@1 84.000 (80.835)\tPrec@5 100.000 (98.714)\n",
      "val Results: Prec@1 80.780 Prec@5 98.710 Loss 5.84132\n",
      "val Class Accuracy: [0.931,0.957,0.802,0.662,0.815,0.755,0.855,0.743,0.760,0.798]\n",
      "Best Prec@1: 80.780\n",
      "\n",
      "Epoch: [163][0/97], lr: 0.00010\tTime 0.560 (0.560)\tData 0.353 (0.353)\tLoss 3.6884 (3.6884)\tPrec@1 84.375 (84.375)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [163][10/97], lr: 0.00010\tTime 0.298 (0.336)\tData 0.000 (0.046)\tLoss 2.6870 (2.9333)\tPrec@1 86.719 (86.364)\tPrec@5 100.000 (99.148)\n",
      "Epoch: [163][20/97], lr: 0.00010\tTime 0.296 (0.321)\tData 0.000 (0.032)\tLoss 5.4353 (2.7606)\tPrec@1 85.938 (87.760)\tPrec@5 98.438 (99.293)\n",
      "Epoch: [163][30/97], lr: 0.00010\tTime 0.295 (0.315)\tData 0.000 (0.027)\tLoss 4.9845 (3.0008)\tPrec@1 93.750 (88.105)\tPrec@5 100.000 (99.345)\n",
      "Epoch: [163][40/97], lr: 0.00010\tTime 0.305 (0.316)\tData 0.000 (0.025)\tLoss 1.9320 (2.9408)\tPrec@1 89.844 (88.377)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [163][50/97], lr: 0.00010\tTime 0.302 (0.314)\tData 0.000 (0.023)\tLoss 1.1500 (2.9181)\tPrec@1 92.969 (88.496)\tPrec@5 99.219 (99.418)\n",
      "Epoch: [163][60/97], lr: 0.00010\tTime 0.300 (0.315)\tData 0.000 (0.022)\tLoss 1.1658 (2.9199)\tPrec@1 89.844 (88.550)\tPrec@5 100.000 (99.462)\n",
      "Epoch: [163][70/97], lr: 0.00010\tTime 0.342 (0.315)\tData 0.000 (0.021)\tLoss 5.7093 (2.9133)\tPrec@1 90.625 (88.765)\tPrec@5 99.219 (99.461)\n",
      "Epoch: [163][80/97], lr: 0.00010\tTime 0.320 (0.317)\tData 0.000 (0.021)\tLoss 1.6664 (2.9605)\tPrec@1 87.500 (88.590)\tPrec@5 99.219 (99.460)\n",
      "Epoch: [163][90/97], lr: 0.00010\tTime 0.298 (0.318)\tData 0.000 (0.020)\tLoss 2.2430 (2.9765)\tPrec@1 91.406 (88.565)\tPrec@5 100.000 (99.502)\n",
      "Epoch: [163][96/97], lr: 0.00010\tTime 0.280 (0.317)\tData 0.000 (0.021)\tLoss 3.7222 (3.0168)\tPrec@1 87.288 (88.530)\tPrec@5 100.000 (99.516)\n",
      "Test: [0/100]\tTime 0.320 (0.320)\tLoss 2.4954 (2.4954)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.095)\tLoss 4.2373 (5.7530)\tPrec@1 83.000 (82.182)\tPrec@5 99.000 (99.000)\n",
      "Test: [20/100]\tTime 0.072 (0.084)\tLoss 5.1842 (5.6365)\tPrec@1 77.000 (81.714)\tPrec@5 99.000 (98.762)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 3.8363 (5.6783)\tPrec@1 83.000 (81.581)\tPrec@5 98.000 (98.516)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.1703 (5.8245)\tPrec@1 85.000 (81.244)\tPrec@5 99.000 (98.415)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 6.5906 (5.8521)\tPrec@1 79.000 (81.431)\tPrec@5 98.000 (98.510)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 7.3734 (5.8709)\tPrec@1 80.000 (81.230)\tPrec@5 99.000 (98.590)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.5517 (5.8514)\tPrec@1 83.000 (81.211)\tPrec@5 99.000 (98.690)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.0918 (5.8125)\tPrec@1 83.000 (81.321)\tPrec@5 99.000 (98.741)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 5.8584 (5.8363)\tPrec@1 84.000 (81.088)\tPrec@5 100.000 (98.736)\n",
      "val Results: Prec@1 81.010 Prec@5 98.770 Loss 5.83577\n",
      "val Class Accuracy: [0.921,0.962,0.810,0.710,0.817,0.735,0.856,0.730,0.782,0.778]\n",
      "Best Prec@1: 81.010\n",
      "\n",
      "Epoch: [164][0/97], lr: 0.00010\tTime 0.856 (0.856)\tData 0.556 (0.556)\tLoss 2.2090 (2.2090)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [164][10/97], lr: 0.00010\tTime 0.316 (0.378)\tData 0.000 (0.061)\tLoss 3.8959 (2.5851)\tPrec@1 91.406 (89.915)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [164][20/97], lr: 0.00010\tTime 0.326 (0.354)\tData 0.000 (0.039)\tLoss 1.7371 (2.9659)\tPrec@1 95.312 (88.802)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [164][30/97], lr: 0.00010\tTime 0.311 (0.343)\tData 0.000 (0.031)\tLoss 3.0346 (2.9637)\tPrec@1 89.844 (88.886)\tPrec@5 99.219 (99.471)\n",
      "Epoch: [164][40/97], lr: 0.00010\tTime 0.316 (0.339)\tData 0.000 (0.028)\tLoss 3.3858 (2.7065)\tPrec@1 84.375 (88.872)\tPrec@5 98.438 (99.447)\n",
      "Epoch: [164][50/97], lr: 0.00010\tTime 0.330 (0.337)\tData 0.000 (0.025)\tLoss 8.1357 (2.8212)\tPrec@1 89.062 (88.771)\tPrec@5 99.219 (99.479)\n",
      "Epoch: [164][60/97], lr: 0.00010\tTime 0.341 (0.336)\tData 0.000 (0.024)\tLoss 1.4586 (2.8403)\tPrec@1 89.844 (88.627)\tPrec@5 99.219 (99.462)\n",
      "Epoch: [164][70/97], lr: 0.00010\tTime 0.322 (0.335)\tData 0.000 (0.022)\tLoss 0.9409 (2.7867)\tPrec@1 92.969 (88.897)\tPrec@5 99.219 (99.472)\n",
      "Epoch: [164][80/97], lr: 0.00010\tTime 0.310 (0.334)\tData 0.000 (0.022)\tLoss 1.9290 (2.7165)\tPrec@1 88.281 (89.005)\tPrec@5 100.000 (99.489)\n",
      "Epoch: [164][90/97], lr: 0.00010\tTime 0.316 (0.333)\tData 0.000 (0.021)\tLoss 5.1532 (2.7833)\tPrec@1 85.156 (88.977)\tPrec@5 100.000 (99.493)\n",
      "Epoch: [164][96/97], lr: 0.00010\tTime 0.318 (0.333)\tData 0.001 (0.021)\tLoss 2.6118 (2.7975)\tPrec@1 87.288 (88.836)\tPrec@5 100.000 (99.460)\n",
      "Test: [0/100]\tTime 0.537 (0.537)\tLoss 2.7248 (2.7248)\tPrec@1 86.000 (86.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.116)\tLoss 4.3257 (5.8244)\tPrec@1 83.000 (82.273)\tPrec@5 100.000 (99.091)\n",
      "Test: [20/100]\tTime 0.073 (0.096)\tLoss 5.2810 (5.7175)\tPrec@1 80.000 (82.095)\tPrec@5 99.000 (98.810)\n",
      "Test: [30/100]\tTime 0.073 (0.089)\tLoss 3.6053 (5.7645)\tPrec@1 83.000 (81.742)\tPrec@5 99.000 (98.581)\n",
      "Test: [40/100]\tTime 0.074 (0.085)\tLoss 4.2504 (5.9187)\tPrec@1 86.000 (81.634)\tPrec@5 99.000 (98.463)\n",
      "Test: [50/100]\tTime 0.073 (0.083)\tLoss 7.0145 (5.9408)\tPrec@1 78.000 (81.824)\tPrec@5 99.000 (98.529)\n",
      "Test: [60/100]\tTime 0.074 (0.081)\tLoss 7.2243 (5.9554)\tPrec@1 82.000 (81.705)\tPrec@5 99.000 (98.623)\n",
      "Test: [70/100]\tTime 0.074 (0.080)\tLoss 3.2015 (5.9283)\tPrec@1 83.000 (81.577)\tPrec@5 98.000 (98.704)\n",
      "Test: [80/100]\tTime 0.073 (0.080)\tLoss 6.3858 (5.8801)\tPrec@1 82.000 (81.667)\tPrec@5 99.000 (98.778)\n",
      "Test: [90/100]\tTime 0.075 (0.079)\tLoss 5.8121 (5.8969)\tPrec@1 86.000 (81.484)\tPrec@5 100.000 (98.758)\n",
      "val Results: Prec@1 81.440 Prec@5 98.790 Loss 5.89398\n",
      "val Class Accuracy: [0.923,0.967,0.811,0.716,0.814,0.745,0.861,0.752,0.784,0.771]\n",
      "Best Prec@1: 81.440\n",
      "\n",
      "Epoch: [165][0/97], lr: 0.00010\tTime 0.655 (0.655)\tData 0.422 (0.422)\tLoss 1.9269 (1.9269)\tPrec@1 91.406 (91.406)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [165][10/97], lr: 0.00010\tTime 0.319 (0.372)\tData 0.001 (0.051)\tLoss 1.5004 (2.2657)\tPrec@1 87.500 (87.926)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [165][20/97], lr: 0.00010\tTime 0.319 (0.346)\tData 0.001 (0.034)\tLoss 2.6284 (2.4973)\tPrec@1 90.625 (88.132)\tPrec@5 99.219 (99.442)\n",
      "Epoch: [165][30/97], lr: 0.00010\tTime 0.330 (0.345)\tData 0.000 (0.028)\tLoss 1.4304 (2.4543)\tPrec@1 89.062 (88.634)\tPrec@5 100.000 (99.420)\n",
      "Epoch: [165][40/97], lr: 0.00010\tTime 0.330 (0.345)\tData 0.000 (0.025)\tLoss 5.5643 (2.5732)\tPrec@1 85.938 (88.338)\tPrec@5 100.000 (99.486)\n",
      "Epoch: [165][50/97], lr: 0.00010\tTime 0.334 (0.345)\tData 0.000 (0.023)\tLoss 3.7211 (2.5939)\tPrec@1 88.281 (88.649)\tPrec@5 96.875 (99.418)\n",
      "Epoch: [165][60/97], lr: 0.00010\tTime 0.318 (0.343)\tData 0.000 (0.022)\tLoss 2.5516 (2.6714)\tPrec@1 86.719 (88.550)\tPrec@5 99.219 (99.372)\n",
      "Epoch: [165][70/97], lr: 0.00010\tTime 0.333 (0.339)\tData 0.000 (0.021)\tLoss 4.1240 (2.7335)\tPrec@1 84.375 (88.501)\tPrec@5 100.000 (99.417)\n",
      "Epoch: [165][80/97], lr: 0.00010\tTime 0.342 (0.340)\tData 0.000 (0.020)\tLoss 2.4795 (2.7334)\tPrec@1 89.844 (88.609)\tPrec@5 99.219 (99.383)\n",
      "Epoch: [165][90/97], lr: 0.00010\tTime 0.326 (0.340)\tData 0.000 (0.019)\tLoss 4.5233 (2.6969)\tPrec@1 90.625 (88.693)\tPrec@5 100.000 (99.433)\n",
      "Epoch: [165][96/97], lr: 0.00010\tTime 0.308 (0.339)\tData 0.000 (0.020)\tLoss 1.4603 (2.6636)\tPrec@1 93.220 (88.828)\tPrec@5 100.000 (99.468)\n",
      "Test: [0/100]\tTime 0.502 (0.502)\tLoss 2.6874 (2.6874)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.113)\tLoss 4.4843 (5.8195)\tPrec@1 81.000 (81.455)\tPrec@5 100.000 (99.091)\n",
      "Test: [20/100]\tTime 0.073 (0.094)\tLoss 5.1937 (5.6731)\tPrec@1 78.000 (81.524)\tPrec@5 99.000 (98.762)\n",
      "Test: [30/100]\tTime 0.073 (0.088)\tLoss 3.8771 (5.7846)\tPrec@1 85.000 (81.484)\tPrec@5 99.000 (98.548)\n",
      "Test: [40/100]\tTime 0.074 (0.084)\tLoss 4.4693 (5.9367)\tPrec@1 84.000 (81.463)\tPrec@5 99.000 (98.463)\n",
      "Test: [50/100]\tTime 0.074 (0.082)\tLoss 6.8185 (5.9623)\tPrec@1 78.000 (81.529)\tPrec@5 99.000 (98.549)\n",
      "Test: [60/100]\tTime 0.073 (0.081)\tLoss 7.6553 (5.9584)\tPrec@1 80.000 (81.492)\tPrec@5 99.000 (98.623)\n",
      "Test: [70/100]\tTime 0.075 (0.080)\tLoss 3.5052 (5.9551)\tPrec@1 83.000 (81.451)\tPrec@5 99.000 (98.704)\n",
      "Test: [80/100]\tTime 0.075 (0.079)\tLoss 6.5350 (5.9164)\tPrec@1 83.000 (81.556)\tPrec@5 98.000 (98.741)\n",
      "Test: [90/100]\tTime 0.073 (0.079)\tLoss 6.0371 (5.9329)\tPrec@1 85.000 (81.418)\tPrec@5 100.000 (98.725)\n",
      "val Results: Prec@1 81.320 Prec@5 98.750 Loss 5.93743\n",
      "val Class Accuracy: [0.928,0.966,0.813,0.706,0.812,0.753,0.869,0.735,0.800,0.750]\n",
      "Best Prec@1: 81.440\n",
      "\n",
      "Epoch: [166][0/97], lr: 0.00010\tTime 0.669 (0.669)\tData 0.468 (0.468)\tLoss 1.9611 (1.9611)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [166][10/97], lr: 0.00010\tTime 0.320 (0.359)\tData 0.000 (0.055)\tLoss 0.9494 (2.3339)\tPrec@1 96.094 (90.412)\tPrec@5 99.219 (99.787)\n",
      "Epoch: [166][20/97], lr: 0.00010\tTime 0.308 (0.337)\tData 0.000 (0.036)\tLoss 2.3945 (2.3441)\tPrec@1 92.969 (89.621)\tPrec@5 100.000 (99.740)\n",
      "Epoch: [166][30/97], lr: 0.00010\tTime 0.308 (0.332)\tData 0.000 (0.030)\tLoss 2.3698 (2.4620)\tPrec@1 86.719 (89.390)\tPrec@5 100.000 (99.597)\n",
      "Epoch: [166][40/97], lr: 0.00010\tTime 0.338 (0.332)\tData 0.000 (0.026)\tLoss 2.6405 (2.4691)\tPrec@1 89.844 (89.348)\tPrec@5 99.219 (99.486)\n",
      "Epoch: [166][50/97], lr: 0.00010\tTime 0.348 (0.334)\tData 0.001 (0.024)\tLoss 3.5151 (2.3923)\tPrec@1 88.281 (89.645)\tPrec@5 99.219 (99.525)\n",
      "Epoch: [166][60/97], lr: 0.00010\tTime 0.332 (0.337)\tData 0.000 (0.023)\tLoss 2.7599 (2.5578)\tPrec@1 89.844 (89.485)\tPrec@5 98.438 (99.488)\n",
      "Epoch: [166][70/97], lr: 0.00010\tTime 0.313 (0.336)\tData 0.001 (0.022)\tLoss 1.1243 (2.5507)\tPrec@1 91.406 (89.437)\tPrec@5 100.000 (99.450)\n",
      "Epoch: [166][80/97], lr: 0.00010\tTime 0.315 (0.334)\tData 0.000 (0.021)\tLoss 2.8069 (2.5490)\tPrec@1 89.844 (89.361)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [166][90/97], lr: 0.00010\tTime 0.319 (0.334)\tData 0.000 (0.020)\tLoss 1.9458 (2.5888)\tPrec@1 87.500 (89.251)\tPrec@5 99.219 (99.476)\n",
      "Epoch: [166][96/97], lr: 0.00010\tTime 0.281 (0.332)\tData 0.000 (0.021)\tLoss 2.8284 (2.6266)\tPrec@1 91.525 (89.271)\tPrec@5 100.000 (99.484)\n",
      "Test: [0/100]\tTime 0.311 (0.311)\tLoss 2.5057 (2.5057)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.072 (0.094)\tLoss 4.4377 (5.7816)\tPrec@1 84.000 (81.909)\tPrec@5 100.000 (98.909)\n",
      "Test: [20/100]\tTime 0.072 (0.084)\tLoss 5.0739 (5.6507)\tPrec@1 80.000 (81.857)\tPrec@5 99.000 (98.667)\n",
      "Test: [30/100]\tTime 0.072 (0.080)\tLoss 3.8862 (5.7644)\tPrec@1 85.000 (81.839)\tPrec@5 98.000 (98.452)\n",
      "Test: [40/100]\tTime 0.072 (0.078)\tLoss 4.4547 (5.9089)\tPrec@1 84.000 (81.732)\tPrec@5 99.000 (98.366)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.7596 (5.9211)\tPrec@1 80.000 (81.902)\tPrec@5 99.000 (98.471)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.5830 (5.9059)\tPrec@1 80.000 (81.754)\tPrec@5 99.000 (98.574)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.1101 (5.9002)\tPrec@1 83.000 (81.718)\tPrec@5 98.000 (98.634)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.6178 (5.8666)\tPrec@1 84.000 (81.765)\tPrec@5 98.000 (98.667)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.9399 (5.8875)\tPrec@1 85.000 (81.582)\tPrec@5 100.000 (98.648)\n",
      "val Results: Prec@1 81.520 Prec@5 98.650 Loss 5.89173\n",
      "val Class Accuracy: [0.923,0.966,0.826,0.714,0.821,0.766,0.840,0.754,0.796,0.746]\n",
      "Best Prec@1: 81.520\n",
      "\n",
      "Epoch: [167][0/97], lr: 0.00010\tTime 0.363 (0.363)\tData 0.228 (0.228)\tLoss 1.6661 (1.6661)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [167][10/97], lr: 0.00010\tTime 0.296 (0.309)\tData 0.000 (0.036)\tLoss 1.9476 (2.3180)\tPrec@1 90.625 (89.702)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [167][20/97], lr: 0.00010\tTime 0.296 (0.303)\tData 0.000 (0.027)\tLoss 3.5774 (2.4514)\tPrec@1 93.750 (89.881)\tPrec@5 99.219 (99.554)\n",
      "Epoch: [167][30/97], lr: 0.00010\tTime 0.298 (0.302)\tData 0.000 (0.024)\tLoss 1.7098 (2.2745)\tPrec@1 88.281 (90.096)\tPrec@5 99.219 (99.546)\n",
      "Epoch: [167][40/97], lr: 0.00010\tTime 0.315 (0.303)\tData 0.000 (0.022)\tLoss 1.9553 (2.4489)\tPrec@1 89.844 (89.634)\tPrec@5 99.219 (99.466)\n",
      "Epoch: [167][50/97], lr: 0.00010\tTime 0.329 (0.310)\tData 0.001 (0.021)\tLoss 3.3738 (2.6116)\tPrec@1 85.938 (89.507)\tPrec@5 100.000 (99.387)\n",
      "Epoch: [167][60/97], lr: 0.00010\tTime 0.327 (0.316)\tData 0.000 (0.020)\tLoss 1.2107 (2.5105)\tPrec@1 92.188 (89.600)\tPrec@5 100.000 (99.449)\n",
      "Epoch: [167][70/97], lr: 0.00010\tTime 0.304 (0.317)\tData 0.000 (0.019)\tLoss 2.6548 (2.4714)\tPrec@1 88.281 (89.668)\tPrec@5 98.438 (99.417)\n",
      "Epoch: [167][80/97], lr: 0.00010\tTime 0.295 (0.316)\tData 0.000 (0.019)\tLoss 3.5826 (2.5274)\tPrec@1 88.281 (89.535)\tPrec@5 100.000 (99.363)\n",
      "Epoch: [167][90/97], lr: 0.00010\tTime 0.297 (0.314)\tData 0.000 (0.019)\tLoss 2.1072 (2.5232)\tPrec@1 82.031 (89.380)\tPrec@5 99.219 (99.399)\n",
      "Epoch: [167][96/97], lr: 0.00010\tTime 0.290 (0.313)\tData 0.000 (0.020)\tLoss 2.2358 (2.5109)\tPrec@1 86.441 (89.384)\tPrec@5 100.000 (99.412)\n",
      "Test: [0/100]\tTime 0.248 (0.248)\tLoss 2.5772 (2.5772)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.072 (0.089)\tLoss 4.4262 (5.8343)\tPrec@1 83.000 (81.182)\tPrec@5 100.000 (98.818)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.2650 (5.7075)\tPrec@1 79.000 (81.571)\tPrec@5 99.000 (98.571)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 4.0788 (5.7910)\tPrec@1 85.000 (81.484)\tPrec@5 98.000 (98.419)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 4.7400 (5.9454)\tPrec@1 86.000 (81.512)\tPrec@5 99.000 (98.366)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.6208 (5.9569)\tPrec@1 80.000 (81.824)\tPrec@5 99.000 (98.471)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 7.6918 (5.9531)\tPrec@1 81.000 (81.754)\tPrec@5 99.000 (98.541)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.3197 (5.9546)\tPrec@1 84.000 (81.662)\tPrec@5 99.000 (98.620)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.6290 (5.9292)\tPrec@1 83.000 (81.778)\tPrec@5 98.000 (98.654)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.8669 (5.9515)\tPrec@1 86.000 (81.626)\tPrec@5 100.000 (98.637)\n",
      "val Results: Prec@1 81.590 Prec@5 98.640 Loss 5.95770\n",
      "val Class Accuracy: [0.917,0.971,0.823,0.737,0.815,0.739,0.865,0.757,0.790,0.745]\n",
      "Best Prec@1: 81.590\n",
      "\n",
      "Epoch: [168][0/97], lr: 0.00010\tTime 0.352 (0.352)\tData 0.199 (0.199)\tLoss 1.9340 (1.9340)\tPrec@1 92.188 (92.188)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [168][10/97], lr: 0.00010\tTime 0.299 (0.305)\tData 0.000 (0.033)\tLoss 2.3520 (2.0992)\tPrec@1 87.500 (89.631)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [168][20/97], lr: 0.00010\tTime 0.294 (0.301)\tData 0.000 (0.026)\tLoss 2.1581 (2.4134)\tPrec@1 87.500 (88.988)\tPrec@5 100.000 (99.368)\n",
      "Epoch: [168][30/97], lr: 0.00010\tTime 0.295 (0.301)\tData 0.000 (0.023)\tLoss 1.7534 (2.5312)\tPrec@1 95.312 (89.390)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [168][40/97], lr: 0.00010\tTime 0.301 (0.300)\tData 0.000 (0.022)\tLoss 4.4210 (2.6099)\tPrec@1 87.500 (89.158)\tPrec@5 97.656 (99.371)\n",
      "Epoch: [168][50/97], lr: 0.00010\tTime 0.294 (0.299)\tData 0.000 (0.021)\tLoss 5.3660 (2.6654)\tPrec@1 89.844 (89.323)\tPrec@5 100.000 (99.357)\n",
      "Epoch: [168][60/97], lr: 0.00010\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.7326 (2.5867)\tPrec@1 92.188 (89.408)\tPrec@5 99.219 (99.334)\n",
      "Epoch: [168][70/97], lr: 0.00010\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 1.5717 (2.5328)\tPrec@1 90.625 (89.426)\tPrec@5 100.000 (99.373)\n",
      "Epoch: [168][80/97], lr: 0.00010\tTime 0.295 (0.299)\tData 0.000 (0.019)\tLoss 2.2255 (2.5020)\tPrec@1 89.844 (89.410)\tPrec@5 100.000 (99.421)\n",
      "Epoch: [168][90/97], lr: 0.00010\tTime 0.297 (0.299)\tData 0.000 (0.019)\tLoss 2.9855 (2.4862)\tPrec@1 86.719 (89.560)\tPrec@5 100.000 (99.425)\n",
      "Epoch: [168][96/97], lr: 0.00010\tTime 0.293 (0.299)\tData 0.000 (0.020)\tLoss 2.6911 (2.4591)\tPrec@1 93.220 (89.666)\tPrec@5 100.000 (99.420)\n",
      "Test: [0/100]\tTime 0.256 (0.256)\tLoss 2.5448 (2.5448)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 4.3172 (5.7909)\tPrec@1 84.000 (82.273)\tPrec@5 100.000 (98.727)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.2159 (5.7069)\tPrec@1 79.000 (82.000)\tPrec@5 99.000 (98.571)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 4.0851 (5.7576)\tPrec@1 85.000 (82.097)\tPrec@5 98.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 4.7988 (5.9207)\tPrec@1 85.000 (81.976)\tPrec@5 98.000 (98.317)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.3213 (5.9164)\tPrec@1 80.000 (82.137)\tPrec@5 99.000 (98.451)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.4756 (5.8971)\tPrec@1 83.000 (82.115)\tPrec@5 99.000 (98.557)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.2143 (5.9006)\tPrec@1 84.000 (81.915)\tPrec@5 99.000 (98.648)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.8030 (5.8860)\tPrec@1 84.000 (81.963)\tPrec@5 98.000 (98.667)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.7274 (5.9118)\tPrec@1 86.000 (81.791)\tPrec@5 100.000 (98.615)\n",
      "val Results: Prec@1 81.660 Prec@5 98.620 Loss 5.91835\n",
      "val Class Accuracy: [0.927,0.971,0.830,0.717,0.842,0.755,0.847,0.756,0.771,0.750]\n",
      "Best Prec@1: 81.660\n",
      "\n",
      "Epoch: [169][0/97], lr: 0.00010\tTime 0.340 (0.340)\tData 0.195 (0.195)\tLoss 3.0603 (3.0603)\tPrec@1 91.406 (91.406)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [169][10/97], lr: 0.00010\tTime 0.296 (0.306)\tData 0.000 (0.033)\tLoss 2.9279 (2.1972)\tPrec@1 89.844 (89.489)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [169][20/97], lr: 0.00010\tTime 0.295 (0.303)\tData 0.000 (0.026)\tLoss 1.9408 (2.3412)\tPrec@1 92.969 (89.546)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [169][30/97], lr: 0.00010\tTime 0.297 (0.302)\tData 0.000 (0.023)\tLoss 3.1030 (2.2551)\tPrec@1 89.062 (89.793)\tPrec@5 97.656 (99.521)\n",
      "Epoch: [169][40/97], lr: 0.00010\tTime 0.300 (0.302)\tData 0.000 (0.022)\tLoss 7.1168 (2.4146)\tPrec@1 88.281 (89.577)\tPrec@5 100.000 (99.543)\n",
      "Epoch: [169][50/97], lr: 0.00010\tTime 0.321 (0.305)\tData 0.000 (0.021)\tLoss 1.7070 (2.4082)\tPrec@1 91.406 (89.507)\tPrec@5 100.000 (99.571)\n",
      "Epoch: [169][60/97], lr: 0.00010\tTime 0.313 (0.306)\tData 0.000 (0.020)\tLoss 1.6655 (2.3699)\tPrec@1 92.188 (89.985)\tPrec@5 100.000 (99.577)\n",
      "Epoch: [169][70/97], lr: 0.00010\tTime 0.316 (0.308)\tData 0.000 (0.019)\tLoss 2.4256 (2.3529)\tPrec@1 91.406 (90.064)\tPrec@5 100.000 (99.604)\n",
      "Epoch: [169][80/97], lr: 0.00010\tTime 0.309 (0.310)\tData 0.000 (0.019)\tLoss 2.0648 (2.4085)\tPrec@1 90.625 (89.940)\tPrec@5 100.000 (99.624)\n",
      "Epoch: [169][90/97], lr: 0.00010\tTime 0.328 (0.312)\tData 0.000 (0.019)\tLoss 1.1685 (2.3771)\tPrec@1 91.406 (90.007)\tPrec@5 99.219 (99.596)\n",
      "Epoch: [169][96/97], lr: 0.00010\tTime 0.327 (0.313)\tData 0.000 (0.019)\tLoss 3.5530 (2.4138)\tPrec@1 88.136 (90.021)\tPrec@5 99.153 (99.565)\n",
      "Test: [0/100]\tTime 0.397 (0.397)\tLoss 2.4646 (2.4646)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.103)\tLoss 4.9876 (6.2056)\tPrec@1 83.000 (82.091)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.072 (0.089)\tLoss 5.4977 (6.0340)\tPrec@1 79.000 (81.857)\tPrec@5 99.000 (98.476)\n",
      "Test: [30/100]\tTime 0.073 (0.084)\tLoss 3.9738 (6.1056)\tPrec@1 87.000 (81.968)\tPrec@5 99.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.081)\tLoss 5.3708 (6.3025)\tPrec@1 84.000 (81.805)\tPrec@5 99.000 (98.341)\n",
      "Test: [50/100]\tTime 0.073 (0.080)\tLoss 6.7714 (6.3276)\tPrec@1 80.000 (82.059)\tPrec@5 99.000 (98.471)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 7.6914 (6.2943)\tPrec@1 82.000 (82.016)\tPrec@5 99.000 (98.607)\n",
      "Test: [70/100]\tTime 0.073 (0.078)\tLoss 3.7310 (6.3117)\tPrec@1 85.000 (81.958)\tPrec@5 99.000 (98.690)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 7.6788 (6.3111)\tPrec@1 82.000 (82.000)\tPrec@5 98.000 (98.716)\n",
      "Test: [90/100]\tTime 0.073 (0.077)\tLoss 5.8706 (6.3440)\tPrec@1 88.000 (81.835)\tPrec@5 100.000 (98.681)\n",
      "val Results: Prec@1 81.750 Prec@5 98.700 Loss 6.35173\n",
      "val Class Accuracy: [0.924,0.972,0.841,0.720,0.824,0.762,0.845,0.775,0.798,0.714]\n",
      "Best Prec@1: 81.750\n",
      "\n",
      "Epoch: [170][0/97], lr: 0.00010\tTime 0.925 (0.925)\tData 0.581 (0.581)\tLoss 1.1422 (1.1422)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [170][10/97], lr: 0.00010\tTime 0.328 (0.391)\tData 0.000 (0.061)\tLoss 0.9726 (2.3562)\tPrec@1 95.312 (91.690)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [170][20/97], lr: 0.00010\tTime 0.318 (0.360)\tData 0.000 (0.039)\tLoss 3.9402 (2.2824)\tPrec@1 88.281 (90.923)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [170][30/97], lr: 0.00010\tTime 0.331 (0.351)\tData 0.000 (0.032)\tLoss 1.7157 (2.1698)\tPrec@1 93.750 (91.079)\tPrec@5 100.000 (99.546)\n",
      "Epoch: [170][40/97], lr: 0.00010\tTime 0.322 (0.347)\tData 0.001 (0.028)\tLoss 2.3805 (2.3439)\tPrec@1 89.062 (90.415)\tPrec@5 100.000 (99.562)\n",
      "Epoch: [170][50/97], lr: 0.00010\tTime 0.331 (0.340)\tData 0.000 (0.025)\tLoss 1.3925 (2.3740)\tPrec@1 92.188 (90.349)\tPrec@5 99.219 (99.510)\n",
      "Epoch: [170][60/97], lr: 0.00010\tTime 0.295 (0.333)\tData 0.000 (0.024)\tLoss 3.3444 (2.4727)\tPrec@1 92.188 (90.241)\tPrec@5 100.000 (99.488)\n",
      "Epoch: [170][70/97], lr: 0.00010\tTime 0.295 (0.328)\tData 0.000 (0.023)\tLoss 1.1512 (2.4559)\tPrec@1 91.406 (90.097)\tPrec@5 100.000 (99.505)\n",
      "Epoch: [170][80/97], lr: 0.00010\tTime 0.304 (0.325)\tData 0.001 (0.022)\tLoss 4.5124 (2.4670)\tPrec@1 89.844 (90.143)\tPrec@5 100.000 (99.489)\n",
      "Epoch: [170][90/97], lr: 0.00010\tTime 0.347 (0.326)\tData 0.000 (0.022)\tLoss 1.3575 (2.3863)\tPrec@1 92.969 (90.264)\tPrec@5 100.000 (99.528)\n",
      "Epoch: [170][96/97], lr: 0.00010\tTime 0.314 (0.326)\tData 0.000 (0.022)\tLoss 3.1776 (2.3628)\tPrec@1 90.678 (90.222)\tPrec@5 100.000 (99.541)\n",
      "Test: [0/100]\tTime 0.446 (0.446)\tLoss 2.6512 (2.6512)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.075 (0.108)\tLoss 4.7174 (5.7362)\tPrec@1 83.000 (82.364)\tPrec@5 99.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.092)\tLoss 5.3642 (5.6498)\tPrec@1 79.000 (82.095)\tPrec@5 99.000 (98.524)\n",
      "Test: [30/100]\tTime 0.073 (0.086)\tLoss 3.8652 (5.6852)\tPrec@1 85.000 (82.226)\tPrec@5 98.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.083)\tLoss 4.7938 (5.8529)\tPrec@1 87.000 (82.146)\tPrec@5 98.000 (98.268)\n",
      "Test: [50/100]\tTime 0.073 (0.081)\tLoss 6.4646 (5.8568)\tPrec@1 82.000 (82.373)\tPrec@5 99.000 (98.333)\n",
      "Test: [60/100]\tTime 0.073 (0.080)\tLoss 7.3738 (5.8357)\tPrec@1 81.000 (82.197)\tPrec@5 99.000 (98.459)\n",
      "Test: [70/100]\tTime 0.073 (0.079)\tLoss 3.2219 (5.8272)\tPrec@1 85.000 (82.211)\tPrec@5 98.000 (98.549)\n",
      "Test: [80/100]\tTime 0.074 (0.078)\tLoss 6.7918 (5.8059)\tPrec@1 84.000 (82.309)\tPrec@5 97.000 (98.593)\n",
      "Test: [90/100]\tTime 0.073 (0.078)\tLoss 5.3598 (5.8305)\tPrec@1 88.000 (82.066)\tPrec@5 100.000 (98.571)\n",
      "val Results: Prec@1 81.850 Prec@5 98.590 Loss 5.83640\n",
      "val Class Accuracy: [0.923,0.970,0.829,0.704,0.834,0.769,0.865,0.757,0.776,0.758]\n",
      "Best Prec@1: 81.850\n",
      "\n",
      "Epoch: [171][0/97], lr: 0.00010\tTime 0.385 (0.385)\tData 0.223 (0.223)\tLoss 0.8952 (0.8952)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [171][10/97], lr: 0.00010\tTime 0.298 (0.309)\tData 0.000 (0.034)\tLoss 1.9001 (2.3496)\tPrec@1 92.188 (89.631)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [171][20/97], lr: 0.00010\tTime 0.299 (0.305)\tData 0.000 (0.026)\tLoss 0.7366 (2.3090)\tPrec@1 94.531 (89.807)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [171][30/97], lr: 0.00010\tTime 0.297 (0.303)\tData 0.000 (0.023)\tLoss 1.5482 (2.4388)\tPrec@1 89.844 (90.247)\tPrec@5 100.000 (99.471)\n",
      "Epoch: [171][40/97], lr: 0.00010\tTime 0.298 (0.302)\tData 0.000 (0.022)\tLoss 1.9236 (2.4433)\tPrec@1 93.750 (90.091)\tPrec@5 100.000 (99.486)\n",
      "Epoch: [171][50/97], lr: 0.00010\tTime 0.297 (0.301)\tData 0.000 (0.021)\tLoss 2.7739 (2.5040)\tPrec@1 93.750 (89.982)\tPrec@5 99.219 (99.449)\n",
      "Epoch: [171][60/97], lr: 0.00010\tTime 0.298 (0.301)\tData 0.000 (0.021)\tLoss 2.4211 (2.4246)\tPrec@1 89.844 (89.805)\tPrec@5 99.219 (99.462)\n",
      "Epoch: [171][70/97], lr: 0.00010\tTime 0.308 (0.301)\tData 0.000 (0.020)\tLoss 2.1904 (2.4368)\tPrec@1 88.281 (89.745)\tPrec@5 98.438 (99.472)\n",
      "Epoch: [171][80/97], lr: 0.00010\tTime 0.297 (0.301)\tData 0.000 (0.020)\tLoss 1.4744 (2.4445)\tPrec@1 89.844 (89.786)\tPrec@5 98.438 (99.421)\n",
      "Epoch: [171][90/97], lr: 0.00010\tTime 0.303 (0.301)\tData 0.000 (0.019)\tLoss 2.4762 (2.4607)\tPrec@1 90.625 (89.672)\tPrec@5 100.000 (99.433)\n",
      "Epoch: [171][96/97], lr: 0.00010\tTime 0.310 (0.301)\tData 0.000 (0.020)\tLoss 3.6159 (2.4400)\tPrec@1 93.220 (89.852)\tPrec@5 99.153 (99.444)\n",
      "Test: [0/100]\tTime 0.277 (0.277)\tLoss 2.7649 (2.7649)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 4.8557 (5.9772)\tPrec@1 83.000 (82.091)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.3103 (5.8924)\tPrec@1 80.000 (82.000)\tPrec@5 99.000 (98.429)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 4.3749 (5.9449)\tPrec@1 84.000 (82.032)\tPrec@5 98.000 (98.323)\n",
      "Test: [40/100]\tTime 0.072 (0.078)\tLoss 4.9123 (6.1066)\tPrec@1 85.000 (81.854)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.072 (0.077)\tLoss 6.6217 (6.1176)\tPrec@1 81.000 (82.000)\tPrec@5 99.000 (98.333)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.5598 (6.0859)\tPrec@1 80.000 (81.934)\tPrec@5 99.000 (98.443)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.3434 (6.0729)\tPrec@1 85.000 (82.014)\tPrec@5 99.000 (98.521)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 7.2148 (6.0550)\tPrec@1 82.000 (82.037)\tPrec@5 97.000 (98.568)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.5211 (6.0850)\tPrec@1 88.000 (81.824)\tPrec@5 100.000 (98.538)\n",
      "val Results: Prec@1 81.670 Prec@5 98.560 Loss 6.09948\n",
      "val Class Accuracy: [0.932,0.971,0.832,0.728,0.820,0.756,0.864,0.760,0.761,0.743]\n",
      "Best Prec@1: 81.850\n",
      "\n",
      "Epoch: [172][0/97], lr: 0.00010\tTime 0.387 (0.387)\tData 0.232 (0.232)\tLoss 3.4662 (3.4662)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [172][10/97], lr: 0.00010\tTime 0.305 (0.308)\tData 0.000 (0.036)\tLoss 2.8697 (2.7747)\tPrec@1 91.406 (90.909)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [172][20/97], lr: 0.00010\tTime 0.294 (0.305)\tData 0.000 (0.027)\tLoss 2.4456 (2.3962)\tPrec@1 86.719 (90.848)\tPrec@5 99.219 (99.516)\n",
      "Epoch: [172][30/97], lr: 0.00010\tTime 0.296 (0.303)\tData 0.000 (0.024)\tLoss 0.9455 (2.2759)\tPrec@1 91.406 (90.776)\tPrec@5 99.219 (99.496)\n",
      "Epoch: [172][40/97], lr: 0.00010\tTime 0.300 (0.302)\tData 0.000 (0.022)\tLoss 1.7327 (2.4210)\tPrec@1 93.750 (90.434)\tPrec@5 99.219 (99.409)\n",
      "Epoch: [172][50/97], lr: 0.00010\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 0.8728 (2.3615)\tPrec@1 91.406 (90.518)\tPrec@5 100.000 (99.464)\n",
      "Epoch: [172][60/97], lr: 0.00010\tTime 0.295 (0.301)\tData 0.000 (0.021)\tLoss 3.6234 (2.2803)\tPrec@1 83.594 (90.458)\tPrec@5 99.219 (99.449)\n",
      "Epoch: [172][70/97], lr: 0.00010\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 1.6624 (2.2612)\tPrec@1 92.188 (90.427)\tPrec@5 100.000 (99.483)\n",
      "Epoch: [172][80/97], lr: 0.00010\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 4.7145 (2.2405)\tPrec@1 85.938 (90.538)\tPrec@5 99.219 (99.431)\n",
      "Epoch: [172][90/97], lr: 0.00010\tTime 0.294 (0.300)\tData 0.000 (0.019)\tLoss 1.2733 (2.2549)\tPrec@1 92.969 (90.582)\tPrec@5 99.219 (99.451)\n",
      "Epoch: [172][96/97], lr: 0.00010\tTime 0.290 (0.300)\tData 0.000 (0.020)\tLoss 2.6153 (2.2947)\tPrec@1 87.288 (90.384)\tPrec@5 99.153 (99.444)\n",
      "Test: [0/100]\tTime 0.262 (0.262)\tLoss 2.1158 (2.1158)\tPrec@1 86.000 (86.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.072 (0.090)\tLoss 4.3314 (5.3864)\tPrec@1 82.000 (82.727)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 5.2472 (5.3790)\tPrec@1 80.000 (82.429)\tPrec@5 99.000 (98.429)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 3.9147 (5.4117)\tPrec@1 85.000 (82.677)\tPrec@5 98.000 (98.419)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 4.4794 (5.5705)\tPrec@1 85.000 (82.512)\tPrec@5 98.000 (98.341)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 6.2316 (5.5573)\tPrec@1 82.000 (82.627)\tPrec@5 99.000 (98.392)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.4423 (5.5435)\tPrec@1 82.000 (82.541)\tPrec@5 99.000 (98.508)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 2.6000 (5.5200)\tPrec@1 85.000 (82.507)\tPrec@5 98.000 (98.592)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.5359 (5.5002)\tPrec@1 83.000 (82.580)\tPrec@5 97.000 (98.642)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.8477 (5.5124)\tPrec@1 87.000 (82.308)\tPrec@5 100.000 (98.615)\n",
      "val Results: Prec@1 82.110 Prec@5 98.630 Loss 5.52479\n",
      "val Class Accuracy: [0.923,0.969,0.832,0.732,0.835,0.742,0.861,0.768,0.776,0.773]\n",
      "Best Prec@1: 82.110\n",
      "\n",
      "Epoch: [173][0/97], lr: 0.00010\tTime 0.376 (0.376)\tData 0.233 (0.233)\tLoss 3.4143 (3.4143)\tPrec@1 86.719 (86.719)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [173][10/97], lr: 0.00010\tTime 0.297 (0.307)\tData 0.000 (0.036)\tLoss 1.8386 (2.1939)\tPrec@1 88.281 (89.347)\tPrec@5 99.219 (99.858)\n",
      "Epoch: [173][20/97], lr: 0.00010\tTime 0.296 (0.302)\tData 0.000 (0.027)\tLoss 1.4258 (2.1506)\tPrec@1 92.969 (89.769)\tPrec@5 99.219 (99.665)\n",
      "Epoch: [173][30/97], lr: 0.00010\tTime 0.296 (0.301)\tData 0.000 (0.024)\tLoss 3.1886 (2.4122)\tPrec@1 92.188 (89.642)\tPrec@5 100.000 (99.446)\n",
      "Epoch: [173][40/97], lr: 0.00010\tTime 0.300 (0.301)\tData 0.000 (0.022)\tLoss 3.4989 (2.3796)\tPrec@1 84.375 (89.215)\tPrec@5 98.438 (99.486)\n",
      "Epoch: [173][50/97], lr: 0.00010\tTime 0.297 (0.301)\tData 0.000 (0.021)\tLoss 0.9928 (2.3771)\tPrec@1 95.312 (89.415)\tPrec@5 99.219 (99.494)\n",
      "Epoch: [173][60/97], lr: 0.00010\tTime 0.297 (0.300)\tData 0.000 (0.021)\tLoss 1.6736 (2.3513)\tPrec@1 89.844 (89.690)\tPrec@5 98.438 (99.462)\n",
      "Epoch: [173][70/97], lr: 0.00010\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 1.6870 (2.4252)\tPrec@1 90.625 (89.745)\tPrec@5 100.000 (99.505)\n",
      "Epoch: [173][80/97], lr: 0.00010\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 4.4177 (2.4463)\tPrec@1 89.844 (89.680)\tPrec@5 100.000 (99.527)\n",
      "Epoch: [173][90/97], lr: 0.00010\tTime 0.309 (0.300)\tData 0.000 (0.020)\tLoss 1.4294 (2.3986)\tPrec@1 89.844 (89.689)\tPrec@5 100.000 (99.502)\n",
      "Epoch: [173][96/97], lr: 0.00010\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 1.6612 (2.3834)\tPrec@1 89.831 (89.658)\tPrec@5 100.000 (99.500)\n",
      "Test: [0/100]\tTime 0.255 (0.255)\tLoss 2.3410 (2.3410)\tPrec@1 86.000 (86.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.072 (0.089)\tLoss 4.8549 (5.6231)\tPrec@1 84.000 (83.091)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.3829 (5.5991)\tPrec@1 80.000 (82.667)\tPrec@5 98.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 3.6577 (5.6123)\tPrec@1 86.000 (82.516)\tPrec@5 98.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 4.5259 (5.7340)\tPrec@1 87.000 (82.488)\tPrec@5 98.000 (98.268)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.7347 (5.7361)\tPrec@1 81.000 (82.686)\tPrec@5 99.000 (98.373)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.9898 (5.6981)\tPrec@1 82.000 (82.574)\tPrec@5 99.000 (98.475)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 2.5906 (5.6835)\tPrec@1 85.000 (82.606)\tPrec@5 98.000 (98.521)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.4673 (5.6360)\tPrec@1 84.000 (82.728)\tPrec@5 97.000 (98.556)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.9967 (5.6484)\tPrec@1 87.000 (82.527)\tPrec@5 100.000 (98.549)\n",
      "val Results: Prec@1 82.410 Prec@5 98.550 Loss 5.66587\n",
      "val Class Accuracy: [0.930,0.966,0.835,0.730,0.835,0.770,0.845,0.787,0.769,0.774]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [174][0/97], lr: 0.00010\tTime 0.375 (0.375)\tData 0.238 (0.238)\tLoss 3.8081 (3.8081)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [174][10/97], lr: 0.00010\tTime 0.295 (0.307)\tData 0.000 (0.037)\tLoss 4.1444 (2.7679)\tPrec@1 90.625 (89.702)\tPrec@5 99.219 (99.503)\n",
      "Epoch: [174][20/97], lr: 0.00010\tTime 0.295 (0.302)\tData 0.000 (0.028)\tLoss 3.7859 (2.8246)\tPrec@1 85.938 (89.286)\tPrec@5 97.656 (99.219)\n",
      "Epoch: [174][30/97], lr: 0.00010\tTime 0.295 (0.301)\tData 0.000 (0.024)\tLoss 1.4490 (2.6662)\tPrec@1 87.500 (89.264)\tPrec@5 100.000 (99.320)\n",
      "Epoch: [174][40/97], lr: 0.00010\tTime 0.288 (0.300)\tData 0.000 (0.023)\tLoss 5.4559 (2.6811)\tPrec@1 85.156 (89.386)\tPrec@5 100.000 (99.371)\n",
      "Epoch: [174][50/97], lr: 0.00010\tTime 0.296 (0.300)\tData 0.000 (0.022)\tLoss 2.3478 (2.6121)\tPrec@1 87.500 (89.292)\tPrec@5 97.656 (99.387)\n",
      "Epoch: [174][60/97], lr: 0.00010\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 3.0193 (2.5274)\tPrec@1 85.156 (89.421)\tPrec@5 99.219 (99.398)\n",
      "Epoch: [174][70/97], lr: 0.00010\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.9828 (2.4443)\tPrec@1 91.406 (89.679)\tPrec@5 99.219 (99.417)\n",
      "Epoch: [174][80/97], lr: 0.00010\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 2.7447 (2.4232)\tPrec@1 89.062 (89.603)\tPrec@5 99.219 (99.402)\n",
      "Epoch: [174][90/97], lr: 0.00010\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.4299 (2.3742)\tPrec@1 92.188 (89.775)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [174][96/97], lr: 0.00010\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 1.9494 (2.3221)\tPrec@1 87.288 (89.779)\tPrec@5 99.153 (99.468)\n",
      "Test: [0/100]\tTime 0.249 (0.249)\tLoss 2.5068 (2.5068)\tPrec@1 86.000 (86.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.072 (0.089)\tLoss 5.1685 (6.0297)\tPrec@1 83.000 (82.091)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.8285 (5.9805)\tPrec@1 80.000 (82.000)\tPrec@5 98.000 (98.333)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 3.9634 (6.0057)\tPrec@1 84.000 (82.065)\tPrec@5 99.000 (98.290)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 5.1357 (6.1839)\tPrec@1 86.000 (81.976)\tPrec@5 98.000 (98.220)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.9201 (6.1859)\tPrec@1 80.000 (82.196)\tPrec@5 99.000 (98.294)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 7.5152 (6.1660)\tPrec@1 82.000 (82.148)\tPrec@5 99.000 (98.393)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.4589 (6.1623)\tPrec@1 86.000 (82.155)\tPrec@5 99.000 (98.451)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.4350 (6.1251)\tPrec@1 82.000 (82.272)\tPrec@5 97.000 (98.494)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.3627 (6.1323)\tPrec@1 86.000 (82.044)\tPrec@5 100.000 (98.495)\n",
      "val Results: Prec@1 81.820 Prec@5 98.500 Loss 6.15620\n",
      "val Class Accuracy: [0.926,0.972,0.831,0.732,0.843,0.750,0.872,0.740,0.779,0.737]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [175][0/97], lr: 0.00010\tTime 0.379 (0.379)\tData 0.222 (0.222)\tLoss 2.8172 (2.8172)\tPrec@1 87.500 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [175][10/97], lr: 0.00010\tTime 0.297 (0.309)\tData 0.000 (0.035)\tLoss 1.4802 (2.3192)\tPrec@1 92.188 (89.560)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [175][20/97], lr: 0.00010\tTime 0.301 (0.304)\tData 0.000 (0.027)\tLoss 4.1673 (2.1942)\tPrec@1 89.844 (90.327)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [175][30/97], lr: 0.00010\tTime 0.297 (0.304)\tData 0.000 (0.024)\tLoss 1.3251 (2.2074)\tPrec@1 90.625 (90.297)\tPrec@5 100.000 (99.546)\n",
      "Epoch: [175][40/97], lr: 0.00010\tTime 0.298 (0.303)\tData 0.000 (0.022)\tLoss 1.8237 (2.1699)\tPrec@1 91.406 (90.091)\tPrec@5 99.219 (99.524)\n",
      "Epoch: [175][50/97], lr: 0.00010\tTime 0.297 (0.302)\tData 0.000 (0.021)\tLoss 2.0624 (2.1603)\tPrec@1 85.938 (90.089)\tPrec@5 99.219 (99.494)\n",
      "Epoch: [175][60/97], lr: 0.00010\tTime 0.296 (0.302)\tData 0.000 (0.021)\tLoss 1.5469 (2.1437)\tPrec@1 89.062 (90.113)\tPrec@5 99.219 (99.411)\n",
      "Epoch: [175][70/97], lr: 0.00010\tTime 0.294 (0.301)\tData 0.000 (0.020)\tLoss 1.5231 (2.1403)\tPrec@1 89.062 (90.251)\tPrec@5 100.000 (99.439)\n",
      "Epoch: [175][80/97], lr: 0.00010\tTime 0.297 (0.301)\tData 0.000 (0.020)\tLoss 0.9379 (2.1213)\tPrec@1 90.625 (90.027)\tPrec@5 100.000 (99.450)\n",
      "Epoch: [175][90/97], lr: 0.00010\tTime 0.300 (0.301)\tData 0.000 (0.020)\tLoss 0.8788 (2.1833)\tPrec@1 91.406 (89.912)\tPrec@5 100.000 (99.493)\n",
      "Epoch: [175][96/97], lr: 0.00010\tTime 0.290 (0.301)\tData 0.000 (0.020)\tLoss 1.3498 (2.1738)\tPrec@1 94.068 (90.013)\tPrec@5 99.153 (99.500)\n",
      "Test: [0/100]\tTime 0.254 (0.254)\tLoss 2.0726 (2.0726)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.072 (0.089)\tLoss 4.7754 (5.5396)\tPrec@1 84.000 (82.727)\tPrec@5 99.000 (98.636)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 5.2162 (5.5349)\tPrec@1 82.000 (82.429)\tPrec@5 98.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 3.9288 (5.5242)\tPrec@1 82.000 (82.290)\tPrec@5 99.000 (98.452)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 4.5753 (5.6560)\tPrec@1 86.000 (82.390)\tPrec@5 98.000 (98.366)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 6.3476 (5.6579)\tPrec@1 83.000 (82.686)\tPrec@5 99.000 (98.412)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.4311 (5.6273)\tPrec@1 82.000 (82.541)\tPrec@5 98.000 (98.459)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 2.5116 (5.6001)\tPrec@1 86.000 (82.549)\tPrec@5 98.000 (98.549)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.7179 (5.5758)\tPrec@1 83.000 (82.654)\tPrec@5 97.000 (98.580)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.8757 (5.5813)\tPrec@1 86.000 (82.451)\tPrec@5 100.000 (98.582)\n",
      "val Results: Prec@1 82.340 Prec@5 98.610 Loss 5.59215\n",
      "val Class Accuracy: [0.930,0.965,0.836,0.722,0.820,0.780,0.854,0.783,0.759,0.785]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [176][0/97], lr: 0.00010\tTime 0.380 (0.380)\tData 0.219 (0.219)\tLoss 1.6033 (1.6033)\tPrec@1 87.500 (87.500)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [176][10/97], lr: 0.00010\tTime 0.298 (0.311)\tData 0.000 (0.035)\tLoss 0.6715 (1.8959)\tPrec@1 95.312 (89.062)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [176][20/97], lr: 0.00010\tTime 0.296 (0.304)\tData 0.000 (0.027)\tLoss 2.8667 (2.0628)\tPrec@1 96.094 (89.881)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [176][30/97], lr: 0.00010\tTime 0.294 (0.302)\tData 0.000 (0.024)\tLoss 3.4584 (2.2329)\tPrec@1 84.375 (89.441)\tPrec@5 99.219 (99.496)\n",
      "Epoch: [176][40/97], lr: 0.00010\tTime 0.298 (0.301)\tData 0.000 (0.022)\tLoss 1.4199 (2.1653)\tPrec@1 89.062 (89.596)\tPrec@5 99.219 (99.543)\n",
      "Epoch: [176][50/97], lr: 0.00010\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 1.3432 (2.0970)\tPrec@1 89.062 (89.737)\tPrec@5 100.000 (99.571)\n",
      "Epoch: [176][60/97], lr: 0.00010\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 4.3286 (2.0457)\tPrec@1 83.594 (90.023)\tPrec@5 100.000 (99.629)\n",
      "Epoch: [176][70/97], lr: 0.00010\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 1.3287 (2.0264)\tPrec@1 89.844 (90.240)\tPrec@5 99.219 (99.604)\n",
      "Epoch: [176][80/97], lr: 0.00010\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 1.4379 (2.0465)\tPrec@1 89.062 (90.230)\tPrec@5 100.000 (99.556)\n",
      "Epoch: [176][90/97], lr: 0.00010\tTime 0.294 (0.299)\tData 0.000 (0.019)\tLoss 1.4762 (2.0892)\tPrec@1 88.281 (90.136)\tPrec@5 100.000 (99.588)\n",
      "Epoch: [176][96/97], lr: 0.00010\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 2.5094 (2.1172)\tPrec@1 88.983 (90.166)\tPrec@5 100.000 (99.589)\n",
      "Test: [0/100]\tTime 0.257 (0.257)\tLoss 2.1122 (2.1122)\tPrec@1 86.000 (86.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 4.5819 (5.4898)\tPrec@1 84.000 (82.182)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 5.4172 (5.4668)\tPrec@1 79.000 (82.238)\tPrec@5 98.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 3.5976 (5.4302)\tPrec@1 83.000 (82.355)\tPrec@5 98.000 (98.419)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 4.8844 (5.5853)\tPrec@1 84.000 (82.341)\tPrec@5 98.000 (98.390)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 6.2929 (5.5710)\tPrec@1 82.000 (82.490)\tPrec@5 99.000 (98.451)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.2108 (5.5479)\tPrec@1 83.000 (82.426)\tPrec@5 98.000 (98.492)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 2.3279 (5.5276)\tPrec@1 85.000 (82.535)\tPrec@5 98.000 (98.606)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 6.6061 (5.5155)\tPrec@1 81.000 (82.642)\tPrec@5 97.000 (98.642)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 4.9273 (5.5259)\tPrec@1 86.000 (82.396)\tPrec@5 100.000 (98.604)\n",
      "val Results: Prec@1 82.260 Prec@5 98.620 Loss 5.54410\n",
      "val Class Accuracy: [0.933,0.968,0.826,0.753,0.833,0.736,0.843,0.786,0.766,0.782]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [177][0/97], lr: 0.00010\tTime 0.364 (0.364)\tData 0.217 (0.217)\tLoss 1.6494 (1.6494)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [177][10/97], lr: 0.00010\tTime 0.297 (0.307)\tData 0.000 (0.035)\tLoss 2.1513 (1.9618)\tPrec@1 92.188 (90.625)\tPrec@5 99.219 (99.716)\n",
      "Epoch: [177][20/97], lr: 0.00010\tTime 0.296 (0.303)\tData 0.000 (0.027)\tLoss 0.9723 (1.8230)\tPrec@1 92.188 (90.290)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [177][30/97], lr: 0.00010\tTime 0.297 (0.302)\tData 0.000 (0.024)\tLoss 1.6272 (1.9027)\tPrec@1 86.719 (89.793)\tPrec@5 99.219 (99.546)\n",
      "Epoch: [177][40/97], lr: 0.00010\tTime 0.299 (0.301)\tData 0.000 (0.022)\tLoss 1.3062 (1.8972)\tPrec@1 92.969 (90.244)\tPrec@5 100.000 (99.486)\n",
      "Epoch: [177][50/97], lr: 0.00010\tTime 0.297 (0.301)\tData 0.000 (0.021)\tLoss 0.3460 (1.9405)\tPrec@1 97.656 (90.411)\tPrec@5 100.000 (99.510)\n",
      "Epoch: [177][60/97], lr: 0.00010\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 1.3088 (1.9963)\tPrec@1 89.062 (90.407)\tPrec@5 99.219 (99.552)\n",
      "Epoch: [177][70/97], lr: 0.00010\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 3.0377 (2.0787)\tPrec@1 92.188 (90.251)\tPrec@5 99.219 (99.505)\n",
      "Epoch: [177][80/97], lr: 0.00010\tTime 0.297 (0.300)\tData 0.000 (0.020)\tLoss 2.8104 (2.1019)\tPrec@1 87.500 (90.355)\tPrec@5 99.219 (99.498)\n",
      "Epoch: [177][90/97], lr: 0.00010\tTime 0.298 (0.300)\tData 0.000 (0.020)\tLoss 5.0649 (2.1331)\tPrec@1 88.281 (90.376)\tPrec@5 97.656 (99.442)\n",
      "Epoch: [177][96/97], lr: 0.00010\tTime 0.288 (0.300)\tData 0.000 (0.020)\tLoss 4.2670 (2.1465)\tPrec@1 86.441 (90.400)\tPrec@5 98.305 (99.444)\n",
      "Test: [0/100]\tTime 0.253 (0.253)\tLoss 2.2392 (2.2392)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 5.1497 (6.0472)\tPrec@1 83.000 (82.273)\tPrec@5 99.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.6098 (5.9682)\tPrec@1 79.000 (82.048)\tPrec@5 97.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 3.9208 (5.9481)\tPrec@1 87.000 (82.194)\tPrec@5 99.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.1457 (6.0912)\tPrec@1 85.000 (82.122)\tPrec@5 98.000 (98.268)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.7828 (6.0832)\tPrec@1 83.000 (82.431)\tPrec@5 99.000 (98.294)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.4653 (6.0432)\tPrec@1 83.000 (82.361)\tPrec@5 98.000 (98.344)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.2662 (6.0402)\tPrec@1 85.000 (82.366)\tPrec@5 99.000 (98.408)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.3430 (6.0204)\tPrec@1 84.000 (82.556)\tPrec@5 97.000 (98.481)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.2880 (6.0283)\tPrec@1 87.000 (82.374)\tPrec@5 100.000 (98.484)\n",
      "val Results: Prec@1 82.230 Prec@5 98.500 Loss 6.06167\n",
      "val Class Accuracy: [0.928,0.970,0.828,0.713,0.846,0.781,0.860,0.779,0.775,0.743]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [178][0/97], lr: 0.00010\tTime 0.370 (0.370)\tData 0.222 (0.222)\tLoss 2.2916 (2.2916)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [178][10/97], lr: 0.00010\tTime 0.295 (0.306)\tData 0.000 (0.035)\tLoss 1.1345 (2.0312)\tPrec@1 92.969 (89.844)\tPrec@5 99.219 (99.645)\n",
      "Epoch: [178][20/97], lr: 0.00010\tTime 0.295 (0.302)\tData 0.000 (0.026)\tLoss 1.4769 (2.2449)\tPrec@1 90.625 (90.402)\tPrec@5 99.219 (99.628)\n",
      "Epoch: [178][30/97], lr: 0.00010\tTime 0.295 (0.301)\tData 0.000 (0.023)\tLoss 1.7220 (2.1112)\tPrec@1 89.062 (90.449)\tPrec@5 99.219 (99.622)\n",
      "Epoch: [178][40/97], lr: 0.00010\tTime 0.295 (0.300)\tData 0.000 (0.022)\tLoss 2.4074 (2.1069)\tPrec@1 86.719 (90.530)\tPrec@5 100.000 (99.676)\n",
      "Epoch: [178][50/97], lr: 0.00010\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 1.4094 (2.1508)\tPrec@1 90.625 (90.564)\tPrec@5 100.000 (99.602)\n",
      "Epoch: [178][60/97], lr: 0.00010\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 3.1825 (2.1383)\tPrec@1 89.062 (90.523)\tPrec@5 100.000 (99.603)\n",
      "Epoch: [178][70/97], lr: 0.00010\tTime 0.313 (0.300)\tData 0.000 (0.020)\tLoss 1.6145 (2.1434)\tPrec@1 92.969 (90.471)\tPrec@5 100.000 (99.582)\n",
      "Epoch: [178][80/97], lr: 0.00010\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 1.5659 (2.1153)\tPrec@1 90.625 (90.461)\tPrec@5 100.000 (99.605)\n",
      "Epoch: [178][90/97], lr: 0.00010\tTime 0.301 (0.299)\tData 0.000 (0.019)\tLoss 2.2699 (2.0991)\tPrec@1 88.281 (90.393)\tPrec@5 100.000 (99.622)\n",
      "Epoch: [178][96/97], lr: 0.00010\tTime 0.287 (0.299)\tData 0.000 (0.020)\tLoss 3.4069 (2.1316)\tPrec@1 89.831 (90.376)\tPrec@5 97.458 (99.581)\n",
      "Test: [0/100]\tTime 0.263 (0.263)\tLoss 2.7406 (2.7406)\tPrec@1 86.000 (86.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.072 (0.090)\tLoss 6.0180 (6.9156)\tPrec@1 82.000 (81.364)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.1389 (6.7582)\tPrec@1 79.000 (81.429)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 4.8131 (6.7483)\tPrec@1 85.000 (81.710)\tPrec@5 99.000 (98.258)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.9250 (6.9279)\tPrec@1 84.000 (81.780)\tPrec@5 98.000 (98.122)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.3888 (6.9658)\tPrec@1 82.000 (82.059)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.6093 (6.9170)\tPrec@1 82.000 (82.016)\tPrec@5 98.000 (98.262)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 4.4551 (6.9355)\tPrec@1 84.000 (81.915)\tPrec@5 99.000 (98.352)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 8.8310 (6.9315)\tPrec@1 82.000 (82.037)\tPrec@5 97.000 (98.444)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.0377 (6.9374)\tPrec@1 85.000 (81.813)\tPrec@5 100.000 (98.429)\n",
      "val Results: Prec@1 81.620 Prec@5 98.450 Loss 6.98361\n",
      "val Class Accuracy: [0.932,0.983,0.830,0.750,0.836,0.774,0.854,0.750,0.769,0.684]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [179][0/97], lr: 0.00010\tTime 0.418 (0.418)\tData 0.257 (0.257)\tLoss 2.3026 (2.3026)\tPrec@1 86.719 (86.719)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [179][10/97], lr: 0.00010\tTime 0.295 (0.311)\tData 0.000 (0.038)\tLoss 2.8635 (2.1922)\tPrec@1 87.500 (89.418)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [179][20/97], lr: 0.00010\tTime 0.295 (0.305)\tData 0.000 (0.028)\tLoss 2.5214 (2.1816)\tPrec@1 92.969 (90.216)\tPrec@5 100.000 (99.368)\n",
      "Epoch: [179][30/97], lr: 0.00010\tTime 0.296 (0.303)\tData 0.000 (0.025)\tLoss 1.9878 (2.4059)\tPrec@1 90.625 (90.045)\tPrec@5 100.000 (99.471)\n",
      "Epoch: [179][40/97], lr: 0.00010\tTime 0.299 (0.302)\tData 0.000 (0.023)\tLoss 1.2145 (2.3378)\tPrec@1 90.625 (90.168)\tPrec@5 100.000 (99.486)\n",
      "Epoch: [179][50/97], lr: 0.00010\tTime 0.295 (0.301)\tData 0.000 (0.022)\tLoss 4.2137 (2.2786)\tPrec@1 89.844 (90.150)\tPrec@5 99.219 (99.494)\n",
      "Epoch: [179][60/97], lr: 0.00010\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 1.8456 (2.2000)\tPrec@1 92.188 (90.279)\tPrec@5 100.000 (99.475)\n",
      "Epoch: [179][70/97], lr: 0.00010\tTime 0.299 (0.301)\tData 0.000 (0.021)\tLoss 2.4580 (2.1598)\tPrec@1 86.719 (90.449)\tPrec@5 100.000 (99.472)\n",
      "Epoch: [179][80/97], lr: 0.00010\tTime 0.300 (0.301)\tData 0.000 (0.020)\tLoss 1.5036 (2.1447)\tPrec@1 92.188 (90.529)\tPrec@5 99.219 (99.508)\n",
      "Epoch: [179][90/97], lr: 0.00010\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 2.5221 (2.1445)\tPrec@1 91.406 (90.556)\tPrec@5 98.438 (99.493)\n",
      "Epoch: [179][96/97], lr: 0.00010\tTime 0.292 (0.300)\tData 0.000 (0.020)\tLoss 2.1012 (2.1469)\tPrec@1 94.915 (90.593)\tPrec@5 100.000 (99.516)\n",
      "Test: [0/100]\tTime 0.285 (0.285)\tLoss 2.3995 (2.3995)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 5.3891 (6.3597)\tPrec@1 83.000 (81.909)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 5.9742 (6.3001)\tPrec@1 79.000 (81.810)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.2935 (6.2874)\tPrec@1 86.000 (82.161)\tPrec@5 99.000 (98.290)\n",
      "Test: [40/100]\tTime 0.072 (0.078)\tLoss 5.5749 (6.4687)\tPrec@1 84.000 (81.951)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.1777 (6.4802)\tPrec@1 81.000 (82.314)\tPrec@5 99.000 (98.294)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.8256 (6.4202)\tPrec@1 83.000 (82.279)\tPrec@5 99.000 (98.344)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.5094 (6.4207)\tPrec@1 85.000 (82.239)\tPrec@5 99.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.3292 (6.3962)\tPrec@1 82.000 (82.432)\tPrec@5 97.000 (98.469)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.6969 (6.4051)\tPrec@1 85.000 (82.143)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 81.990 Prec@5 98.500 Loss 6.44848\n",
      "val Class Accuracy: [0.933,0.975,0.834,0.750,0.849,0.749,0.851,0.769,0.783,0.706]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [180][0/97], lr: 0.00000\tTime 0.384 (0.384)\tData 0.241 (0.241)\tLoss 0.9927 (0.9927)\tPrec@1 92.188 (92.188)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [180][10/97], lr: 0.00000\tTime 0.297 (0.308)\tData 0.000 (0.037)\tLoss 2.0231 (2.1106)\tPrec@1 91.406 (90.554)\tPrec@5 99.219 (99.858)\n",
      "Epoch: [180][20/97], lr: 0.00000\tTime 0.328 (0.308)\tData 0.000 (0.028)\tLoss 1.3544 (2.1424)\tPrec@1 91.406 (90.737)\tPrec@5 100.000 (99.665)\n",
      "Epoch: [180][30/97], lr: 0.00000\tTime 0.324 (0.317)\tData 0.001 (0.024)\tLoss 1.8221 (2.1114)\tPrec@1 94.531 (91.356)\tPrec@5 100.000 (99.723)\n",
      "Epoch: [180][40/97], lr: 0.00000\tTime 0.332 (0.320)\tData 0.000 (0.022)\tLoss 2.9753 (2.1808)\tPrec@1 92.188 (91.273)\tPrec@5 99.219 (99.562)\n",
      "Epoch: [180][50/97], lr: 0.00000\tTime 0.314 (0.321)\tData 0.000 (0.021)\tLoss 1.3183 (2.1556)\tPrec@1 92.188 (91.314)\tPrec@5 100.000 (99.540)\n",
      "Epoch: [180][60/97], lr: 0.00000\tTime 0.318 (0.322)\tData 0.001 (0.020)\tLoss 2.5151 (2.0777)\tPrec@1 88.281 (91.099)\tPrec@5 99.219 (99.539)\n",
      "Epoch: [180][70/97], lr: 0.00000\tTime 0.322 (0.323)\tData 0.000 (0.019)\tLoss 3.5754 (2.1148)\tPrec@1 94.531 (91.032)\tPrec@5 99.219 (99.560)\n",
      "Epoch: [180][80/97], lr: 0.00000\tTime 0.329 (0.324)\tData 0.000 (0.019)\tLoss 0.9119 (2.0724)\tPrec@1 92.188 (91.069)\tPrec@5 100.000 (99.595)\n",
      "Epoch: [180][90/97], lr: 0.00000\tTime 0.323 (0.325)\tData 0.000 (0.018)\tLoss 1.9215 (2.0089)\tPrec@1 92.969 (91.252)\tPrec@5 99.219 (99.622)\n",
      "Epoch: [180][96/97], lr: 0.00000\tTime 0.304 (0.324)\tData 0.000 (0.019)\tLoss 1.7007 (2.0398)\tPrec@1 86.441 (91.141)\tPrec@5 100.000 (99.613)\n",
      "Test: [0/100]\tTime 0.325 (0.325)\tLoss 2.4121 (2.4121)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.096)\tLoss 5.3826 (6.3477)\tPrec@1 82.000 (82.273)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 5.9307 (6.3034)\tPrec@1 80.000 (82.333)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.072 (0.081)\tLoss 4.4363 (6.2909)\tPrec@1 85.000 (82.484)\tPrec@5 99.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 5.5105 (6.4654)\tPrec@1 85.000 (82.268)\tPrec@5 98.000 (98.220)\n",
      "Test: [50/100]\tTime 0.072 (0.078)\tLoss 7.0088 (6.4720)\tPrec@1 81.000 (82.529)\tPrec@5 99.000 (98.235)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 7.9687 (6.4100)\tPrec@1 83.000 (82.590)\tPrec@5 99.000 (98.311)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.4921 (6.4078)\tPrec@1 85.000 (82.535)\tPrec@5 99.000 (98.352)\n",
      "Test: [80/100]\tTime 0.072 (0.076)\tLoss 8.1740 (6.3789)\tPrec@1 83.000 (82.704)\tPrec@5 97.000 (98.444)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.7245 (6.3899)\tPrec@1 85.000 (82.440)\tPrec@5 100.000 (98.429)\n",
      "val Results: Prec@1 82.230 Prec@5 98.470 Loss 6.42921\n",
      "val Class Accuracy: [0.930,0.976,0.828,0.753,0.851,0.750,0.862,0.784,0.777,0.712]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [181][0/97], lr: 0.00000\tTime 0.483 (0.483)\tData 0.326 (0.326)\tLoss 1.9219 (1.9219)\tPrec@1 92.188 (92.188)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [181][10/97], lr: 0.00000\tTime 0.296 (0.318)\tData 0.000 (0.044)\tLoss 1.7246 (2.0322)\tPrec@1 92.188 (90.696)\tPrec@5 98.438 (99.645)\n",
      "Epoch: [181][20/97], lr: 0.00000\tTime 0.324 (0.313)\tData 0.000 (0.031)\tLoss 3.4491 (2.0700)\tPrec@1 89.844 (90.699)\tPrec@5 100.000 (99.702)\n",
      "Epoch: [181][30/97], lr: 0.00000\tTime 0.341 (0.328)\tData 0.001 (0.026)\tLoss 3.5815 (2.0936)\tPrec@1 90.625 (90.751)\tPrec@5 100.000 (99.723)\n",
      "Epoch: [181][40/97], lr: 0.00000\tTime 0.320 (0.333)\tData 0.000 (0.023)\tLoss 1.8526 (2.2295)\tPrec@1 87.500 (90.606)\tPrec@5 99.219 (99.600)\n",
      "Epoch: [181][50/97], lr: 0.00000\tTime 0.318 (0.330)\tData 0.000 (0.022)\tLoss 2.8691 (2.2934)\tPrec@1 89.062 (90.640)\tPrec@5 100.000 (99.617)\n",
      "Epoch: [181][60/97], lr: 0.00000\tTime 0.297 (0.329)\tData 0.000 (0.021)\tLoss 1.5501 (2.2513)\tPrec@1 90.625 (90.740)\tPrec@5 100.000 (99.565)\n",
      "Epoch: [181][70/97], lr: 0.00000\tTime 0.299 (0.324)\tData 0.000 (0.020)\tLoss 2.2457 (2.2305)\tPrec@1 89.844 (90.812)\tPrec@5 99.219 (99.538)\n",
      "Epoch: [181][80/97], lr: 0.00000\tTime 0.295 (0.321)\tData 0.000 (0.020)\tLoss 2.1179 (2.2359)\tPrec@1 87.500 (90.664)\tPrec@5 100.000 (99.518)\n",
      "Epoch: [181][90/97], lr: 0.00000\tTime 0.294 (0.319)\tData 0.000 (0.020)\tLoss 1.8533 (2.2204)\tPrec@1 91.406 (90.737)\tPrec@5 99.219 (99.485)\n",
      "Epoch: [181][96/97], lr: 0.00000\tTime 0.290 (0.317)\tData 0.000 (0.020)\tLoss 1.6257 (2.1854)\tPrec@1 91.525 (90.892)\tPrec@5 99.153 (99.492)\n",
      "Test: [0/100]\tTime 0.274 (0.274)\tLoss 2.5976 (2.5976)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 5.5902 (6.5514)\tPrec@1 82.000 (81.545)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.0133 (6.4620)\tPrec@1 80.000 (81.714)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.7585 (6.4442)\tPrec@1 85.000 (81.935)\tPrec@5 99.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 5.6460 (6.6218)\tPrec@1 84.000 (81.756)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.072 (0.077)\tLoss 7.1123 (6.6343)\tPrec@1 81.000 (82.118)\tPrec@5 99.000 (98.275)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.2532 (6.5756)\tPrec@1 83.000 (82.197)\tPrec@5 99.000 (98.344)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.7207 (6.5810)\tPrec@1 85.000 (82.113)\tPrec@5 99.000 (98.394)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.5080 (6.5610)\tPrec@1 82.000 (82.296)\tPrec@5 97.000 (98.469)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 5.9086 (6.5749)\tPrec@1 85.000 (82.033)\tPrec@5 100.000 (98.451)\n",
      "val Results: Prec@1 81.900 Prec@5 98.480 Loss 6.61361\n",
      "val Class Accuracy: [0.931,0.979,0.831,0.764,0.840,0.749,0.853,0.771,0.769,0.703]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [182][0/97], lr: 0.00000\tTime 0.531 (0.531)\tData 0.334 (0.334)\tLoss 2.4946 (2.4946)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [182][10/97], lr: 0.00000\tTime 0.301 (0.333)\tData 0.000 (0.045)\tLoss 4.2639 (2.2895)\tPrec@1 89.062 (90.980)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [182][20/97], lr: 0.00000\tTime 0.296 (0.317)\tData 0.000 (0.031)\tLoss 1.0047 (2.0639)\tPrec@1 94.531 (91.220)\tPrec@5 100.000 (99.554)\n",
      "Epoch: [182][30/97], lr: 0.00000\tTime 0.295 (0.311)\tData 0.000 (0.027)\tLoss 0.5016 (2.0582)\tPrec@1 95.312 (90.801)\tPrec@5 100.000 (99.622)\n",
      "Epoch: [182][40/97], lr: 0.00000\tTime 0.294 (0.308)\tData 0.000 (0.024)\tLoss 1.8996 (2.0720)\tPrec@1 89.844 (90.816)\tPrec@5 100.000 (99.600)\n",
      "Epoch: [182][50/97], lr: 0.00000\tTime 0.295 (0.306)\tData 0.000 (0.023)\tLoss 1.8618 (2.0865)\tPrec@1 91.406 (90.794)\tPrec@5 100.000 (99.617)\n",
      "Epoch: [182][60/97], lr: 0.00000\tTime 0.296 (0.305)\tData 0.000 (0.022)\tLoss 1.3464 (2.0802)\tPrec@1 90.625 (90.715)\tPrec@5 100.000 (99.590)\n",
      "Epoch: [182][70/97], lr: 0.00000\tTime 0.295 (0.304)\tData 0.000 (0.021)\tLoss 1.5014 (2.1190)\tPrec@1 92.188 (90.757)\tPrec@5 100.000 (99.593)\n",
      "Epoch: [182][80/97], lr: 0.00000\tTime 0.295 (0.303)\tData 0.000 (0.021)\tLoss 1.7697 (2.1181)\tPrec@1 93.750 (90.635)\tPrec@5 100.000 (99.614)\n",
      "Epoch: [182][90/97], lr: 0.00000\tTime 0.296 (0.303)\tData 0.000 (0.021)\tLoss 1.1142 (2.0797)\tPrec@1 90.625 (90.788)\tPrec@5 99.219 (99.622)\n",
      "Epoch: [182][96/97], lr: 0.00000\tTime 0.289 (0.302)\tData 0.000 (0.021)\tLoss 2.3276 (2.1008)\tPrec@1 90.678 (90.634)\tPrec@5 99.153 (99.605)\n",
      "Test: [0/100]\tTime 0.267 (0.267)\tLoss 2.4489 (2.4489)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.072 (0.090)\tLoss 5.3792 (6.3747)\tPrec@1 83.000 (82.182)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.9772 (6.3174)\tPrec@1 78.000 (82.095)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.4408 (6.3075)\tPrec@1 85.000 (82.387)\tPrec@5 99.000 (98.323)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 5.6218 (6.4837)\tPrec@1 84.000 (82.146)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.1275 (6.4967)\tPrec@1 81.000 (82.451)\tPrec@5 99.000 (98.294)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 7.8551 (6.4364)\tPrec@1 84.000 (82.459)\tPrec@5 99.000 (98.344)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 3.5394 (6.4373)\tPrec@1 85.000 (82.380)\tPrec@5 99.000 (98.366)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.3570 (6.4114)\tPrec@1 83.000 (82.556)\tPrec@5 97.000 (98.457)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.7044 (6.4227)\tPrec@1 85.000 (82.253)\tPrec@5 100.000 (98.440)\n",
      "val Results: Prec@1 82.070 Prec@5 98.480 Loss 6.46555\n",
      "val Class Accuracy: [0.937,0.975,0.825,0.757,0.856,0.743,0.855,0.771,0.777,0.711]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [183][0/97], lr: 0.00000\tTime 0.354 (0.354)\tData 0.199 (0.199)\tLoss 2.2280 (2.2280)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [183][10/97], lr: 0.00000\tTime 0.298 (0.306)\tData 0.000 (0.033)\tLoss 1.9500 (2.3343)\tPrec@1 89.844 (91.548)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [183][20/97], lr: 0.00000\tTime 0.298 (0.303)\tData 0.000 (0.026)\tLoss 1.9978 (2.1588)\tPrec@1 92.969 (91.555)\tPrec@5 100.000 (99.665)\n",
      "Epoch: [183][30/97], lr: 0.00000\tTime 0.296 (0.301)\tData 0.000 (0.023)\tLoss 3.5545 (2.2664)\tPrec@1 90.625 (91.079)\tPrec@5 100.000 (99.597)\n",
      "Epoch: [183][40/97], lr: 0.00000\tTime 0.299 (0.301)\tData 0.000 (0.022)\tLoss 3.0343 (2.3350)\tPrec@1 87.500 (91.101)\tPrec@5 100.000 (99.619)\n",
      "Epoch: [183][50/97], lr: 0.00000\tTime 0.297 (0.301)\tData 0.000 (0.021)\tLoss 1.9678 (2.2916)\tPrec@1 92.188 (91.299)\tPrec@5 100.000 (99.617)\n",
      "Epoch: [183][60/97], lr: 0.00000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 1.8223 (2.1890)\tPrec@1 89.844 (91.393)\tPrec@5 100.000 (99.577)\n",
      "Epoch: [183][70/97], lr: 0.00000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 1.5679 (2.1597)\tPrec@1 90.625 (91.395)\tPrec@5 99.219 (99.593)\n",
      "Epoch: [183][80/97], lr: 0.00000\tTime 0.297 (0.300)\tData 0.000 (0.020)\tLoss 1.4414 (2.1824)\tPrec@1 91.406 (91.397)\tPrec@5 99.219 (99.595)\n",
      "Epoch: [183][90/97], lr: 0.00000\tTime 0.300 (0.300)\tData 0.000 (0.019)\tLoss 1.0583 (2.1669)\tPrec@1 90.625 (91.415)\tPrec@5 100.000 (99.588)\n",
      "Epoch: [183][96/97], lr: 0.00000\tTime 0.290 (0.300)\tData 0.000 (0.020)\tLoss 1.3044 (2.1770)\tPrec@1 90.678 (91.286)\tPrec@5 99.153 (99.573)\n",
      "Test: [0/100]\tTime 0.251 (0.251)\tLoss 2.5763 (2.5763)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 5.4129 (6.5152)\tPrec@1 83.000 (82.091)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 5.9815 (6.4401)\tPrec@1 79.000 (82.095)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.6047 (6.4255)\tPrec@1 85.000 (82.387)\tPrec@5 99.000 (98.290)\n",
      "Test: [40/100]\tTime 0.072 (0.077)\tLoss 5.6665 (6.6015)\tPrec@1 84.000 (82.000)\tPrec@5 98.000 (98.220)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.9522 (6.6076)\tPrec@1 81.000 (82.314)\tPrec@5 99.000 (98.275)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.1470 (6.5449)\tPrec@1 82.000 (82.377)\tPrec@5 99.000 (98.328)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.7215 (6.5512)\tPrec@1 85.000 (82.310)\tPrec@5 99.000 (98.366)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.5502 (6.5294)\tPrec@1 82.000 (82.444)\tPrec@5 97.000 (98.457)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 5.8746 (6.5454)\tPrec@1 84.000 (82.143)\tPrec@5 100.000 (98.451)\n",
      "val Results: Prec@1 81.970 Prec@5 98.480 Loss 6.58461\n",
      "val Class Accuracy: [0.939,0.976,0.830,0.745,0.856,0.739,0.863,0.776,0.767,0.706]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [184][0/97], lr: 0.00000\tTime 0.368 (0.368)\tData 0.213 (0.213)\tLoss 2.2090 (2.2090)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [184][10/97], lr: 0.00000\tTime 0.296 (0.305)\tData 0.000 (0.033)\tLoss 1.9439 (2.3158)\tPrec@1 90.625 (91.619)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [184][20/97], lr: 0.00000\tTime 0.300 (0.302)\tData 0.000 (0.025)\tLoss 1.7101 (2.3369)\tPrec@1 87.500 (90.290)\tPrec@5 100.000 (99.256)\n",
      "Epoch: [184][30/97], lr: 0.00000\tTime 0.297 (0.301)\tData 0.000 (0.023)\tLoss 1.6856 (2.2187)\tPrec@1 92.969 (90.625)\tPrec@5 99.219 (99.294)\n",
      "Epoch: [184][40/97], lr: 0.00000\tTime 0.288 (0.300)\tData 0.000 (0.021)\tLoss 3.7332 (2.2367)\tPrec@1 89.062 (90.454)\tPrec@5 100.000 (99.352)\n",
      "Epoch: [184][50/97], lr: 0.00000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 2.0355 (2.2144)\tPrec@1 94.531 (90.717)\tPrec@5 100.000 (99.341)\n",
      "Epoch: [184][60/97], lr: 0.00000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 2.1906 (2.1329)\tPrec@1 93.750 (90.817)\tPrec@5 100.000 (99.385)\n",
      "Epoch: [184][70/97], lr: 0.00000\tTime 0.293 (0.299)\tData 0.000 (0.020)\tLoss 1.5084 (2.0719)\tPrec@1 92.969 (90.977)\tPrec@5 98.438 (99.406)\n",
      "Epoch: [184][80/97], lr: 0.00000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 1.3329 (2.0143)\tPrec@1 88.281 (91.030)\tPrec@5 99.219 (99.402)\n",
      "Epoch: [184][90/97], lr: 0.00000\tTime 0.294 (0.299)\tData 0.000 (0.019)\tLoss 5.2524 (2.0618)\tPrec@1 90.625 (91.029)\tPrec@5 99.219 (99.416)\n",
      "Epoch: [184][96/97], lr: 0.00000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 1.2676 (2.0468)\tPrec@1 90.678 (91.101)\tPrec@5 99.153 (99.444)\n",
      "Test: [0/100]\tTime 0.267 (0.267)\tLoss 2.5594 (2.5594)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 5.5777 (6.5679)\tPrec@1 83.000 (81.909)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.072 (0.082)\tLoss 5.9828 (6.4750)\tPrec@1 80.000 (81.905)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.6624 (6.4629)\tPrec@1 85.000 (82.129)\tPrec@5 99.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.7902 (6.6403)\tPrec@1 84.000 (81.902)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.1794 (6.6591)\tPrec@1 81.000 (82.176)\tPrec@5 99.000 (98.275)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.1147 (6.5919)\tPrec@1 82.000 (82.246)\tPrec@5 99.000 (98.328)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 3.6415 (6.5969)\tPrec@1 85.000 (82.183)\tPrec@5 99.000 (98.352)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.5181 (6.5763)\tPrec@1 81.000 (82.309)\tPrec@5 97.000 (98.444)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.8896 (6.5923)\tPrec@1 85.000 (82.033)\tPrec@5 100.000 (98.429)\n",
      "val Results: Prec@1 81.860 Prec@5 98.470 Loss 6.63417\n",
      "val Class Accuracy: [0.937,0.977,0.833,0.749,0.844,0.754,0.850,0.778,0.764,0.700]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [185][0/97], lr: 0.00000\tTime 0.462 (0.462)\tData 0.295 (0.295)\tLoss 1.5778 (1.5778)\tPrec@1 93.750 (93.750)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [185][10/97], lr: 0.00000\tTime 0.296 (0.314)\tData 0.000 (0.042)\tLoss 0.8546 (1.7496)\tPrec@1 91.406 (91.903)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [185][20/97], lr: 0.00000\tTime 0.296 (0.306)\tData 0.000 (0.030)\tLoss 0.8554 (2.0688)\tPrec@1 90.625 (90.774)\tPrec@5 99.219 (99.479)\n",
      "Epoch: [185][30/97], lr: 0.00000\tTime 0.300 (0.305)\tData 0.000 (0.026)\tLoss 1.3609 (1.9715)\tPrec@1 94.531 (91.129)\tPrec@5 100.000 (99.572)\n",
      "Epoch: [185][40/97], lr: 0.00000\tTime 0.300 (0.304)\tData 0.000 (0.024)\tLoss 2.2603 (2.0137)\tPrec@1 89.062 (90.777)\tPrec@5 100.000 (99.562)\n",
      "Epoch: [185][50/97], lr: 0.00000\tTime 0.297 (0.303)\tData 0.000 (0.023)\tLoss 3.0839 (1.9894)\tPrec@1 92.969 (90.916)\tPrec@5 98.438 (99.494)\n",
      "Epoch: [185][60/97], lr: 0.00000\tTime 0.296 (0.303)\tData 0.000 (0.022)\tLoss 1.7667 (1.9701)\tPrec@1 92.969 (90.958)\tPrec@5 99.219 (99.488)\n",
      "Epoch: [185][70/97], lr: 0.00000\tTime 0.297 (0.302)\tData 0.000 (0.021)\tLoss 1.4219 (1.9916)\tPrec@1 91.406 (90.966)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [185][80/97], lr: 0.00000\tTime 0.297 (0.302)\tData 0.000 (0.021)\tLoss 2.4363 (2.0383)\tPrec@1 85.938 (90.760)\tPrec@5 100.000 (99.470)\n",
      "Epoch: [185][90/97], lr: 0.00000\tTime 0.308 (0.302)\tData 0.000 (0.020)\tLoss 1.9647 (2.0815)\tPrec@1 92.969 (90.840)\tPrec@5 100.000 (99.485)\n",
      "Epoch: [185][96/97], lr: 0.00000\tTime 0.290 (0.302)\tData 0.000 (0.021)\tLoss 0.8527 (2.0814)\tPrec@1 91.525 (90.803)\tPrec@5 100.000 (99.484)\n",
      "Test: [0/100]\tTime 0.255 (0.255)\tLoss 2.4616 (2.4616)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 5.2822 (6.3395)\tPrec@1 85.000 (82.182)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.9150 (6.2845)\tPrec@1 79.000 (81.952)\tPrec@5 98.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.4187 (6.2807)\tPrec@1 83.000 (82.000)\tPrec@5 99.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.5328 (6.4543)\tPrec@1 85.000 (81.878)\tPrec@5 98.000 (98.293)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.0233 (6.4591)\tPrec@1 82.000 (82.196)\tPrec@5 99.000 (98.333)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 7.8252 (6.3992)\tPrec@1 84.000 (82.246)\tPrec@5 99.000 (98.377)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.4430 (6.3966)\tPrec@1 85.000 (82.282)\tPrec@5 99.000 (98.394)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.2715 (6.3692)\tPrec@1 82.000 (82.432)\tPrec@5 97.000 (98.481)\n",
      "Test: [90/100]\tTime 0.072 (0.075)\tLoss 5.6937 (6.3809)\tPrec@1 85.000 (82.121)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 81.920 Prec@5 98.500 Loss 6.42163\n",
      "val Class Accuracy: [0.939,0.973,0.817,0.777,0.841,0.728,0.856,0.768,0.773,0.720]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [186][0/97], lr: 0.00000\tTime 0.363 (0.363)\tData 0.213 (0.213)\tLoss 2.5519 (2.5519)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [186][10/97], lr: 0.00000\tTime 0.296 (0.304)\tData 0.000 (0.034)\tLoss 1.3741 (1.7934)\tPrec@1 89.844 (92.116)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [186][20/97], lr: 0.00000\tTime 0.294 (0.301)\tData 0.000 (0.026)\tLoss 0.9690 (1.7600)\tPrec@1 93.750 (91.592)\tPrec@5 100.000 (99.665)\n",
      "Epoch: [186][30/97], lr: 0.00000\tTime 0.295 (0.301)\tData 0.000 (0.023)\tLoss 1.5760 (1.7715)\tPrec@1 90.625 (91.784)\tPrec@5 100.000 (99.622)\n",
      "Epoch: [186][40/97], lr: 0.00000\tTime 0.297 (0.300)\tData 0.000 (0.022)\tLoss 1.4936 (1.8975)\tPrec@1 92.188 (91.425)\tPrec@5 99.219 (99.524)\n",
      "Epoch: [186][50/97], lr: 0.00000\tTime 0.295 (0.299)\tData 0.000 (0.021)\tLoss 2.2604 (2.0443)\tPrec@1 89.844 (91.146)\tPrec@5 99.219 (99.586)\n",
      "Epoch: [186][60/97], lr: 0.00000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.1588 (2.0169)\tPrec@1 89.062 (91.201)\tPrec@5 99.219 (99.603)\n",
      "Epoch: [186][70/97], lr: 0.00000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 2.2707 (1.9943)\tPrec@1 90.625 (91.296)\tPrec@5 100.000 (99.637)\n",
      "Epoch: [186][80/97], lr: 0.00000\tTime 0.303 (0.299)\tData 0.000 (0.020)\tLoss 2.1600 (1.9858)\tPrec@1 88.281 (91.271)\tPrec@5 100.000 (99.624)\n",
      "Epoch: [186][90/97], lr: 0.00000\tTime 0.299 (0.299)\tData 0.000 (0.019)\tLoss 1.5957 (1.9626)\tPrec@1 91.406 (91.269)\tPrec@5 100.000 (99.596)\n",
      "Epoch: [186][96/97], lr: 0.00000\tTime 0.291 (0.299)\tData 0.000 (0.020)\tLoss 4.0085 (2.0257)\tPrec@1 89.831 (91.174)\tPrec@5 99.153 (99.557)\n",
      "Test: [0/100]\tTime 0.258 (0.258)\tLoss 2.4298 (2.4298)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 5.6802 (6.4842)\tPrec@1 82.000 (82.182)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.8973 (6.3873)\tPrec@1 80.000 (81.952)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 4.5278 (6.3741)\tPrec@1 84.000 (81.903)\tPrec@5 99.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.6119 (6.5487)\tPrec@1 84.000 (81.732)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.2141 (6.5648)\tPrec@1 82.000 (82.098)\tPrec@5 99.000 (98.294)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.0719 (6.4957)\tPrec@1 82.000 (82.131)\tPrec@5 99.000 (98.344)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 3.5702 (6.4992)\tPrec@1 85.000 (82.085)\tPrec@5 99.000 (98.394)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.3379 (6.4774)\tPrec@1 82.000 (82.284)\tPrec@5 97.000 (98.481)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.8284 (6.4881)\tPrec@1 85.000 (82.022)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 81.870 Prec@5 98.510 Loss 6.52830\n",
      "val Class Accuracy: [0.931,0.979,0.826,0.739,0.831,0.778,0.851,0.779,0.773,0.700]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [187][0/97], lr: 0.00000\tTime 0.462 (0.462)\tData 0.297 (0.297)\tLoss 2.7510 (2.7510)\tPrec@1 91.406 (91.406)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [187][10/97], lr: 0.00000\tTime 0.301 (0.316)\tData 0.000 (0.042)\tLoss 2.4004 (1.9324)\tPrec@1 90.625 (91.477)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [187][20/97], lr: 0.00000\tTime 0.299 (0.309)\tData 0.000 (0.030)\tLoss 2.0308 (1.8783)\tPrec@1 92.969 (91.332)\tPrec@5 100.000 (99.665)\n",
      "Epoch: [187][30/97], lr: 0.00000\tTime 0.302 (0.306)\tData 0.000 (0.026)\tLoss 2.1008 (1.9484)\tPrec@1 91.406 (91.129)\tPrec@5 100.000 (99.647)\n",
      "Epoch: [187][40/97], lr: 0.00000\tTime 0.293 (0.304)\tData 0.000 (0.024)\tLoss 1.8652 (1.9324)\tPrec@1 89.062 (90.835)\tPrec@5 99.219 (99.600)\n",
      "Epoch: [187][50/97], lr: 0.00000\tTime 0.296 (0.303)\tData 0.000 (0.023)\tLoss 3.0101 (1.8864)\tPrec@1 92.188 (90.977)\tPrec@5 100.000 (99.586)\n",
      "Epoch: [187][60/97], lr: 0.00000\tTime 0.306 (0.303)\tData 0.000 (0.022)\tLoss 1.3471 (1.9317)\tPrec@1 91.406 (90.766)\tPrec@5 100.000 (99.552)\n",
      "Epoch: [187][70/97], lr: 0.00000\tTime 0.297 (0.303)\tData 0.000 (0.021)\tLoss 2.3221 (1.9253)\tPrec@1 91.406 (90.900)\tPrec@5 100.000 (99.582)\n",
      "Epoch: [187][80/97], lr: 0.00000\tTime 0.299 (0.302)\tData 0.000 (0.021)\tLoss 1.3221 (1.9435)\tPrec@1 92.969 (90.924)\tPrec@5 99.219 (99.556)\n",
      "Epoch: [187][90/97], lr: 0.00000\tTime 0.304 (0.302)\tData 0.000 (0.020)\tLoss 1.5467 (1.9592)\tPrec@1 91.406 (90.934)\tPrec@5 100.000 (99.562)\n",
      "Epoch: [187][96/97], lr: 0.00000\tTime 0.291 (0.302)\tData 0.000 (0.021)\tLoss 1.4702 (1.9567)\tPrec@1 94.915 (90.924)\tPrec@5 100.000 (99.565)\n",
      "Test: [0/100]\tTime 0.276 (0.276)\tLoss 2.4803 (2.4803)\tPrec@1 86.000 (86.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 5.6188 (6.4601)\tPrec@1 82.000 (82.182)\tPrec@5 99.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.9172 (6.3665)\tPrec@1 80.000 (82.000)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.6997 (6.3483)\tPrec@1 84.000 (81.968)\tPrec@5 99.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 5.6529 (6.5247)\tPrec@1 84.000 (81.805)\tPrec@5 98.000 (98.268)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.0976 (6.5430)\tPrec@1 81.000 (82.118)\tPrec@5 99.000 (98.314)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.2352 (6.4820)\tPrec@1 80.000 (82.066)\tPrec@5 99.000 (98.361)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.6164 (6.4866)\tPrec@1 85.000 (82.070)\tPrec@5 99.000 (98.408)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 8.2952 (6.4655)\tPrec@1 82.000 (82.272)\tPrec@5 97.000 (98.494)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.7969 (6.4742)\tPrec@1 86.000 (82.066)\tPrec@5 100.000 (98.473)\n",
      "val Results: Prec@1 81.890 Prec@5 98.500 Loss 6.51341\n",
      "val Class Accuracy: [0.933,0.978,0.827,0.750,0.827,0.766,0.860,0.773,0.769,0.706]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [188][0/97], lr: 0.00000\tTime 0.396 (0.396)\tData 0.241 (0.241)\tLoss 1.4777 (1.4777)\tPrec@1 92.969 (92.969)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [188][10/97], lr: 0.00000\tTime 0.311 (0.309)\tData 0.000 (0.037)\tLoss 1.8754 (2.1889)\tPrec@1 89.844 (91.406)\tPrec@5 99.219 (99.787)\n",
      "Epoch: [188][20/97], lr: 0.00000\tTime 0.295 (0.303)\tData 0.000 (0.027)\tLoss 1.8171 (2.2738)\tPrec@1 92.188 (90.923)\tPrec@5 100.000 (99.740)\n",
      "Epoch: [188][30/97], lr: 0.00000\tTime 0.295 (0.301)\tData 0.000 (0.024)\tLoss 2.0094 (2.2370)\tPrec@1 90.625 (90.978)\tPrec@5 99.219 (99.698)\n",
      "Epoch: [188][40/97], lr: 0.00000\tTime 0.302 (0.300)\tData 0.000 (0.022)\tLoss 2.0835 (2.1060)\tPrec@1 92.188 (91.216)\tPrec@5 98.438 (99.676)\n",
      "Epoch: [188][50/97], lr: 0.00000\tTime 0.297 (0.300)\tData 0.000 (0.021)\tLoss 4.2277 (2.0444)\tPrec@1 91.406 (91.330)\tPrec@5 100.000 (99.663)\n",
      "Epoch: [188][60/97], lr: 0.00000\tTime 0.297 (0.300)\tData 0.000 (0.021)\tLoss 0.9500 (2.0260)\tPrec@1 92.969 (91.163)\tPrec@5 100.000 (99.705)\n",
      "Epoch: [188][70/97], lr: 0.00000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 1.6305 (2.0514)\tPrec@1 90.625 (90.977)\tPrec@5 99.219 (99.692)\n",
      "Epoch: [188][80/97], lr: 0.00000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 0.9755 (2.1043)\tPrec@1 89.844 (90.992)\tPrec@5 99.219 (99.662)\n",
      "Epoch: [188][90/97], lr: 0.00000\tTime 0.294 (0.299)\tData 0.000 (0.020)\tLoss 1.6097 (2.0961)\tPrec@1 95.312 (90.968)\tPrec@5 100.000 (99.665)\n",
      "Epoch: [188][96/97], lr: 0.00000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 1.9415 (2.1360)\tPrec@1 91.525 (90.940)\tPrec@5 99.153 (99.653)\n",
      "Test: [0/100]\tTime 0.231 (0.231)\tLoss 2.3786 (2.3786)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 5.3294 (6.2271)\tPrec@1 84.000 (82.091)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 5.7866 (6.1715)\tPrec@1 79.000 (81.905)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 4.3058 (6.1629)\tPrec@1 86.000 (82.161)\tPrec@5 99.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.076)\tLoss 5.4330 (6.3335)\tPrec@1 83.000 (81.902)\tPrec@5 98.000 (98.293)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.0423 (6.3421)\tPrec@1 82.000 (82.235)\tPrec@5 99.000 (98.333)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 7.8317 (6.2829)\tPrec@1 84.000 (82.230)\tPrec@5 99.000 (98.361)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.2390 (6.2795)\tPrec@1 85.000 (82.282)\tPrec@5 99.000 (98.423)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.1685 (6.2555)\tPrec@1 82.000 (82.531)\tPrec@5 97.000 (98.494)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 5.5819 (6.2647)\tPrec@1 85.000 (82.231)\tPrec@5 100.000 (98.473)\n",
      "val Results: Prec@1 82.070 Prec@5 98.530 Loss 6.30420\n",
      "val Class Accuracy: [0.938,0.974,0.825,0.770,0.829,0.744,0.845,0.781,0.773,0.728]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [189][0/97], lr: 0.00000\tTime 0.425 (0.425)\tData 0.257 (0.257)\tLoss 0.8801 (0.8801)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [189][10/97], lr: 0.00000\tTime 0.298 (0.312)\tData 0.000 (0.038)\tLoss 2.7922 (1.9928)\tPrec@1 89.844 (90.341)\tPrec@5 99.219 (99.787)\n",
      "Epoch: [189][20/97], lr: 0.00000\tTime 0.297 (0.306)\tData 0.000 (0.028)\tLoss 1.5468 (2.0538)\tPrec@1 90.625 (90.848)\tPrec@5 99.219 (99.554)\n",
      "Epoch: [189][30/97], lr: 0.00000\tTime 0.298 (0.304)\tData 0.000 (0.025)\tLoss 1.0400 (2.0307)\tPrec@1 94.531 (91.255)\tPrec@5 100.000 (99.622)\n",
      "Epoch: [189][40/97], lr: 0.00000\tTime 0.291 (0.303)\tData 0.000 (0.023)\tLoss 2.1466 (2.0447)\tPrec@1 93.750 (91.216)\tPrec@5 100.000 (99.562)\n",
      "Epoch: [189][50/97], lr: 0.00000\tTime 0.298 (0.303)\tData 0.000 (0.022)\tLoss 1.4803 (1.9539)\tPrec@1 93.750 (91.544)\tPrec@5 100.000 (99.556)\n",
      "Epoch: [189][60/97], lr: 0.00000\tTime 0.299 (0.302)\tData 0.000 (0.021)\tLoss 1.1724 (1.9731)\tPrec@1 92.969 (91.432)\tPrec@5 100.000 (99.526)\n",
      "Epoch: [189][70/97], lr: 0.00000\tTime 0.299 (0.302)\tData 0.000 (0.021)\tLoss 2.3449 (1.9726)\tPrec@1 89.062 (91.527)\tPrec@5 100.000 (99.549)\n",
      "Epoch: [189][80/97], lr: 0.00000\tTime 0.298 (0.302)\tData 0.000 (0.020)\tLoss 3.3902 (2.0791)\tPrec@1 92.188 (91.426)\tPrec@5 99.219 (99.576)\n",
      "Epoch: [189][90/97], lr: 0.00000\tTime 0.313 (0.302)\tData 0.000 (0.020)\tLoss 2.5354 (2.1359)\tPrec@1 90.625 (91.252)\tPrec@5 100.000 (99.588)\n",
      "Epoch: [189][96/97], lr: 0.00000\tTime 0.292 (0.302)\tData 0.000 (0.021)\tLoss 6.0287 (2.1563)\tPrec@1 86.441 (91.206)\tPrec@5 99.153 (99.589)\n",
      "Test: [0/100]\tTime 0.280 (0.280)\tLoss 2.4352 (2.4352)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 5.4550 (6.4012)\tPrec@1 82.000 (82.182)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 5.9683 (6.3321)\tPrec@1 79.000 (82.048)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 4.5783 (6.3219)\tPrec@1 85.000 (82.226)\tPrec@5 99.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 5.6287 (6.5029)\tPrec@1 84.000 (81.976)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.0345 (6.5179)\tPrec@1 81.000 (82.235)\tPrec@5 99.000 (98.294)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.0778 (6.4595)\tPrec@1 83.000 (82.295)\tPrec@5 99.000 (98.344)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.6316 (6.4635)\tPrec@1 85.000 (82.197)\tPrec@5 99.000 (98.394)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 8.3614 (6.4450)\tPrec@1 81.000 (82.420)\tPrec@5 97.000 (98.481)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.7852 (6.4534)\tPrec@1 85.000 (82.154)\tPrec@5 100.000 (98.473)\n",
      "val Results: Prec@1 81.970 Prec@5 98.510 Loss 6.49400\n",
      "val Class Accuracy: [0.930,0.978,0.825,0.763,0.835,0.745,0.863,0.779,0.775,0.704]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [190][0/97], lr: 0.00000\tTime 0.411 (0.411)\tData 0.254 (0.254)\tLoss 2.1208 (2.1208)\tPrec@1 92.969 (92.969)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [190][10/97], lr: 0.00000\tTime 0.296 (0.311)\tData 0.000 (0.038)\tLoss 1.8146 (2.2492)\tPrec@1 88.281 (90.909)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [190][20/97], lr: 0.00000\tTime 0.294 (0.304)\tData 0.000 (0.028)\tLoss 1.5630 (2.0551)\tPrec@1 91.406 (91.183)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [190][30/97], lr: 0.00000\tTime 0.296 (0.303)\tData 0.000 (0.025)\tLoss 1.8338 (2.0124)\tPrec@1 89.062 (90.978)\tPrec@5 99.219 (99.597)\n",
      "Epoch: [190][40/97], lr: 0.00000\tTime 0.298 (0.301)\tData 0.000 (0.023)\tLoss 1.8245 (2.0670)\tPrec@1 93.750 (90.796)\tPrec@5 100.000 (99.600)\n",
      "Epoch: [190][50/97], lr: 0.00000\tTime 0.297 (0.301)\tData 0.000 (0.022)\tLoss 2.4000 (2.0807)\tPrec@1 85.156 (90.824)\tPrec@5 97.656 (99.617)\n",
      "Epoch: [190][60/97], lr: 0.00000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 3.5656 (2.0850)\tPrec@1 89.062 (90.740)\tPrec@5 100.000 (99.629)\n",
      "Epoch: [190][70/97], lr: 0.00000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 1.3789 (2.0901)\tPrec@1 92.969 (90.812)\tPrec@5 100.000 (99.615)\n",
      "Epoch: [190][80/97], lr: 0.00000\tTime 0.298 (0.300)\tData 0.000 (0.020)\tLoss 1.2470 (2.0580)\tPrec@1 91.406 (90.799)\tPrec@5 99.219 (99.595)\n",
      "Epoch: [190][90/97], lr: 0.00000\tTime 0.304 (0.299)\tData 0.000 (0.020)\tLoss 2.0467 (2.0630)\tPrec@1 93.750 (90.917)\tPrec@5 99.219 (99.579)\n",
      "Epoch: [190][96/97], lr: 0.00000\tTime 0.288 (0.299)\tData 0.000 (0.020)\tLoss 3.5685 (2.0710)\tPrec@1 90.678 (90.819)\tPrec@5 98.305 (99.581)\n",
      "Test: [0/100]\tTime 0.240 (0.240)\tLoss 2.4658 (2.4658)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 5.5064 (6.3669)\tPrec@1 83.000 (82.000)\tPrec@5 99.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.8700 (6.2741)\tPrec@1 79.000 (81.905)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.4397 (6.2611)\tPrec@1 85.000 (81.871)\tPrec@5 99.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.5107 (6.4344)\tPrec@1 84.000 (81.732)\tPrec@5 98.000 (98.268)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.1249 (6.4469)\tPrec@1 81.000 (82.118)\tPrec@5 99.000 (98.314)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 7.9686 (6.3854)\tPrec@1 83.000 (82.131)\tPrec@5 99.000 (98.361)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.4615 (6.3840)\tPrec@1 85.000 (82.169)\tPrec@5 99.000 (98.394)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.2232 (6.3608)\tPrec@1 82.000 (82.383)\tPrec@5 97.000 (98.481)\n",
      "Test: [90/100]\tTime 0.072 (0.074)\tLoss 5.6781 (6.3708)\tPrec@1 85.000 (82.110)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 81.940 Prec@5 98.510 Loss 6.41135\n",
      "val Class Accuracy: [0.936,0.978,0.817,0.763,0.836,0.758,0.847,0.764,0.775,0.720]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [191][0/97], lr: 0.00000\tTime 0.377 (0.377)\tData 0.243 (0.243)\tLoss 1.7231 (1.7231)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [191][10/97], lr: 0.00000\tTime 0.299 (0.307)\tData 0.000 (0.037)\tLoss 1.6885 (2.2786)\tPrec@1 92.188 (89.844)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [191][20/97], lr: 0.00000\tTime 0.297 (0.303)\tData 0.000 (0.028)\tLoss 0.8879 (2.2573)\tPrec@1 93.750 (89.955)\tPrec@5 100.000 (99.628)\n",
      "Epoch: [191][30/97], lr: 0.00000\tTime 0.295 (0.301)\tData 0.000 (0.024)\tLoss 3.9180 (2.2987)\tPrec@1 85.938 (90.247)\tPrec@5 98.438 (99.521)\n",
      "Epoch: [191][40/97], lr: 0.00000\tTime 0.297 (0.300)\tData 0.000 (0.023)\tLoss 1.0265 (2.3004)\tPrec@1 93.750 (90.606)\tPrec@5 100.000 (99.543)\n",
      "Epoch: [191][50/97], lr: 0.00000\tTime 0.295 (0.300)\tData 0.000 (0.022)\tLoss 0.9398 (2.2442)\tPrec@1 93.750 (90.533)\tPrec@5 100.000 (99.571)\n",
      "Epoch: [191][60/97], lr: 0.00000\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 1.6829 (2.2640)\tPrec@1 91.406 (90.407)\tPrec@5 99.219 (99.539)\n",
      "Epoch: [191][70/97], lr: 0.00000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 4.4941 (2.3412)\tPrec@1 86.719 (90.306)\tPrec@5 100.000 (99.494)\n",
      "Epoch: [191][80/97], lr: 0.00000\tTime 0.299 (0.299)\tData 0.000 (0.020)\tLoss 1.3346 (2.2948)\tPrec@1 90.625 (90.230)\tPrec@5 100.000 (99.508)\n",
      "Epoch: [191][90/97], lr: 0.00000\tTime 0.293 (0.299)\tData 0.000 (0.020)\tLoss 2.2802 (2.2502)\tPrec@1 88.281 (90.342)\tPrec@5 100.000 (99.519)\n",
      "Epoch: [191][96/97], lr: 0.00000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 1.1925 (2.2616)\tPrec@1 94.068 (90.360)\tPrec@5 100.000 (99.541)\n",
      "Test: [0/100]\tTime 0.242 (0.242)\tLoss 2.7842 (2.7842)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 5.8506 (6.7939)\tPrec@1 83.000 (81.545)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.1625 (6.6772)\tPrec@1 79.000 (81.714)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 5.0460 (6.6584)\tPrec@1 84.000 (81.968)\tPrec@5 99.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.9743 (6.8323)\tPrec@1 83.000 (81.732)\tPrec@5 98.000 (98.293)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.1307 (6.8606)\tPrec@1 81.000 (82.059)\tPrec@5 99.000 (98.333)\n",
      "Test: [60/100]\tTime 0.072 (0.075)\tLoss 8.5066 (6.7970)\tPrec@1 81.000 (82.115)\tPrec@5 99.000 (98.393)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.9787 (6.8072)\tPrec@1 85.000 (82.070)\tPrec@5 99.000 (98.437)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.8440 (6.7905)\tPrec@1 80.000 (82.222)\tPrec@5 97.000 (98.506)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 6.0502 (6.8093)\tPrec@1 85.000 (81.967)\tPrec@5 100.000 (98.484)\n",
      "val Results: Prec@1 81.800 Prec@5 98.510 Loss 6.84891\n",
      "val Class Accuracy: [0.939,0.980,0.834,0.748,0.849,0.754,0.850,0.772,0.760,0.694]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [192][0/97], lr: 0.00000\tTime 0.383 (0.383)\tData 0.221 (0.221)\tLoss 3.6196 (3.6196)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [192][10/97], lr: 0.00000\tTime 0.297 (0.309)\tData 0.000 (0.035)\tLoss 1.6672 (2.4024)\tPrec@1 92.969 (89.276)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [192][20/97], lr: 0.00000\tTime 0.297 (0.304)\tData 0.000 (0.027)\tLoss 2.0879 (2.2517)\tPrec@1 92.188 (89.955)\tPrec@5 99.219 (99.330)\n",
      "Epoch: [192][30/97], lr: 0.00000\tTime 0.296 (0.303)\tData 0.000 (0.024)\tLoss 0.9058 (2.1468)\tPrec@1 94.531 (90.927)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [192][40/97], lr: 0.00000\tTime 0.300 (0.302)\tData 0.000 (0.022)\tLoss 2.1331 (2.1174)\tPrec@1 96.094 (90.854)\tPrec@5 100.000 (99.390)\n",
      "Epoch: [192][50/97], lr: 0.00000\tTime 0.295 (0.302)\tData 0.000 (0.021)\tLoss 2.3588 (2.0710)\tPrec@1 90.625 (91.039)\tPrec@5 99.219 (99.433)\n",
      "Epoch: [192][60/97], lr: 0.00000\tTime 0.296 (0.301)\tData 0.000 (0.020)\tLoss 1.0596 (2.1142)\tPrec@1 96.875 (91.227)\tPrec@5 100.000 (99.475)\n",
      "Epoch: [192][70/97], lr: 0.00000\tTime 0.319 (0.301)\tData 0.000 (0.020)\tLoss 2.1851 (2.1081)\tPrec@1 87.500 (91.065)\tPrec@5 99.219 (99.472)\n",
      "Epoch: [192][80/97], lr: 0.00000\tTime 0.298 (0.301)\tData 0.000 (0.020)\tLoss 2.1009 (2.1428)\tPrec@1 92.188 (91.011)\tPrec@5 100.000 (99.470)\n",
      "Epoch: [192][90/97], lr: 0.00000\tTime 0.297 (0.301)\tData 0.000 (0.019)\tLoss 1.8261 (2.1792)\tPrec@1 92.969 (90.986)\tPrec@5 100.000 (99.468)\n",
      "Epoch: [192][96/97], lr: 0.00000\tTime 0.293 (0.301)\tData 0.000 (0.020)\tLoss 3.5421 (2.1755)\tPrec@1 89.831 (91.004)\tPrec@5 100.000 (99.492)\n",
      "Test: [0/100]\tTime 0.260 (0.260)\tLoss 2.3866 (2.3866)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.072 (0.090)\tLoss 5.4257 (6.2650)\tPrec@1 82.000 (82.455)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.9124 (6.1945)\tPrec@1 79.000 (82.429)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.4376 (6.1692)\tPrec@1 86.000 (82.516)\tPrec@5 99.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.4782 (6.3450)\tPrec@1 85.000 (82.268)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.0300 (6.3476)\tPrec@1 81.000 (82.588)\tPrec@5 99.000 (98.294)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.9515 (6.2961)\tPrec@1 83.000 (82.557)\tPrec@5 99.000 (98.344)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 3.4269 (6.2957)\tPrec@1 85.000 (82.577)\tPrec@5 99.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.0658 (6.2744)\tPrec@1 83.000 (82.765)\tPrec@5 97.000 (98.469)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.6494 (6.2829)\tPrec@1 85.000 (82.538)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 82.380 Prec@5 98.520 Loss 6.32171\n",
      "val Class Accuracy: [0.933,0.977,0.820,0.761,0.859,0.757,0.850,0.773,0.777,0.731]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [193][0/97], lr: 0.00000\tTime 0.349 (0.349)\tData 0.203 (0.203)\tLoss 1.8288 (1.8288)\tPrec@1 92.969 (92.969)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [193][10/97], lr: 0.00000\tTime 0.294 (0.305)\tData 0.000 (0.033)\tLoss 2.6740 (1.8815)\tPrec@1 90.625 (90.909)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [193][20/97], lr: 0.00000\tTime 0.294 (0.301)\tData 0.000 (0.026)\tLoss 2.0483 (2.0119)\tPrec@1 89.844 (90.848)\tPrec@5 98.438 (99.442)\n",
      "Epoch: [193][30/97], lr: 0.00000\tTime 0.297 (0.301)\tData 0.000 (0.023)\tLoss 1.5031 (2.0497)\tPrec@1 93.750 (91.104)\tPrec@5 100.000 (99.471)\n",
      "Epoch: [193][40/97], lr: 0.00000\tTime 0.294 (0.300)\tData 0.000 (0.022)\tLoss 3.5031 (2.1358)\tPrec@1 91.406 (91.368)\tPrec@5 100.000 (99.466)\n",
      "Epoch: [193][50/97], lr: 0.00000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 3.4038 (2.1804)\tPrec@1 92.969 (91.192)\tPrec@5 100.000 (99.403)\n",
      "Epoch: [193][60/97], lr: 0.00000\tTime 0.297 (0.299)\tData 0.000 (0.020)\tLoss 1.6819 (2.1228)\tPrec@1 94.531 (91.342)\tPrec@5 100.000 (99.462)\n",
      "Epoch: [193][70/97], lr: 0.00000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.5709 (2.1114)\tPrec@1 88.281 (91.186)\tPrec@5 98.438 (99.472)\n",
      "Epoch: [193][80/97], lr: 0.00000\tTime 0.296 (0.299)\tData 0.000 (0.019)\tLoss 1.3279 (2.1079)\tPrec@1 89.844 (91.011)\tPrec@5 100.000 (99.489)\n",
      "Epoch: [193][90/97], lr: 0.00000\tTime 0.296 (0.299)\tData 0.000 (0.019)\tLoss 2.1276 (2.0832)\tPrec@1 92.969 (90.977)\tPrec@5 100.000 (99.519)\n",
      "Epoch: [193][96/97], lr: 0.00000\tTime 0.289 (0.299)\tData 0.000 (0.020)\tLoss 2.2785 (2.0613)\tPrec@1 93.220 (91.101)\tPrec@5 98.305 (99.484)\n",
      "Test: [0/100]\tTime 0.256 (0.256)\tLoss 2.3682 (2.3682)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 5.4145 (6.3966)\tPrec@1 83.000 (82.455)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.8747 (6.3297)\tPrec@1 80.000 (82.333)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.3686 (6.3234)\tPrec@1 85.000 (82.452)\tPrec@5 99.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.5876 (6.5045)\tPrec@1 84.000 (82.146)\tPrec@5 98.000 (98.268)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.0717 (6.5162)\tPrec@1 81.000 (82.294)\tPrec@5 99.000 (98.314)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.9486 (6.4461)\tPrec@1 83.000 (82.328)\tPrec@5 99.000 (98.361)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.5026 (6.4478)\tPrec@1 85.000 (82.254)\tPrec@5 99.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.3986 (6.4262)\tPrec@1 81.000 (82.395)\tPrec@5 97.000 (98.457)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.8006 (6.4373)\tPrec@1 85.000 (82.121)\tPrec@5 100.000 (98.451)\n",
      "val Results: Prec@1 81.900 Prec@5 98.490 Loss 6.47901\n",
      "val Class Accuracy: [0.936,0.975,0.824,0.744,0.842,0.750,0.859,0.786,0.774,0.700]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [194][0/97], lr: 0.00000\tTime 0.417 (0.417)\tData 0.265 (0.265)\tLoss 2.7579 (2.7579)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [194][10/97], lr: 0.00000\tTime 0.301 (0.322)\tData 0.000 (0.039)\tLoss 2.6006 (2.2091)\tPrec@1 92.188 (90.554)\tPrec@5 99.219 (99.361)\n",
      "Epoch: [194][20/97], lr: 0.00000\tTime 0.311 (0.318)\tData 0.000 (0.028)\tLoss 1.5329 (2.2427)\tPrec@1 90.625 (90.439)\tPrec@5 98.438 (99.442)\n",
      "Epoch: [194][30/97], lr: 0.00000\tTime 0.355 (0.321)\tData 0.001 (0.025)\tLoss 1.4261 (2.2400)\tPrec@1 89.062 (90.600)\tPrec@5 100.000 (99.471)\n",
      "Epoch: [194][40/97], lr: 0.00000\tTime 0.307 (0.323)\tData 0.000 (0.022)\tLoss 3.5531 (2.3309)\tPrec@1 85.938 (90.511)\tPrec@5 98.438 (99.409)\n",
      "Epoch: [194][50/97], lr: 0.00000\tTime 0.319 (0.326)\tData 0.001 (0.021)\tLoss 0.7763 (2.1760)\tPrec@1 92.969 (90.855)\tPrec@5 100.000 (99.403)\n",
      "Epoch: [194][60/97], lr: 0.00000\tTime 0.360 (0.331)\tData 0.000 (0.020)\tLoss 1.9365 (2.2191)\tPrec@1 90.625 (90.651)\tPrec@5 99.219 (99.385)\n",
      "Epoch: [194][70/97], lr: 0.00000\tTime 0.306 (0.333)\tData 0.000 (0.019)\tLoss 1.6592 (2.2296)\tPrec@1 93.750 (90.647)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [194][80/97], lr: 0.00000\tTime 0.306 (0.332)\tData 0.000 (0.019)\tLoss 2.1253 (2.1861)\tPrec@1 85.938 (90.721)\tPrec@5 100.000 (99.412)\n",
      "Epoch: [194][90/97], lr: 0.00000\tTime 0.303 (0.330)\tData 0.000 (0.019)\tLoss 1.2540 (2.1134)\tPrec@1 92.969 (90.865)\tPrec@5 100.000 (99.425)\n",
      "Epoch: [194][96/97], lr: 0.00000\tTime 0.291 (0.328)\tData 0.000 (0.019)\tLoss 1.2014 (2.0900)\tPrec@1 92.373 (90.956)\tPrec@5 99.153 (99.436)\n",
      "Test: [0/100]\tTime 0.319 (0.319)\tLoss 2.5138 (2.5138)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.072 (0.095)\tLoss 5.3612 (6.4434)\tPrec@1 83.000 (81.818)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.072 (0.084)\tLoss 5.9390 (6.3845)\tPrec@1 79.000 (81.762)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 4.6504 (6.3696)\tPrec@1 85.000 (82.097)\tPrec@5 99.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 5.6355 (6.5396)\tPrec@1 85.000 (81.976)\tPrec@5 98.000 (98.293)\n",
      "Test: [50/100]\tTime 0.072 (0.078)\tLoss 7.0846 (6.5491)\tPrec@1 82.000 (82.294)\tPrec@5 99.000 (98.333)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.0137 (6.4857)\tPrec@1 83.000 (82.311)\tPrec@5 99.000 (98.393)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 3.4575 (6.4883)\tPrec@1 85.000 (82.282)\tPrec@5 99.000 (98.408)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 8.3297 (6.4637)\tPrec@1 83.000 (82.432)\tPrec@5 97.000 (98.481)\n",
      "Test: [90/100]\tTime 0.131 (0.078)\tLoss 5.7860 (6.4788)\tPrec@1 85.000 (82.121)\tPrec@5 100.000 (98.473)\n",
      "val Results: Prec@1 81.970 Prec@5 98.510 Loss 6.51861\n",
      "val Class Accuracy: [0.938,0.976,0.838,0.750,0.841,0.739,0.857,0.777,0.761,0.720]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [195][0/97], lr: 0.00000\tTime 1.367 (1.367)\tData 0.910 (0.910)\tLoss 1.9205 (1.9205)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [195][10/97], lr: 0.00000\tTime 0.327 (0.458)\tData 0.000 (0.090)\tLoss 4.1568 (2.1778)\tPrec@1 86.719 (89.773)\tPrec@5 100.000 (99.787)\n",
      "Epoch: [195][20/97], lr: 0.00000\tTime 0.317 (0.418)\tData 0.000 (0.055)\tLoss 2.8782 (2.1907)\tPrec@1 92.969 (90.216)\tPrec@5 99.219 (99.591)\n",
      "Epoch: [195][30/97], lr: 0.00000\tTime 0.305 (0.387)\tData 0.000 (0.042)\tLoss 2.2965 (2.0431)\tPrec@1 92.969 (90.675)\tPrec@5 99.219 (99.622)\n",
      "Epoch: [195][40/97], lr: 0.00000\tTime 0.297 (0.369)\tData 0.000 (0.036)\tLoss 2.2484 (2.2796)\tPrec@1 94.531 (90.587)\tPrec@5 99.219 (99.600)\n",
      "Epoch: [195][50/97], lr: 0.00000\tTime 0.308 (0.358)\tData 0.000 (0.032)\tLoss 0.9202 (2.2545)\tPrec@1 94.531 (90.656)\tPrec@5 100.000 (99.556)\n",
      "Epoch: [195][60/97], lr: 0.00000\tTime 0.313 (0.353)\tData 0.000 (0.030)\tLoss 2.4775 (2.1592)\tPrec@1 93.750 (90.881)\tPrec@5 100.000 (99.526)\n",
      "Epoch: [195][70/97], lr: 0.00000\tTime 0.307 (0.348)\tData 0.000 (0.028)\tLoss 1.2788 (2.0962)\tPrec@1 90.625 (90.966)\tPrec@5 99.219 (99.494)\n",
      "Epoch: [195][80/97], lr: 0.00000\tTime 0.295 (0.342)\tData 0.000 (0.026)\tLoss 1.5446 (2.0996)\tPrec@1 87.500 (90.644)\tPrec@5 99.219 (99.460)\n",
      "Epoch: [195][90/97], lr: 0.00000\tTime 0.297 (0.338)\tData 0.000 (0.025)\tLoss 5.2888 (2.1014)\tPrec@1 91.406 (90.762)\tPrec@5 100.000 (99.459)\n",
      "Epoch: [195][96/97], lr: 0.00000\tTime 0.304 (0.336)\tData 0.000 (0.025)\tLoss 3.0275 (2.0621)\tPrec@1 87.288 (90.795)\tPrec@5 99.153 (99.476)\n",
      "Test: [0/100]\tTime 0.353 (0.353)\tLoss 2.5151 (2.5151)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.098)\tLoss 5.5789 (6.4578)\tPrec@1 84.000 (82.000)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.086)\tLoss 5.9488 (6.3782)\tPrec@1 79.000 (81.952)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.074 (0.082)\tLoss 4.6880 (6.3692)\tPrec@1 85.000 (82.065)\tPrec@5 99.000 (98.355)\n",
      "Test: [40/100]\tTime 0.075 (0.081)\tLoss 5.6926 (6.5449)\tPrec@1 84.000 (81.878)\tPrec@5 98.000 (98.268)\n",
      "Test: [50/100]\tTime 0.075 (0.080)\tLoss 7.1075 (6.5608)\tPrec@1 81.000 (82.255)\tPrec@5 99.000 (98.294)\n",
      "Test: [60/100]\tTime 0.075 (0.079)\tLoss 8.0425 (6.5015)\tPrec@1 83.000 (82.279)\tPrec@5 99.000 (98.361)\n",
      "Test: [70/100]\tTime 0.075 (0.079)\tLoss 3.5671 (6.5044)\tPrec@1 85.000 (82.296)\tPrec@5 99.000 (98.394)\n",
      "Test: [80/100]\tTime 0.074 (0.079)\tLoss 8.3292 (6.4851)\tPrec@1 81.000 (82.457)\tPrec@5 97.000 (98.481)\n",
      "Test: [90/100]\tTime 0.074 (0.078)\tLoss 5.7984 (6.4985)\tPrec@1 85.000 (82.198)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 82.000 Prec@5 98.490 Loss 6.53861\n",
      "val Class Accuracy: [0.935,0.978,0.819,0.770,0.843,0.753,0.847,0.780,0.768,0.707]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [196][0/97], lr: 0.00000\tTime 0.526 (0.526)\tData 0.330 (0.330)\tLoss 2.7199 (2.7199)\tPrec@1 88.281 (88.281)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [196][10/97], lr: 0.00000\tTime 0.296 (0.331)\tData 0.000 (0.044)\tLoss 1.7757 (1.8475)\tPrec@1 91.406 (90.696)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [196][20/97], lr: 0.00000\tTime 0.304 (0.317)\tData 0.000 (0.031)\tLoss 1.0129 (1.8677)\tPrec@1 92.188 (90.551)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [196][30/97], lr: 0.00000\tTime 0.302 (0.314)\tData 0.000 (0.027)\tLoss 1.2292 (1.9072)\tPrec@1 93.750 (90.801)\tPrec@5 100.000 (99.647)\n",
      "Epoch: [196][40/97], lr: 0.00000\tTime 0.296 (0.313)\tData 0.000 (0.024)\tLoss 2.9172 (2.0197)\tPrec@1 90.625 (90.835)\tPrec@5 100.000 (99.619)\n",
      "Epoch: [196][50/97], lr: 0.00000\tTime 0.295 (0.312)\tData 0.000 (0.023)\tLoss 0.9824 (1.9925)\tPrec@1 91.406 (90.656)\tPrec@5 99.219 (99.571)\n",
      "Epoch: [196][60/97], lr: 0.00000\tTime 0.301 (0.311)\tData 0.000 (0.022)\tLoss 2.1152 (1.9987)\tPrec@1 90.625 (90.817)\tPrec@5 100.000 (99.590)\n",
      "Epoch: [196][70/97], lr: 0.00000\tTime 0.311 (0.313)\tData 0.000 (0.021)\tLoss 1.5703 (2.0625)\tPrec@1 87.500 (90.757)\tPrec@5 99.219 (99.615)\n",
      "Epoch: [196][80/97], lr: 0.00000\tTime 0.302 (0.312)\tData 0.000 (0.021)\tLoss 1.8405 (2.0724)\tPrec@1 89.844 (90.654)\tPrec@5 100.000 (99.614)\n",
      "Epoch: [196][90/97], lr: 0.00000\tTime 0.300 (0.311)\tData 0.000 (0.020)\tLoss 0.7873 (2.0686)\tPrec@1 93.750 (90.762)\tPrec@5 99.219 (99.614)\n",
      "Epoch: [196][96/97], lr: 0.00000\tTime 0.291 (0.311)\tData 0.000 (0.021)\tLoss 1.6978 (2.0753)\tPrec@1 94.068 (90.811)\tPrec@5 100.000 (99.597)\n",
      "Test: [0/100]\tTime 0.328 (0.328)\tLoss 2.4310 (2.4310)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.096)\tLoss 5.4001 (6.3268)\tPrec@1 83.000 (82.182)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 5.8778 (6.2726)\tPrec@1 79.000 (82.000)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 4.5635 (6.2692)\tPrec@1 85.000 (82.194)\tPrec@5 99.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 5.5503 (6.4419)\tPrec@1 84.000 (81.927)\tPrec@5 98.000 (98.220)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 7.1113 (6.4488)\tPrec@1 82.000 (82.275)\tPrec@5 99.000 (98.255)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.0178 (6.3918)\tPrec@1 84.000 (82.328)\tPrec@5 99.000 (98.311)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.3710 (6.3899)\tPrec@1 85.000 (82.324)\tPrec@5 99.000 (98.352)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 8.1817 (6.3657)\tPrec@1 82.000 (82.519)\tPrec@5 97.000 (98.444)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 5.6782 (6.3768)\tPrec@1 86.000 (82.231)\tPrec@5 100.000 (98.429)\n",
      "val Results: Prec@1 82.050 Prec@5 98.470 Loss 6.41571\n",
      "val Class Accuracy: [0.932,0.976,0.837,0.761,0.840,0.748,0.845,0.771,0.773,0.722]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [197][0/97], lr: 0.00000\tTime 0.391 (0.391)\tData 0.230 (0.230)\tLoss 1.8151 (1.8151)\tPrec@1 94.531 (94.531)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [197][10/97], lr: 0.00000\tTime 0.296 (0.307)\tData 0.000 (0.036)\tLoss 3.0519 (2.3961)\tPrec@1 90.625 (91.335)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [197][20/97], lr: 0.00000\tTime 0.297 (0.302)\tData 0.000 (0.027)\tLoss 1.5256 (1.9399)\tPrec@1 85.938 (90.885)\tPrec@5 100.000 (99.628)\n",
      "Epoch: [197][30/97], lr: 0.00000\tTime 0.300 (0.301)\tData 0.000 (0.024)\tLoss 4.4848 (2.0786)\tPrec@1 89.844 (90.726)\tPrec@5 99.219 (99.597)\n",
      "Epoch: [197][40/97], lr: 0.00000\tTime 0.296 (0.301)\tData 0.000 (0.022)\tLoss 2.2938 (2.1408)\tPrec@1 89.844 (90.663)\tPrec@5 99.219 (99.581)\n",
      "Epoch: [197][50/97], lr: 0.00000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 1.5678 (2.1380)\tPrec@1 91.406 (90.579)\tPrec@5 100.000 (99.602)\n",
      "Epoch: [197][60/97], lr: 0.00000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 2.5023 (2.0945)\tPrec@1 89.062 (90.715)\tPrec@5 100.000 (99.654)\n",
      "Epoch: [197][70/97], lr: 0.00000\tTime 0.297 (0.300)\tData 0.000 (0.020)\tLoss 1.5512 (2.0073)\tPrec@1 89.062 (90.900)\tPrec@5 100.000 (99.648)\n",
      "Epoch: [197][80/97], lr: 0.00000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 4.5365 (2.0971)\tPrec@1 86.719 (90.818)\tPrec@5 97.656 (99.605)\n",
      "Epoch: [197][90/97], lr: 0.00000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 3.2635 (2.0778)\tPrec@1 87.500 (90.797)\tPrec@5 99.219 (99.588)\n",
      "Epoch: [197][96/97], lr: 0.00000\tTime 0.290 (0.300)\tData 0.000 (0.020)\tLoss 3.0962 (2.1032)\tPrec@1 87.288 (90.714)\tPrec@5 99.153 (99.573)\n",
      "Test: [0/100]\tTime 0.261 (0.261)\tLoss 2.3971 (2.3971)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 5.2976 (6.2350)\tPrec@1 82.000 (81.818)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.8401 (6.1881)\tPrec@1 78.000 (81.857)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.4218 (6.1793)\tPrec@1 85.000 (82.161)\tPrec@5 99.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.4320 (6.3506)\tPrec@1 84.000 (82.024)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.072 (0.076)\tLoss 7.0060 (6.3533)\tPrec@1 82.000 (82.412)\tPrec@5 99.000 (98.294)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.9648 (6.3001)\tPrec@1 84.000 (82.426)\tPrec@5 99.000 (98.344)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.3438 (6.2967)\tPrec@1 85.000 (82.394)\tPrec@5 99.000 (98.394)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.0653 (6.2689)\tPrec@1 83.000 (82.593)\tPrec@5 97.000 (98.481)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.6005 (6.2772)\tPrec@1 85.000 (82.308)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 82.140 Prec@5 98.500 Loss 6.31451\n",
      "val Class Accuracy: [0.932,0.976,0.825,0.767,0.847,0.742,0.852,0.772,0.775,0.726]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [198][0/97], lr: 0.00000\tTime 0.419 (0.419)\tData 0.260 (0.260)\tLoss 3.3068 (3.3068)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [198][10/97], lr: 0.00000\tTime 0.297 (0.308)\tData 0.000 (0.039)\tLoss 3.0217 (2.5432)\tPrec@1 90.625 (90.980)\tPrec@5 100.000 (99.716)\n",
      "Epoch: [198][20/97], lr: 0.00000\tTime 0.298 (0.303)\tData 0.000 (0.028)\tLoss 2.0606 (2.3841)\tPrec@1 93.750 (90.625)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [198][30/97], lr: 0.00000\tTime 0.291 (0.301)\tData 0.000 (0.025)\tLoss 0.8814 (2.2191)\tPrec@1 92.969 (90.902)\tPrec@5 99.219 (99.420)\n",
      "Epoch: [198][40/97], lr: 0.00000\tTime 0.296 (0.301)\tData 0.000 (0.023)\tLoss 3.3202 (2.0877)\tPrec@1 89.062 (91.025)\tPrec@5 99.219 (99.466)\n",
      "Epoch: [198][50/97], lr: 0.00000\tTime 0.295 (0.300)\tData 0.000 (0.022)\tLoss 1.8852 (2.0652)\tPrec@1 91.406 (91.222)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [198][60/97], lr: 0.00000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 2.4275 (2.1023)\tPrec@1 93.750 (91.253)\tPrec@5 100.000 (99.475)\n",
      "Epoch: [198][70/97], lr: 0.00000\tTime 0.296 (0.300)\tData 0.000 (0.021)\tLoss 0.6534 (2.0973)\tPrec@1 96.094 (91.230)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [198][80/97], lr: 0.00000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 1.5860 (2.0696)\tPrec@1 92.969 (91.127)\tPrec@5 100.000 (99.537)\n",
      "Epoch: [198][90/97], lr: 0.00000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.1185 (2.0474)\tPrec@1 89.062 (91.123)\tPrec@5 100.000 (99.528)\n",
      "Epoch: [198][96/97], lr: 0.00000\tTime 0.279 (0.299)\tData 0.000 (0.020)\tLoss 1.1635 (2.0311)\tPrec@1 88.983 (91.045)\tPrec@5 100.000 (99.508)\n",
      "Test: [0/100]\tTime 0.277 (0.277)\tLoss 2.5859 (2.5859)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 5.4271 (6.5114)\tPrec@1 83.000 (81.636)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.0101 (6.4324)\tPrec@1 79.000 (81.905)\tPrec@5 97.000 (98.429)\n",
      "Test: [30/100]\tTime 0.072 (0.079)\tLoss 4.7121 (6.4276)\tPrec@1 85.000 (82.129)\tPrec@5 99.000 (98.419)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 5.6868 (6.5991)\tPrec@1 85.000 (82.000)\tPrec@5 98.000 (98.293)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.9980 (6.6128)\tPrec@1 82.000 (82.255)\tPrec@5 99.000 (98.314)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.0694 (6.5497)\tPrec@1 82.000 (82.279)\tPrec@5 99.000 (98.377)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 3.6572 (6.5513)\tPrec@1 85.000 (82.296)\tPrec@5 99.000 (98.408)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.3236 (6.5277)\tPrec@1 83.000 (82.444)\tPrec@5 97.000 (98.469)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.8526 (6.5444)\tPrec@1 85.000 (82.154)\tPrec@5 100.000 (98.451)\n",
      "val Results: Prec@1 81.960 Prec@5 98.480 Loss 6.58590\n",
      "val Class Accuracy: [0.937,0.978,0.822,0.769,0.849,0.740,0.858,0.777,0.755,0.711]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [199][0/97], lr: 0.00000\tTime 0.390 (0.390)\tData 0.231 (0.231)\tLoss 1.9987 (1.9987)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [199][10/97], lr: 0.00000\tTime 0.299 (0.310)\tData 0.000 (0.036)\tLoss 2.2127 (2.2637)\tPrec@1 86.719 (90.838)\tPrec@5 99.219 (98.935)\n",
      "Epoch: [199][20/97], lr: 0.00000\tTime 0.296 (0.304)\tData 0.000 (0.027)\tLoss 0.8034 (2.1535)\tPrec@1 94.531 (91.146)\tPrec@5 99.219 (99.033)\n",
      "Epoch: [199][30/97], lr: 0.00000\tTime 0.299 (0.303)\tData 0.000 (0.024)\tLoss 2.5132 (2.2825)\tPrec@1 92.188 (90.953)\tPrec@5 100.000 (99.244)\n",
      "Epoch: [199][40/97], lr: 0.00000\tTime 0.306 (0.302)\tData 0.000 (0.022)\tLoss 4.3656 (2.1864)\tPrec@1 93.750 (91.178)\tPrec@5 100.000 (99.409)\n",
      "Epoch: [199][50/97], lr: 0.00000\tTime 0.296 (0.302)\tData 0.000 (0.021)\tLoss 3.0478 (2.1343)\tPrec@1 89.062 (91.376)\tPrec@5 99.219 (99.449)\n",
      "Epoch: [199][60/97], lr: 0.00000\tTime 0.298 (0.302)\tData 0.000 (0.021)\tLoss 1.6758 (2.0870)\tPrec@1 92.188 (91.457)\tPrec@5 99.219 (99.462)\n",
      "Epoch: [199][70/97], lr: 0.00000\tTime 0.293 (0.301)\tData 0.000 (0.020)\tLoss 1.7857 (2.0369)\tPrec@1 95.312 (91.373)\tPrec@5 100.000 (99.505)\n",
      "Epoch: [199][80/97], lr: 0.00000\tTime 0.299 (0.301)\tData 0.000 (0.020)\tLoss 3.8350 (2.1631)\tPrec@1 85.156 (91.165)\tPrec@5 98.438 (99.508)\n",
      "Epoch: [199][90/97], lr: 0.00000\tTime 0.297 (0.301)\tData 0.000 (0.020)\tLoss 2.1752 (2.1485)\tPrec@1 88.281 (90.934)\tPrec@5 98.438 (99.485)\n",
      "Epoch: [199][96/97], lr: 0.00000\tTime 0.292 (0.301)\tData 0.000 (0.020)\tLoss 1.3535 (2.0928)\tPrec@1 87.288 (90.956)\tPrec@5 100.000 (99.492)\n",
      "Test: [0/100]\tTime 0.239 (0.239)\tLoss 2.4484 (2.4484)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 5.4973 (6.3794)\tPrec@1 82.000 (81.818)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.7893 (6.2989)\tPrec@1 79.000 (81.905)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.5157 (6.2884)\tPrec@1 84.000 (81.968)\tPrec@5 99.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.5231 (6.4572)\tPrec@1 84.000 (81.805)\tPrec@5 98.000 (98.268)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.0229 (6.4644)\tPrec@1 81.000 (82.196)\tPrec@5 99.000 (98.294)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 8.0695 (6.3956)\tPrec@1 81.000 (82.164)\tPrec@5 99.000 (98.361)\n",
      "Test: [70/100]\tTime 0.072 (0.075)\tLoss 3.4462 (6.3969)\tPrec@1 85.000 (82.254)\tPrec@5 99.000 (98.394)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.2138 (6.3743)\tPrec@1 82.000 (82.395)\tPrec@5 97.000 (98.481)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 5.7523 (6.3874)\tPrec@1 85.000 (82.143)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 82.010 Prec@5 98.490 Loss 6.42363\n",
      "val Class Accuracy: [0.933,0.978,0.822,0.754,0.831,0.760,0.849,0.792,0.767,0.715]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [200][0/97], lr: 0.00000\tTime 0.400 (0.400)\tData 0.259 (0.259)\tLoss 1.2204 (1.2204)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [200][10/97], lr: 0.00000\tTime 0.296 (0.308)\tData 0.000 (0.039)\tLoss 1.3852 (2.2556)\tPrec@1 92.969 (90.270)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [200][20/97], lr: 0.00000\tTime 0.297 (0.303)\tData 0.000 (0.028)\tLoss 2.8596 (2.1566)\tPrec@1 87.500 (90.923)\tPrec@5 96.094 (99.293)\n",
      "Epoch: [200][30/97], lr: 0.00000\tTime 0.296 (0.303)\tData 0.000 (0.025)\tLoss 2.0324 (2.0938)\tPrec@1 86.719 (90.978)\tPrec@5 100.000 (99.446)\n",
      "Epoch: [200][40/97], lr: 0.00000\tTime 0.297 (0.302)\tData 0.000 (0.023)\tLoss 2.3784 (2.2580)\tPrec@1 92.969 (90.854)\tPrec@5 99.219 (99.466)\n",
      "Epoch: [200][50/97], lr: 0.00000\tTime 0.295 (0.301)\tData 0.000 (0.022)\tLoss 2.8305 (2.2901)\tPrec@1 88.281 (90.671)\tPrec@5 99.219 (99.387)\n",
      "Epoch: [200][60/97], lr: 0.00000\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 1.6595 (2.2933)\tPrec@1 92.188 (90.599)\tPrec@5 100.000 (99.360)\n",
      "Epoch: [200][70/97], lr: 0.00000\tTime 0.292 (0.300)\tData 0.000 (0.021)\tLoss 2.7648 (2.2728)\tPrec@1 86.719 (90.559)\tPrec@5 98.438 (99.362)\n",
      "Epoch: [200][80/97], lr: 0.00000\tTime 0.302 (0.301)\tData 0.000 (0.020)\tLoss 1.0966 (2.2124)\tPrec@1 90.625 (90.664)\tPrec@5 98.438 (99.354)\n",
      "Epoch: [200][90/97], lr: 0.00000\tTime 0.303 (0.301)\tData 0.000 (0.020)\tLoss 1.0147 (2.1754)\tPrec@1 90.625 (90.745)\tPrec@5 100.000 (99.390)\n",
      "Epoch: [200][96/97], lr: 0.00000\tTime 0.289 (0.301)\tData 0.000 (0.020)\tLoss 3.1154 (2.1809)\tPrec@1 89.831 (90.803)\tPrec@5 100.000 (99.428)\n",
      "Test: [0/100]\tTime 0.237 (0.237)\tLoss 2.4603 (2.4603)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 5.4375 (6.3826)\tPrec@1 83.000 (82.000)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.072 (0.080)\tLoss 5.8892 (6.3183)\tPrec@1 80.000 (82.048)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 4.6074 (6.3104)\tPrec@1 85.000 (82.161)\tPrec@5 99.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.5818 (6.4832)\tPrec@1 84.000 (81.976)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.0108 (6.4884)\tPrec@1 81.000 (82.333)\tPrec@5 99.000 (98.294)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 8.0452 (6.4242)\tPrec@1 82.000 (82.344)\tPrec@5 99.000 (98.361)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.5097 (6.4254)\tPrec@1 85.000 (82.324)\tPrec@5 99.000 (98.394)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.2760 (6.4051)\tPrec@1 82.000 (82.531)\tPrec@5 97.000 (98.481)\n",
      "Test: [90/100]\tTime 0.073 (0.074)\tLoss 5.7853 (6.4192)\tPrec@1 85.000 (82.242)\tPrec@5 100.000 (98.462)\n",
      "val Results: Prec@1 82.100 Prec@5 98.490 Loss 6.45775\n",
      "val Class Accuracy: [0.934,0.978,0.826,0.756,0.843,0.751,0.852,0.786,0.770,0.714]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [201][0/97], lr: 0.00000\tTime 0.347 (0.347)\tData 0.210 (0.210)\tLoss 1.9802 (1.9802)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [201][10/97], lr: 0.00000\tTime 0.300 (0.306)\tData 0.000 (0.034)\tLoss 3.7481 (2.2879)\tPrec@1 86.719 (89.844)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [201][20/97], lr: 0.00000\tTime 0.297 (0.303)\tData 0.000 (0.026)\tLoss 1.2872 (2.0931)\tPrec@1 92.188 (90.997)\tPrec@5 98.438 (99.628)\n",
      "Epoch: [201][30/97], lr: 0.00000\tTime 0.297 (0.301)\tData 0.000 (0.023)\tLoss 2.4493 (1.9957)\tPrec@1 91.406 (91.053)\tPrec@5 100.000 (99.471)\n",
      "Epoch: [201][40/97], lr: 0.00000\tTime 0.299 (0.301)\tData 0.000 (0.022)\tLoss 1.6207 (1.9488)\tPrec@1 92.188 (91.254)\tPrec@5 100.000 (99.524)\n",
      "Epoch: [201][50/97], lr: 0.00000\tTime 0.297 (0.301)\tData 0.000 (0.021)\tLoss 2.4584 (2.0837)\tPrec@1 88.281 (90.947)\tPrec@5 99.219 (99.556)\n",
      "Epoch: [201][60/97], lr: 0.00000\tTime 0.297 (0.301)\tData 0.000 (0.020)\tLoss 4.2561 (2.1354)\tPrec@1 85.156 (90.715)\tPrec@5 98.438 (99.552)\n",
      "Epoch: [201][70/97], lr: 0.00000\tTime 0.295 (0.301)\tData 0.000 (0.020)\tLoss 2.6256 (2.0877)\tPrec@1 88.281 (90.746)\tPrec@5 100.000 (99.538)\n",
      "Epoch: [201][80/97], lr: 0.00000\tTime 0.297 (0.301)\tData 0.000 (0.020)\tLoss 1.7209 (2.1103)\tPrec@1 92.188 (90.828)\tPrec@5 99.219 (99.556)\n",
      "Epoch: [201][90/97], lr: 0.00000\tTime 0.318 (0.301)\tData 0.000 (0.019)\tLoss 2.5147 (2.1616)\tPrec@1 87.500 (90.891)\tPrec@5 100.000 (99.588)\n",
      "Epoch: [201][96/97], lr: 0.00000\tTime 0.290 (0.301)\tData 0.000 (0.020)\tLoss 2.3939 (2.1655)\tPrec@1 92.373 (90.932)\tPrec@5 99.153 (99.549)\n",
      "Test: [0/100]\tTime 0.244 (0.244)\tLoss 2.4794 (2.4794)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 5.4807 (6.4405)\tPrec@1 83.000 (81.636)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.8915 (6.3674)\tPrec@1 79.000 (82.000)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.6080 (6.3658)\tPrec@1 85.000 (82.097)\tPrec@5 99.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.6216 (6.5424)\tPrec@1 85.000 (81.902)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.0990 (6.5497)\tPrec@1 81.000 (82.235)\tPrec@5 99.000 (98.294)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.9931 (6.4867)\tPrec@1 82.000 (82.279)\tPrec@5 99.000 (98.344)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.5324 (6.4850)\tPrec@1 85.000 (82.282)\tPrec@5 99.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.3105 (6.4589)\tPrec@1 82.000 (82.506)\tPrec@5 97.000 (98.457)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.8374 (6.4730)\tPrec@1 85.000 (82.253)\tPrec@5 100.000 (98.440)\n",
      "val Results: Prec@1 82.090 Prec@5 98.470 Loss 6.51049\n",
      "val Class Accuracy: [0.931,0.977,0.824,0.768,0.852,0.748,0.849,0.771,0.773,0.716]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [202][0/97], lr: 0.00000\tTime 0.366 (0.366)\tData 0.222 (0.222)\tLoss 1.3366 (1.3366)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [202][10/97], lr: 0.00000\tTime 0.295 (0.306)\tData 0.000 (0.035)\tLoss 1.3446 (1.6973)\tPrec@1 90.625 (90.057)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [202][20/97], lr: 0.00000\tTime 0.295 (0.301)\tData 0.000 (0.027)\tLoss 2.4463 (1.8801)\tPrec@1 91.406 (90.513)\tPrec@5 100.000 (99.591)\n",
      "Epoch: [202][30/97], lr: 0.00000\tTime 0.297 (0.301)\tData 0.000 (0.024)\tLoss 1.4221 (1.7914)\tPrec@1 87.500 (90.171)\tPrec@5 98.438 (99.471)\n",
      "Epoch: [202][40/97], lr: 0.00000\tTime 0.300 (0.300)\tData 0.000 (0.022)\tLoss 3.0558 (1.8642)\tPrec@1 92.188 (90.396)\tPrec@5 100.000 (99.505)\n",
      "Epoch: [202][50/97], lr: 0.00000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 3.0227 (1.9122)\tPrec@1 88.281 (90.625)\tPrec@5 99.219 (99.494)\n",
      "Epoch: [202][60/97], lr: 0.00000\tTime 0.299 (0.300)\tData 0.000 (0.020)\tLoss 1.7590 (1.9777)\tPrec@1 89.062 (90.497)\tPrec@5 100.000 (99.513)\n",
      "Epoch: [202][70/97], lr: 0.00000\tTime 0.292 (0.299)\tData 0.000 (0.020)\tLoss 1.8248 (2.0174)\tPrec@1 92.188 (90.405)\tPrec@5 100.000 (99.549)\n",
      "Epoch: [202][80/97], lr: 0.00000\tTime 0.301 (0.299)\tData 0.000 (0.020)\tLoss 0.8460 (2.0482)\tPrec@1 94.531 (90.500)\tPrec@5 100.000 (99.508)\n",
      "Epoch: [202][90/97], lr: 0.00000\tTime 0.304 (0.299)\tData 0.000 (0.019)\tLoss 1.8527 (2.1075)\tPrec@1 92.188 (90.325)\tPrec@5 98.438 (99.451)\n",
      "Epoch: [202][96/97], lr: 0.00000\tTime 0.288 (0.299)\tData 0.000 (0.020)\tLoss 3.7332 (2.1587)\tPrec@1 88.136 (90.311)\tPrec@5 100.000 (99.468)\n",
      "Test: [0/100]\tTime 0.317 (0.317)\tLoss 2.4149 (2.4149)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.095)\tLoss 5.4001 (6.3713)\tPrec@1 83.000 (82.091)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 5.9057 (6.3054)\tPrec@1 80.000 (82.048)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 4.5103 (6.2991)\tPrec@1 85.000 (82.323)\tPrec@5 99.000 (98.290)\n",
      "Test: [40/100]\tTime 0.072 (0.079)\tLoss 5.6800 (6.4799)\tPrec@1 83.000 (82.049)\tPrec@5 98.000 (98.220)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.0705 (6.4945)\tPrec@1 81.000 (82.392)\tPrec@5 99.000 (98.275)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 7.9808 (6.4362)\tPrec@1 84.000 (82.426)\tPrec@5 99.000 (98.328)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.4869 (6.4398)\tPrec@1 85.000 (82.310)\tPrec@5 99.000 (98.366)\n",
      "Test: [80/100]\tTime 0.072 (0.076)\tLoss 8.2639 (6.4200)\tPrec@1 82.000 (82.469)\tPrec@5 97.000 (98.457)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.7548 (6.4309)\tPrec@1 85.000 (82.165)\tPrec@5 100.000 (98.440)\n",
      "val Results: Prec@1 81.990 Prec@5 98.480 Loss 6.47150\n",
      "val Class Accuracy: [0.935,0.977,0.828,0.760,0.842,0.746,0.852,0.776,0.774,0.709]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [203][0/97], lr: 0.00000\tTime 0.334 (0.334)\tData 0.191 (0.191)\tLoss 2.3537 (2.3537)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [203][10/97], lr: 0.00000\tTime 0.295 (0.301)\tData 0.000 (0.032)\tLoss 3.1962 (1.8973)\tPrec@1 89.062 (90.696)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [203][20/97], lr: 0.00000\tTime 0.299 (0.300)\tData 0.000 (0.025)\tLoss 1.4765 (2.0112)\tPrec@1 96.094 (91.369)\tPrec@5 100.000 (99.330)\n",
      "Epoch: [203][30/97], lr: 0.00000\tTime 0.300 (0.300)\tData 0.000 (0.022)\tLoss 4.0975 (2.2817)\tPrec@1 88.281 (90.575)\tPrec@5 99.219 (99.320)\n",
      "Epoch: [203][40/97], lr: 0.00000\tTime 0.293 (0.300)\tData 0.000 (0.021)\tLoss 3.7299 (2.2343)\tPrec@1 89.062 (91.006)\tPrec@5 100.000 (99.352)\n",
      "Epoch: [203][50/97], lr: 0.00000\tTime 0.298 (0.300)\tData 0.000 (0.020)\tLoss 1.9051 (2.1301)\tPrec@1 91.406 (91.115)\tPrec@5 99.219 (99.418)\n",
      "Epoch: [203][60/97], lr: 0.00000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.9231 (2.0855)\tPrec@1 92.188 (91.176)\tPrec@5 100.000 (99.462)\n",
      "Epoch: [203][70/97], lr: 0.00000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.6426 (2.0329)\tPrec@1 87.500 (91.197)\tPrec@5 99.219 (99.472)\n",
      "Epoch: [203][80/97], lr: 0.00000\tTime 0.298 (0.299)\tData 0.000 (0.019)\tLoss 4.3384 (2.0990)\tPrec@1 91.406 (91.194)\tPrec@5 100.000 (99.470)\n",
      "Epoch: [203][90/97], lr: 0.00000\tTime 0.305 (0.299)\tData 0.000 (0.019)\tLoss 1.3150 (2.0961)\tPrec@1 89.844 (91.089)\tPrec@5 99.219 (99.468)\n",
      "Epoch: [203][96/97], lr: 0.00000\tTime 0.291 (0.299)\tData 0.000 (0.020)\tLoss 2.4708 (2.1061)\tPrec@1 85.593 (90.924)\tPrec@5 100.000 (99.492)\n",
      "Test: [0/100]\tTime 0.261 (0.261)\tLoss 2.3567 (2.3567)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 5.3379 (6.1680)\tPrec@1 83.000 (82.091)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.072 (0.081)\tLoss 5.7522 (6.1107)\tPrec@1 79.000 (82.095)\tPrec@5 97.000 (98.238)\n",
      "Test: [30/100]\tTime 0.072 (0.078)\tLoss 4.3925 (6.0910)\tPrec@1 84.000 (82.290)\tPrec@5 99.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.3747 (6.2616)\tPrec@1 84.000 (82.073)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.0023 (6.2662)\tPrec@1 82.000 (82.471)\tPrec@5 99.000 (98.294)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.9857 (6.2089)\tPrec@1 82.000 (82.475)\tPrec@5 98.000 (98.328)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.2381 (6.2053)\tPrec@1 85.000 (82.521)\tPrec@5 99.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.9697 (6.1825)\tPrec@1 83.000 (82.704)\tPrec@5 97.000 (98.469)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.5973 (6.1928)\tPrec@1 85.000 (82.473)\tPrec@5 100.000 (98.451)\n",
      "val Results: Prec@1 82.330 Prec@5 98.500 Loss 6.22666\n",
      "val Class Accuracy: [0.930,0.978,0.826,0.758,0.837,0.758,0.850,0.789,0.771,0.736]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [204][0/97], lr: 0.00000\tTime 0.370 (0.370)\tData 0.231 (0.231)\tLoss 5.7532 (5.7532)\tPrec@1 92.188 (92.188)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [204][10/97], lr: 0.00000\tTime 0.299 (0.310)\tData 0.000 (0.036)\tLoss 2.0359 (1.8727)\tPrec@1 89.062 (91.335)\tPrec@5 98.438 (99.361)\n",
      "Epoch: [204][20/97], lr: 0.00000\tTime 0.297 (0.305)\tData 0.000 (0.027)\tLoss 3.0862 (1.9359)\tPrec@1 91.406 (91.555)\tPrec@5 98.438 (99.479)\n",
      "Epoch: [204][30/97], lr: 0.00000\tTime 0.296 (0.303)\tData 0.000 (0.024)\tLoss 1.8837 (1.9234)\tPrec@1 91.406 (91.507)\tPrec@5 98.438 (99.471)\n",
      "Epoch: [204][40/97], lr: 0.00000\tTime 0.290 (0.302)\tData 0.000 (0.022)\tLoss 6.3556 (2.0540)\tPrec@1 85.938 (91.025)\tPrec@5 100.000 (99.486)\n",
      "Epoch: [204][50/97], lr: 0.00000\tTime 0.296 (0.302)\tData 0.000 (0.021)\tLoss 1.3320 (2.0130)\tPrec@1 90.625 (91.008)\tPrec@5 99.219 (99.510)\n",
      "Epoch: [204][60/97], lr: 0.00000\tTime 0.296 (0.301)\tData 0.000 (0.021)\tLoss 1.1649 (2.0536)\tPrec@1 89.844 (90.804)\tPrec@5 99.219 (99.501)\n",
      "Epoch: [204][70/97], lr: 0.00000\tTime 0.296 (0.301)\tData 0.000 (0.020)\tLoss 1.9224 (1.9899)\tPrec@1 93.750 (91.021)\tPrec@5 99.219 (99.494)\n",
      "Epoch: [204][80/97], lr: 0.00000\tTime 0.298 (0.301)\tData 0.000 (0.020)\tLoss 2.0520 (1.9790)\tPrec@1 89.844 (91.040)\tPrec@5 99.219 (99.479)\n",
      "Epoch: [204][90/97], lr: 0.00000\tTime 0.296 (0.301)\tData 0.000 (0.020)\tLoss 3.0944 (1.9955)\tPrec@1 89.844 (91.003)\tPrec@5 99.219 (99.476)\n",
      "Epoch: [204][96/97], lr: 0.00000\tTime 0.291 (0.301)\tData 0.000 (0.020)\tLoss 1.9526 (1.9889)\tPrec@1 94.068 (91.053)\tPrec@5 99.153 (99.484)\n",
      "Test: [0/100]\tTime 0.245 (0.245)\tLoss 2.4606 (2.4606)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 5.4045 (6.3776)\tPrec@1 83.000 (82.182)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 5.9109 (6.3030)\tPrec@1 79.000 (82.095)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.5027 (6.2886)\tPrec@1 85.000 (82.355)\tPrec@5 99.000 (98.258)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.6410 (6.4625)\tPrec@1 84.000 (82.122)\tPrec@5 98.000 (98.220)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 6.9779 (6.4788)\tPrec@1 81.000 (82.275)\tPrec@5 99.000 (98.255)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.0926 (6.4144)\tPrec@1 82.000 (82.311)\tPrec@5 99.000 (98.295)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.5328 (6.4184)\tPrec@1 85.000 (82.296)\tPrec@5 99.000 (98.338)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.2646 (6.4003)\tPrec@1 82.000 (82.457)\tPrec@5 97.000 (98.432)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.7632 (6.4129)\tPrec@1 85.000 (82.176)\tPrec@5 100.000 (98.418)\n",
      "val Results: Prec@1 82.000 Prec@5 98.470 Loss 6.45111\n",
      "val Class Accuracy: [0.938,0.978,0.827,0.741,0.845,0.748,0.857,0.788,0.767,0.711]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [205][0/97], lr: 0.00000\tTime 0.373 (0.373)\tData 0.217 (0.217)\tLoss 2.1095 (2.1095)\tPrec@1 92.188 (92.188)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [205][10/97], lr: 0.00000\tTime 0.298 (0.306)\tData 0.000 (0.035)\tLoss 1.1284 (2.2429)\tPrec@1 92.969 (90.838)\tPrec@5 100.000 (99.716)\n",
      "Epoch: [205][20/97], lr: 0.00000\tTime 0.295 (0.302)\tData 0.000 (0.026)\tLoss 2.1325 (2.4200)\tPrec@1 92.969 (90.365)\tPrec@5 100.000 (99.777)\n",
      "Epoch: [205][30/97], lr: 0.00000\tTime 0.300 (0.301)\tData 0.000 (0.023)\tLoss 4.3970 (2.4217)\tPrec@1 86.719 (90.197)\tPrec@5 100.000 (99.723)\n",
      "Epoch: [205][40/97], lr: 0.00000\tTime 0.298 (0.301)\tData 0.000 (0.022)\tLoss 1.8920 (2.3973)\tPrec@1 86.719 (90.111)\tPrec@5 98.438 (99.638)\n",
      "Epoch: [205][50/97], lr: 0.00000\tTime 0.295 (0.300)\tData 0.000 (0.021)\tLoss 3.0191 (2.2323)\tPrec@1 91.406 (90.349)\tPrec@5 98.438 (99.632)\n",
      "Epoch: [205][60/97], lr: 0.00000\tTime 0.301 (0.300)\tData 0.000 (0.020)\tLoss 1.2406 (2.2203)\tPrec@1 88.281 (90.138)\tPrec@5 100.000 (99.603)\n",
      "Epoch: [205][70/97], lr: 0.00000\tTime 0.295 (0.300)\tData 0.000 (0.020)\tLoss 2.6246 (2.2488)\tPrec@1 88.281 (90.075)\tPrec@5 98.438 (99.615)\n",
      "Epoch: [205][80/97], lr: 0.00000\tTime 0.296 (0.300)\tData 0.000 (0.020)\tLoss 1.3022 (2.2467)\tPrec@1 94.531 (90.268)\tPrec@5 100.000 (99.605)\n",
      "Epoch: [205][90/97], lr: 0.00000\tTime 0.296 (0.299)\tData 0.000 (0.019)\tLoss 1.2871 (2.2231)\tPrec@1 93.750 (90.453)\tPrec@5 99.219 (99.614)\n",
      "Epoch: [205][96/97], lr: 0.00000\tTime 0.290 (0.299)\tData 0.000 (0.020)\tLoss 0.7625 (2.2331)\tPrec@1 95.763 (90.593)\tPrec@5 100.000 (99.621)\n",
      "Test: [0/100]\tTime 0.268 (0.268)\tLoss 2.6079 (2.6079)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 5.7707 (6.6342)\tPrec@1 85.000 (81.818)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.0565 (6.5304)\tPrec@1 80.000 (81.667)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.9254 (6.5139)\tPrec@1 84.000 (81.774)\tPrec@5 99.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.8750 (6.6874)\tPrec@1 84.000 (81.634)\tPrec@5 98.000 (98.220)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 7.2030 (6.7136)\tPrec@1 80.000 (81.922)\tPrec@5 99.000 (98.255)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.0820 (6.6517)\tPrec@1 83.000 (81.951)\tPrec@5 99.000 (98.311)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.6663 (6.6565)\tPrec@1 85.000 (82.014)\tPrec@5 99.000 (98.352)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.4992 (6.6401)\tPrec@1 81.000 (82.173)\tPrec@5 97.000 (98.444)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.8937 (6.6570)\tPrec@1 85.000 (81.912)\tPrec@5 100.000 (98.440)\n",
      "val Results: Prec@1 81.710 Prec@5 98.480 Loss 6.70088\n",
      "val Class Accuracy: [0.941,0.980,0.822,0.760,0.840,0.758,0.849,0.762,0.757,0.702]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [206][0/97], lr: 0.00000\tTime 0.376 (0.376)\tData 0.223 (0.223)\tLoss 1.2192 (1.2192)\tPrec@1 94.531 (94.531)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [206][10/97], lr: 0.00000\tTime 0.296 (0.308)\tData 0.000 (0.036)\tLoss 1.3698 (1.9542)\tPrec@1 90.625 (91.406)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [206][20/97], lr: 0.00000\tTime 0.300 (0.304)\tData 0.000 (0.027)\tLoss 2.7299 (2.1374)\tPrec@1 91.406 (90.439)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [206][30/97], lr: 0.00000\tTime 0.297 (0.302)\tData 0.000 (0.024)\tLoss 2.0038 (2.2229)\tPrec@1 92.188 (90.726)\tPrec@5 100.000 (99.546)\n",
      "Epoch: [206][40/97], lr: 0.00000\tTime 0.299 (0.302)\tData 0.000 (0.022)\tLoss 2.7391 (2.1034)\tPrec@1 86.719 (90.549)\tPrec@5 100.000 (99.600)\n",
      "Epoch: [206][50/97], lr: 0.00000\tTime 0.299 (0.301)\tData 0.000 (0.021)\tLoss 1.1592 (2.1433)\tPrec@1 92.969 (90.671)\tPrec@5 100.000 (99.602)\n",
      "Epoch: [206][60/97], lr: 0.00000\tTime 0.297 (0.301)\tData 0.000 (0.021)\tLoss 2.4603 (2.1482)\tPrec@1 91.406 (90.766)\tPrec@5 100.000 (99.577)\n",
      "Epoch: [206][70/97], lr: 0.00000\tTime 0.296 (0.301)\tData 0.000 (0.020)\tLoss 3.3468 (2.2041)\tPrec@1 91.406 (90.757)\tPrec@5 98.438 (99.604)\n",
      "Epoch: [206][80/97], lr: 0.00000\tTime 0.301 (0.301)\tData 0.000 (0.020)\tLoss 1.2430 (2.2072)\tPrec@1 91.406 (90.866)\tPrec@5 100.000 (99.605)\n",
      "Epoch: [206][90/97], lr: 0.00000\tTime 0.301 (0.301)\tData 0.000 (0.020)\tLoss 1.7170 (2.2224)\tPrec@1 89.062 (90.719)\tPrec@5 96.875 (99.571)\n",
      "Epoch: [206][96/97], lr: 0.00000\tTime 0.291 (0.300)\tData 0.000 (0.020)\tLoss 2.7742 (2.1877)\tPrec@1 87.288 (90.666)\tPrec@5 99.153 (99.532)\n",
      "Test: [0/100]\tTime 0.281 (0.281)\tLoss 2.3722 (2.3722)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 5.5078 (6.3634)\tPrec@1 82.000 (81.636)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 5.8506 (6.3004)\tPrec@1 79.000 (81.762)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.4694 (6.3043)\tPrec@1 84.000 (81.742)\tPrec@5 99.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 5.5262 (6.4781)\tPrec@1 85.000 (81.634)\tPrec@5 98.000 (98.220)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.1911 (6.4825)\tPrec@1 81.000 (82.078)\tPrec@5 99.000 (98.275)\n",
      "Test: [60/100]\tTime 0.072 (0.076)\tLoss 7.8389 (6.4187)\tPrec@1 83.000 (82.115)\tPrec@5 99.000 (98.328)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 3.4663 (6.4129)\tPrec@1 85.000 (82.169)\tPrec@5 99.000 (98.366)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.2117 (6.3824)\tPrec@1 83.000 (82.358)\tPrec@5 97.000 (98.444)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.7633 (6.3950)\tPrec@1 85.000 (82.132)\tPrec@5 100.000 (98.429)\n",
      "val Results: Prec@1 81.990 Prec@5 98.480 Loss 6.43318\n",
      "val Class Accuracy: [0.930,0.974,0.823,0.759,0.839,0.763,0.850,0.768,0.782,0.711]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [207][0/97], lr: 0.00000\tTime 0.344 (0.344)\tData 0.208 (0.208)\tLoss 1.3601 (1.3601)\tPrec@1 92.188 (92.188)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [207][10/97], lr: 0.00000\tTime 0.295 (0.302)\tData 0.000 (0.034)\tLoss 1.6759 (2.3489)\tPrec@1 90.625 (90.696)\tPrec@5 98.438 (99.148)\n",
      "Epoch: [207][20/97], lr: 0.00000\tTime 0.295 (0.300)\tData 0.000 (0.026)\tLoss 3.0963 (2.1382)\tPrec@1 90.625 (90.960)\tPrec@5 98.438 (99.368)\n",
      "Epoch: [207][30/97], lr: 0.00000\tTime 0.295 (0.299)\tData 0.000 (0.023)\tLoss 1.7595 (2.0813)\tPrec@1 90.625 (90.953)\tPrec@5 97.656 (99.345)\n",
      "Epoch: [207][40/97], lr: 0.00000\tTime 0.288 (0.299)\tData 0.000 (0.022)\tLoss 2.9727 (2.0814)\tPrec@1 90.625 (91.063)\tPrec@5 98.438 (99.352)\n",
      "Epoch: [207][50/97], lr: 0.00000\tTime 0.296 (0.299)\tData 0.000 (0.021)\tLoss 1.3802 (2.1515)\tPrec@1 91.406 (90.901)\tPrec@5 100.000 (99.372)\n",
      "Epoch: [207][60/97], lr: 0.00000\tTime 0.296 (0.299)\tData 0.000 (0.020)\tLoss 1.0902 (2.0719)\tPrec@1 91.406 (90.907)\tPrec@5 99.219 (99.398)\n",
      "Epoch: [207][70/97], lr: 0.00000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 1.6731 (2.1177)\tPrec@1 88.281 (90.834)\tPrec@5 98.438 (99.395)\n",
      "Epoch: [207][80/97], lr: 0.00000\tTime 0.295 (0.299)\tData 0.000 (0.020)\tLoss 2.3297 (2.1352)\tPrec@1 93.750 (90.885)\tPrec@5 100.000 (99.412)\n",
      "Epoch: [207][90/97], lr: 0.00000\tTime 0.294 (0.299)\tData 0.000 (0.019)\tLoss 2.1324 (2.1211)\tPrec@1 93.750 (90.917)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [207][96/97], lr: 0.00000\tTime 0.286 (0.299)\tData 0.000 (0.020)\tLoss 1.7856 (2.1429)\tPrec@1 89.831 (90.819)\tPrec@5 99.153 (99.444)\n",
      "Test: [0/100]\tTime 0.271 (0.271)\tLoss 2.4123 (2.4123)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 5.1521 (6.1215)\tPrec@1 84.000 (82.545)\tPrec@5 99.000 (98.636)\n",
      "Test: [20/100]\tTime 0.072 (0.082)\tLoss 5.8272 (6.0796)\tPrec@1 79.000 (82.333)\tPrec@5 97.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 4.4127 (6.0679)\tPrec@1 84.000 (82.419)\tPrec@5 99.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 5.3699 (6.2313)\tPrec@1 84.000 (82.244)\tPrec@5 98.000 (98.268)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.8681 (6.2358)\tPrec@1 82.000 (82.569)\tPrec@5 99.000 (98.314)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.7827 (6.1842)\tPrec@1 84.000 (82.557)\tPrec@5 99.000 (98.361)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 3.1132 (6.1743)\tPrec@1 84.000 (82.535)\tPrec@5 99.000 (98.408)\n",
      "Test: [80/100]\tTime 0.072 (0.075)\tLoss 7.8199 (6.1475)\tPrec@1 82.000 (82.704)\tPrec@5 97.000 (98.494)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 5.4968 (6.1590)\tPrec@1 85.000 (82.396)\tPrec@5 100.000 (98.473)\n",
      "val Results: Prec@1 82.190 Prec@5 98.520 Loss 6.19733\n",
      "val Class Accuracy: [0.938,0.977,0.820,0.767,0.840,0.743,0.854,0.769,0.765,0.746]\n",
      "Best Prec@1: 82.410\n",
      "\n",
      "Epoch: [208][0/97], lr: 0.00000\tTime 0.394 (0.394)\tData 0.250 (0.250)\tLoss 1.5214 (1.5214)\tPrec@1 92.969 (92.969)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [208][10/97], lr: 0.00000\tTime 0.299 (0.312)\tData 0.000 (0.038)\tLoss 1.1027 (1.6431)\tPrec@1 92.188 (90.909)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [208][20/97], lr: 0.00000\tTime 0.298 (0.306)\tData 0.000 (0.028)\tLoss 0.7397 (1.7414)\tPrec@1 92.188 (90.923)\tPrec@5 100.000 (99.665)\n",
      "Epoch: [208][30/97], lr: 0.00000\tTime 0.304 (0.304)\tData 0.000 (0.025)\tLoss 1.5640 (1.8548)\tPrec@1 91.406 (90.953)\tPrec@5 100.000 (99.546)\n",
      "Epoch: [208][40/97], lr: 0.00000\tTime 0.297 (0.304)\tData 0.000 (0.023)\tLoss 2.5585 (1.9699)\tPrec@1 91.406 (91.139)\tPrec@5 98.438 (99.543)\n",
      "Epoch: [208][50/97], lr: 0.00000\tTime 0.298 (0.303)\tData 0.000 (0.022)\tLoss 1.5206 (1.9272)\tPrec@1 90.625 (91.131)\tPrec@5 98.438 (99.540)\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"cifar_train_lorot-E.py\", line 672, in <module>\n",
      "    main()\n",
      "  File \"cifar_train_lorot-E.py\", line 181, in main\n",
      "    main_worker(args.gpu, ngpus_per_node, args)\n",
      "  File \"cifar_train_lorot-E.py\", line 361, in main_worker\n",
      "    train(\n",
      "  File \"cifar_train_lorot-E.py\", line 525, in train\n",
      "    loss.backward()\n",
      "  File \"/home/aristo/miniconda3/envs/pytorch-test/lib/python3.8/site-packages/torch/_tensor.py\", line 396, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/home/aristo/miniconda3/envs/pytorch-test/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 173, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python cifar_train_lorot-E.py --gpu 0 --imb_type exp --exp_str lorotldamdrw01 --imb_factor 0.01 --loss_type LDAM --train_rule DRW -m \"rot\" --epochs 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar_train_lorot-E.py:175: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.\n",
      "  warnings.warn(\n",
      "Use GPU: 0 for training\n",
      "=> creating model 'resnet32'\n",
      "num_trans : 16\n",
      "num_flipped : 4\n",
      "num shuffled channel : 24\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[5000, 2997, 1796, 1077, 645, 387, 232, 139, 83, 50]\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "/media/aristo/Data A/documents/kuliah/Project/Localizable-Rotation/Imbalanced/losses.py:49: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/TensorCompare.cpp:402.)\n",
      "  output = torch.where(index, x_m, x)\n",
      "Epoch: [0][0/97], lr: 0.00200\tTime 4.583 (4.583)\tData 0.581 (0.581)\tLoss 14.3182 (14.3182)\tPrec@1 1.562 (1.562)\tPrec@5 77.344 (77.344)\n",
      "Epoch: [0][10/97], lr: 0.00200\tTime 0.335 (0.704)\tData 0.000 (0.053)\tLoss 8.3075 (9.0869)\tPrec@1 50.000 (34.943)\tPrec@5 92.188 (91.477)\n",
      "Epoch: [0][20/97], lr: 0.00200\tTime 0.337 (0.530)\tData 0.000 (0.036)\tLoss 7.1701 (8.5205)\tPrec@1 52.344 (42.076)\tPrec@5 89.062 (90.699)\n",
      "Epoch: [0][30/97], lr: 0.00200\tTime 0.338 (0.469)\tData 0.000 (0.029)\tLoss 8.0005 (7.9753)\tPrec@1 52.344 (46.220)\tPrec@5 91.406 (91.179)\n",
      "Epoch: [0][40/97], lr: 0.00200\tTime 0.337 (0.436)\tData 0.000 (0.026)\tLoss 6.8829 (7.6063)\tPrec@1 53.906 (48.838)\tPrec@5 91.406 (91.521)\n",
      "Epoch: [0][50/97], lr: 0.00200\tTime 0.324 (0.416)\tData 0.000 (0.024)\tLoss 6.3699 (7.4320)\tPrec@1 52.344 (49.893)\tPrec@5 93.750 (91.544)\n",
      "Epoch: [0][60/97], lr: 0.00200\tTime 0.342 (0.403)\tData 0.000 (0.023)\tLoss 6.1978 (7.2021)\tPrec@1 60.156 (50.845)\tPrec@5 95.312 (91.957)\n",
      "Epoch: [0][70/97], lr: 0.00200\tTime 0.331 (0.393)\tData 0.000 (0.022)\tLoss 5.6422 (7.0314)\tPrec@1 55.469 (51.618)\tPrec@5 93.750 (92.188)\n",
      "Epoch: [0][80/97], lr: 0.00200\tTime 0.333 (0.386)\tData 0.000 (0.021)\tLoss 6.1218 (6.8882)\tPrec@1 56.250 (52.566)\tPrec@5 95.312 (92.438)\n",
      "Epoch: [0][90/97], lr: 0.00200\tTime 0.332 (0.381)\tData 0.000 (0.021)\tLoss 4.8739 (6.7604)\tPrec@1 66.406 (53.468)\tPrec@5 97.656 (92.531)\n",
      "Epoch: [0][96/97], lr: 0.00200\tTime 0.922 (0.384)\tData 0.000 (0.021)\tLoss 5.6947 (6.6880)\tPrec@1 57.627 (53.845)\tPrec@5 92.373 (92.633)\n",
      "Gated Network Weight Gate= Flip:0.97, Sc:0.03\n",
      "Test: [0/100]\tTime 0.359 (0.359)\tLoss 16.3803 (16.3803)\tPrec@1 16.000 (16.000)\tPrec@5 70.000 (70.000)\n",
      "Test: [10/100]\tTime 0.073 (0.099)\tLoss 15.5782 (15.2313)\tPrec@1 25.000 (20.364)\tPrec@5 66.000 (69.455)\n",
      "Test: [20/100]\tTime 0.073 (0.087)\tLoss 13.3955 (14.9939)\tPrec@1 27.000 (21.238)\tPrec@5 81.000 (70.952)\n",
      "Test: [30/100]\tTime 0.073 (0.082)\tLoss 14.1161 (14.8577)\tPrec@1 23.000 (21.645)\tPrec@5 74.000 (71.194)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 15.8552 (14.8690)\tPrec@1 20.000 (21.634)\tPrec@5 67.000 (70.829)\n",
      "Test: [50/100]\tTime 0.073 (0.079)\tLoss 14.7862 (14.7680)\tPrec@1 17.000 (21.608)\tPrec@5 70.000 (71.353)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 13.1753 (14.7749)\tPrec@1 25.000 (21.557)\tPrec@5 77.000 (70.951)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 14.2431 (14.7335)\tPrec@1 23.000 (21.662)\tPrec@5 64.000 (70.831)\n",
      "Test: [80/100]\tTime 0.073 (0.077)\tLoss 13.3591 (14.6664)\tPrec@1 28.000 (21.975)\tPrec@5 73.000 (71.086)\n",
      "Test: [90/100]\tTime 0.075 (0.076)\tLoss 14.0404 (14.7367)\tPrec@1 27.000 (21.626)\tPrec@5 76.000 (70.934)\n",
      "val Results: Prec@1 21.670 Prec@5 70.880 Loss 14.75836\n",
      "val Class Accuracy: [0.650,0.917,0.379,0.006,0.156,0.059,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 21.670\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [1][0/97], lr: 0.00400\tTime 0.640 (0.640)\tData 0.304 (0.304)\tLoss 5.3034 (5.3034)\tPrec@1 64.062 (64.062)\tPrec@5 96.875 (96.875)\n",
      "Epoch: [1][10/97], lr: 0.00400\tTime 0.475 (0.494)\tData 0.001 (0.040)\tLoss 4.9488 (5.6488)\tPrec@1 64.062 (61.577)\tPrec@5 93.750 (92.116)\n",
      "Epoch: [1][20/97], lr: 0.00400\tTime 0.488 (0.487)\tData 0.001 (0.027)\tLoss 5.0955 (5.6873)\tPrec@1 64.062 (59.412)\tPrec@5 92.188 (92.448)\n",
      "Epoch: [1][30/97], lr: 0.00400\tTime 0.415 (0.472)\tData 0.000 (0.022)\tLoss 5.4389 (5.6526)\tPrec@1 58.594 (58.921)\tPrec@5 93.750 (92.339)\n",
      "Epoch: [1][40/97], lr: 0.00400\tTime 0.484 (0.467)\tData 0.001 (0.020)\tLoss 6.6065 (5.6530)\tPrec@1 53.125 (58.632)\tPrec@5 92.188 (92.626)\n",
      "Epoch: [1][50/97], lr: 0.00400\tTime 0.403 (0.462)\tData 0.001 (0.018)\tLoss 4.9565 (5.5867)\tPrec@1 63.281 (58.808)\tPrec@5 93.750 (92.249)\n",
      "Epoch: [1][60/97], lr: 0.00400\tTime 0.436 (0.458)\tData 0.001 (0.018)\tLoss 5.3856 (5.6034)\tPrec@1 61.719 (58.747)\tPrec@5 93.750 (92.469)\n",
      "Epoch: [1][70/97], lr: 0.00400\tTime 0.350 (0.448)\tData 0.000 (0.017)\tLoss 5.3448 (5.5891)\tPrec@1 60.156 (58.649)\tPrec@5 96.094 (92.573)\n",
      "Epoch: [1][80/97], lr: 0.00400\tTime 0.366 (0.437)\tData 0.000 (0.017)\tLoss 5.5602 (5.5551)\tPrec@1 58.594 (58.787)\tPrec@5 94.531 (92.776)\n",
      "Epoch: [1][90/97], lr: 0.00400\tTime 0.344 (0.430)\tData 0.000 (0.017)\tLoss 5.1519 (5.5507)\tPrec@1 61.719 (58.765)\tPrec@5 89.844 (92.625)\n",
      "Epoch: [1][96/97], lr: 0.00400\tTime 0.321 (0.425)\tData 0.000 (0.017)\tLoss 5.7947 (5.5462)\tPrec@1 58.475 (58.722)\tPrec@5 91.525 (92.616)\n",
      "Gated Network Weight Gate= Flip:0.95, Sc:0.05\n",
      "Test: [0/100]\tTime 0.353 (0.353)\tLoss 13.1927 (13.1927)\tPrec@1 22.000 (22.000)\tPrec@5 76.000 (76.000)\n",
      "Test: [10/100]\tTime 0.073 (0.099)\tLoss 10.8573 (12.3341)\tPrec@1 36.000 (26.000)\tPrec@5 79.000 (76.545)\n",
      "Test: [20/100]\tTime 0.073 (0.087)\tLoss 11.3199 (12.3149)\tPrec@1 28.000 (25.429)\tPrec@5 83.000 (77.762)\n",
      "Test: [30/100]\tTime 0.074 (0.082)\tLoss 11.2395 (12.2523)\tPrec@1 27.000 (25.226)\tPrec@5 78.000 (77.710)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 12.8966 (12.2935)\tPrec@1 20.000 (25.122)\tPrec@5 68.000 (77.000)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 11.8129 (12.1854)\tPrec@1 27.000 (25.882)\tPrec@5 80.000 (77.569)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 10.7097 (12.1561)\tPrec@1 27.000 (25.738)\tPrec@5 79.000 (77.508)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 11.8298 (12.1196)\tPrec@1 23.000 (25.901)\tPrec@5 77.000 (77.254)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 11.0789 (12.0694)\tPrec@1 35.000 (26.210)\tPrec@5 72.000 (77.407)\n",
      "Test: [90/100]\tTime 0.073 (0.077)\tLoss 11.8479 (12.1129)\tPrec@1 29.000 (26.154)\tPrec@5 81.000 (77.319)\n",
      "val Results: Prec@1 26.100 Prec@5 77.100 Loss 12.14663\n",
      "val Class Accuracy: [0.815,0.812,0.185,0.776,0.022,0.000,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 26.100\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [2][0/97], lr: 0.00600\tTime 0.730 (0.730)\tData 0.430 (0.430)\tLoss 5.8075 (5.8075)\tPrec@1 52.344 (52.344)\tPrec@5 94.531 (94.531)\n",
      "Epoch: [2][10/97], lr: 0.00600\tTime 0.342 (0.399)\tData 0.000 (0.051)\tLoss 4.7892 (5.3258)\tPrec@1 65.625 (60.511)\tPrec@5 93.750 (93.608)\n",
      "Epoch: [2][20/97], lr: 0.00600\tTime 0.331 (0.370)\tData 0.000 (0.035)\tLoss 5.1323 (5.2003)\tPrec@1 61.719 (61.458)\tPrec@5 93.750 (94.271)\n",
      "Epoch: [2][30/97], lr: 0.00600\tTime 0.325 (0.357)\tData 0.000 (0.029)\tLoss 6.2394 (5.2563)\tPrec@1 55.469 (61.290)\tPrec@5 93.750 (93.926)\n",
      "Epoch: [2][40/97], lr: 0.00600\tTime 0.322 (0.351)\tData 0.000 (0.026)\tLoss 5.4057 (5.2857)\tPrec@1 60.938 (60.957)\tPrec@5 95.312 (94.188)\n",
      "Epoch: [2][50/97], lr: 0.00600\tTime 0.341 (0.348)\tData 0.000 (0.024)\tLoss 4.8539 (5.2622)\tPrec@1 67.969 (61.213)\tPrec@5 94.531 (94.056)\n",
      "Epoch: [2][60/97], lr: 0.00600\tTime 0.346 (0.347)\tData 0.000 (0.023)\tLoss 4.6605 (5.2654)\tPrec@1 67.188 (61.283)\tPrec@5 94.531 (94.211)\n",
      "Epoch: [2][70/97], lr: 0.00600\tTime 0.346 (0.346)\tData 0.000 (0.022)\tLoss 4.2632 (5.2118)\tPrec@1 70.312 (61.796)\tPrec@5 96.875 (94.179)\n",
      "Epoch: [2][80/97], lr: 0.00600\tTime 0.378 (0.346)\tData 0.000 (0.021)\tLoss 5.2002 (5.1624)\tPrec@1 61.719 (62.124)\tPrec@5 92.188 (94.290)\n",
      "Epoch: [2][90/97], lr: 0.00600\tTime 0.343 (0.349)\tData 0.000 (0.021)\tLoss 5.5990 (5.1549)\tPrec@1 62.500 (62.234)\tPrec@5 93.750 (94.179)\n",
      "Epoch: [2][96/97], lr: 0.00600\tTime 0.338 (0.349)\tData 0.000 (0.021)\tLoss 5.4325 (5.1394)\tPrec@1 55.932 (62.172)\tPrec@5 95.763 (94.285)\n",
      "Gated Network Weight Gate= Flip:0.92, Sc:0.08\n",
      "Test: [0/100]\tTime 0.366 (0.366)\tLoss 14.5960 (14.5960)\tPrec@1 20.000 (20.000)\tPrec@5 72.000 (72.000)\n",
      "Test: [10/100]\tTime 0.073 (0.100)\tLoss 12.0160 (13.5618)\tPrec@1 32.000 (21.636)\tPrec@5 76.000 (74.364)\n",
      "Test: [20/100]\tTime 0.073 (0.087)\tLoss 12.6941 (13.6432)\tPrec@1 25.000 (21.238)\tPrec@5 80.000 (75.238)\n",
      "Test: [30/100]\tTime 0.074 (0.083)\tLoss 12.5517 (13.5669)\tPrec@1 27.000 (21.742)\tPrec@5 78.000 (75.355)\n",
      "Test: [40/100]\tTime 0.074 (0.081)\tLoss 14.4759 (13.6361)\tPrec@1 19.000 (21.610)\tPrec@5 68.000 (74.732)\n",
      "Test: [50/100]\tTime 0.082 (0.080)\tLoss 13.5071 (13.5166)\tPrec@1 24.000 (22.255)\tPrec@5 74.000 (75.059)\n",
      "Test: [60/100]\tTime 0.078 (0.079)\tLoss 12.1044 (13.4779)\tPrec@1 21.000 (22.000)\tPrec@5 80.000 (75.082)\n",
      "Test: [70/100]\tTime 0.074 (0.078)\tLoss 13.3146 (13.4468)\tPrec@1 28.000 (22.296)\tPrec@5 72.000 (75.070)\n",
      "Test: [80/100]\tTime 0.074 (0.078)\tLoss 12.0947 (13.3801)\tPrec@1 28.000 (22.704)\tPrec@5 79.000 (75.210)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 13.5616 (13.4278)\tPrec@1 22.000 (22.604)\tPrec@5 79.000 (74.978)\n",
      "val Results: Prec@1 22.620 Prec@5 74.890 Loss 13.44987\n",
      "val Class Accuracy: [0.640,0.971,0.066,0.511,0.074,0.000,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 26.100\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [3][0/97], lr: 0.00800\tTime 0.848 (0.848)\tData 0.458 (0.458)\tLoss 5.5152 (5.5152)\tPrec@1 60.938 (60.938)\tPrec@5 96.094 (96.094)\n",
      "Epoch: [3][10/97], lr: 0.00800\tTime 0.381 (0.432)\tData 0.000 (0.052)\tLoss 4.7164 (5.2152)\tPrec@1 64.844 (60.653)\tPrec@5 95.312 (94.176)\n",
      "Epoch: [3][20/97], lr: 0.00800\tTime 0.340 (0.393)\tData 0.000 (0.035)\tLoss 4.9930 (5.0440)\tPrec@1 64.844 (62.500)\tPrec@5 95.312 (94.643)\n",
      "Epoch: [3][30/97], lr: 0.00800\tTime 0.384 (0.390)\tData 0.000 (0.029)\tLoss 5.0857 (5.0712)\tPrec@1 63.281 (62.576)\tPrec@5 95.312 (94.657)\n",
      "Epoch: [3][40/97], lr: 0.00800\tTime 0.343 (0.383)\tData 0.000 (0.025)\tLoss 5.2140 (5.0500)\tPrec@1 60.156 (62.519)\tPrec@5 94.531 (94.303)\n",
      "Epoch: [3][50/97], lr: 0.00800\tTime 0.344 (0.375)\tData 0.001 (0.024)\tLoss 4.6774 (5.0413)\tPrec@1 66.406 (62.638)\tPrec@5 91.406 (94.026)\n",
      "Epoch: [3][60/97], lr: 0.00800\tTime 0.340 (0.370)\tData 0.001 (0.022)\tLoss 4.3889 (4.9956)\tPrec@1 73.438 (63.230)\tPrec@5 94.531 (94.249)\n",
      "Epoch: [3][70/97], lr: 0.00800\tTime 0.341 (0.365)\tData 0.000 (0.022)\tLoss 4.3188 (4.9442)\tPrec@1 67.969 (63.523)\tPrec@5 96.875 (94.399)\n",
      "Epoch: [3][80/97], lr: 0.00800\tTime 0.345 (0.363)\tData 0.000 (0.021)\tLoss 4.7146 (4.9153)\tPrec@1 67.188 (63.696)\tPrec@5 96.875 (94.608)\n",
      "Epoch: [3][90/97], lr: 0.00800\tTime 0.341 (0.362)\tData 0.000 (0.020)\tLoss 4.2986 (4.8999)\tPrec@1 67.188 (63.814)\tPrec@5 96.094 (94.660)\n",
      "Epoch: [3][96/97], lr: 0.00800\tTime 0.330 (0.361)\tData 0.000 (0.021)\tLoss 4.5992 (4.9104)\tPrec@1 62.712 (63.768)\tPrec@5 95.763 (94.656)\n",
      "Gated Network Weight Gate= Flip:0.94, Sc:0.06\n",
      "Test: [0/100]\tTime 0.336 (0.336)\tLoss 12.8956 (12.8956)\tPrec@1 26.000 (26.000)\tPrec@5 82.000 (82.000)\n",
      "Test: [10/100]\tTime 0.073 (0.097)\tLoss 10.2045 (12.0477)\tPrec@1 38.000 (29.182)\tPrec@5 86.000 (81.545)\n",
      "Test: [20/100]\tTime 0.074 (0.086)\tLoss 11.3750 (12.1542)\tPrec@1 29.000 (27.714)\tPrec@5 86.000 (82.238)\n",
      "Test: [30/100]\tTime 0.074 (0.082)\tLoss 11.3778 (12.1122)\tPrec@1 31.000 (27.871)\tPrec@5 89.000 (81.903)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 12.8956 (12.1813)\tPrec@1 23.000 (27.707)\tPrec@5 82.000 (81.439)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 11.7437 (12.0736)\tPrec@1 32.000 (28.569)\tPrec@5 88.000 (81.922)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 10.3160 (12.0328)\tPrec@1 34.000 (28.377)\tPrec@5 80.000 (81.721)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 11.7732 (11.9795)\tPrec@1 28.000 (28.592)\tPrec@5 86.000 (81.746)\n",
      "Test: [80/100]\tTime 0.073 (0.077)\tLoss 10.8196 (11.9264)\tPrec@1 36.000 (28.877)\tPrec@5 83.000 (81.802)\n",
      "Test: [90/100]\tTime 0.073 (0.077)\tLoss 11.5073 (11.9866)\tPrec@1 35.000 (28.582)\tPrec@5 79.000 (81.736)\n",
      "val Results: Prec@1 28.480 Prec@5 81.800 Loss 12.01479\n",
      "val Class Accuracy: [0.903,0.839,0.511,0.559,0.000,0.000,0.036,0.000,0.000,0.000]\n",
      "Best Prec@1: 28.480\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [4][0/97], lr: 0.01000\tTime 0.945 (0.945)\tData 0.569 (0.569)\tLoss 5.3804 (5.3804)\tPrec@1 61.719 (61.719)\tPrec@5 89.062 (89.062)\n",
      "Epoch: [4][10/97], lr: 0.01000\tTime 0.347 (0.434)\tData 0.000 (0.063)\tLoss 5.0901 (4.7896)\tPrec@1 60.156 (64.773)\tPrec@5 94.531 (93.466)\n",
      "Epoch: [4][20/97], lr: 0.01000\tTime 0.352 (0.403)\tData 0.000 (0.041)\tLoss 4.7338 (4.8978)\tPrec@1 63.281 (64.583)\tPrec@5 96.094 (94.234)\n",
      "Epoch: [4][30/97], lr: 0.01000\tTime 0.341 (0.386)\tData 0.000 (0.033)\tLoss 5.7926 (4.8672)\tPrec@1 57.031 (65.146)\tPrec@5 94.531 (94.859)\n",
      "Epoch: [4][40/97], lr: 0.01000\tTime 0.371 (0.377)\tData 0.000 (0.029)\tLoss 4.6344 (4.7946)\tPrec@1 61.719 (65.530)\tPrec@5 98.438 (95.160)\n",
      "Epoch: [4][50/97], lr: 0.01000\tTime 0.340 (0.373)\tData 0.000 (0.026)\tLoss 4.3111 (4.7371)\tPrec@1 69.531 (65.947)\tPrec@5 97.656 (95.358)\n",
      "Epoch: [4][60/97], lr: 0.01000\tTime 0.348 (0.369)\tData 0.000 (0.024)\tLoss 4.6001 (4.7343)\tPrec@1 61.719 (65.548)\tPrec@5 96.875 (95.466)\n",
      "Epoch: [4][70/97], lr: 0.01000\tTime 0.352 (0.365)\tData 0.000 (0.023)\tLoss 4.9138 (4.7492)\tPrec@1 64.844 (65.482)\tPrec@5 94.531 (95.268)\n",
      "Epoch: [4][80/97], lr: 0.01000\tTime 0.351 (0.364)\tData 0.000 (0.022)\tLoss 5.1288 (4.7569)\tPrec@1 64.062 (65.422)\tPrec@5 95.312 (95.197)\n",
      "Epoch: [4][90/97], lr: 0.01000\tTime 0.343 (0.363)\tData 0.000 (0.022)\tLoss 5.5009 (4.7276)\tPrec@1 57.031 (65.470)\tPrec@5 94.531 (95.338)\n",
      "Epoch: [4][96/97], lr: 0.01000\tTime 0.325 (0.361)\tData 0.000 (0.022)\tLoss 4.4740 (4.7190)\tPrec@1 72.034 (65.589)\tPrec@5 96.610 (95.333)\n",
      "Gated Network Weight Gate= Flip:0.89, Sc:0.11\n",
      "Test: [0/100]\tTime 0.334 (0.334)\tLoss 15.2461 (15.2461)\tPrec@1 16.000 (16.000)\tPrec@5 63.000 (63.000)\n",
      "Test: [10/100]\tTime 0.074 (0.097)\tLoss 12.6909 (14.4213)\tPrec@1 25.000 (20.182)\tPrec@5 70.000 (67.364)\n",
      "Test: [20/100]\tTime 0.074 (0.086)\tLoss 13.3109 (14.4758)\tPrec@1 18.000 (19.143)\tPrec@5 75.000 (68.000)\n",
      "Test: [30/100]\tTime 0.073 (0.082)\tLoss 13.2118 (14.3935)\tPrec@1 21.000 (19.355)\tPrec@5 72.000 (68.548)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 14.7797 (14.4716)\tPrec@1 17.000 (19.390)\tPrec@5 63.000 (68.634)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 14.2586 (14.4233)\tPrec@1 19.000 (19.529)\tPrec@5 71.000 (69.137)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 11.9279 (14.3412)\tPrec@1 28.000 (19.475)\tPrec@5 81.000 (69.852)\n",
      "Test: [70/100]\tTime 0.074 (0.078)\tLoss 13.8926 (14.3057)\tPrec@1 21.000 (19.648)\tPrec@5 71.000 (70.085)\n",
      "Test: [80/100]\tTime 0.085 (0.077)\tLoss 13.4600 (14.2732)\tPrec@1 26.000 (19.765)\tPrec@5 71.000 (70.111)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 13.9593 (14.3408)\tPrec@1 28.000 (19.560)\tPrec@5 70.000 (69.736)\n",
      "val Results: Prec@1 19.520 Prec@5 69.620 Loss 14.37250\n",
      "val Class Accuracy: [0.993,0.424,0.434,0.101,0.000,0.000,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 28.480\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [5][0/97], lr: 0.01000\tTime 0.768 (0.768)\tData 0.419 (0.419)\tLoss 4.1574 (4.1574)\tPrec@1 69.531 (69.531)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [5][10/97], lr: 0.01000\tTime 0.358 (0.403)\tData 0.000 (0.050)\tLoss 4.8670 (4.5749)\tPrec@1 65.625 (67.756)\tPrec@5 96.094 (96.307)\n",
      "Epoch: [5][20/97], lr: 0.01000\tTime 0.355 (0.391)\tData 0.000 (0.034)\tLoss 4.5710 (4.5728)\tPrec@1 63.281 (67.485)\tPrec@5 97.656 (96.057)\n",
      "Epoch: [5][30/97], lr: 0.01000\tTime 0.341 (0.378)\tData 0.000 (0.028)\tLoss 4.6751 (4.5936)\tPrec@1 63.281 (67.490)\tPrec@5 96.094 (95.791)\n",
      "Epoch: [5][40/97], lr: 0.01000\tTime 0.343 (0.371)\tData 0.000 (0.025)\tLoss 4.9504 (4.6333)\tPrec@1 67.969 (67.054)\tPrec@5 94.531 (95.846)\n",
      "Epoch: [5][50/97], lr: 0.01000\tTime 0.332 (0.364)\tData 0.000 (0.023)\tLoss 5.3663 (4.5753)\tPrec@1 63.281 (67.463)\tPrec@5 95.312 (95.849)\n",
      "Epoch: [5][60/97], lr: 0.01000\tTime 0.331 (0.360)\tData 0.000 (0.022)\tLoss 4.4285 (4.5534)\tPrec@1 67.188 (67.367)\tPrec@5 96.094 (95.633)\n",
      "Epoch: [5][70/97], lr: 0.01000\tTime 0.362 (0.360)\tData 0.000 (0.022)\tLoss 4.3871 (4.5045)\tPrec@1 68.750 (67.540)\tPrec@5 97.656 (95.885)\n",
      "Epoch: [5][80/97], lr: 0.01000\tTime 0.381 (0.361)\tData 0.000 (0.021)\tLoss 4.2460 (4.4891)\tPrec@1 66.406 (67.689)\tPrec@5 92.969 (95.853)\n",
      "Epoch: [5][90/97], lr: 0.01000\tTime 0.468 (0.364)\tData 0.000 (0.020)\tLoss 4.0315 (4.4774)\tPrec@1 71.875 (67.840)\tPrec@5 96.094 (95.768)\n",
      "Epoch: [5][96/97], lr: 0.01000\tTime 0.408 (0.366)\tData 0.000 (0.020)\tLoss 3.6394 (4.4656)\tPrec@1 71.186 (67.919)\tPrec@5 95.763 (95.897)\n",
      "Gated Network Weight Gate= Flip:0.83, Sc:0.17\n",
      "Test: [0/100]\tTime 0.627 (0.627)\tLoss 14.4146 (14.4146)\tPrec@1 22.000 (22.000)\tPrec@5 79.000 (79.000)\n",
      "Test: [10/100]\tTime 0.076 (0.125)\tLoss 11.9880 (13.3754)\tPrec@1 33.000 (26.727)\tPrec@5 74.000 (82.364)\n",
      "Test: [20/100]\tTime 0.077 (0.101)\tLoss 12.0584 (13.4156)\tPrec@1 30.000 (25.619)\tPrec@5 86.000 (82.381)\n",
      "Test: [30/100]\tTime 0.074 (0.092)\tLoss 12.7795 (13.3525)\tPrec@1 30.000 (26.161)\tPrec@5 84.000 (82.645)\n",
      "Test: [40/100]\tTime 0.073 (0.088)\tLoss 13.9035 (13.4262)\tPrec@1 25.000 (26.220)\tPrec@5 78.000 (82.732)\n",
      "Test: [50/100]\tTime 0.073 (0.085)\tLoss 13.3123 (13.3668)\tPrec@1 27.000 (26.451)\tPrec@5 79.000 (82.765)\n",
      "Test: [60/100]\tTime 0.073 (0.083)\tLoss 11.4234 (13.3338)\tPrec@1 31.000 (26.213)\tPrec@5 84.000 (82.410)\n",
      "Test: [70/100]\tTime 0.073 (0.082)\tLoss 12.6995 (13.2944)\tPrec@1 31.000 (26.493)\tPrec@5 84.000 (82.507)\n",
      "Test: [80/100]\tTime 0.073 (0.081)\tLoss 12.0890 (13.2210)\tPrec@1 34.000 (26.963)\tPrec@5 85.000 (82.778)\n",
      "Test: [90/100]\tTime 0.073 (0.080)\tLoss 12.7451 (13.2612)\tPrec@1 29.000 (26.670)\tPrec@5 91.000 (82.659)\n",
      "val Results: Prec@1 26.690 Prec@5 82.550 Loss 13.28773\n",
      "val Class Accuracy: [0.814,0.987,0.478,0.116,0.157,0.117,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 28.480\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [6][0/97], lr: 0.01000\tTime 0.697 (0.697)\tData 0.413 (0.413)\tLoss 3.7829 (3.7829)\tPrec@1 71.875 (71.875)\tPrec@5 96.875 (96.875)\n",
      "Epoch: [6][10/97], lr: 0.01000\tTime 0.415 (0.399)\tData 0.000 (0.051)\tLoss 4.4393 (4.3088)\tPrec@1 67.969 (69.247)\tPrec@5 96.875 (96.023)\n",
      "Epoch: [6][20/97], lr: 0.01000\tTime 0.325 (0.376)\tData 0.000 (0.033)\tLoss 4.3206 (4.2604)\tPrec@1 73.438 (70.089)\tPrec@5 96.094 (96.057)\n",
      "Epoch: [6][30/97], lr: 0.01000\tTime 0.370 (0.366)\tData 0.000 (0.028)\tLoss 4.3081 (4.2606)\tPrec@1 71.094 (69.607)\tPrec@5 96.094 (96.245)\n",
      "Epoch: [6][40/97], lr: 0.01000\tTime 0.400 (0.383)\tData 0.000 (0.025)\tLoss 3.9777 (4.2627)\tPrec@1 75.781 (69.741)\tPrec@5 96.875 (96.322)\n",
      "Epoch: [6][50/97], lr: 0.01000\tTime 0.372 (0.392)\tData 0.000 (0.023)\tLoss 3.7612 (4.2668)\tPrec@1 72.656 (69.547)\tPrec@5 97.656 (96.385)\n",
      "Epoch: [6][60/97], lr: 0.01000\tTime 0.447 (0.395)\tData 0.000 (0.022)\tLoss 3.6524 (4.2897)\tPrec@1 77.344 (69.314)\tPrec@5 97.656 (96.324)\n",
      "Epoch: [6][70/97], lr: 0.01000\tTime 0.330 (0.390)\tData 0.000 (0.021)\tLoss 4.3642 (4.2768)\tPrec@1 74.219 (69.454)\tPrec@5 97.656 (96.160)\n",
      "Epoch: [6][80/97], lr: 0.01000\tTime 0.316 (0.381)\tData 0.000 (0.020)\tLoss 3.7503 (4.2641)\tPrec@1 74.219 (69.512)\tPrec@5 95.312 (96.171)\n",
      "Epoch: [6][90/97], lr: 0.01000\tTime 0.351 (0.376)\tData 0.000 (0.020)\tLoss 3.5770 (4.2650)\tPrec@1 72.656 (69.643)\tPrec@5 98.438 (96.145)\n",
      "Epoch: [6][96/97], lr: 0.01000\tTime 0.318 (0.374)\tData 0.000 (0.020)\tLoss 4.2353 (4.2612)\tPrec@1 73.729 (69.757)\tPrec@5 97.458 (96.212)\n",
      "Gated Network Weight Gate= Flip:0.84, Sc:0.16\n",
      "Test: [0/100]\tTime 0.294 (0.294)\tLoss 12.9533 (12.9533)\tPrec@1 30.000 (30.000)\tPrec@5 88.000 (88.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 10.0737 (12.1764)\tPrec@1 40.000 (30.273)\tPrec@5 93.000 (86.818)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 11.0533 (12.2284)\tPrec@1 32.000 (29.571)\tPrec@5 87.000 (86.333)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 11.3443 (12.1584)\tPrec@1 34.000 (30.065)\tPrec@5 88.000 (86.129)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 12.8053 (12.2499)\tPrec@1 28.000 (29.659)\tPrec@5 84.000 (85.805)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 12.2966 (12.1718)\tPrec@1 26.000 (29.980)\tPrec@5 84.000 (86.020)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 10.4581 (12.1452)\tPrec@1 36.000 (29.754)\tPrec@5 89.000 (85.820)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 12.0169 (12.0916)\tPrec@1 28.000 (29.803)\tPrec@5 82.000 (85.845)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 10.7659 (12.0264)\tPrec@1 42.000 (30.111)\tPrec@5 83.000 (86.000)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.8426 (12.0689)\tPrec@1 32.000 (29.912)\tPrec@5 94.000 (86.033)\n",
      "val Results: Prec@1 29.850 Prec@5 86.020 Loss 12.10491\n",
      "val Class Accuracy: [0.916,0.966,0.355,0.506,0.200,0.000,0.042,0.000,0.000,0.000]\n",
      "Best Prec@1: 29.850\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [7][0/97], lr: 0.01000\tTime 0.465 (0.465)\tData 0.247 (0.247)\tLoss 5.1178 (5.1178)\tPrec@1 64.844 (64.844)\tPrec@5 95.312 (95.312)\n",
      "Epoch: [7][10/97], lr: 0.01000\tTime 0.316 (0.343)\tData 0.000 (0.037)\tLoss 3.7173 (4.1814)\tPrec@1 71.875 (70.028)\tPrec@5 96.875 (96.236)\n",
      "Epoch: [7][20/97], lr: 0.01000\tTime 0.388 (0.338)\tData 0.000 (0.027)\tLoss 4.8098 (4.2564)\tPrec@1 65.625 (69.940)\tPrec@5 95.312 (95.982)\n",
      "Epoch: [7][30/97], lr: 0.01000\tTime 0.317 (0.341)\tData 0.000 (0.023)\tLoss 3.4060 (4.1977)\tPrec@1 70.312 (70.161)\tPrec@5 96.094 (95.892)\n",
      "Epoch: [7][40/97], lr: 0.01000\tTime 0.320 (0.336)\tData 0.000 (0.022)\tLoss 4.8946 (4.1673)\tPrec@1 66.406 (70.351)\tPrec@5 94.531 (96.265)\n",
      "Epoch: [7][50/97], lr: 0.01000\tTime 0.342 (0.336)\tData 0.000 (0.021)\tLoss 4.4825 (4.1871)\tPrec@1 73.438 (70.435)\tPrec@5 93.750 (96.400)\n",
      "Epoch: [7][60/97], lr: 0.01000\tTime 0.316 (0.334)\tData 0.000 (0.020)\tLoss 3.8276 (4.1696)\tPrec@1 70.312 (70.389)\tPrec@5 96.094 (96.491)\n",
      "Epoch: [7][70/97], lr: 0.01000\tTime 0.313 (0.332)\tData 0.000 (0.020)\tLoss 4.0283 (4.1715)\tPrec@1 67.188 (70.368)\tPrec@5 95.312 (96.413)\n",
      "Epoch: [7][80/97], lr: 0.01000\tTime 0.317 (0.330)\tData 0.000 (0.019)\tLoss 4.0068 (4.1756)\tPrec@1 73.438 (70.399)\tPrec@5 98.438 (96.393)\n",
      "Epoch: [7][90/97], lr: 0.01000\tTime 0.313 (0.328)\tData 0.000 (0.019)\tLoss 3.5457 (4.1682)\tPrec@1 75.000 (70.390)\tPrec@5 94.531 (96.265)\n",
      "Epoch: [7][96/97], lr: 0.01000\tTime 0.302 (0.327)\tData 0.000 (0.020)\tLoss 4.3323 (4.1845)\tPrec@1 66.949 (70.297)\tPrec@5 98.305 (96.252)\n",
      "Gated Network Weight Gate= Flip:0.73, Sc:0.27\n",
      "Test: [0/100]\tTime 0.233 (0.233)\tLoss 12.5369 (12.5369)\tPrec@1 23.000 (23.000)\tPrec@5 89.000 (89.000)\n",
      "Test: [10/100]\tTime 0.072 (0.087)\tLoss 10.0870 (11.9128)\tPrec@1 39.000 (27.818)\tPrec@5 90.000 (85.273)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 10.7562 (11.9713)\tPrec@1 35.000 (27.619)\tPrec@5 85.000 (84.810)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 11.2915 (11.9132)\tPrec@1 32.000 (27.871)\tPrec@5 84.000 (84.484)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 12.7987 (11.9847)\tPrec@1 23.000 (27.780)\tPrec@5 80.000 (84.098)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 11.6971 (11.9046)\tPrec@1 31.000 (28.235)\tPrec@5 81.000 (84.314)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 10.4833 (11.8965)\tPrec@1 31.000 (27.967)\tPrec@5 89.000 (84.049)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 11.8346 (11.8493)\tPrec@1 31.000 (28.268)\tPrec@5 82.000 (84.127)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 10.5969 (11.7844)\tPrec@1 41.000 (28.654)\tPrec@5 85.000 (84.222)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.5365 (11.8168)\tPrec@1 29.000 (28.407)\tPrec@5 91.000 (84.275)\n",
      "val Results: Prec@1 28.330 Prec@5 84.130 Loss 11.84983\n",
      "val Class Accuracy: [0.856,0.985,0.373,0.607,0.006,0.000,0.006,0.000,0.000,0.000]\n",
      "Best Prec@1: 29.850\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [8][0/97], lr: 0.01000\tTime 0.419 (0.419)\tData 0.236 (0.236)\tLoss 4.3005 (4.3005)\tPrec@1 67.969 (67.969)\tPrec@5 96.094 (96.094)\n",
      "Epoch: [8][10/97], lr: 0.01000\tTime 0.316 (0.329)\tData 0.000 (0.037)\tLoss 3.1418 (4.0434)\tPrec@1 80.469 (72.159)\tPrec@5 96.094 (96.236)\n",
      "Epoch: [8][20/97], lr: 0.01000\tTime 0.313 (0.322)\tData 0.000 (0.027)\tLoss 4.2462 (4.0787)\tPrec@1 69.531 (71.763)\tPrec@5 96.875 (96.466)\n",
      "Epoch: [8][30/97], lr: 0.01000\tTime 0.315 (0.320)\tData 0.000 (0.024)\tLoss 3.9874 (4.1521)\tPrec@1 72.656 (71.018)\tPrec@5 96.094 (96.144)\n",
      "Epoch: [8][40/97], lr: 0.01000\tTime 0.316 (0.319)\tData 0.000 (0.022)\tLoss 4.7168 (4.2140)\tPrec@1 67.969 (70.751)\tPrec@5 95.312 (96.303)\n",
      "Epoch: [8][50/97], lr: 0.01000\tTime 0.314 (0.318)\tData 0.000 (0.021)\tLoss 4.2038 (4.1797)\tPrec@1 69.531 (70.925)\tPrec@5 96.875 (96.400)\n",
      "Epoch: [8][60/97], lr: 0.01000\tTime 0.316 (0.318)\tData 0.000 (0.021)\tLoss 3.4153 (4.1133)\tPrec@1 80.469 (71.491)\tPrec@5 94.531 (96.491)\n",
      "Epoch: [8][70/97], lr: 0.01000\tTime 0.316 (0.318)\tData 0.000 (0.020)\tLoss 4.6534 (4.0911)\tPrec@1 64.062 (71.666)\tPrec@5 96.875 (96.512)\n",
      "Epoch: [8][80/97], lr: 0.01000\tTime 0.316 (0.317)\tData 0.000 (0.020)\tLoss 3.8209 (4.0857)\tPrec@1 75.781 (71.798)\tPrec@5 96.875 (96.499)\n",
      "Epoch: [8][90/97], lr: 0.01000\tTime 0.316 (0.317)\tData 0.000 (0.020)\tLoss 3.8238 (4.0942)\tPrec@1 78.125 (71.849)\tPrec@5 98.438 (96.480)\n",
      "Epoch: [8][96/97], lr: 0.01000\tTime 0.304 (0.317)\tData 0.000 (0.020)\tLoss 4.9701 (4.1014)\tPrec@1 61.864 (71.852)\tPrec@5 96.610 (96.461)\n",
      "Gated Network Weight Gate= Flip:0.69, Sc:0.31\n",
      "Test: [0/100]\tTime 0.232 (0.232)\tLoss 11.9857 (11.9857)\tPrec@1 31.000 (31.000)\tPrec@5 89.000 (89.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 9.3114 (11.0628)\tPrec@1 42.000 (34.273)\tPrec@5 91.000 (86.182)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 10.1541 (11.0051)\tPrec@1 35.000 (34.524)\tPrec@5 91.000 (86.810)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 10.0460 (10.9202)\tPrec@1 40.000 (34.935)\tPrec@5 88.000 (86.484)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 11.3355 (10.9419)\tPrec@1 34.000 (34.927)\tPrec@5 90.000 (86.537)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 10.6256 (10.8508)\tPrec@1 35.000 (35.392)\tPrec@5 90.000 (86.980)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 9.5024 (10.8078)\tPrec@1 37.000 (35.279)\tPrec@5 92.000 (87.033)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 10.5111 (10.7852)\tPrec@1 35.000 (35.408)\tPrec@5 89.000 (87.014)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.9853 (10.7466)\tPrec@1 42.000 (35.593)\tPrec@5 87.000 (87.148)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 10.5960 (10.7958)\tPrec@1 37.000 (35.275)\tPrec@5 84.000 (86.945)\n",
      "val Results: Prec@1 35.190 Prec@5 86.880 Loss 10.81788\n",
      "val Class Accuracy: [0.837,0.916,0.378,0.786,0.585,0.000,0.015,0.002,0.000,0.000]\n",
      "Best Prec@1: 35.190\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [9][0/97], lr: 0.01000\tTime 0.415 (0.415)\tData 0.246 (0.246)\tLoss 3.4616 (3.4616)\tPrec@1 75.781 (75.781)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [9][10/97], lr: 0.01000\tTime 0.315 (0.326)\tData 0.000 (0.037)\tLoss 4.0290 (3.9559)\tPrec@1 69.531 (72.514)\tPrec@5 96.094 (96.733)\n",
      "Epoch: [9][20/97], lr: 0.01000\tTime 0.313 (0.320)\tData 0.000 (0.028)\tLoss 4.3489 (4.0516)\tPrec@1 74.219 (71.987)\tPrec@5 97.656 (96.838)\n",
      "Epoch: [9][30/97], lr: 0.01000\tTime 0.313 (0.319)\tData 0.000 (0.024)\tLoss 4.0840 (4.1049)\tPrec@1 71.094 (71.522)\tPrec@5 99.219 (96.799)\n",
      "Epoch: [9][40/97], lr: 0.01000\tTime 0.315 (0.318)\tData 0.000 (0.023)\tLoss 4.0706 (4.1039)\tPrec@1 71.875 (71.456)\tPrec@5 96.094 (96.932)\n",
      "Epoch: [9][50/97], lr: 0.01000\tTime 0.315 (0.317)\tData 0.000 (0.021)\tLoss 4.0672 (4.0498)\tPrec@1 71.875 (71.814)\tPrec@5 99.219 (97.013)\n",
      "Epoch: [9][60/97], lr: 0.01000\tTime 0.313 (0.317)\tData 0.000 (0.021)\tLoss 3.9485 (4.0245)\tPrec@1 73.438 (72.016)\tPrec@5 94.531 (97.067)\n",
      "Epoch: [9][70/97], lr: 0.01000\tTime 0.319 (0.317)\tData 0.000 (0.020)\tLoss 3.6652 (4.0088)\tPrec@1 75.781 (72.282)\tPrec@5 94.531 (96.963)\n",
      "Epoch: [9][80/97], lr: 0.01000\tTime 0.317 (0.317)\tData 0.000 (0.020)\tLoss 3.5969 (3.9900)\tPrec@1 75.781 (72.357)\tPrec@5 96.875 (96.885)\n",
      "Epoch: [9][90/97], lr: 0.01000\tTime 0.315 (0.317)\tData 0.000 (0.020)\tLoss 4.0587 (3.9801)\tPrec@1 71.094 (72.424)\tPrec@5 95.312 (96.841)\n",
      "Epoch: [9][96/97], lr: 0.01000\tTime 0.303 (0.317)\tData 0.000 (0.020)\tLoss 3.3742 (3.9678)\tPrec@1 78.814 (72.586)\tPrec@5 99.153 (96.832)\n",
      "Gated Network Weight Gate= Flip:0.68, Sc:0.32\n",
      "Test: [0/100]\tTime 0.224 (0.224)\tLoss 13.4765 (13.4765)\tPrec@1 20.000 (20.000)\tPrec@5 90.000 (90.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 10.7778 (12.4126)\tPrec@1 39.000 (28.818)\tPrec@5 89.000 (89.000)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 11.0344 (12.3804)\tPrec@1 31.000 (28.952)\tPrec@5 91.000 (88.571)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 11.3503 (12.3185)\tPrec@1 36.000 (29.290)\tPrec@5 88.000 (88.323)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 12.4363 (12.3784)\tPrec@1 30.000 (29.488)\tPrec@5 89.000 (88.463)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 11.9149 (12.2747)\tPrec@1 32.000 (30.137)\tPrec@5 91.000 (88.510)\n",
      "Test: [60/100]\tTime 0.073 (0.075)\tLoss 10.9288 (12.2610)\tPrec@1 34.000 (30.016)\tPrec@5 90.000 (88.492)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 11.8335 (12.2271)\tPrec@1 32.000 (30.155)\tPrec@5 94.000 (88.493)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 10.9431 (12.1791)\tPrec@1 37.000 (30.432)\tPrec@5 87.000 (88.642)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 12.3755 (12.2210)\tPrec@1 31.000 (30.264)\tPrec@5 89.000 (88.516)\n",
      "val Results: Prec@1 30.270 Prec@5 88.330 Loss 12.24899\n",
      "val Class Accuracy: [0.696,0.993,0.260,0.531,0.547,0.000,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 35.190\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [10][0/97], lr: 0.01000\tTime 0.399 (0.399)\tData 0.209 (0.209)\tLoss 4.7348 (4.7348)\tPrec@1 65.625 (65.625)\tPrec@5 96.875 (96.875)\n",
      "Epoch: [10][10/97], lr: 0.01000\tTime 0.318 (0.327)\tData 0.000 (0.034)\tLoss 4.1208 (4.0261)\tPrec@1 71.094 (72.727)\tPrec@5 98.438 (97.159)\n",
      "Epoch: [10][20/97], lr: 0.01000\tTime 0.319 (0.322)\tData 0.000 (0.026)\tLoss 3.4221 (3.9154)\tPrec@1 77.344 (73.438)\tPrec@5 96.875 (96.949)\n",
      "Epoch: [10][30/97], lr: 0.01000\tTime 0.314 (0.319)\tData 0.000 (0.023)\tLoss 4.3831 (3.8643)\tPrec@1 71.094 (73.690)\tPrec@5 96.875 (97.177)\n",
      "Epoch: [10][40/97], lr: 0.01000\tTime 0.315 (0.318)\tData 0.000 (0.022)\tLoss 3.7616 (3.8510)\tPrec@1 72.656 (73.514)\tPrec@5 96.875 (97.275)\n",
      "Epoch: [10][50/97], lr: 0.01000\tTime 0.313 (0.318)\tData 0.000 (0.021)\tLoss 4.1220 (3.8733)\tPrec@1 71.094 (73.330)\tPrec@5 98.438 (97.120)\n",
      "Epoch: [10][60/97], lr: 0.01000\tTime 0.313 (0.317)\tData 0.000 (0.020)\tLoss 4.5296 (3.8422)\tPrec@1 68.750 (73.745)\tPrec@5 96.094 (97.054)\n",
      "Epoch: [10][70/97], lr: 0.01000\tTime 0.318 (0.317)\tData 0.000 (0.020)\tLoss 3.2656 (3.8272)\tPrec@1 77.344 (73.867)\tPrec@5 99.219 (97.106)\n",
      "Epoch: [10][80/97], lr: 0.01000\tTime 0.316 (0.317)\tData 0.000 (0.020)\tLoss 4.2254 (3.8173)\tPrec@1 71.875 (73.756)\tPrec@5 95.312 (97.078)\n",
      "Epoch: [10][90/97], lr: 0.01000\tTime 0.313 (0.317)\tData 0.000 (0.019)\tLoss 4.1027 (3.8252)\tPrec@1 76.562 (73.841)\tPrec@5 97.656 (96.901)\n",
      "Epoch: [10][96/97], lr: 0.01000\tTime 0.303 (0.317)\tData 0.000 (0.020)\tLoss 3.4517 (3.8405)\tPrec@1 77.966 (73.827)\tPrec@5 96.610 (96.856)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.228 (0.228)\tLoss 12.5677 (12.5677)\tPrec@1 27.000 (27.000)\tPrec@5 89.000 (89.000)\n",
      "Test: [10/100]\tTime 0.073 (0.087)\tLoss 9.8486 (11.5036)\tPrec@1 41.000 (33.364)\tPrec@5 93.000 (90.909)\n",
      "Test: [20/100]\tTime 0.073 (0.080)\tLoss 10.6187 (11.4669)\tPrec@1 33.000 (34.095)\tPrec@5 90.000 (90.333)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 10.5648 (11.3595)\tPrec@1 36.000 (34.806)\tPrec@5 91.000 (89.968)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 11.6206 (11.4008)\tPrec@1 34.000 (34.585)\tPrec@5 90.000 (89.854)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 11.0500 (11.2966)\tPrec@1 39.000 (35.176)\tPrec@5 85.000 (89.922)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 9.8167 (11.2843)\tPrec@1 41.000 (34.951)\tPrec@5 92.000 (89.836)\n",
      "Test: [70/100]\tTime 0.074 (0.075)\tLoss 10.9682 (11.2478)\tPrec@1 37.000 (35.183)\tPrec@5 91.000 (89.690)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 10.1949 (11.2178)\tPrec@1 43.000 (35.321)\tPrec@5 88.000 (89.827)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 11.0261 (11.2754)\tPrec@1 37.000 (34.956)\tPrec@5 94.000 (89.857)\n",
      "val Results: Prec@1 34.810 Prec@5 89.770 Loss 11.31338\n",
      "val Class Accuracy: [0.908,0.987,0.443,0.449,0.676,0.000,0.018,0.000,0.000,0.000]\n",
      "Best Prec@1: 35.190\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [11][0/97], lr: 0.01000\tTime 0.401 (0.401)\tData 0.208 (0.208)\tLoss 4.0508 (4.0508)\tPrec@1 74.219 (74.219)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [11][10/97], lr: 0.01000\tTime 0.314 (0.325)\tData 0.000 (0.034)\tLoss 3.4059 (3.9042)\tPrec@1 75.781 (73.651)\tPrec@5 97.656 (97.514)\n",
      "Epoch: [11][20/97], lr: 0.01000\tTime 0.327 (0.322)\tData 0.000 (0.026)\tLoss 3.3817 (3.8575)\tPrec@1 76.562 (73.512)\tPrec@5 95.312 (97.359)\n",
      "Epoch: [11][30/97], lr: 0.01000\tTime 0.315 (0.320)\tData 0.000 (0.023)\tLoss 3.7374 (3.8386)\tPrec@1 74.219 (73.589)\tPrec@5 98.438 (97.429)\n",
      "Epoch: [11][40/97], lr: 0.01000\tTime 0.316 (0.319)\tData 0.000 (0.022)\tLoss 4.1336 (3.8067)\tPrec@1 72.656 (73.933)\tPrec@5 95.312 (97.447)\n",
      "Epoch: [11][50/97], lr: 0.01000\tTime 0.316 (0.318)\tData 0.000 (0.021)\tLoss 3.2732 (3.8106)\tPrec@1 77.344 (74.004)\tPrec@5 95.312 (97.365)\n",
      "Epoch: [11][60/97], lr: 0.01000\tTime 0.314 (0.318)\tData 0.000 (0.020)\tLoss 4.4474 (3.7887)\tPrec@1 67.969 (74.244)\tPrec@5 96.094 (97.208)\n",
      "Epoch: [11][70/97], lr: 0.01000\tTime 0.318 (0.318)\tData 0.000 (0.020)\tLoss 3.7026 (3.7935)\tPrec@1 74.219 (74.032)\tPrec@5 98.438 (97.216)\n",
      "Epoch: [11][80/97], lr: 0.01000\tTime 0.318 (0.318)\tData 0.000 (0.019)\tLoss 3.5624 (3.7825)\tPrec@1 77.344 (74.171)\tPrec@5 97.656 (97.193)\n",
      "Epoch: [11][90/97], lr: 0.01000\tTime 0.314 (0.318)\tData 0.000 (0.019)\tLoss 3.8240 (3.8099)\tPrec@1 72.656 (73.935)\tPrec@5 98.438 (97.124)\n",
      "Epoch: [11][96/97], lr: 0.01000\tTime 0.306 (0.317)\tData 0.000 (0.020)\tLoss 3.2893 (3.8079)\tPrec@1 76.271 (74.053)\tPrec@5 95.763 (97.058)\n",
      "Gated Network Weight Gate= Flip:0.66, Sc:0.34\n",
      "Test: [0/100]\tTime 0.236 (0.236)\tLoss 14.2809 (14.2809)\tPrec@1 22.000 (22.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 11.2609 (13.0418)\tPrec@1 33.000 (27.909)\tPrec@5 91.000 (92.545)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 12.1194 (13.1140)\tPrec@1 31.000 (27.524)\tPrec@5 93.000 (91.476)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 12.7196 (13.0279)\tPrec@1 27.000 (28.226)\tPrec@5 91.000 (91.258)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 13.9654 (13.0437)\tPrec@1 20.000 (28.098)\tPrec@5 87.000 (91.293)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 13.1648 (13.0044)\tPrec@1 29.000 (28.471)\tPrec@5 92.000 (91.275)\n",
      "Test: [60/100]\tTime 0.074 (0.076)\tLoss 11.0116 (13.0275)\tPrec@1 35.000 (28.066)\tPrec@5 95.000 (91.246)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 12.7008 (13.0003)\tPrec@1 32.000 (28.324)\tPrec@5 92.000 (91.338)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 11.7941 (12.9529)\tPrec@1 36.000 (28.642)\tPrec@5 91.000 (91.395)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 12.1351 (13.0081)\tPrec@1 37.000 (28.484)\tPrec@5 95.000 (91.407)\n",
      "val Results: Prec@1 28.460 Prec@5 91.440 Loss 13.03752\n",
      "val Class Accuracy: [0.872,0.989,0.830,0.109,0.020,0.003,0.014,0.009,0.000,0.000]\n",
      "Best Prec@1: 35.190\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [12][0/97], lr: 0.01000\tTime 0.366 (0.366)\tData 0.200 (0.200)\tLoss 3.6433 (3.6433)\tPrec@1 75.781 (75.781)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [12][10/97], lr: 0.01000\tTime 0.314 (0.322)\tData 0.000 (0.033)\tLoss 3.2123 (3.6627)\tPrec@1 76.562 (75.497)\tPrec@5 98.438 (97.372)\n",
      "Epoch: [12][20/97], lr: 0.01000\tTime 0.316 (0.319)\tData 0.000 (0.025)\tLoss 3.9749 (3.7984)\tPrec@1 75.781 (74.888)\tPrec@5 95.312 (97.284)\n",
      "Epoch: [12][30/97], lr: 0.01000\tTime 0.313 (0.318)\tData 0.000 (0.023)\tLoss 3.7840 (3.8362)\tPrec@1 75.000 (74.471)\tPrec@5 98.438 (97.228)\n",
      "Epoch: [12][40/97], lr: 0.01000\tTime 0.317 (0.318)\tData 0.000 (0.021)\tLoss 4.1861 (3.8138)\tPrec@1 73.438 (74.562)\tPrec@5 97.656 (97.351)\n",
      "Epoch: [12][50/97], lr: 0.01000\tTime 0.317 (0.317)\tData 0.000 (0.021)\tLoss 3.3704 (3.8046)\tPrec@1 76.562 (74.663)\tPrec@5 97.656 (97.243)\n",
      "Epoch: [12][60/97], lr: 0.01000\tTime 0.314 (0.317)\tData 0.000 (0.020)\tLoss 4.2547 (3.8052)\tPrec@1 69.531 (74.577)\tPrec@5 93.750 (97.131)\n",
      "Epoch: [12][70/97], lr: 0.01000\tTime 0.317 (0.317)\tData 0.000 (0.020)\tLoss 3.6176 (3.7959)\tPrec@1 75.781 (74.725)\tPrec@5 97.656 (97.172)\n",
      "Epoch: [12][80/97], lr: 0.01000\tTime 0.316 (0.317)\tData 0.000 (0.019)\tLoss 3.7315 (3.7922)\tPrec@1 71.875 (74.518)\tPrec@5 96.875 (97.078)\n",
      "Epoch: [12][90/97], lr: 0.01000\tTime 0.347 (0.321)\tData 0.000 (0.019)\tLoss 2.9728 (3.7548)\tPrec@1 78.125 (74.742)\tPrec@5 99.219 (97.167)\n",
      "Epoch: [12][96/97], lr: 0.01000\tTime 0.313 (0.322)\tData 0.000 (0.020)\tLoss 3.7576 (3.7396)\tPrec@1 74.576 (74.875)\tPrec@5 94.915 (97.155)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.238 (0.238)\tLoss 12.8935 (12.8935)\tPrec@1 27.000 (27.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 10.5565 (11.9403)\tPrec@1 39.000 (32.091)\tPrec@5 88.000 (87.818)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 11.1562 (11.8385)\tPrec@1 38.000 (33.381)\tPrec@5 89.000 (88.429)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 10.6754 (11.7305)\tPrec@1 40.000 (34.548)\tPrec@5 85.000 (88.645)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 12.1106 (11.7702)\tPrec@1 29.000 (34.317)\tPrec@5 94.000 (88.780)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 11.0895 (11.6645)\tPrec@1 36.000 (34.980)\tPrec@5 91.000 (89.078)\n",
      "Test: [60/100]\tTime 0.074 (0.076)\tLoss 10.6380 (11.6683)\tPrec@1 39.000 (34.607)\tPrec@5 90.000 (89.180)\n",
      "Test: [70/100]\tTime 0.079 (0.076)\tLoss 10.5647 (11.6231)\tPrec@1 43.000 (34.944)\tPrec@5 92.000 (89.070)\n",
      "Test: [80/100]\tTime 0.075 (0.076)\tLoss 10.1465 (11.5725)\tPrec@1 48.000 (35.358)\tPrec@5 89.000 (89.099)\n",
      "Test: [90/100]\tTime 0.075 (0.076)\tLoss 11.8336 (11.6533)\tPrec@1 37.000 (34.857)\tPrec@5 90.000 (88.835)\n",
      "val Results: Prec@1 34.770 Prec@5 88.800 Loss 11.69503\n",
      "val Class Accuracy: [0.795,0.986,0.614,0.296,0.707,0.016,0.004,0.059,0.000,0.000]\n",
      "Best Prec@1: 35.190\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [13][0/97], lr: 0.01000\tTime 1.015 (1.015)\tData 0.636 (0.636)\tLoss 4.0180 (4.0180)\tPrec@1 71.875 (71.875)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [13][10/97], lr: 0.01000\tTime 0.349 (0.440)\tData 0.000 (0.069)\tLoss 3.9660 (3.7282)\tPrec@1 71.094 (74.219)\tPrec@5 96.875 (97.514)\n",
      "Epoch: [13][20/97], lr: 0.01000\tTime 0.353 (0.401)\tData 0.000 (0.044)\tLoss 4.1534 (3.7519)\tPrec@1 70.312 (74.070)\tPrec@5 96.875 (97.284)\n",
      "Epoch: [13][30/97], lr: 0.01000\tTime 0.350 (0.389)\tData 0.000 (0.035)\tLoss 4.3679 (3.6468)\tPrec@1 69.531 (74.748)\tPrec@5 99.219 (97.303)\n",
      "Epoch: [13][40/97], lr: 0.01000\tTime 0.353 (0.381)\tData 0.000 (0.030)\tLoss 3.5488 (3.7023)\tPrec@1 79.688 (74.638)\tPrec@5 97.656 (97.161)\n",
      "Epoch: [13][50/97], lr: 0.01000\tTime 0.338 (0.375)\tData 0.000 (0.028)\tLoss 3.4925 (3.6764)\tPrec@1 78.125 (74.985)\tPrec@5 96.875 (97.044)\n",
      "Epoch: [13][60/97], lr: 0.01000\tTime 0.349 (0.372)\tData 0.000 (0.026)\tLoss 3.6572 (3.6972)\tPrec@1 75.781 (74.782)\tPrec@5 97.656 (96.926)\n",
      "Epoch: [13][70/97], lr: 0.01000\tTime 0.335 (0.368)\tData 0.000 (0.024)\tLoss 3.8626 (3.7014)\tPrec@1 74.219 (74.670)\tPrec@5 97.656 (97.073)\n",
      "Epoch: [13][80/97], lr: 0.01000\tTime 0.342 (0.364)\tData 0.000 (0.023)\tLoss 4.3355 (3.7079)\tPrec@1 72.656 (74.653)\tPrec@5 98.438 (97.087)\n",
      "Epoch: [13][90/97], lr: 0.01000\tTime 0.336 (0.362)\tData 0.000 (0.023)\tLoss 2.8399 (3.7009)\tPrec@1 84.375 (74.742)\tPrec@5 98.438 (97.047)\n",
      "Epoch: [13][96/97], lr: 0.01000\tTime 0.326 (0.361)\tData 0.000 (0.023)\tLoss 3.3792 (3.6931)\tPrec@1 77.966 (74.811)\tPrec@5 97.458 (97.090)\n",
      "Gated Network Weight Gate= Flip:0.60, Sc:0.40\n",
      "Test: [0/100]\tTime 0.351 (0.351)\tLoss 12.5417 (12.5417)\tPrec@1 29.000 (29.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.076 (0.099)\tLoss 9.4358 (11.4773)\tPrec@1 47.000 (35.545)\tPrec@5 94.000 (89.909)\n",
      "Test: [20/100]\tTime 0.076 (0.087)\tLoss 10.2202 (11.4837)\tPrec@1 43.000 (34.810)\tPrec@5 89.000 (89.619)\n",
      "Test: [30/100]\tTime 0.076 (0.083)\tLoss 11.1535 (11.3790)\tPrec@1 31.000 (35.419)\tPrec@5 91.000 (89.839)\n",
      "Test: [40/100]\tTime 0.074 (0.081)\tLoss 11.9495 (11.4122)\tPrec@1 30.000 (35.512)\tPrec@5 82.000 (89.195)\n",
      "Test: [50/100]\tTime 0.074 (0.080)\tLoss 11.4198 (11.3235)\tPrec@1 37.000 (36.078)\tPrec@5 89.000 (89.373)\n",
      "Test: [60/100]\tTime 0.075 (0.079)\tLoss 9.2724 (11.3102)\tPrec@1 43.000 (35.639)\tPrec@5 94.000 (89.508)\n",
      "Test: [70/100]\tTime 0.079 (0.079)\tLoss 11.0029 (11.2718)\tPrec@1 41.000 (35.972)\tPrec@5 89.000 (89.493)\n",
      "Test: [80/100]\tTime 0.075 (0.079)\tLoss 10.7296 (11.2334)\tPrec@1 43.000 (36.247)\tPrec@5 84.000 (89.642)\n",
      "Test: [90/100]\tTime 0.075 (0.078)\tLoss 10.8079 (11.2870)\tPrec@1 42.000 (36.077)\tPrec@5 95.000 (89.681)\n",
      "val Results: Prec@1 35.970 Prec@5 89.760 Loss 11.31989\n",
      "val Class Accuracy: [0.913,0.978,0.731,0.383,0.241,0.037,0.312,0.002,0.000,0.000]\n",
      "Best Prec@1: 35.970\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [14][0/97], lr: 0.01000\tTime 1.003 (1.003)\tData 0.627 (0.627)\tLoss 3.2060 (3.2060)\tPrec@1 78.125 (78.125)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [14][10/97], lr: 0.01000\tTime 0.330 (0.442)\tData 0.000 (0.068)\tLoss 3.7673 (3.4814)\tPrec@1 75.781 (76.278)\tPrec@5 97.656 (98.580)\n",
      "Epoch: [14][20/97], lr: 0.01000\tTime 0.317 (0.390)\tData 0.000 (0.044)\tLoss 3.5187 (3.4346)\tPrec@1 77.344 (77.083)\tPrec@5 96.875 (98.028)\n",
      "Epoch: [14][30/97], lr: 0.01000\tTime 0.313 (0.367)\tData 0.000 (0.035)\tLoss 3.8264 (3.4936)\tPrec@1 78.906 (77.167)\tPrec@5 97.656 (97.782)\n",
      "Epoch: [14][40/97], lr: 0.01000\tTime 0.317 (0.355)\tData 0.000 (0.031)\tLoss 4.3250 (3.5391)\tPrec@1 71.094 (76.677)\tPrec@5 96.875 (97.694)\n",
      "Epoch: [14][50/97], lr: 0.01000\tTime 0.316 (0.347)\tData 0.000 (0.028)\tLoss 3.0757 (3.5512)\tPrec@1 78.906 (76.823)\tPrec@5 96.875 (97.595)\n",
      "Epoch: [14][60/97], lr: 0.01000\tTime 0.313 (0.342)\tData 0.000 (0.026)\tLoss 3.4369 (3.5632)\tPrec@1 77.344 (76.716)\tPrec@5 98.438 (97.464)\n",
      "Epoch: [14][70/97], lr: 0.01000\tTime 0.319 (0.339)\tData 0.000 (0.025)\tLoss 3.3941 (3.6038)\tPrec@1 77.344 (76.265)\tPrec@5 100.000 (97.524)\n",
      "Epoch: [14][80/97], lr: 0.01000\tTime 0.317 (0.336)\tData 0.000 (0.024)\tLoss 3.2934 (3.5654)\tPrec@1 77.344 (76.649)\tPrec@5 97.656 (97.560)\n",
      "Epoch: [14][90/97], lr: 0.01000\tTime 0.314 (0.334)\tData 0.000 (0.023)\tLoss 3.4219 (3.5883)\tPrec@1 78.125 (76.417)\tPrec@5 98.438 (97.536)\n",
      "Epoch: [14][96/97], lr: 0.01000\tTime 0.308 (0.333)\tData 0.000 (0.024)\tLoss 4.3754 (3.5837)\tPrec@1 70.339 (76.495)\tPrec@5 97.458 (97.525)\n",
      "Gated Network Weight Gate= Flip:0.68, Sc:0.32\n",
      "Test: [0/100]\tTime 0.248 (0.248)\tLoss 14.3664 (14.3664)\tPrec@1 17.000 (17.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 10.9773 (12.6871)\tPrec@1 36.000 (28.182)\tPrec@5 90.000 (91.727)\n",
      "Test: [20/100]\tTime 0.074 (0.082)\tLoss 11.5710 (12.6222)\tPrec@1 31.000 (29.524)\tPrec@5 96.000 (90.905)\n",
      "Test: [30/100]\tTime 0.074 (0.079)\tLoss 11.7215 (12.4611)\tPrec@1 32.000 (30.323)\tPrec@5 89.000 (90.710)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 12.6861 (12.4825)\tPrec@1 27.000 (30.366)\tPrec@5 90.000 (90.780)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 11.8644 (12.3493)\tPrec@1 35.000 (31.137)\tPrec@5 87.000 (90.784)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 11.1095 (12.3239)\tPrec@1 37.000 (31.082)\tPrec@5 94.000 (90.820)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 11.7685 (12.2992)\tPrec@1 34.000 (31.296)\tPrec@5 93.000 (90.761)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 10.8765 (12.2404)\tPrec@1 39.000 (31.568)\tPrec@5 97.000 (90.889)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 11.9465 (12.2936)\tPrec@1 33.000 (31.264)\tPrec@5 94.000 (90.846)\n",
      "val Results: Prec@1 31.150 Prec@5 91.020 Loss 12.31961\n",
      "val Class Accuracy: [0.689,0.997,0.498,0.122,0.735,0.036,0.027,0.011,0.000,0.000]\n",
      "Best Prec@1: 35.970\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [15][0/97], lr: 0.01000\tTime 0.428 (0.428)\tData 0.220 (0.220)\tLoss 3.4371 (3.4371)\tPrec@1 81.250 (81.250)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [15][10/97], lr: 0.01000\tTime 0.315 (0.332)\tData 0.000 (0.035)\tLoss 3.1940 (3.4887)\tPrec@1 78.125 (76.634)\tPrec@5 96.875 (97.727)\n",
      "Epoch: [15][20/97], lr: 0.01000\tTime 0.317 (0.325)\tData 0.000 (0.027)\tLoss 4.0718 (3.5696)\tPrec@1 73.438 (76.525)\tPrec@5 94.531 (97.582)\n",
      "Epoch: [15][30/97], lr: 0.01000\tTime 0.315 (0.323)\tData 0.000 (0.024)\tLoss 3.4336 (3.5345)\tPrec@1 77.344 (76.890)\tPrec@5 97.656 (97.732)\n",
      "Epoch: [15][40/97], lr: 0.01000\tTime 0.321 (0.322)\tData 0.000 (0.022)\tLoss 3.6289 (3.4942)\tPrec@1 75.000 (76.963)\tPrec@5 98.438 (97.923)\n",
      "Epoch: [15][50/97], lr: 0.01000\tTime 0.314 (0.321)\tData 0.000 (0.021)\tLoss 2.9152 (3.5065)\tPrec@1 82.031 (76.900)\tPrec@5 97.656 (97.794)\n",
      "Epoch: [15][60/97], lr: 0.01000\tTime 0.303 (0.321)\tData 0.000 (0.020)\tLoss 4.7870 (3.5116)\tPrec@1 66.406 (76.716)\tPrec@5 97.656 (97.784)\n",
      "Epoch: [15][70/97], lr: 0.01000\tTime 0.320 (0.320)\tData 0.000 (0.020)\tLoss 3.7107 (3.5340)\tPrec@1 75.000 (76.551)\tPrec@5 98.438 (97.777)\n",
      "Epoch: [15][80/97], lr: 0.01000\tTime 0.315 (0.320)\tData 0.000 (0.020)\tLoss 3.9354 (3.5320)\tPrec@1 74.219 (76.562)\tPrec@5 96.094 (97.733)\n",
      "Epoch: [15][90/97], lr: 0.01000\tTime 0.316 (0.320)\tData 0.000 (0.019)\tLoss 2.9045 (3.5450)\tPrec@1 83.594 (76.511)\tPrec@5 98.438 (97.691)\n",
      "Epoch: [15][96/97], lr: 0.01000\tTime 0.305 (0.319)\tData 0.000 (0.020)\tLoss 3.8828 (3.5485)\tPrec@1 76.271 (76.511)\tPrec@5 98.305 (97.695)\n",
      "Gated Network Weight Gate= Flip:0.66, Sc:0.34\n",
      "Test: [0/100]\tTime 0.263 (0.263)\tLoss 12.1426 (12.1426)\tPrec@1 30.000 (30.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 9.7008 (11.1379)\tPrec@1 45.000 (37.455)\tPrec@5 87.000 (91.364)\n",
      "Test: [20/100]\tTime 0.074 (0.082)\tLoss 9.5495 (11.1391)\tPrec@1 43.000 (37.333)\tPrec@5 95.000 (91.190)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 10.2683 (11.0591)\tPrec@1 42.000 (38.194)\tPrec@5 89.000 (91.194)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 11.4921 (11.0509)\tPrec@1 34.000 (38.512)\tPrec@5 90.000 (91.146)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 10.6902 (10.9577)\tPrec@1 42.000 (39.431)\tPrec@5 90.000 (91.216)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 9.3956 (10.9369)\tPrec@1 48.000 (39.164)\tPrec@5 93.000 (91.033)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 10.5242 (10.9129)\tPrec@1 40.000 (39.169)\tPrec@5 92.000 (90.972)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 9.7136 (10.8769)\tPrec@1 46.000 (39.086)\tPrec@5 91.000 (91.099)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 10.4469 (10.9476)\tPrec@1 43.000 (38.670)\tPrec@5 95.000 (91.088)\n",
      "val Results: Prec@1 38.550 Prec@5 91.220 Loss 10.97424\n",
      "val Class Accuracy: [0.963,0.978,0.587,0.297,0.407,0.453,0.099,0.068,0.000,0.003]\n",
      "Best Prec@1: 38.550\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [16][0/97], lr: 0.01000\tTime 0.445 (0.445)\tData 0.253 (0.253)\tLoss 3.1024 (3.1024)\tPrec@1 76.562 (76.562)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [16][10/97], lr: 0.01000\tTime 0.314 (0.334)\tData 0.000 (0.037)\tLoss 3.7228 (3.5166)\tPrec@1 76.562 (76.065)\tPrec@5 99.219 (97.514)\n",
      "Epoch: [16][20/97], lr: 0.01000\tTime 0.322 (0.326)\tData 0.000 (0.028)\tLoss 4.5421 (3.6218)\tPrec@1 71.875 (75.632)\tPrec@5 98.438 (97.917)\n",
      "Epoch: [16][30/97], lr: 0.01000\tTime 0.317 (0.323)\tData 0.000 (0.024)\tLoss 3.8382 (3.5340)\tPrec@1 75.000 (76.638)\tPrec@5 98.438 (97.858)\n",
      "Epoch: [16][40/97], lr: 0.01000\tTime 0.316 (0.321)\tData 0.000 (0.022)\tLoss 3.6808 (3.5557)\tPrec@1 75.000 (76.391)\tPrec@5 94.531 (97.580)\n",
      "Epoch: [16][50/97], lr: 0.01000\tTime 0.315 (0.320)\tData 0.000 (0.021)\tLoss 3.0935 (3.5076)\tPrec@1 82.031 (77.053)\tPrec@5 96.875 (97.718)\n",
      "Epoch: [16][60/97], lr: 0.01000\tTime 0.314 (0.320)\tData 0.000 (0.021)\tLoss 5.0607 (3.5249)\tPrec@1 63.281 (76.883)\tPrec@5 94.531 (97.579)\n",
      "Epoch: [16][70/97], lr: 0.01000\tTime 0.315 (0.320)\tData 0.000 (0.020)\tLoss 2.5266 (3.5054)\tPrec@1 85.156 (77.091)\tPrec@5 98.438 (97.623)\n",
      "Epoch: [16][80/97], lr: 0.01000\tTime 0.320 (0.319)\tData 0.000 (0.020)\tLoss 3.5869 (3.4645)\tPrec@1 75.781 (77.411)\tPrec@5 96.875 (97.627)\n",
      "Epoch: [16][90/97], lr: 0.01000\tTime 0.314 (0.319)\tData 0.000 (0.020)\tLoss 4.1944 (3.4653)\tPrec@1 74.219 (77.344)\tPrec@5 98.438 (97.613)\n",
      "Epoch: [16][96/97], lr: 0.01000\tTime 0.306 (0.319)\tData 0.000 (0.020)\tLoss 3.4840 (3.4576)\tPrec@1 76.271 (77.382)\tPrec@5 98.305 (97.638)\n",
      "Gated Network Weight Gate= Flip:0.64, Sc:0.36\n",
      "Test: [0/100]\tTime 0.246 (0.246)\tLoss 11.7278 (11.7278)\tPrec@1 42.000 (42.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 8.2446 (10.7095)\tPrec@1 59.000 (42.364)\tPrec@5 95.000 (91.909)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 10.1398 (10.7552)\tPrec@1 42.000 (41.286)\tPrec@5 95.000 (92.810)\n",
      "Test: [30/100]\tTime 0.074 (0.079)\tLoss 10.3382 (10.7573)\tPrec@1 41.000 (41.161)\tPrec@5 89.000 (92.452)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 11.2713 (10.8318)\tPrec@1 39.000 (40.683)\tPrec@5 87.000 (92.049)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 10.2791 (10.6881)\tPrec@1 47.000 (41.824)\tPrec@5 88.000 (92.235)\n",
      "Test: [60/100]\tTime 0.074 (0.076)\tLoss 9.3573 (10.6520)\tPrec@1 46.000 (41.820)\tPrec@5 93.000 (92.328)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 10.2114 (10.6367)\tPrec@1 48.000 (41.972)\tPrec@5 90.000 (92.197)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 9.3995 (10.5982)\tPrec@1 51.000 (42.198)\tPrec@5 90.000 (92.173)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 10.7228 (10.6541)\tPrec@1 37.000 (41.857)\tPrec@5 94.000 (92.198)\n",
      "val Results: Prec@1 41.750 Prec@5 92.110 Loss 10.68797\n",
      "val Class Accuracy: [0.954,0.893,0.388,0.730,0.552,0.024,0.632,0.000,0.000,0.002]\n",
      "Best Prec@1: 41.750\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [17][0/97], lr: 0.01000\tTime 0.463 (0.463)\tData 0.257 (0.257)\tLoss 3.6488 (3.6488)\tPrec@1 75.000 (75.000)\tPrec@5 96.094 (96.094)\n",
      "Epoch: [17][10/97], lr: 0.01000\tTime 0.315 (0.334)\tData 0.000 (0.038)\tLoss 3.1065 (3.6262)\tPrec@1 78.125 (75.355)\tPrec@5 100.000 (97.159)\n",
      "Epoch: [17][20/97], lr: 0.01000\tTime 0.317 (0.325)\tData 0.000 (0.028)\tLoss 3.3699 (3.6604)\tPrec@1 79.688 (74.963)\tPrec@5 96.875 (97.470)\n",
      "Epoch: [17][30/97], lr: 0.01000\tTime 0.315 (0.323)\tData 0.000 (0.025)\tLoss 2.6036 (3.4615)\tPrec@1 82.031 (76.815)\tPrec@5 98.438 (97.732)\n",
      "Epoch: [17][40/97], lr: 0.01000\tTime 0.321 (0.322)\tData 0.000 (0.023)\tLoss 2.5412 (3.4238)\tPrec@1 82.812 (77.287)\tPrec@5 99.219 (97.713)\n",
      "Epoch: [17][50/97], lr: 0.01000\tTime 0.315 (0.321)\tData 0.000 (0.022)\tLoss 2.9987 (3.4217)\tPrec@1 78.906 (77.099)\tPrec@5 98.438 (97.702)\n",
      "Epoch: [17][60/97], lr: 0.01000\tTime 0.315 (0.321)\tData 0.000 (0.021)\tLoss 3.7574 (3.4337)\tPrec@1 75.000 (77.036)\tPrec@5 96.875 (97.707)\n",
      "Epoch: [17][70/97], lr: 0.01000\tTime 0.322 (0.320)\tData 0.000 (0.021)\tLoss 3.2831 (3.4449)\tPrec@1 78.906 (77.003)\tPrec@5 99.219 (97.722)\n",
      "Epoch: [17][80/97], lr: 0.01000\tTime 0.315 (0.320)\tData 0.000 (0.020)\tLoss 3.3802 (3.4354)\tPrec@1 78.125 (77.132)\tPrec@5 97.656 (97.733)\n",
      "Epoch: [17][90/97], lr: 0.01000\tTime 0.315 (0.320)\tData 0.000 (0.020)\tLoss 3.5828 (3.4440)\tPrec@1 75.781 (77.060)\tPrec@5 97.656 (97.751)\n",
      "Epoch: [17][96/97], lr: 0.01000\tTime 0.305 (0.320)\tData 0.000 (0.020)\tLoss 2.7402 (3.4363)\tPrec@1 82.203 (77.140)\tPrec@5 98.305 (97.743)\n",
      "Gated Network Weight Gate= Flip:0.51, Sc:0.49\n",
      "Test: [0/100]\tTime 0.250 (0.250)\tLoss 11.9887 (11.9887)\tPrec@1 38.000 (38.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 9.1378 (11.1120)\tPrec@1 50.000 (40.727)\tPrec@5 92.000 (91.636)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 10.2917 (11.1406)\tPrec@1 42.000 (40.476)\tPrec@5 91.000 (91.667)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 10.4906 (11.1459)\tPrec@1 42.000 (40.419)\tPrec@5 91.000 (91.710)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 11.3907 (11.2204)\tPrec@1 41.000 (40.000)\tPrec@5 91.000 (91.415)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 10.7693 (11.0948)\tPrec@1 41.000 (40.667)\tPrec@5 90.000 (91.627)\n",
      "Test: [60/100]\tTime 0.074 (0.076)\tLoss 9.7394 (11.0788)\tPrec@1 43.000 (40.459)\tPrec@5 91.000 (91.344)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 10.2761 (11.0504)\tPrec@1 46.000 (40.690)\tPrec@5 92.000 (91.155)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 9.8643 (11.0167)\tPrec@1 47.000 (40.840)\tPrec@5 86.000 (91.160)\n",
      "Test: [90/100]\tTime 0.074 (0.075)\tLoss 11.3997 (11.0718)\tPrec@1 39.000 (40.462)\tPrec@5 94.000 (91.143)\n",
      "val Results: Prec@1 40.400 Prec@5 91.020 Loss 11.10658\n",
      "val Class Accuracy: [0.916,0.967,0.576,0.768,0.388,0.000,0.415,0.010,0.000,0.000]\n",
      "Best Prec@1: 41.750\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [18][0/97], lr: 0.01000\tTime 0.508 (0.508)\tData 0.271 (0.271)\tLoss 3.3842 (3.3842)\tPrec@1 75.781 (75.781)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [18][10/97], lr: 0.01000\tTime 0.317 (0.343)\tData 0.000 (0.039)\tLoss 3.7491 (3.5530)\tPrec@1 74.219 (75.852)\tPrec@5 97.656 (97.798)\n",
      "Epoch: [18][20/97], lr: 0.01000\tTime 0.323 (0.330)\tData 0.000 (0.028)\tLoss 2.9648 (3.3360)\tPrec@1 80.469 (78.088)\tPrec@5 100.000 (98.065)\n",
      "Epoch: [18][30/97], lr: 0.01000\tTime 0.313 (0.325)\tData 0.000 (0.025)\tLoss 2.9169 (3.3168)\tPrec@1 80.469 (78.175)\tPrec@5 98.438 (98.059)\n",
      "Epoch: [18][40/97], lr: 0.01000\tTime 0.314 (0.323)\tData 0.000 (0.023)\tLoss 3.9032 (3.3098)\tPrec@1 75.781 (78.354)\tPrec@5 97.656 (97.961)\n",
      "Epoch: [18][50/97], lr: 0.01000\tTime 0.314 (0.323)\tData 0.000 (0.022)\tLoss 3.2901 (3.2908)\tPrec@1 76.562 (78.477)\tPrec@5 95.312 (97.779)\n",
      "Epoch: [18][60/97], lr: 0.01000\tTime 0.317 (0.322)\tData 0.000 (0.021)\tLoss 3.2387 (3.2632)\tPrec@1 77.344 (78.624)\tPrec@5 96.875 (97.836)\n",
      "Epoch: [18][70/97], lr: 0.01000\tTime 0.317 (0.321)\tData 0.000 (0.020)\tLoss 2.8993 (3.2782)\tPrec@1 80.469 (78.576)\tPrec@5 100.000 (97.975)\n",
      "Epoch: [18][80/97], lr: 0.01000\tTime 0.349 (0.325)\tData 0.000 (0.020)\tLoss 3.6872 (3.2988)\tPrec@1 75.781 (78.520)\tPrec@5 96.094 (97.917)\n",
      "Epoch: [18][90/97], lr: 0.01000\tTime 0.371 (0.329)\tData 0.000 (0.020)\tLoss 2.6275 (3.3177)\tPrec@1 82.031 (78.408)\tPrec@5 100.000 (97.957)\n",
      "Epoch: [18][96/97], lr: 0.01000\tTime 0.313 (0.329)\tData 0.000 (0.020)\tLoss 3.9912 (3.3381)\tPrec@1 77.966 (78.236)\tPrec@5 97.458 (97.977)\n",
      "Gated Network Weight Gate= Flip:0.49, Sc:0.51\n",
      "Test: [0/100]\tTime 0.286 (0.286)\tLoss 14.1383 (14.1383)\tPrec@1 24.000 (24.000)\tPrec@5 89.000 (89.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 10.8912 (13.1521)\tPrec@1 42.000 (29.727)\tPrec@5 84.000 (88.273)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 12.2747 (13.2883)\tPrec@1 34.000 (29.381)\tPrec@5 90.000 (88.429)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 11.8677 (13.1991)\tPrec@1 35.000 (29.355)\tPrec@5 91.000 (88.806)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 13.6651 (13.2221)\tPrec@1 29.000 (29.293)\tPrec@5 88.000 (88.707)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 13.1481 (13.0922)\tPrec@1 31.000 (29.980)\tPrec@5 86.000 (89.020)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 12.0752 (13.0688)\tPrec@1 30.000 (29.885)\tPrec@5 89.000 (88.836)\n",
      "Test: [70/100]\tTime 0.075 (0.077)\tLoss 12.6832 (13.0241)\tPrec@1 33.000 (30.155)\tPrec@5 92.000 (88.901)\n",
      "Test: [80/100]\tTime 0.075 (0.076)\tLoss 11.4876 (12.9746)\tPrec@1 43.000 (30.247)\tPrec@5 92.000 (89.086)\n",
      "Test: [90/100]\tTime 0.082 (0.076)\tLoss 12.6749 (13.0500)\tPrec@1 31.000 (29.769)\tPrec@5 93.000 (88.923)\n",
      "val Results: Prec@1 29.810 Prec@5 88.960 Loss 13.07346\n",
      "val Class Accuracy: [0.989,0.955,0.299,0.178,0.387,0.101,0.051,0.021,0.000,0.000]\n",
      "Best Prec@1: 41.750\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [19][0/97], lr: 0.01000\tTime 0.805 (0.805)\tData 0.487 (0.487)\tLoss 3.5925 (3.5925)\tPrec@1 76.562 (76.562)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [19][10/97], lr: 0.01000\tTime 0.334 (0.368)\tData 0.000 (0.056)\tLoss 3.3516 (3.1842)\tPrec@1 77.344 (78.764)\tPrec@5 98.438 (97.585)\n",
      "Epoch: [19][20/97], lr: 0.01000\tTime 0.333 (0.346)\tData 0.000 (0.037)\tLoss 4.0005 (3.3447)\tPrec@1 77.344 (78.311)\tPrec@5 97.656 (97.433)\n",
      "Epoch: [19][30/97], lr: 0.01000\tTime 0.322 (0.343)\tData 0.000 (0.031)\tLoss 3.0025 (3.2855)\tPrec@1 78.906 (78.654)\tPrec@5 96.875 (97.681)\n",
      "Epoch: [19][40/97], lr: 0.01000\tTime 0.391 (0.343)\tData 0.000 (0.027)\tLoss 3.4475 (3.2474)\tPrec@1 78.906 (79.021)\tPrec@5 98.438 (97.809)\n",
      "Epoch: [19][50/97], lr: 0.01000\tTime 0.329 (0.342)\tData 0.000 (0.025)\tLoss 2.6485 (3.2556)\tPrec@1 86.719 (79.105)\tPrec@5 98.438 (97.901)\n",
      "Epoch: [19][60/97], lr: 0.01000\tTime 0.315 (0.339)\tData 0.000 (0.024)\tLoss 4.6486 (3.2986)\tPrec@1 70.312 (78.893)\tPrec@5 97.656 (97.964)\n",
      "Epoch: [19][70/97], lr: 0.01000\tTime 0.333 (0.338)\tData 0.000 (0.023)\tLoss 2.9390 (3.3084)\tPrec@1 80.469 (78.862)\tPrec@5 98.438 (97.865)\n",
      "Epoch: [19][80/97], lr: 0.01000\tTime 0.325 (0.337)\tData 0.000 (0.022)\tLoss 3.3328 (3.3033)\tPrec@1 79.688 (78.877)\tPrec@5 98.438 (97.955)\n",
      "Epoch: [19][90/97], lr: 0.01000\tTime 0.343 (0.337)\tData 0.000 (0.021)\tLoss 2.7911 (3.2867)\tPrec@1 80.469 (78.949)\tPrec@5 100.000 (98.034)\n",
      "Epoch: [19][96/97], lr: 0.01000\tTime 0.308 (0.336)\tData 0.000 (0.022)\tLoss 3.2871 (3.2851)\tPrec@1 79.661 (78.938)\tPrec@5 100.000 (98.049)\n",
      "Gated Network Weight Gate= Flip:0.66, Sc:0.34\n",
      "Test: [0/100]\tTime 0.255 (0.255)\tLoss 13.4200 (13.4200)\tPrec@1 35.000 (35.000)\tPrec@5 90.000 (90.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 10.5941 (12.4047)\tPrec@1 46.000 (35.455)\tPrec@5 91.000 (91.909)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 11.5050 (12.5872)\tPrec@1 36.000 (34.190)\tPrec@5 92.000 (91.095)\n",
      "Test: [30/100]\tTime 0.074 (0.079)\tLoss 11.4638 (12.5114)\tPrec@1 40.000 (34.548)\tPrec@5 90.000 (90.774)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 13.6854 (12.6014)\tPrec@1 28.000 (33.878)\tPrec@5 88.000 (90.610)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 12.6241 (12.5008)\tPrec@1 31.000 (34.157)\tPrec@5 87.000 (90.941)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 10.6280 (12.4893)\tPrec@1 42.000 (34.033)\tPrec@5 91.000 (90.820)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 12.2014 (12.4468)\tPrec@1 38.000 (34.310)\tPrec@5 89.000 (90.577)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 10.7860 (12.3979)\tPrec@1 45.000 (34.543)\tPrec@5 93.000 (90.630)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 12.5591 (12.4545)\tPrec@1 36.000 (34.209)\tPrec@5 86.000 (90.527)\n",
      "val Results: Prec@1 34.130 Prec@5 90.540 Loss 12.48246\n",
      "val Class Accuracy: [0.922,0.994,0.596,0.339,0.271,0.032,0.237,0.022,0.000,0.000]\n",
      "Best Prec@1: 41.750\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [20][0/97], lr: 0.01000\tTime 0.462 (0.462)\tData 0.258 (0.258)\tLoss 2.6994 (2.6994)\tPrec@1 83.594 (83.594)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [20][10/97], lr: 0.01000\tTime 0.317 (0.332)\tData 0.000 (0.038)\tLoss 2.8269 (3.2307)\tPrec@1 78.125 (78.267)\tPrec@5 97.656 (98.438)\n",
      "Epoch: [20][20/97], lr: 0.01000\tTime 0.325 (0.324)\tData 0.000 (0.028)\tLoss 2.9369 (3.1486)\tPrec@1 81.250 (79.167)\tPrec@5 98.438 (98.475)\n",
      "Epoch: [20][30/97], lr: 0.01000\tTime 0.327 (0.327)\tData 0.000 (0.025)\tLoss 3.1357 (3.1412)\tPrec@1 76.562 (79.284)\tPrec@5 98.438 (98.236)\n",
      "Epoch: [20][40/97], lr: 0.01000\tTime 0.334 (0.330)\tData 0.000 (0.023)\tLoss 3.6311 (3.1559)\tPrec@1 77.344 (79.306)\tPrec@5 97.656 (98.133)\n",
      "Epoch: [20][50/97], lr: 0.01000\tTime 0.317 (0.328)\tData 0.000 (0.021)\tLoss 3.1120 (3.1805)\tPrec@1 81.250 (79.151)\tPrec@5 98.438 (98.192)\n",
      "Epoch: [20][60/97], lr: 0.01000\tTime 0.331 (0.327)\tData 0.000 (0.021)\tLoss 3.8161 (3.2175)\tPrec@1 77.344 (79.022)\tPrec@5 96.875 (98.309)\n",
      "Epoch: [20][70/97], lr: 0.01000\tTime 0.441 (0.333)\tData 0.000 (0.020)\tLoss 3.6813 (3.2078)\tPrec@1 77.344 (79.027)\tPrec@5 97.656 (98.360)\n",
      "Epoch: [20][80/97], lr: 0.01000\tTime 0.342 (0.338)\tData 0.000 (0.019)\tLoss 2.8610 (3.1944)\tPrec@1 83.594 (79.041)\tPrec@5 97.656 (98.341)\n",
      "Epoch: [20][90/97], lr: 0.01000\tTime 0.326 (0.338)\tData 0.000 (0.019)\tLoss 3.8562 (3.1905)\tPrec@1 72.656 (79.009)\tPrec@5 97.656 (98.352)\n",
      "Epoch: [20][96/97], lr: 0.01000\tTime 0.307 (0.337)\tData 0.000 (0.020)\tLoss 3.2393 (3.2041)\tPrec@1 77.119 (78.938)\tPrec@5 97.458 (98.364)\n",
      "Gated Network Weight Gate= Flip:0.63, Sc:0.37\n",
      "Test: [0/100]\tTime 0.309 (0.309)\tLoss 11.4289 (11.4289)\tPrec@1 41.000 (41.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.095)\tLoss 8.6361 (10.7222)\tPrec@1 54.000 (41.364)\tPrec@5 96.000 (94.091)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 9.7714 (10.7376)\tPrec@1 43.000 (40.762)\tPrec@5 96.000 (93.857)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 10.2878 (10.6663)\tPrec@1 43.000 (41.161)\tPrec@5 95.000 (93.774)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 11.2518 (10.7235)\tPrec@1 39.000 (40.659)\tPrec@5 90.000 (93.659)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 10.3320 (10.6243)\tPrec@1 43.000 (41.471)\tPrec@5 93.000 (93.725)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 9.5009 (10.6357)\tPrec@1 44.000 (41.262)\tPrec@5 93.000 (93.541)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 10.0291 (10.5958)\tPrec@1 44.000 (41.549)\tPrec@5 92.000 (93.549)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 9.7492 (10.5707)\tPrec@1 46.000 (41.679)\tPrec@5 88.000 (93.642)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 10.5331 (10.6284)\tPrec@1 39.000 (41.516)\tPrec@5 98.000 (93.736)\n",
      "val Results: Prec@1 41.360 Prec@5 93.640 Loss 10.65341\n",
      "val Class Accuracy: [0.920,0.967,0.791,0.648,0.285,0.062,0.408,0.052,0.002,0.001]\n",
      "Best Prec@1: 41.750\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [21][0/97], lr: 0.01000\tTime 0.504 (0.504)\tData 0.257 (0.257)\tLoss 3.2608 (3.2608)\tPrec@1 76.562 (76.562)\tPrec@5 96.875 (96.875)\n",
      "Epoch: [21][10/97], lr: 0.01000\tTime 0.339 (0.361)\tData 0.000 (0.036)\tLoss 2.8562 (3.1324)\tPrec@1 78.906 (79.830)\tPrec@5 99.219 (98.366)\n",
      "Epoch: [21][20/97], lr: 0.01000\tTime 0.343 (0.354)\tData 0.000 (0.027)\tLoss 3.5053 (3.0934)\tPrec@1 78.125 (80.208)\tPrec@5 99.219 (98.698)\n",
      "Epoch: [21][30/97], lr: 0.01000\tTime 0.323 (0.347)\tData 0.000 (0.024)\tLoss 3.0125 (3.1167)\tPrec@1 84.375 (80.343)\tPrec@5 97.656 (98.614)\n",
      "Epoch: [21][40/97], lr: 0.01000\tTime 0.313 (0.340)\tData 0.000 (0.022)\tLoss 3.8029 (3.1487)\tPrec@1 75.000 (80.107)\tPrec@5 96.875 (98.361)\n",
      "Epoch: [21][50/97], lr: 0.01000\tTime 0.323 (0.338)\tData 0.000 (0.021)\tLoss 3.0887 (3.1629)\tPrec@1 81.250 (79.979)\tPrec@5 96.875 (98.238)\n",
      "Epoch: [21][60/97], lr: 0.01000\tTime 0.347 (0.335)\tData 0.000 (0.020)\tLoss 2.5240 (3.1853)\tPrec@1 82.031 (79.700)\tPrec@5 96.875 (98.245)\n",
      "Epoch: [21][70/97], lr: 0.01000\tTime 0.319 (0.335)\tData 0.000 (0.020)\tLoss 3.2534 (3.1843)\tPrec@1 78.125 (79.588)\tPrec@5 96.094 (98.173)\n",
      "Epoch: [21][80/97], lr: 0.01000\tTime 0.313 (0.333)\tData 0.000 (0.019)\tLoss 3.2298 (3.1933)\tPrec@1 78.125 (79.495)\tPrec@5 100.000 (98.196)\n",
      "Epoch: [21][90/97], lr: 0.01000\tTime 0.316 (0.331)\tData 0.000 (0.019)\tLoss 2.5885 (3.2052)\tPrec@1 81.250 (79.430)\tPrec@5 99.219 (98.103)\n",
      "Epoch: [21][96/97], lr: 0.01000\tTime 0.310 (0.331)\tData 0.000 (0.020)\tLoss 3.6523 (3.2214)\tPrec@1 74.576 (79.268)\tPrec@5 95.763 (98.122)\n",
      "Gated Network Weight Gate= Flip:0.59, Sc:0.41\n",
      "Test: [0/100]\tTime 0.232 (0.232)\tLoss 10.8729 (10.8729)\tPrec@1 42.000 (42.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 8.2303 (10.3465)\tPrec@1 55.000 (45.364)\tPrec@5 94.000 (93.273)\n",
      "Test: [20/100]\tTime 0.074 (0.081)\tLoss 9.6181 (10.3845)\tPrec@1 45.000 (44.524)\tPrec@5 96.000 (93.286)\n",
      "Test: [30/100]\tTime 0.074 (0.079)\tLoss 9.5776 (10.2887)\tPrec@1 46.000 (45.323)\tPrec@5 93.000 (93.258)\n",
      "Test: [40/100]\tTime 0.075 (0.078)\tLoss 10.8098 (10.3316)\tPrec@1 41.000 (45.024)\tPrec@5 92.000 (93.195)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 10.1204 (10.2406)\tPrec@1 46.000 (45.647)\tPrec@5 92.000 (93.314)\n",
      "Test: [60/100]\tTime 0.074 (0.076)\tLoss 8.9217 (10.2019)\tPrec@1 49.000 (45.672)\tPrec@5 91.000 (93.066)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 9.8528 (10.1865)\tPrec@1 49.000 (45.775)\tPrec@5 96.000 (92.972)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 9.3896 (10.1611)\tPrec@1 49.000 (45.901)\tPrec@5 91.000 (93.025)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 9.9915 (10.1955)\tPrec@1 45.000 (45.791)\tPrec@5 94.000 (92.967)\n",
      "val Results: Prec@1 45.730 Prec@5 92.990 Loss 10.21700\n",
      "val Class Accuracy: [0.958,0.961,0.694,0.653,0.280,0.255,0.655,0.116,0.001,0.000]\n",
      "Best Prec@1: 45.730\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [22][0/97], lr: 0.01000\tTime 0.477 (0.477)\tData 0.268 (0.268)\tLoss 3.7365 (3.7365)\tPrec@1 72.656 (72.656)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [22][10/97], lr: 0.01000\tTime 0.372 (0.353)\tData 0.000 (0.040)\tLoss 2.8869 (3.2045)\tPrec@1 78.125 (78.409)\tPrec@5 97.656 (98.153)\n",
      "Epoch: [22][20/97], lr: 0.01000\tTime 0.335 (0.345)\tData 0.000 (0.028)\tLoss 2.8196 (3.1408)\tPrec@1 80.469 (78.943)\tPrec@5 98.438 (98.103)\n",
      "Epoch: [22][30/97], lr: 0.01000\tTime 0.316 (0.341)\tData 0.000 (0.025)\tLoss 3.4336 (3.0920)\tPrec@1 78.125 (79.435)\tPrec@5 96.875 (98.185)\n",
      "Epoch: [22][40/97], lr: 0.01000\tTime 0.314 (0.338)\tData 0.000 (0.023)\tLoss 3.1160 (3.1301)\tPrec@1 79.688 (79.325)\tPrec@5 100.000 (98.152)\n",
      "Epoch: [22][50/97], lr: 0.01000\tTime 0.314 (0.334)\tData 0.000 (0.022)\tLoss 2.6385 (3.1126)\tPrec@1 82.812 (79.381)\tPrec@5 96.875 (98.238)\n",
      "Epoch: [22][60/97], lr: 0.01000\tTime 0.313 (0.331)\tData 0.000 (0.021)\tLoss 4.2158 (3.1735)\tPrec@1 71.875 (79.022)\tPrec@5 97.656 (98.258)\n",
      "Epoch: [22][70/97], lr: 0.01000\tTime 0.317 (0.329)\tData 0.000 (0.020)\tLoss 2.5293 (3.1909)\tPrec@1 82.812 (79.115)\tPrec@5 99.219 (98.283)\n",
      "Epoch: [22][80/97], lr: 0.01000\tTime 0.317 (0.327)\tData 0.000 (0.020)\tLoss 3.2190 (3.2086)\tPrec@1 82.031 (78.993)\tPrec@5 99.219 (98.283)\n",
      "Epoch: [22][90/97], lr: 0.01000\tTime 0.313 (0.326)\tData 0.000 (0.020)\tLoss 3.8497 (3.2080)\tPrec@1 73.438 (79.147)\tPrec@5 97.656 (98.249)\n",
      "Epoch: [22][96/97], lr: 0.01000\tTime 0.304 (0.326)\tData 0.000 (0.020)\tLoss 3.3342 (3.1987)\tPrec@1 78.814 (79.196)\tPrec@5 100.000 (98.315)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.271 (0.271)\tLoss 13.3780 (13.3780)\tPrec@1 28.000 (28.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 10.5636 (12.5445)\tPrec@1 40.000 (32.818)\tPrec@5 95.000 (90.909)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 12.2025 (12.5667)\tPrec@1 34.000 (32.333)\tPrec@5 89.000 (90.762)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 11.7669 (12.4533)\tPrec@1 38.000 (33.484)\tPrec@5 91.000 (90.419)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 13.3335 (12.4920)\tPrec@1 29.000 (33.439)\tPrec@5 87.000 (90.366)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 12.5303 (12.4065)\tPrec@1 35.000 (33.961)\tPrec@5 87.000 (90.569)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 10.7796 (12.4355)\tPrec@1 43.000 (33.770)\tPrec@5 89.000 (90.197)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 11.7693 (12.4041)\tPrec@1 37.000 (33.972)\tPrec@5 92.000 (90.380)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 11.7424 (12.3655)\tPrec@1 39.000 (34.136)\tPrec@5 88.000 (90.531)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 11.5334 (12.4347)\tPrec@1 37.000 (33.835)\tPrec@5 92.000 (90.549)\n",
      "val Results: Prec@1 33.770 Prec@5 90.530 Loss 12.45377\n",
      "val Class Accuracy: [0.920,0.943,0.874,0.101,0.232,0.052,0.238,0.015,0.000,0.002]\n",
      "Best Prec@1: 45.730\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [23][0/97], lr: 0.01000\tTime 0.440 (0.440)\tData 0.218 (0.218)\tLoss 2.4961 (2.4961)\tPrec@1 82.031 (82.031)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [23][10/97], lr: 0.01000\tTime 0.330 (0.350)\tData 0.000 (0.034)\tLoss 3.4761 (3.2078)\tPrec@1 78.906 (79.119)\tPrec@5 98.438 (98.295)\n",
      "Epoch: [23][20/97], lr: 0.01000\tTime 0.372 (0.349)\tData 0.000 (0.026)\tLoss 3.0904 (3.1546)\tPrec@1 79.688 (79.874)\tPrec@5 99.219 (98.438)\n",
      "Epoch: [23][30/97], lr: 0.01000\tTime 0.360 (0.344)\tData 0.000 (0.023)\tLoss 3.1816 (3.1271)\tPrec@1 77.344 (80.091)\tPrec@5 100.000 (98.538)\n",
      "Epoch: [23][40/97], lr: 0.01000\tTime 0.326 (0.341)\tData 0.000 (0.021)\tLoss 3.2697 (3.1218)\tPrec@1 81.250 (80.278)\tPrec@5 99.219 (98.571)\n",
      "Epoch: [23][50/97], lr: 0.01000\tTime 0.353 (0.340)\tData 0.000 (0.021)\tLoss 3.1098 (3.1046)\tPrec@1 80.469 (80.147)\tPrec@5 97.656 (98.438)\n",
      "Epoch: [23][60/97], lr: 0.01000\tTime 0.363 (0.343)\tData 0.000 (0.020)\tLoss 3.4044 (3.1259)\tPrec@1 77.344 (80.033)\tPrec@5 97.656 (98.361)\n",
      "Epoch: [23][70/97], lr: 0.01000\tTime 0.331 (0.342)\tData 0.000 (0.019)\tLoss 2.8082 (3.1221)\tPrec@1 84.375 (79.974)\tPrec@5 99.219 (98.338)\n",
      "Epoch: [23][80/97], lr: 0.01000\tTime 0.318 (0.340)\tData 0.000 (0.019)\tLoss 4.1848 (3.1219)\tPrec@1 71.875 (79.900)\tPrec@5 97.656 (98.341)\n",
      "Epoch: [23][90/97], lr: 0.01000\tTime 0.318 (0.337)\tData 0.000 (0.019)\tLoss 3.7721 (3.1362)\tPrec@1 76.562 (79.808)\tPrec@5 98.438 (98.283)\n",
      "Epoch: [23][96/97], lr: 0.01000\tTime 0.307 (0.336)\tData 0.000 (0.019)\tLoss 3.0776 (3.1292)\tPrec@1 79.661 (79.760)\tPrec@5 99.153 (98.259)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.241 (0.241)\tLoss 13.3398 (13.3398)\tPrec@1 33.000 (33.000)\tPrec@5 85.000 (85.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 10.5116 (12.8429)\tPrec@1 46.000 (35.091)\tPrec@5 89.000 (88.182)\n",
      "Test: [20/100]\tTime 0.074 (0.082)\tLoss 11.5952 (12.9433)\tPrec@1 37.000 (33.810)\tPrec@5 88.000 (88.095)\n",
      "Test: [30/100]\tTime 0.074 (0.079)\tLoss 12.5243 (12.9003)\tPrec@1 34.000 (33.774)\tPrec@5 87.000 (87.871)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 13.9894 (12.9988)\tPrec@1 27.000 (33.341)\tPrec@5 82.000 (87.756)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 12.3281 (12.8869)\tPrec@1 40.000 (33.765)\tPrec@5 86.000 (88.137)\n",
      "Test: [60/100]\tTime 0.074 (0.076)\tLoss 10.9544 (12.8823)\tPrec@1 40.000 (33.623)\tPrec@5 91.000 (88.164)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 12.2501 (12.8481)\tPrec@1 36.000 (33.676)\tPrec@5 91.000 (88.310)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 11.2242 (12.7761)\tPrec@1 43.000 (34.173)\tPrec@5 89.000 (88.605)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 12.7374 (12.8294)\tPrec@1 37.000 (34.044)\tPrec@5 88.000 (88.516)\n",
      "val Results: Prec@1 34.160 Prec@5 88.450 Loss 12.84454\n",
      "val Class Accuracy: [0.879,0.994,0.687,0.451,0.056,0.048,0.292,0.009,0.000,0.000]\n",
      "Best Prec@1: 45.730\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [24][0/97], lr: 0.01000\tTime 0.480 (0.480)\tData 0.271 (0.271)\tLoss 2.9837 (2.9837)\tPrec@1 79.688 (79.688)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [24][10/97], lr: 0.01000\tTime 0.327 (0.348)\tData 0.000 (0.039)\tLoss 2.8369 (3.1350)\tPrec@1 78.906 (79.759)\tPrec@5 100.000 (98.011)\n",
      "Epoch: [24][20/97], lr: 0.01000\tTime 0.321 (0.336)\tData 0.000 (0.028)\tLoss 3.3644 (3.0756)\tPrec@1 78.125 (80.283)\tPrec@5 98.438 (97.991)\n",
      "Epoch: [24][30/97], lr: 0.01000\tTime 0.394 (0.343)\tData 0.000 (0.025)\tLoss 3.6595 (3.0989)\tPrec@1 75.781 (79.990)\tPrec@5 100.000 (98.211)\n",
      "Epoch: [24][40/97], lr: 0.01000\tTime 0.376 (0.351)\tData 0.000 (0.022)\tLoss 3.1472 (3.1152)\tPrec@1 76.562 (79.840)\tPrec@5 98.438 (98.171)\n",
      "Epoch: [24][50/97], lr: 0.01000\tTime 0.321 (0.350)\tData 0.000 (0.021)\tLoss 2.9508 (3.1082)\tPrec@1 80.469 (79.933)\tPrec@5 97.656 (98.208)\n",
      "Epoch: [24][60/97], lr: 0.01000\tTime 0.330 (0.347)\tData 0.000 (0.020)\tLoss 3.4557 (3.1245)\tPrec@1 78.125 (79.700)\tPrec@5 99.219 (98.207)\n",
      "Epoch: [24][70/97], lr: 0.01000\tTime 0.323 (0.344)\tData 0.000 (0.020)\tLoss 2.6042 (3.1200)\tPrec@1 80.469 (79.599)\tPrec@5 99.219 (98.272)\n",
      "Epoch: [24][80/97], lr: 0.01000\tTime 0.323 (0.342)\tData 0.000 (0.019)\tLoss 3.9340 (3.1351)\tPrec@1 73.438 (79.504)\tPrec@5 97.656 (98.302)\n",
      "Epoch: [24][90/97], lr: 0.01000\tTime 0.340 (0.341)\tData 0.000 (0.019)\tLoss 3.8482 (3.1227)\tPrec@1 75.781 (79.670)\tPrec@5 97.656 (98.343)\n",
      "Epoch: [24][96/97], lr: 0.01000\tTime 0.389 (0.342)\tData 0.000 (0.019)\tLoss 2.8815 (3.1147)\tPrec@1 80.508 (79.768)\tPrec@5 99.153 (98.323)\n",
      "Gated Network Weight Gate= Flip:0.53, Sc:0.47\n",
      "Test: [0/100]\tTime 0.528 (0.528)\tLoss 12.1084 (12.1084)\tPrec@1 34.000 (34.000)\tPrec@5 87.000 (87.000)\n",
      "Test: [10/100]\tTime 0.074 (0.115)\tLoss 8.8433 (10.9482)\tPrec@1 52.000 (41.818)\tPrec@5 92.000 (86.364)\n",
      "Test: [20/100]\tTime 0.073 (0.096)\tLoss 9.6036 (11.0096)\tPrec@1 48.000 (40.810)\tPrec@5 94.000 (86.143)\n",
      "Test: [30/100]\tTime 0.074 (0.088)\tLoss 10.3538 (10.9070)\tPrec@1 39.000 (41.613)\tPrec@5 89.000 (86.097)\n",
      "Test: [40/100]\tTime 0.074 (0.085)\tLoss 11.7059 (10.9685)\tPrec@1 37.000 (41.268)\tPrec@5 84.000 (85.976)\n",
      "Test: [50/100]\tTime 0.074 (0.083)\tLoss 10.4059 (10.8908)\tPrec@1 48.000 (41.627)\tPrec@5 88.000 (85.980)\n",
      "Test: [60/100]\tTime 0.074 (0.081)\tLoss 8.8579 (10.8222)\tPrec@1 52.000 (41.967)\tPrec@5 90.000 (86.082)\n",
      "Test: [70/100]\tTime 0.074 (0.080)\tLoss 11.0152 (10.8031)\tPrec@1 39.000 (42.197)\tPrec@5 86.000 (85.930)\n",
      "Test: [80/100]\tTime 0.074 (0.080)\tLoss 10.1314 (10.7675)\tPrec@1 46.000 (42.358)\tPrec@5 87.000 (85.975)\n",
      "Test: [90/100]\tTime 0.074 (0.079)\tLoss 10.8383 (10.8427)\tPrec@1 40.000 (41.868)\tPrec@5 85.000 (85.824)\n",
      "val Results: Prec@1 41.870 Prec@5 85.770 Loss 10.85853\n",
      "val Class Accuracy: [0.955,0.882,0.747,0.204,0.107,0.630,0.515,0.147,0.000,0.000]\n",
      "Best Prec@1: 45.730\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [25][0/97], lr: 0.01000\tTime 0.606 (0.606)\tData 0.338 (0.338)\tLoss 3.1882 (3.1882)\tPrec@1 80.469 (80.469)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [25][10/97], lr: 0.01000\tTime 0.338 (0.382)\tData 0.000 (0.044)\tLoss 3.0531 (3.1444)\tPrec@1 81.250 (80.256)\tPrec@5 97.656 (97.585)\n",
      "Epoch: [25][20/97], lr: 0.01000\tTime 0.345 (0.364)\tData 0.000 (0.031)\tLoss 2.7774 (3.0866)\tPrec@1 82.812 (80.543)\tPrec@5 99.219 (97.731)\n",
      "Epoch: [25][30/97], lr: 0.01000\tTime 0.340 (0.356)\tData 0.000 (0.026)\tLoss 2.9036 (3.0829)\tPrec@1 78.125 (80.318)\tPrec@5 97.656 (97.757)\n",
      "Epoch: [25][40/97], lr: 0.01000\tTime 0.338 (0.354)\tData 0.000 (0.024)\tLoss 2.5149 (3.0679)\tPrec@1 84.375 (80.431)\tPrec@5 100.000 (98.018)\n",
      "Epoch: [25][50/97], lr: 0.01000\tTime 0.342 (0.352)\tData 0.000 (0.022)\tLoss 2.5289 (3.0534)\tPrec@1 83.594 (80.453)\tPrec@5 100.000 (98.162)\n",
      "Epoch: [25][60/97], lr: 0.01000\tTime 0.348 (0.351)\tData 0.000 (0.021)\tLoss 3.6534 (3.0422)\tPrec@1 80.469 (80.622)\tPrec@5 97.656 (98.220)\n",
      "Epoch: [25][70/97], lr: 0.01000\tTime 0.356 (0.353)\tData 0.000 (0.021)\tLoss 4.0150 (3.0665)\tPrec@1 72.656 (80.359)\tPrec@5 95.312 (98.217)\n",
      "Epoch: [25][80/97], lr: 0.01000\tTime 0.366 (0.353)\tData 0.000 (0.020)\tLoss 2.8205 (3.0823)\tPrec@1 85.156 (80.208)\tPrec@5 98.438 (98.264)\n",
      "Epoch: [25][90/97], lr: 0.01000\tTime 0.339 (0.353)\tData 0.000 (0.020)\tLoss 2.8976 (3.0846)\tPrec@1 82.812 (80.271)\tPrec@5 97.656 (98.223)\n",
      "Epoch: [25][96/97], lr: 0.01000\tTime 0.334 (0.353)\tData 0.000 (0.020)\tLoss 3.3167 (3.0913)\tPrec@1 79.661 (80.251)\tPrec@5 98.305 (98.227)\n",
      "Gated Network Weight Gate= Flip:0.54, Sc:0.46\n",
      "Test: [0/100]\tTime 0.325 (0.325)\tLoss 12.2817 (12.2817)\tPrec@1 34.000 (34.000)\tPrec@5 89.000 (89.000)\n",
      "Test: [10/100]\tTime 0.074 (0.097)\tLoss 9.3603 (11.1445)\tPrec@1 52.000 (40.455)\tPrec@5 86.000 (88.909)\n",
      "Test: [20/100]\tTime 0.073 (0.086)\tLoss 9.7550 (11.1460)\tPrec@1 49.000 (40.143)\tPrec@5 91.000 (89.095)\n",
      "Test: [30/100]\tTime 0.074 (0.082)\tLoss 10.5063 (11.0815)\tPrec@1 41.000 (40.419)\tPrec@5 90.000 (89.742)\n",
      "Test: [40/100]\tTime 0.074 (0.080)\tLoss 11.7353 (11.1308)\tPrec@1 39.000 (40.293)\tPrec@5 84.000 (89.683)\n",
      "Test: [50/100]\tTime 0.075 (0.079)\tLoss 10.2603 (11.0036)\tPrec@1 48.000 (40.961)\tPrec@5 92.000 (89.667)\n",
      "Test: [60/100]\tTime 0.075 (0.078)\tLoss 8.9489 (10.9584)\tPrec@1 48.000 (41.016)\tPrec@5 89.000 (89.492)\n",
      "Test: [70/100]\tTime 0.075 (0.078)\tLoss 11.1941 (10.9377)\tPrec@1 40.000 (41.254)\tPrec@5 88.000 (89.239)\n",
      "Test: [80/100]\tTime 0.075 (0.077)\tLoss 10.1311 (10.8765)\tPrec@1 50.000 (41.679)\tPrec@5 93.000 (89.494)\n",
      "Test: [90/100]\tTime 0.075 (0.077)\tLoss 10.8128 (10.9588)\tPrec@1 41.000 (41.253)\tPrec@5 89.000 (89.121)\n",
      "val Results: Prec@1 41.140 Prec@5 89.190 Loss 10.97470\n",
      "val Class Accuracy: [0.956,0.951,0.500,0.278,0.271,0.861,0.165,0.127,0.000,0.005]\n",
      "Best Prec@1: 45.730\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [26][0/97], lr: 0.01000\tTime 0.856 (0.856)\tData 0.476 (0.476)\tLoss 2.9463 (2.9463)\tPrec@1 81.250 (81.250)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [26][10/97], lr: 0.01000\tTime 0.352 (0.419)\tData 0.000 (0.055)\tLoss 2.7679 (2.9019)\tPrec@1 82.031 (81.605)\tPrec@5 100.000 (99.006)\n",
      "Epoch: [26][20/97], lr: 0.01000\tTime 0.360 (0.388)\tData 0.000 (0.037)\tLoss 2.0769 (2.9490)\tPrec@1 85.938 (81.436)\tPrec@5 100.000 (98.847)\n",
      "Epoch: [26][30/97], lr: 0.01000\tTime 0.341 (0.373)\tData 0.000 (0.030)\tLoss 2.6378 (3.0048)\tPrec@1 82.812 (80.998)\tPrec@5 99.219 (98.589)\n",
      "Epoch: [26][40/97], lr: 0.01000\tTime 0.338 (0.366)\tData 0.000 (0.027)\tLoss 2.5391 (3.0230)\tPrec@1 85.156 (81.098)\tPrec@5 98.438 (98.457)\n",
      "Epoch: [26][50/97], lr: 0.01000\tTime 0.347 (0.361)\tData 0.000 (0.025)\tLoss 3.3995 (3.0621)\tPrec@1 75.781 (80.729)\tPrec@5 97.656 (98.330)\n",
      "Epoch: [26][60/97], lr: 0.01000\tTime 0.341 (0.358)\tData 0.000 (0.023)\tLoss 3.0667 (3.0428)\tPrec@1 78.906 (80.827)\tPrec@5 99.219 (98.220)\n",
      "Epoch: [26][70/97], lr: 0.01000\tTime 0.344 (0.356)\tData 0.000 (0.022)\tLoss 2.4198 (3.0795)\tPrec@1 85.156 (80.546)\tPrec@5 100.000 (98.261)\n",
      "Epoch: [26][80/97], lr: 0.01000\tTime 0.362 (0.357)\tData 0.000 (0.022)\tLoss 3.0855 (3.0844)\tPrec@1 82.031 (80.498)\tPrec@5 96.875 (98.274)\n",
      "Epoch: [26][90/97], lr: 0.01000\tTime 0.340 (0.356)\tData 0.000 (0.021)\tLoss 4.1981 (3.1013)\tPrec@1 71.094 (80.340)\tPrec@5 98.438 (98.317)\n",
      "Epoch: [26][96/97], lr: 0.01000\tTime 0.319 (0.355)\tData 0.000 (0.021)\tLoss 2.6126 (3.0829)\tPrec@1 84.746 (80.477)\tPrec@5 98.305 (98.356)\n",
      "Gated Network Weight Gate= Flip:0.61, Sc:0.39\n",
      "Test: [0/100]\tTime 0.308 (0.308)\tLoss 12.1017 (12.1017)\tPrec@1 32.000 (32.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.074 (0.095)\tLoss 9.5856 (11.3181)\tPrec@1 47.000 (39.727)\tPrec@5 93.000 (92.636)\n",
      "Test: [20/100]\tTime 0.074 (0.085)\tLoss 10.0367 (11.3111)\tPrec@1 45.000 (39.524)\tPrec@5 93.000 (91.952)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 10.6289 (11.3031)\tPrec@1 42.000 (39.516)\tPrec@5 93.000 (91.968)\n",
      "Test: [40/100]\tTime 0.075 (0.080)\tLoss 12.5394 (11.3750)\tPrec@1 33.000 (39.098)\tPrec@5 85.000 (91.707)\n",
      "Test: [50/100]\tTime 0.077 (0.079)\tLoss 10.8384 (11.2530)\tPrec@1 44.000 (40.039)\tPrec@5 94.000 (91.863)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 10.0966 (11.2409)\tPrec@1 43.000 (39.672)\tPrec@5 92.000 (91.754)\n",
      "Test: [70/100]\tTime 0.075 (0.078)\tLoss 10.8291 (11.2189)\tPrec@1 43.000 (39.915)\tPrec@5 94.000 (91.690)\n",
      "Test: [80/100]\tTime 0.076 (0.077)\tLoss 10.1619 (11.1619)\tPrec@1 47.000 (40.235)\tPrec@5 90.000 (91.667)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 11.1123 (11.2236)\tPrec@1 40.000 (39.901)\tPrec@5 88.000 (91.407)\n",
      "val Results: Prec@1 39.800 Prec@5 91.460 Loss 11.24200\n",
      "val Class Accuracy: [0.957,0.948,0.510,0.863,0.039,0.263,0.202,0.164,0.004,0.030]\n",
      "Best Prec@1: 45.730\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [27][0/97], lr: 0.01000\tTime 0.970 (0.970)\tData 0.602 (0.602)\tLoss 2.9275 (2.9275)\tPrec@1 80.469 (80.469)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [27][10/97], lr: 0.01000\tTime 0.349 (0.434)\tData 0.000 (0.066)\tLoss 3.0183 (3.1885)\tPrec@1 79.688 (79.261)\tPrec@5 99.219 (97.869)\n",
      "Epoch: [27][20/97], lr: 0.01000\tTime 0.375 (0.399)\tData 0.000 (0.042)\tLoss 3.4638 (3.1210)\tPrec@1 76.562 (79.874)\tPrec@5 98.438 (98.140)\n",
      "Epoch: [27][30/97], lr: 0.01000\tTime 0.387 (0.387)\tData 0.000 (0.033)\tLoss 3.9096 (3.0201)\tPrec@1 70.312 (80.292)\tPrec@5 96.875 (98.211)\n",
      "Epoch: [27][40/97], lr: 0.01000\tTime 0.356 (0.381)\tData 0.000 (0.029)\tLoss 3.4833 (3.0142)\tPrec@1 77.344 (80.412)\tPrec@5 95.312 (98.247)\n",
      "Epoch: [27][50/97], lr: 0.01000\tTime 0.348 (0.378)\tData 0.000 (0.026)\tLoss 3.3424 (3.0360)\tPrec@1 80.469 (80.515)\tPrec@5 96.094 (98.162)\n",
      "Epoch: [27][60/97], lr: 0.01000\tTime 0.368 (0.376)\tData 0.000 (0.025)\tLoss 3.1900 (3.0359)\tPrec@1 77.344 (80.686)\tPrec@5 100.000 (98.156)\n",
      "Epoch: [27][70/97], lr: 0.01000\tTime 0.351 (0.373)\tData 0.000 (0.023)\tLoss 2.7918 (3.0336)\tPrec@1 80.469 (80.612)\tPrec@5 98.438 (98.217)\n",
      "Epoch: [27][80/97], lr: 0.01000\tTime 0.385 (0.372)\tData 0.000 (0.022)\tLoss 2.9145 (3.0111)\tPrec@1 81.250 (80.845)\tPrec@5 99.219 (98.274)\n",
      "Epoch: [27][90/97], lr: 0.01000\tTime 0.366 (0.371)\tData 0.000 (0.022)\tLoss 2.7088 (3.0062)\tPrec@1 78.906 (80.941)\tPrec@5 100.000 (98.292)\n",
      "Epoch: [27][96/97], lr: 0.01000\tTime 0.329 (0.369)\tData 0.000 (0.022)\tLoss 2.8300 (3.0024)\tPrec@1 80.508 (80.993)\tPrec@5 98.305 (98.323)\n",
      "Gated Network Weight Gate= Flip:0.52, Sc:0.48\n",
      "Test: [0/100]\tTime 0.355 (0.355)\tLoss 11.7043 (11.7043)\tPrec@1 40.000 (40.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.074 (0.099)\tLoss 8.5930 (10.6800)\tPrec@1 56.000 (46.818)\tPrec@5 91.000 (92.000)\n",
      "Test: [20/100]\tTime 0.075 (0.087)\tLoss 9.8983 (10.6425)\tPrec@1 48.000 (46.381)\tPrec@5 97.000 (92.476)\n",
      "Test: [30/100]\tTime 0.074 (0.084)\tLoss 9.6940 (10.5788)\tPrec@1 50.000 (46.452)\tPrec@5 92.000 (92.419)\n",
      "Test: [40/100]\tTime 0.075 (0.082)\tLoss 11.1251 (10.6256)\tPrec@1 44.000 (46.341)\tPrec@5 93.000 (92.390)\n",
      "Test: [50/100]\tTime 0.074 (0.080)\tLoss 10.1382 (10.5206)\tPrec@1 49.000 (46.922)\tPrec@5 90.000 (92.392)\n",
      "Test: [60/100]\tTime 0.075 (0.079)\tLoss 8.7358 (10.4808)\tPrec@1 55.000 (46.852)\tPrec@5 92.000 (92.279)\n",
      "Test: [70/100]\tTime 0.075 (0.079)\tLoss 10.2199 (10.4837)\tPrec@1 48.000 (46.831)\tPrec@5 95.000 (92.254)\n",
      "Test: [80/100]\tTime 0.075 (0.078)\tLoss 9.5771 (10.4639)\tPrec@1 53.000 (46.901)\tPrec@5 86.000 (92.272)\n",
      "Test: [90/100]\tTime 0.074 (0.078)\tLoss 10.2283 (10.5332)\tPrec@1 47.000 (46.516)\tPrec@5 93.000 (92.110)\n",
      "val Results: Prec@1 46.310 Prec@5 92.200 Loss 10.57420\n",
      "val Class Accuracy: [0.968,0.915,0.699,0.252,0.763,0.487,0.439,0.108,0.000,0.000]\n",
      "Best Prec@1: 46.310\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [28][0/97], lr: 0.01000\tTime 0.584 (0.584)\tData 0.306 (0.306)\tLoss 2.9340 (2.9340)\tPrec@1 81.250 (81.250)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [28][10/97], lr: 0.01000\tTime 0.336 (0.375)\tData 0.000 (0.041)\tLoss 3.0638 (2.8302)\tPrec@1 82.031 (82.031)\tPrec@5 97.656 (99.077)\n",
      "Epoch: [28][20/97], lr: 0.01000\tTime 0.341 (0.360)\tData 0.000 (0.030)\tLoss 2.6903 (2.7989)\tPrec@1 84.375 (82.143)\tPrec@5 100.000 (98.847)\n",
      "Epoch: [28][30/97], lr: 0.01000\tTime 0.347 (0.357)\tData 0.000 (0.025)\tLoss 2.1997 (2.8498)\tPrec@1 84.375 (81.830)\tPrec@5 99.219 (98.841)\n",
      "Epoch: [28][40/97], lr: 0.01000\tTime 0.338 (0.357)\tData 0.000 (0.023)\tLoss 2.5277 (2.8845)\tPrec@1 86.719 (81.650)\tPrec@5 99.219 (98.761)\n",
      "Epoch: [28][50/97], lr: 0.01000\tTime 0.350 (0.354)\tData 0.000 (0.022)\tLoss 2.9154 (2.9202)\tPrec@1 80.469 (81.526)\tPrec@5 98.438 (98.667)\n",
      "Epoch: [28][60/97], lr: 0.01000\tTime 0.342 (0.352)\tData 0.000 (0.021)\tLoss 2.7585 (2.9123)\tPrec@1 82.812 (81.557)\tPrec@5 98.438 (98.732)\n",
      "Epoch: [28][70/97], lr: 0.01000\tTime 0.358 (0.353)\tData 0.000 (0.020)\tLoss 2.3282 (2.9025)\tPrec@1 81.250 (81.525)\tPrec@5 98.438 (98.691)\n",
      "Epoch: [28][80/97], lr: 0.01000\tTime 0.386 (0.354)\tData 0.000 (0.020)\tLoss 3.2029 (2.9202)\tPrec@1 78.125 (81.346)\tPrec@5 100.000 (98.650)\n",
      "Epoch: [28][90/97], lr: 0.01000\tTime 0.356 (0.354)\tData 0.000 (0.019)\tLoss 3.0930 (2.9290)\tPrec@1 83.594 (81.362)\tPrec@5 98.438 (98.644)\n",
      "Epoch: [28][96/97], lr: 0.01000\tTime 0.343 (0.354)\tData 0.000 (0.020)\tLoss 3.1052 (2.9479)\tPrec@1 81.356 (81.219)\tPrec@5 99.153 (98.622)\n",
      "Gated Network Weight Gate= Flip:0.54, Sc:0.46\n",
      "Test: [0/100]\tTime 0.347 (0.347)\tLoss 10.8395 (10.8395)\tPrec@1 48.000 (48.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.074 (0.099)\tLoss 7.8090 (9.9361)\tPrec@1 58.000 (48.273)\tPrec@5 94.000 (95.273)\n",
      "Test: [20/100]\tTime 0.073 (0.087)\tLoss 9.7984 (9.8822)\tPrec@1 47.000 (48.190)\tPrec@5 95.000 (95.810)\n",
      "Test: [30/100]\tTime 0.074 (0.083)\tLoss 9.0971 (9.8741)\tPrec@1 50.000 (48.226)\tPrec@5 93.000 (95.419)\n",
      "Test: [40/100]\tTime 0.074 (0.080)\tLoss 10.4322 (9.9169)\tPrec@1 42.000 (48.073)\tPrec@5 93.000 (95.098)\n",
      "Test: [50/100]\tTime 0.075 (0.079)\tLoss 9.0385 (9.7787)\tPrec@1 52.000 (48.804)\tPrec@5 95.000 (95.196)\n",
      "Test: [60/100]\tTime 0.074 (0.079)\tLoss 8.8684 (9.7518)\tPrec@1 51.000 (48.754)\tPrec@5 95.000 (95.098)\n",
      "Test: [70/100]\tTime 0.075 (0.078)\tLoss 9.3964 (9.7323)\tPrec@1 50.000 (48.930)\tPrec@5 97.000 (94.986)\n",
      "Test: [80/100]\tTime 0.074 (0.078)\tLoss 8.9296 (9.7060)\tPrec@1 54.000 (49.086)\tPrec@5 96.000 (95.062)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 9.6709 (9.7687)\tPrec@1 52.000 (48.857)\tPrec@5 97.000 (94.923)\n",
      "val Results: Prec@1 48.810 Prec@5 95.040 Loss 9.79319\n",
      "val Class Accuracy: [0.944,0.951,0.626,0.719,0.736,0.221,0.449,0.227,0.000,0.008]\n",
      "Best Prec@1: 48.810\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [29][0/97], lr: 0.01000\tTime 0.775 (0.775)\tData 0.487 (0.487)\tLoss 2.0829 (2.0829)\tPrec@1 86.719 (86.719)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [29][10/97], lr: 0.01000\tTime 0.351 (0.418)\tData 0.000 (0.056)\tLoss 2.8672 (2.8789)\tPrec@1 79.688 (81.889)\tPrec@5 98.438 (98.651)\n",
      "Epoch: [29][20/97], lr: 0.01000\tTime 0.347 (0.387)\tData 0.000 (0.037)\tLoss 2.7248 (2.9300)\tPrec@1 83.594 (81.622)\tPrec@5 99.219 (98.847)\n",
      "Epoch: [29][30/97], lr: 0.01000\tTime 0.337 (0.372)\tData 0.000 (0.030)\tLoss 2.9041 (2.9378)\tPrec@1 80.469 (81.502)\tPrec@5 99.219 (98.765)\n",
      "Epoch: [29][40/97], lr: 0.01000\tTime 0.343 (0.365)\tData 0.000 (0.027)\tLoss 2.9719 (2.8857)\tPrec@1 82.812 (81.898)\tPrec@5 99.219 (98.761)\n",
      "Epoch: [29][50/97], lr: 0.01000\tTime 0.353 (0.361)\tData 0.000 (0.025)\tLoss 3.2252 (2.9125)\tPrec@1 79.688 (81.602)\tPrec@5 97.656 (98.698)\n",
      "Epoch: [29][60/97], lr: 0.01000\tTime 0.352 (0.359)\tData 0.000 (0.023)\tLoss 3.2404 (2.9342)\tPrec@1 77.344 (81.301)\tPrec@5 98.438 (98.681)\n",
      "Epoch: [29][70/97], lr: 0.01000\tTime 0.388 (0.363)\tData 0.000 (0.022)\tLoss 2.1925 (2.9528)\tPrec@1 83.594 (81.096)\tPrec@5 99.219 (98.603)\n",
      "Epoch: [29][80/97], lr: 0.01000\tTime 0.361 (0.366)\tData 0.000 (0.022)\tLoss 3.3455 (2.9570)\tPrec@1 74.219 (80.980)\tPrec@5 97.656 (98.563)\n",
      "Epoch: [29][90/97], lr: 0.01000\tTime 0.337 (0.365)\tData 0.000 (0.021)\tLoss 2.4545 (2.9300)\tPrec@1 84.375 (81.138)\tPrec@5 99.219 (98.601)\n",
      "Epoch: [29][96/97], lr: 0.01000\tTime 0.331 (0.364)\tData 0.000 (0.021)\tLoss 2.8779 (2.9395)\tPrec@1 81.356 (81.106)\tPrec@5 99.153 (98.581)\n",
      "Gated Network Weight Gate= Flip:0.64, Sc:0.36\n",
      "Test: [0/100]\tTime 0.312 (0.312)\tLoss 12.3057 (12.3057)\tPrec@1 40.000 (40.000)\tPrec@5 88.000 (88.000)\n",
      "Test: [10/100]\tTime 0.073 (0.095)\tLoss 10.1172 (11.2501)\tPrec@1 46.000 (42.364)\tPrec@5 84.000 (90.727)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 10.2178 (11.1214)\tPrec@1 49.000 (42.857)\tPrec@5 92.000 (90.857)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 9.2113 (11.0104)\tPrec@1 55.000 (43.548)\tPrec@5 94.000 (90.548)\n",
      "Test: [40/100]\tTime 0.075 (0.080)\tLoss 11.8265 (11.0386)\tPrec@1 35.000 (43.317)\tPrec@5 86.000 (90.634)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 11.2169 (10.9180)\tPrec@1 42.000 (43.824)\tPrec@5 91.000 (91.059)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 10.0936 (10.9339)\tPrec@1 47.000 (43.508)\tPrec@5 88.000 (90.869)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 10.5012 (10.9073)\tPrec@1 46.000 (43.732)\tPrec@5 92.000 (90.817)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 9.9614 (10.8619)\tPrec@1 46.000 (43.901)\tPrec@5 95.000 (90.988)\n",
      "Test: [90/100]\tTime 0.075 (0.077)\tLoss 10.6488 (10.9254)\tPrec@1 46.000 (43.769)\tPrec@5 94.000 (90.835)\n",
      "val Results: Prec@1 43.660 Prec@5 90.800 Loss 10.96169\n",
      "val Class Accuracy: [0.930,0.991,0.486,0.761,0.671,0.153,0.056,0.302,0.001,0.015]\n",
      "Best Prec@1: 48.810\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [30][0/97], lr: 0.01000\tTime 0.820 (0.820)\tData 0.436 (0.436)\tLoss 2.5473 (2.5473)\tPrec@1 85.156 (85.156)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [30][10/97], lr: 0.01000\tTime 0.380 (0.422)\tData 0.000 (0.052)\tLoss 2.2827 (2.7266)\tPrec@1 83.594 (82.528)\tPrec@5 100.000 (99.006)\n",
      "Epoch: [30][20/97], lr: 0.01000\tTime 0.367 (0.391)\tData 0.000 (0.035)\tLoss 2.4557 (2.7733)\tPrec@1 84.375 (82.515)\tPrec@5 98.438 (98.624)\n",
      "Epoch: [30][30/97], lr: 0.01000\tTime 0.355 (0.380)\tData 0.000 (0.028)\tLoss 2.2779 (2.8277)\tPrec@1 85.938 (82.208)\tPrec@5 100.000 (98.362)\n",
      "Epoch: [30][40/97], lr: 0.01000\tTime 0.347 (0.374)\tData 0.000 (0.025)\tLoss 2.9694 (2.8705)\tPrec@1 82.031 (81.993)\tPrec@5 99.219 (98.304)\n",
      "Epoch: [30][50/97], lr: 0.01000\tTime 0.369 (0.373)\tData 0.000 (0.023)\tLoss 2.4523 (2.8555)\tPrec@1 85.938 (81.847)\tPrec@5 99.219 (98.330)\n",
      "Epoch: [30][60/97], lr: 0.01000\tTime 0.363 (0.372)\tData 0.000 (0.022)\tLoss 3.0739 (2.8786)\tPrec@1 81.250 (81.788)\tPrec@5 97.656 (98.399)\n",
      "Epoch: [30][70/97], lr: 0.01000\tTime 0.352 (0.371)\tData 0.000 (0.021)\tLoss 3.0140 (2.9007)\tPrec@1 80.469 (81.635)\tPrec@5 98.438 (98.393)\n",
      "Epoch: [30][80/97], lr: 0.01000\tTime 0.359 (0.369)\tData 0.000 (0.021)\tLoss 3.5974 (2.9241)\tPrec@1 78.906 (81.481)\tPrec@5 96.094 (98.341)\n",
      "Epoch: [30][90/97], lr: 0.01000\tTime 0.380 (0.369)\tData 0.000 (0.020)\tLoss 2.8215 (2.9359)\tPrec@1 81.250 (81.465)\tPrec@5 97.656 (98.360)\n",
      "Epoch: [30][96/97], lr: 0.01000\tTime 0.334 (0.368)\tData 0.000 (0.020)\tLoss 2.6836 (2.9383)\tPrec@1 84.746 (81.420)\tPrec@5 97.458 (98.404)\n",
      "Gated Network Weight Gate= Flip:0.54, Sc:0.46\n",
      "Test: [0/100]\tTime 0.363 (0.363)\tLoss 10.9061 (10.9061)\tPrec@1 43.000 (43.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.100)\tLoss 8.0967 (10.5083)\tPrec@1 54.000 (43.818)\tPrec@5 96.000 (94.636)\n",
      "Test: [20/100]\tTime 0.074 (0.087)\tLoss 10.7458 (10.5715)\tPrec@1 39.000 (43.667)\tPrec@5 96.000 (95.143)\n",
      "Test: [30/100]\tTime 0.074 (0.083)\tLoss 9.7700 (10.5143)\tPrec@1 47.000 (44.323)\tPrec@5 97.000 (94.935)\n",
      "Test: [40/100]\tTime 0.074 (0.081)\tLoss 11.6523 (10.6113)\tPrec@1 36.000 (43.951)\tPrec@5 93.000 (94.610)\n",
      "Test: [50/100]\tTime 0.075 (0.080)\tLoss 10.8150 (10.5287)\tPrec@1 43.000 (44.294)\tPrec@5 96.000 (94.745)\n",
      "Test: [60/100]\tTime 0.075 (0.079)\tLoss 9.7720 (10.5485)\tPrec@1 47.000 (44.049)\tPrec@5 96.000 (94.525)\n",
      "Test: [70/100]\tTime 0.074 (0.078)\tLoss 9.6196 (10.5063)\tPrec@1 48.000 (44.268)\tPrec@5 94.000 (94.239)\n",
      "Test: [80/100]\tTime 0.074 (0.078)\tLoss 9.7379 (10.4803)\tPrec@1 50.000 (44.506)\tPrec@5 92.000 (94.247)\n",
      "Test: [90/100]\tTime 0.075 (0.078)\tLoss 9.9930 (10.5275)\tPrec@1 50.000 (44.297)\tPrec@5 96.000 (94.187)\n",
      "val Results: Prec@1 44.150 Prec@5 94.100 Loss 10.55794\n",
      "val Class Accuracy: [0.825,0.970,0.863,0.524,0.469,0.084,0.576,0.093,0.001,0.010]\n",
      "Best Prec@1: 48.810\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [31][0/97], lr: 0.01000\tTime 0.579 (0.579)\tData 0.326 (0.326)\tLoss 3.4116 (3.4116)\tPrec@1 77.344 (77.344)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [31][10/97], lr: 0.01000\tTime 0.341 (0.380)\tData 0.000 (0.042)\tLoss 3.1487 (3.0928)\tPrec@1 78.906 (80.256)\tPrec@5 99.219 (98.580)\n",
      "Epoch: [31][20/97], lr: 0.01000\tTime 0.345 (0.364)\tData 0.000 (0.030)\tLoss 2.1504 (2.9717)\tPrec@1 85.156 (80.915)\tPrec@5 100.000 (98.624)\n",
      "Epoch: [31][30/97], lr: 0.01000\tTime 0.347 (0.357)\tData 0.000 (0.026)\tLoss 2.4355 (2.8614)\tPrec@1 84.375 (81.754)\tPrec@5 100.000 (98.740)\n",
      "Epoch: [31][40/97], lr: 0.01000\tTime 0.342 (0.356)\tData 0.000 (0.023)\tLoss 3.0130 (2.8803)\tPrec@1 80.469 (81.669)\tPrec@5 98.438 (98.647)\n",
      "Epoch: [31][50/97], lr: 0.01000\tTime 0.350 (0.354)\tData 0.000 (0.022)\tLoss 3.1256 (2.8627)\tPrec@1 77.344 (81.893)\tPrec@5 99.219 (98.591)\n",
      "Epoch: [31][60/97], lr: 0.01000\tTime 0.351 (0.354)\tData 0.000 (0.021)\tLoss 3.0375 (2.8381)\tPrec@1 80.469 (82.006)\tPrec@5 100.000 (98.591)\n",
      "Epoch: [31][70/97], lr: 0.01000\tTime 0.347 (0.354)\tData 0.000 (0.020)\tLoss 3.3927 (2.8522)\tPrec@1 75.781 (81.866)\tPrec@5 100.000 (98.625)\n",
      "Epoch: [31][80/97], lr: 0.01000\tTime 0.355 (0.355)\tData 0.000 (0.020)\tLoss 3.3672 (2.8579)\tPrec@1 79.688 (81.858)\tPrec@5 97.656 (98.621)\n",
      "Epoch: [31][90/97], lr: 0.01000\tTime 0.350 (0.354)\tData 0.000 (0.019)\tLoss 3.0345 (2.8578)\tPrec@1 79.688 (81.825)\tPrec@5 100.000 (98.626)\n",
      "Epoch: [31][96/97], lr: 0.01000\tTime 0.346 (0.354)\tData 0.000 (0.020)\tLoss 2.9756 (2.8461)\tPrec@1 81.356 (81.936)\tPrec@5 99.153 (98.654)\n",
      "Gated Network Weight Gate= Flip:0.51, Sc:0.49\n",
      "Test: [0/100]\tTime 0.367 (0.367)\tLoss 11.6211 (11.6211)\tPrec@1 43.000 (43.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.074 (0.101)\tLoss 9.1599 (10.8939)\tPrec@1 53.000 (44.182)\tPrec@5 92.000 (95.182)\n",
      "Test: [20/100]\tTime 0.079 (0.088)\tLoss 10.8415 (10.9192)\tPrec@1 40.000 (42.762)\tPrec@5 96.000 (94.810)\n",
      "Test: [30/100]\tTime 0.074 (0.084)\tLoss 11.2043 (10.9045)\tPrec@1 37.000 (42.710)\tPrec@5 95.000 (94.774)\n",
      "Test: [40/100]\tTime 0.074 (0.082)\tLoss 11.9732 (10.9942)\tPrec@1 36.000 (42.415)\tPrec@5 95.000 (94.634)\n",
      "Test: [50/100]\tTime 0.075 (0.081)\tLoss 11.1618 (10.9103)\tPrec@1 43.000 (42.824)\tPrec@5 90.000 (94.725)\n",
      "Test: [60/100]\tTime 0.074 (0.080)\tLoss 9.5433 (10.9199)\tPrec@1 49.000 (42.738)\tPrec@5 96.000 (94.754)\n",
      "Test: [70/100]\tTime 0.074 (0.079)\tLoss 10.3154 (10.8889)\tPrec@1 44.000 (42.859)\tPrec@5 98.000 (94.549)\n",
      "Test: [80/100]\tTime 0.075 (0.078)\tLoss 9.7794 (10.8244)\tPrec@1 49.000 (43.210)\tPrec@5 96.000 (94.741)\n",
      "Test: [90/100]\tTime 0.075 (0.078)\tLoss 10.7039 (10.8611)\tPrec@1 42.000 (43.011)\tPrec@5 98.000 (94.637)\n",
      "val Results: Prec@1 42.950 Prec@5 94.630 Loss 10.87509\n",
      "val Class Accuracy: [0.850,0.977,0.826,0.824,0.288,0.045,0.395,0.022,0.067,0.001]\n",
      "Best Prec@1: 48.810\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [32][0/97], lr: 0.01000\tTime 0.751 (0.751)\tData 0.406 (0.406)\tLoss 3.0873 (3.0873)\tPrec@1 76.562 (76.562)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [32][10/97], lr: 0.01000\tTime 0.361 (0.417)\tData 0.000 (0.049)\tLoss 3.4657 (3.1028)\tPrec@1 78.125 (79.474)\tPrec@5 99.219 (98.793)\n",
      "Epoch: [32][20/97], lr: 0.01000\tTime 0.403 (0.395)\tData 0.000 (0.033)\tLoss 3.2402 (2.9082)\tPrec@1 78.125 (80.952)\tPrec@5 99.219 (98.996)\n",
      "Epoch: [32][30/97], lr: 0.01000\tTime 0.338 (0.381)\tData 0.000 (0.028)\tLoss 3.9969 (2.9229)\tPrec@1 71.875 (80.746)\tPrec@5 96.875 (98.765)\n",
      "Epoch: [32][40/97], lr: 0.01000\tTime 0.349 (0.373)\tData 0.000 (0.025)\tLoss 3.2398 (2.8153)\tPrec@1 78.125 (81.650)\tPrec@5 99.219 (98.704)\n",
      "Epoch: [32][50/97], lr: 0.01000\tTime 0.352 (0.368)\tData 0.000 (0.023)\tLoss 3.2995 (2.8155)\tPrec@1 78.906 (81.725)\tPrec@5 96.875 (98.637)\n",
      "Epoch: [32][60/97], lr: 0.01000\tTime 0.338 (0.364)\tData 0.000 (0.022)\tLoss 3.1632 (2.8171)\tPrec@1 78.906 (81.814)\tPrec@5 100.000 (98.668)\n",
      "Epoch: [32][70/97], lr: 0.01000\tTime 0.348 (0.361)\tData 0.000 (0.021)\tLoss 3.0574 (2.8130)\tPrec@1 82.031 (82.009)\tPrec@5 99.219 (98.636)\n",
      "Epoch: [32][80/97], lr: 0.01000\tTime 0.345 (0.361)\tData 0.000 (0.021)\tLoss 3.1244 (2.8254)\tPrec@1 79.688 (81.838)\tPrec@5 98.438 (98.669)\n",
      "Epoch: [32][90/97], lr: 0.01000\tTime 0.331 (0.359)\tData 0.000 (0.020)\tLoss 2.6956 (2.8300)\tPrec@1 80.469 (81.757)\tPrec@5 100.000 (98.669)\n",
      "Epoch: [32][96/97], lr: 0.01000\tTime 0.335 (0.358)\tData 0.000 (0.021)\tLoss 3.1230 (2.8437)\tPrec@1 78.814 (81.670)\tPrec@5 99.153 (98.670)\n",
      "Gated Network Weight Gate= Flip:0.63, Sc:0.37\n",
      "Test: [0/100]\tTime 0.294 (0.294)\tLoss 11.9707 (11.9707)\tPrec@1 42.000 (42.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.074 (0.094)\tLoss 9.1864 (10.9147)\tPrec@1 48.000 (44.000)\tPrec@5 94.000 (90.909)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 10.1751 (10.8198)\tPrec@1 46.000 (44.429)\tPrec@5 95.000 (91.810)\n",
      "Test: [30/100]\tTime 0.075 (0.081)\tLoss 9.5977 (10.7506)\tPrec@1 48.000 (44.710)\tPrec@5 94.000 (91.290)\n",
      "Test: [40/100]\tTime 0.074 (0.080)\tLoss 11.5610 (10.8221)\tPrec@1 40.000 (44.512)\tPrec@5 94.000 (91.317)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 10.6462 (10.7306)\tPrec@1 42.000 (45.098)\tPrec@5 90.000 (91.235)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 8.8454 (10.6969)\tPrec@1 56.000 (45.131)\tPrec@5 92.000 (91.279)\n",
      "Test: [70/100]\tTime 0.075 (0.077)\tLoss 10.0977 (10.6792)\tPrec@1 51.000 (45.282)\tPrec@5 92.000 (91.113)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 10.1384 (10.6496)\tPrec@1 50.000 (45.444)\tPrec@5 87.000 (91.123)\n",
      "Test: [90/100]\tTime 0.075 (0.077)\tLoss 9.8256 (10.7015)\tPrec@1 53.000 (45.154)\tPrec@5 91.000 (90.989)\n",
      "val Results: Prec@1 45.070 Prec@5 90.860 Loss 10.73657\n",
      "val Class Accuracy: [0.899,0.926,0.731,0.686,0.717,0.065,0.412,0.071,0.000,0.000]\n",
      "Best Prec@1: 48.810\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [33][0/97], lr: 0.01000\tTime 0.708 (0.708)\tData 0.394 (0.394)\tLoss 1.9920 (1.9920)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [33][10/97], lr: 0.01000\tTime 0.367 (0.417)\tData 0.000 (0.048)\tLoss 2.6292 (2.4673)\tPrec@1 85.938 (84.659)\tPrec@5 100.000 (98.935)\n",
      "Epoch: [33][20/97], lr: 0.01000\tTime 0.366 (0.391)\tData 0.000 (0.032)\tLoss 3.1044 (2.6546)\tPrec@1 81.250 (83.445)\tPrec@5 98.438 (98.847)\n",
      "Epoch: [33][30/97], lr: 0.01000\tTime 0.358 (0.380)\tData 0.000 (0.027)\tLoss 2.7736 (2.7356)\tPrec@1 83.594 (82.737)\tPrec@5 99.219 (98.866)\n",
      "Epoch: [33][40/97], lr: 0.01000\tTime 0.357 (0.376)\tData 0.000 (0.024)\tLoss 3.3203 (2.8190)\tPrec@1 76.562 (81.936)\tPrec@5 98.438 (98.761)\n",
      "Epoch: [33][50/97], lr: 0.01000\tTime 0.357 (0.373)\tData 0.000 (0.023)\tLoss 3.2444 (2.8837)\tPrec@1 78.906 (81.480)\tPrec@5 96.875 (98.683)\n",
      "Epoch: [33][60/97], lr: 0.01000\tTime 0.365 (0.370)\tData 0.000 (0.022)\tLoss 4.0719 (2.9093)\tPrec@1 72.656 (81.340)\tPrec@5 96.094 (98.514)\n",
      "Epoch: [33][70/97], lr: 0.01000\tTime 0.358 (0.370)\tData 0.000 (0.021)\tLoss 2.8957 (2.9005)\tPrec@1 82.031 (81.426)\tPrec@5 100.000 (98.526)\n",
      "Epoch: [33][80/97], lr: 0.01000\tTime 0.354 (0.368)\tData 0.000 (0.020)\tLoss 2.3867 (2.9126)\tPrec@1 85.156 (81.289)\tPrec@5 100.000 (98.515)\n",
      "Epoch: [33][90/97], lr: 0.01000\tTime 0.353 (0.368)\tData 0.000 (0.020)\tLoss 3.0070 (2.8889)\tPrec@1 81.250 (81.533)\tPrec@5 98.438 (98.541)\n",
      "Epoch: [33][96/97], lr: 0.01000\tTime 0.348 (0.368)\tData 0.000 (0.020)\tLoss 2.9038 (2.8928)\tPrec@1 83.051 (81.525)\tPrec@5 97.458 (98.541)\n",
      "Gated Network Weight Gate= Flip:0.48, Sc:0.52\n",
      "Test: [0/100]\tTime 0.354 (0.354)\tLoss 10.9701 (10.9701)\tPrec@1 48.000 (48.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.073 (0.099)\tLoss 7.9891 (10.2829)\tPrec@1 59.000 (47.727)\tPrec@5 93.000 (94.818)\n",
      "Test: [20/100]\tTime 0.075 (0.087)\tLoss 9.3235 (10.2506)\tPrec@1 52.000 (47.667)\tPrec@5 94.000 (94.333)\n",
      "Test: [30/100]\tTime 0.075 (0.083)\tLoss 9.3228 (10.2337)\tPrec@1 51.000 (47.452)\tPrec@5 94.000 (94.065)\n",
      "Test: [40/100]\tTime 0.075 (0.081)\tLoss 10.8281 (10.2907)\tPrec@1 43.000 (47.220)\tPrec@5 95.000 (93.927)\n",
      "Test: [50/100]\tTime 0.076 (0.081)\tLoss 10.1811 (10.1705)\tPrec@1 46.000 (48.020)\tPrec@5 94.000 (94.176)\n",
      "Test: [60/100]\tTime 0.075 (0.080)\tLoss 8.4247 (10.1333)\tPrec@1 57.000 (48.131)\tPrec@5 95.000 (94.295)\n",
      "Test: [70/100]\tTime 0.075 (0.079)\tLoss 9.9786 (10.1186)\tPrec@1 48.000 (48.211)\tPrec@5 98.000 (94.366)\n",
      "Test: [80/100]\tTime 0.076 (0.079)\tLoss 9.0786 (10.0770)\tPrec@1 55.000 (48.272)\tPrec@5 92.000 (94.543)\n",
      "Test: [90/100]\tTime 0.075 (0.078)\tLoss 10.0770 (10.1345)\tPrec@1 47.000 (48.000)\tPrec@5 98.000 (94.484)\n",
      "val Results: Prec@1 47.930 Prec@5 94.380 Loss 10.16337\n",
      "val Class Accuracy: [0.957,0.994,0.755,0.652,0.334,0.329,0.503,0.256,0.007,0.006]\n",
      "Best Prec@1: 48.810\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [34][0/97], lr: 0.01000\tTime 0.857 (0.857)\tData 0.479 (0.479)\tLoss 2.3574 (2.3574)\tPrec@1 85.156 (85.156)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [34][10/97], lr: 0.01000\tTime 0.343 (0.416)\tData 0.000 (0.055)\tLoss 3.0191 (2.6293)\tPrec@1 81.250 (84.020)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [34][20/97], lr: 0.01000\tTime 0.386 (0.388)\tData 0.000 (0.036)\tLoss 2.8791 (2.6777)\tPrec@1 82.812 (83.519)\tPrec@5 98.438 (98.996)\n",
      "Epoch: [34][30/97], lr: 0.01000\tTime 0.340 (0.375)\tData 0.000 (0.030)\tLoss 2.8709 (2.7263)\tPrec@1 80.469 (83.115)\tPrec@5 98.438 (98.866)\n",
      "Epoch: [34][40/97], lr: 0.01000\tTime 0.335 (0.367)\tData 0.000 (0.026)\tLoss 2.3772 (2.7482)\tPrec@1 86.719 (82.908)\tPrec@5 98.438 (98.761)\n",
      "Epoch: [34][50/97], lr: 0.01000\tTime 0.342 (0.362)\tData 0.000 (0.024)\tLoss 2.6970 (2.7807)\tPrec@1 83.594 (82.583)\tPrec@5 99.219 (98.637)\n",
      "Epoch: [34][60/97], lr: 0.01000\tTime 0.341 (0.359)\tData 0.000 (0.023)\tLoss 2.6836 (2.7687)\tPrec@1 84.375 (82.646)\tPrec@5 98.438 (98.668)\n",
      "Epoch: [34][70/97], lr: 0.01000\tTime 0.341 (0.357)\tData 0.000 (0.022)\tLoss 3.3560 (2.7800)\tPrec@1 77.344 (82.515)\tPrec@5 98.438 (98.757)\n",
      "Epoch: [34][80/97], lr: 0.01000\tTime 0.344 (0.356)\tData 0.000 (0.021)\tLoss 2.4271 (2.7725)\tPrec@1 84.375 (82.533)\tPrec@5 99.219 (98.708)\n",
      "Epoch: [34][90/97], lr: 0.01000\tTime 0.348 (0.356)\tData 0.000 (0.021)\tLoss 3.2722 (2.7978)\tPrec@1 78.125 (82.357)\tPrec@5 96.875 (98.661)\n",
      "Epoch: [34][96/97], lr: 0.01000\tTime 0.338 (0.356)\tData 0.000 (0.021)\tLoss 2.0662 (2.8025)\tPrec@1 87.288 (82.307)\tPrec@5 99.153 (98.702)\n",
      "Gated Network Weight Gate= Flip:0.63, Sc:0.37\n",
      "Test: [0/100]\tTime 0.366 (0.366)\tLoss 11.6846 (11.6846)\tPrec@1 41.000 (41.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.102)\tLoss 8.8389 (11.0775)\tPrec@1 52.000 (44.364)\tPrec@5 91.000 (89.818)\n",
      "Test: [20/100]\tTime 0.074 (0.089)\tLoss 10.1531 (11.0155)\tPrec@1 47.000 (44.143)\tPrec@5 96.000 (90.810)\n",
      "Test: [30/100]\tTime 0.075 (0.084)\tLoss 10.2489 (10.9924)\tPrec@1 42.000 (44.097)\tPrec@5 92.000 (91.065)\n",
      "Test: [40/100]\tTime 0.075 (0.082)\tLoss 12.1826 (11.0841)\tPrec@1 37.000 (43.585)\tPrec@5 86.000 (90.732)\n",
      "Test: [50/100]\tTime 0.075 (0.081)\tLoss 10.9449 (10.9874)\tPrec@1 44.000 (44.157)\tPrec@5 91.000 (90.627)\n",
      "Test: [60/100]\tTime 0.075 (0.080)\tLoss 9.2575 (10.9548)\tPrec@1 54.000 (44.115)\tPrec@5 89.000 (90.557)\n",
      "Test: [70/100]\tTime 0.075 (0.079)\tLoss 10.0110 (10.9274)\tPrec@1 49.000 (44.268)\tPrec@5 93.000 (90.324)\n",
      "Test: [80/100]\tTime 0.074 (0.079)\tLoss 10.4407 (10.8745)\tPrec@1 44.000 (44.593)\tPrec@5 88.000 (90.395)\n",
      "Test: [90/100]\tTime 0.075 (0.078)\tLoss 9.8289 (10.9405)\tPrec@1 53.000 (44.264)\tPrec@5 92.000 (90.220)\n",
      "val Results: Prec@1 44.180 Prec@5 90.150 Loss 10.97485\n",
      "val Class Accuracy: [0.863,0.954,0.892,0.168,0.250,0.530,0.582,0.178,0.000,0.001]\n",
      "Best Prec@1: 48.810\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [35][0/97], lr: 0.01000\tTime 0.854 (0.854)\tData 0.500 (0.500)\tLoss 2.7664 (2.7664)\tPrec@1 82.812 (82.812)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [35][10/97], lr: 0.01000\tTime 0.374 (0.428)\tData 0.000 (0.057)\tLoss 3.2271 (2.7015)\tPrec@1 78.125 (82.670)\tPrec@5 97.656 (99.077)\n",
      "Epoch: [35][20/97], lr: 0.01000\tTime 0.363 (0.397)\tData 0.000 (0.037)\tLoss 3.1652 (2.7289)\tPrec@1 82.812 (82.999)\tPrec@5 97.656 (98.772)\n",
      "Epoch: [35][30/97], lr: 0.01000\tTime 0.393 (0.388)\tData 0.000 (0.030)\tLoss 2.7681 (2.7126)\tPrec@1 80.469 (82.888)\tPrec@5 97.656 (98.715)\n",
      "Epoch: [35][40/97], lr: 0.01000\tTime 0.343 (0.387)\tData 0.000 (0.027)\tLoss 3.9005 (2.8219)\tPrec@1 76.562 (82.260)\tPrec@5 97.656 (98.666)\n",
      "Epoch: [35][50/97], lr: 0.01000\tTime 0.343 (0.379)\tData 0.000 (0.025)\tLoss 2.5584 (2.8048)\tPrec@1 83.594 (82.384)\tPrec@5 99.219 (98.698)\n",
      "Epoch: [35][60/97], lr: 0.01000\tTime 0.342 (0.373)\tData 0.000 (0.023)\tLoss 2.2918 (2.7954)\tPrec@1 85.938 (82.518)\tPrec@5 100.000 (98.706)\n",
      "Epoch: [35][70/97], lr: 0.01000\tTime 0.343 (0.369)\tData 0.000 (0.022)\tLoss 2.8130 (2.7971)\tPrec@1 79.688 (82.537)\tPrec@5 99.219 (98.691)\n",
      "Epoch: [35][80/97], lr: 0.01000\tTime 0.400 (0.367)\tData 0.000 (0.022)\tLoss 3.7927 (2.7848)\tPrec@1 78.125 (82.629)\tPrec@5 99.219 (98.717)\n",
      "Epoch: [35][90/97], lr: 0.01000\tTime 0.530 (0.376)\tData 0.000 (0.021)\tLoss 2.2569 (2.7892)\tPrec@1 85.938 (82.452)\tPrec@5 100.000 (98.798)\n",
      "Epoch: [35][96/97], lr: 0.01000\tTime 0.342 (0.377)\tData 0.000 (0.021)\tLoss 3.0112 (2.7982)\tPrec@1 83.051 (82.404)\tPrec@5 98.305 (98.831)\n",
      "Gated Network Weight Gate= Flip:0.54, Sc:0.46\n",
      "Test: [0/100]\tTime 0.418 (0.418)\tLoss 11.0220 (11.0220)\tPrec@1 44.000 (44.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.074 (0.105)\tLoss 8.1083 (10.3516)\tPrec@1 57.000 (48.455)\tPrec@5 91.000 (92.364)\n",
      "Test: [20/100]\tTime 0.074 (0.090)\tLoss 9.6071 (10.3456)\tPrec@1 47.000 (47.048)\tPrec@5 92.000 (92.714)\n",
      "Test: [30/100]\tTime 0.074 (0.085)\tLoss 9.2754 (10.2583)\tPrec@1 52.000 (47.871)\tPrec@5 95.000 (92.581)\n",
      "Test: [40/100]\tTime 0.074 (0.082)\tLoss 11.1813 (10.3196)\tPrec@1 42.000 (47.634)\tPrec@5 92.000 (92.537)\n",
      "Test: [50/100]\tTime 0.075 (0.081)\tLoss 9.6959 (10.2229)\tPrec@1 52.000 (48.333)\tPrec@5 92.000 (92.392)\n",
      "Test: [60/100]\tTime 0.074 (0.080)\tLoss 8.3763 (10.2009)\tPrec@1 59.000 (48.148)\tPrec@5 92.000 (92.393)\n",
      "Test: [70/100]\tTime 0.075 (0.079)\tLoss 9.8343 (10.1878)\tPrec@1 51.000 (48.197)\tPrec@5 93.000 (92.296)\n",
      "Test: [80/100]\tTime 0.075 (0.079)\tLoss 9.6032 (10.1510)\tPrec@1 52.000 (48.235)\tPrec@5 86.000 (92.395)\n",
      "Test: [90/100]\tTime 0.075 (0.078)\tLoss 10.2980 (10.2178)\tPrec@1 45.000 (47.912)\tPrec@5 94.000 (92.176)\n",
      "val Results: Prec@1 47.880 Prec@5 92.120 Loss 10.24320\n",
      "val Class Accuracy: [0.959,0.973,0.818,0.364,0.456,0.422,0.586,0.204,0.005,0.001]\n",
      "Best Prec@1: 48.810\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [36][0/97], lr: 0.01000\tTime 0.711 (0.711)\tData 0.412 (0.412)\tLoss 2.4224 (2.4224)\tPrec@1 86.719 (86.719)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [36][10/97], lr: 0.01000\tTime 0.361 (0.423)\tData 0.000 (0.048)\tLoss 2.6165 (2.8665)\tPrec@1 85.938 (81.605)\tPrec@5 100.000 (98.793)\n",
      "Epoch: [36][20/97], lr: 0.01000\tTime 0.379 (0.394)\tData 0.000 (0.033)\tLoss 2.6606 (2.7809)\tPrec@1 81.250 (82.143)\tPrec@5 96.875 (98.698)\n",
      "Epoch: [36][30/97], lr: 0.01000\tTime 0.354 (0.382)\tData 0.000 (0.027)\tLoss 2.5654 (2.7310)\tPrec@1 82.031 (82.359)\tPrec@5 99.219 (98.765)\n",
      "Epoch: [36][40/97], lr: 0.01000\tTime 0.350 (0.377)\tData 0.000 (0.024)\tLoss 3.6085 (2.7709)\tPrec@1 78.125 (81.993)\tPrec@5 96.875 (98.685)\n",
      "Epoch: [36][50/97], lr: 0.01000\tTime 0.360 (0.374)\tData 0.000 (0.023)\tLoss 2.8554 (2.8026)\tPrec@1 79.688 (81.817)\tPrec@5 99.219 (98.775)\n",
      "Epoch: [36][60/97], lr: 0.01000\tTime 0.355 (0.371)\tData 0.000 (0.022)\tLoss 2.1364 (2.7927)\tPrec@1 87.500 (82.018)\tPrec@5 98.438 (98.758)\n",
      "Epoch: [36][70/97], lr: 0.01000\tTime 0.355 (0.371)\tData 0.000 (0.021)\tLoss 3.0912 (2.8303)\tPrec@1 81.250 (81.976)\tPrec@5 98.438 (98.801)\n",
      "Epoch: [36][80/97], lr: 0.01000\tTime 0.358 (0.370)\tData 0.000 (0.020)\tLoss 2.1756 (2.8009)\tPrec@1 86.719 (82.224)\tPrec@5 98.438 (98.794)\n",
      "Epoch: [36][90/97], lr: 0.01000\tTime 0.372 (0.369)\tData 0.000 (0.020)\tLoss 2.8365 (2.7670)\tPrec@1 81.250 (82.426)\tPrec@5 99.219 (98.789)\n",
      "Epoch: [36][96/97], lr: 0.01000\tTime 0.343 (0.368)\tData 0.000 (0.020)\tLoss 2.7213 (2.7545)\tPrec@1 82.203 (82.541)\tPrec@5 99.153 (98.783)\n",
      "Gated Network Weight Gate= Flip:0.38, Sc:0.62\n",
      "Test: [0/100]\tTime 0.334 (0.334)\tLoss 11.1162 (11.1162)\tPrec@1 44.000 (44.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.074 (0.097)\tLoss 8.2246 (9.8275)\tPrec@1 54.000 (48.727)\tPrec@5 93.000 (93.818)\n",
      "Test: [20/100]\tTime 0.075 (0.086)\tLoss 9.0630 (9.6172)\tPrec@1 50.000 (50.143)\tPrec@5 95.000 (93.619)\n",
      "Test: [30/100]\tTime 0.075 (0.083)\tLoss 8.5814 (9.5886)\tPrec@1 57.000 (50.387)\tPrec@5 95.000 (93.226)\n",
      "Test: [40/100]\tTime 0.075 (0.081)\tLoss 10.0013 (9.6285)\tPrec@1 51.000 (50.707)\tPrec@5 91.000 (92.951)\n",
      "Test: [50/100]\tTime 0.076 (0.080)\tLoss 8.5117 (9.4804)\tPrec@1 56.000 (51.627)\tPrec@5 94.000 (93.157)\n",
      "Test: [60/100]\tTime 0.076 (0.079)\tLoss 8.1470 (9.4475)\tPrec@1 56.000 (51.836)\tPrec@5 94.000 (93.115)\n",
      "Test: [70/100]\tTime 0.075 (0.078)\tLoss 8.6163 (9.4228)\tPrec@1 58.000 (52.085)\tPrec@5 96.000 (93.070)\n",
      "Test: [80/100]\tTime 0.076 (0.078)\tLoss 9.2792 (9.3650)\tPrec@1 55.000 (52.395)\tPrec@5 91.000 (93.148)\n",
      "Test: [90/100]\tTime 0.075 (0.078)\tLoss 9.7936 (9.4469)\tPrec@1 53.000 (51.967)\tPrec@5 96.000 (92.956)\n",
      "val Results: Prec@1 51.800 Prec@5 92.980 Loss 9.47827\n",
      "val Class Accuracy: [0.894,0.896,0.564,0.760,0.678,0.586,0.288,0.474,0.026,0.014]\n",
      "Best Prec@1: 51.800\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [37][0/97], lr: 0.01000\tTime 0.877 (0.877)\tData 0.500 (0.500)\tLoss 2.1851 (2.1851)\tPrec@1 85.156 (85.156)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [37][10/97], lr: 0.01000\tTime 0.359 (0.429)\tData 0.000 (0.056)\tLoss 3.3531 (2.7235)\tPrec@1 80.469 (82.599)\tPrec@5 98.438 (98.722)\n",
      "Epoch: [37][20/97], lr: 0.01000\tTime 0.348 (0.395)\tData 0.000 (0.037)\tLoss 3.5484 (2.7426)\tPrec@1 77.344 (82.478)\tPrec@5 97.656 (98.624)\n",
      "Epoch: [37][30/97], lr: 0.01000\tTime 0.359 (0.384)\tData 0.000 (0.030)\tLoss 3.1762 (2.7624)\tPrec@1 82.812 (82.586)\tPrec@5 97.656 (98.513)\n",
      "Epoch: [37][40/97], lr: 0.01000\tTime 0.371 (0.381)\tData 0.000 (0.027)\tLoss 3.3481 (2.7678)\tPrec@1 80.469 (82.565)\tPrec@5 96.094 (98.495)\n",
      "Epoch: [37][50/97], lr: 0.01000\tTime 0.359 (0.376)\tData 0.000 (0.025)\tLoss 3.2433 (2.7990)\tPrec@1 79.688 (82.292)\tPrec@5 100.000 (98.483)\n",
      "Epoch: [37][60/97], lr: 0.01000\tTime 0.348 (0.374)\tData 0.000 (0.023)\tLoss 3.2223 (2.7923)\tPrec@1 76.562 (82.223)\tPrec@5 97.656 (98.591)\n",
      "Epoch: [37][70/97], lr: 0.01000\tTime 0.351 (0.372)\tData 0.000 (0.022)\tLoss 2.8941 (2.7909)\tPrec@1 81.250 (82.053)\tPrec@5 99.219 (98.636)\n",
      "Epoch: [37][80/97], lr: 0.01000\tTime 0.380 (0.370)\tData 0.000 (0.021)\tLoss 2.7502 (2.7667)\tPrec@1 82.812 (82.330)\tPrec@5 97.656 (98.621)\n",
      "Epoch: [37][90/97], lr: 0.01000\tTime 0.377 (0.369)\tData 0.000 (0.021)\tLoss 2.8823 (2.7901)\tPrec@1 82.812 (82.177)\tPrec@5 99.219 (98.618)\n",
      "Epoch: [37][96/97], lr: 0.01000\tTime 0.332 (0.369)\tData 0.000 (0.021)\tLoss 2.7450 (2.7986)\tPrec@1 83.051 (82.170)\tPrec@5 100.000 (98.630)\n",
      "Gated Network Weight Gate= Flip:0.46, Sc:0.54\n",
      "Test: [0/100]\tTime 0.376 (0.376)\tLoss 11.0831 (11.0831)\tPrec@1 44.000 (44.000)\tPrec@5 90.000 (90.000)\n",
      "Test: [10/100]\tTime 0.074 (0.101)\tLoss 8.6313 (10.4887)\tPrec@1 56.000 (46.000)\tPrec@5 95.000 (94.727)\n",
      "Test: [20/100]\tTime 0.075 (0.088)\tLoss 9.6330 (10.5287)\tPrec@1 47.000 (44.762)\tPrec@5 97.000 (95.000)\n",
      "Test: [30/100]\tTime 0.074 (0.084)\tLoss 9.6908 (10.4684)\tPrec@1 45.000 (44.839)\tPrec@5 97.000 (94.806)\n",
      "Test: [40/100]\tTime 0.075 (0.082)\tLoss 11.3394 (10.4829)\tPrec@1 42.000 (44.780)\tPrec@5 94.000 (94.634)\n",
      "Test: [50/100]\tTime 0.075 (0.080)\tLoss 10.7585 (10.4209)\tPrec@1 42.000 (45.157)\tPrec@5 94.000 (94.706)\n",
      "Test: [60/100]\tTime 0.075 (0.080)\tLoss 8.9346 (10.3917)\tPrec@1 51.000 (45.082)\tPrec@5 98.000 (94.820)\n",
      "Test: [70/100]\tTime 0.076 (0.079)\tLoss 10.8524 (10.3915)\tPrec@1 44.000 (45.056)\tPrec@5 95.000 (94.887)\n",
      "Test: [80/100]\tTime 0.075 (0.079)\tLoss 9.7160 (10.3589)\tPrec@1 52.000 (45.247)\tPrec@5 92.000 (95.012)\n",
      "Test: [90/100]\tTime 0.075 (0.078)\tLoss 10.1515 (10.4196)\tPrec@1 50.000 (44.967)\tPrec@5 95.000 (94.956)\n",
      "val Results: Prec@1 44.960 Prec@5 94.830 Loss 10.43210\n",
      "val Class Accuracy: [0.964,0.967,0.841,0.314,0.326,0.261,0.397,0.402,0.021,0.003]\n",
      "Best Prec@1: 51.800\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [38][0/97], lr: 0.01000\tTime 0.804 (0.804)\tData 0.412 (0.412)\tLoss 2.8080 (2.8080)\tPrec@1 82.812 (82.812)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [38][10/97], lr: 0.01000\tTime 0.352 (0.428)\tData 0.000 (0.045)\tLoss 3.4306 (2.6901)\tPrec@1 77.344 (83.594)\tPrec@5 98.438 (98.580)\n",
      "Epoch: [38][20/97], lr: 0.01000\tTime 0.374 (0.402)\tData 0.000 (0.031)\tLoss 2.6619 (2.6658)\tPrec@1 82.812 (83.296)\tPrec@5 98.438 (98.772)\n",
      "Epoch: [38][30/97], lr: 0.01000\tTime 0.355 (0.389)\tData 0.001 (0.026)\tLoss 3.0632 (2.6874)\tPrec@1 76.562 (83.039)\tPrec@5 99.219 (98.614)\n",
      "Epoch: [38][40/97], lr: 0.01000\tTime 0.356 (0.383)\tData 0.000 (0.024)\tLoss 3.1244 (2.7091)\tPrec@1 79.688 (82.908)\tPrec@5 100.000 (98.647)\n",
      "Epoch: [38][50/97], lr: 0.01000\tTime 0.361 (0.380)\tData 0.000 (0.022)\tLoss 2.8441 (2.7278)\tPrec@1 82.812 (82.843)\tPrec@5 98.438 (98.606)\n",
      "Epoch: [38][60/97], lr: 0.01000\tTime 0.366 (0.376)\tData 0.000 (0.021)\tLoss 3.0871 (2.7503)\tPrec@1 81.250 (82.659)\tPrec@5 99.219 (98.553)\n",
      "Epoch: [38][70/97], lr: 0.01000\tTime 0.354 (0.374)\tData 0.000 (0.020)\tLoss 3.2607 (2.7574)\tPrec@1 80.469 (82.559)\tPrec@5 96.875 (98.548)\n",
      "Epoch: [38][80/97], lr: 0.01000\tTime 0.358 (0.373)\tData 0.000 (0.020)\tLoss 2.2999 (2.7523)\tPrec@1 85.156 (82.591)\tPrec@5 98.438 (98.601)\n",
      "Epoch: [38][90/97], lr: 0.01000\tTime 0.376 (0.373)\tData 0.000 (0.019)\tLoss 2.5568 (2.7433)\tPrec@1 85.156 (82.787)\tPrec@5 99.219 (98.618)\n",
      "Epoch: [38][96/97], lr: 0.01000\tTime 0.340 (0.372)\tData 0.000 (0.020)\tLoss 3.3716 (2.7611)\tPrec@1 80.508 (82.637)\tPrec@5 99.153 (98.614)\n",
      "Gated Network Weight Gate= Flip:0.52, Sc:0.48\n",
      "Test: [0/100]\tTime 0.335 (0.335)\tLoss 11.0748 (11.0748)\tPrec@1 44.000 (44.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.075 (0.099)\tLoss 8.5840 (10.5099)\tPrec@1 58.000 (45.909)\tPrec@5 95.000 (93.818)\n",
      "Test: [20/100]\tTime 0.075 (0.087)\tLoss 9.5885 (10.5842)\tPrec@1 48.000 (45.190)\tPrec@5 97.000 (93.429)\n",
      "Test: [30/100]\tTime 0.074 (0.083)\tLoss 10.0021 (10.5421)\tPrec@1 48.000 (45.355)\tPrec@5 95.000 (93.258)\n",
      "Test: [40/100]\tTime 0.074 (0.081)\tLoss 11.6409 (10.5802)\tPrec@1 39.000 (45.220)\tPrec@5 86.000 (92.780)\n",
      "Test: [50/100]\tTime 0.076 (0.080)\tLoss 9.9550 (10.4574)\tPrec@1 47.000 (45.882)\tPrec@5 95.000 (92.784)\n",
      "Test: [60/100]\tTime 0.075 (0.079)\tLoss 9.0488 (10.4512)\tPrec@1 51.000 (45.770)\tPrec@5 91.000 (92.672)\n",
      "Test: [70/100]\tTime 0.074 (0.079)\tLoss 10.0408 (10.4250)\tPrec@1 48.000 (45.915)\tPrec@5 95.000 (92.479)\n",
      "Test: [80/100]\tTime 0.074 (0.078)\tLoss 9.4658 (10.3829)\tPrec@1 52.000 (46.198)\tPrec@5 88.000 (92.469)\n",
      "Test: [90/100]\tTime 0.076 (0.078)\tLoss 9.9578 (10.4355)\tPrec@1 51.000 (45.923)\tPrec@5 92.000 (92.319)\n",
      "val Results: Prec@1 45.950 Prec@5 92.260 Loss 10.44490\n",
      "val Class Accuracy: [0.886,0.974,0.827,0.526,0.126,0.600,0.442,0.196,0.013,0.005]\n",
      "Best Prec@1: 51.800\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [39][0/97], lr: 0.01000\tTime 0.606 (0.606)\tData 0.341 (0.341)\tLoss 2.7037 (2.7037)\tPrec@1 82.812 (82.812)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [39][10/97], lr: 0.01000\tTime 0.344 (0.388)\tData 0.000 (0.043)\tLoss 2.9265 (2.7997)\tPrec@1 83.594 (83.097)\tPrec@5 96.875 (98.651)\n",
      "Epoch: [39][20/97], lr: 0.01000\tTime 0.366 (0.371)\tData 0.000 (0.031)\tLoss 2.7586 (2.7629)\tPrec@1 83.594 (83.110)\tPrec@5 99.219 (98.661)\n",
      "Epoch: [39][30/97], lr: 0.01000\tTime 0.342 (0.366)\tData 0.000 (0.026)\tLoss 2.9389 (2.8504)\tPrec@1 82.812 (82.334)\tPrec@5 99.219 (98.564)\n",
      "Epoch: [39][40/97], lr: 0.01000\tTime 0.339 (0.363)\tData 0.000 (0.024)\tLoss 2.7795 (2.8255)\tPrec@1 81.250 (82.203)\tPrec@5 100.000 (98.704)\n",
      "Epoch: [39][50/97], lr: 0.01000\tTime 0.348 (0.360)\tData 0.000 (0.022)\tLoss 2.4857 (2.8134)\tPrec@1 85.938 (82.184)\tPrec@5 98.438 (98.790)\n",
      "Epoch: [39][60/97], lr: 0.01000\tTime 0.337 (0.357)\tData 0.000 (0.021)\tLoss 2.3168 (2.8515)\tPrec@1 85.156 (81.775)\tPrec@5 99.219 (98.783)\n",
      "Epoch: [39][70/97], lr: 0.01000\tTime 0.357 (0.356)\tData 0.000 (0.021)\tLoss 2.8235 (2.8254)\tPrec@1 78.125 (81.833)\tPrec@5 100.000 (98.845)\n",
      "Epoch: [39][80/97], lr: 0.01000\tTime 0.365 (0.358)\tData 0.000 (0.020)\tLoss 2.5831 (2.8226)\tPrec@1 85.156 (81.916)\tPrec@5 98.438 (98.746)\n",
      "Epoch: [39][90/97], lr: 0.01000\tTime 0.373 (0.361)\tData 0.000 (0.020)\tLoss 2.2370 (2.7916)\tPrec@1 85.938 (82.160)\tPrec@5 97.656 (98.721)\n",
      "Epoch: [39][96/97], lr: 0.01000\tTime 0.350 (0.362)\tData 0.000 (0.020)\tLoss 3.0705 (2.8040)\tPrec@1 82.203 (82.130)\tPrec@5 98.305 (98.710)\n",
      "Gated Network Weight Gate= Flip:0.46, Sc:0.54\n",
      "Test: [0/100]\tTime 0.575 (0.575)\tLoss 10.7481 (10.7481)\tPrec@1 44.000 (44.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.073 (0.121)\tLoss 8.0554 (9.7499)\tPrec@1 52.000 (50.182)\tPrec@5 91.000 (93.636)\n",
      "Test: [20/100]\tTime 0.074 (0.099)\tLoss 8.8718 (9.7062)\tPrec@1 54.000 (49.667)\tPrec@5 95.000 (93.714)\n",
      "Test: [30/100]\tTime 0.085 (0.091)\tLoss 9.4175 (9.7024)\tPrec@1 49.000 (49.742)\tPrec@5 93.000 (93.774)\n",
      "Test: [40/100]\tTime 0.075 (0.088)\tLoss 10.1521 (9.6770)\tPrec@1 48.000 (50.220)\tPrec@5 91.000 (93.732)\n",
      "Test: [50/100]\tTime 0.075 (0.086)\tLoss 9.4440 (9.5872)\tPrec@1 52.000 (50.686)\tPrec@5 96.000 (93.902)\n",
      "Test: [60/100]\tTime 0.076 (0.085)\tLoss 8.4045 (9.5478)\tPrec@1 59.000 (50.836)\tPrec@5 91.000 (93.951)\n",
      "Test: [70/100]\tTime 0.074 (0.084)\tLoss 9.2287 (9.5488)\tPrec@1 51.000 (50.775)\tPrec@5 94.000 (93.746)\n",
      "Test: [80/100]\tTime 0.085 (0.083)\tLoss 8.7116 (9.4941)\tPrec@1 59.000 (51.000)\tPrec@5 91.000 (93.667)\n",
      "Test: [90/100]\tTime 0.074 (0.082)\tLoss 9.6613 (9.5503)\tPrec@1 49.000 (50.670)\tPrec@5 96.000 (93.527)\n",
      "val Results: Prec@1 50.720 Prec@5 93.480 Loss 9.55602\n",
      "val Class Accuracy: [0.933,0.950,0.734,0.799,0.434,0.375,0.242,0.559,0.004,0.042]\n",
      "Best Prec@1: 51.800\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [40][0/97], lr: 0.01000\tTime 0.613 (0.613)\tData 0.382 (0.382)\tLoss 1.6568 (1.6568)\tPrec@1 91.406 (91.406)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [40][10/97], lr: 0.01000\tTime 0.371 (0.402)\tData 0.000 (0.048)\tLoss 1.8113 (2.4520)\tPrec@1 89.062 (84.659)\tPrec@5 100.000 (98.864)\n",
      "Epoch: [40][20/97], lr: 0.01000\tTime 0.375 (0.391)\tData 0.001 (0.033)\tLoss 3.2733 (2.5798)\tPrec@1 78.125 (83.854)\tPrec@5 96.094 (98.921)\n",
      "Epoch: [40][30/97], lr: 0.01000\tTime 0.368 (0.395)\tData 0.000 (0.027)\tLoss 2.8340 (2.6196)\tPrec@1 82.031 (83.468)\tPrec@5 99.219 (99.042)\n",
      "Epoch: [40][40/97], lr: 0.01000\tTime 0.404 (0.401)\tData 0.000 (0.024)\tLoss 2.9721 (2.6245)\tPrec@1 80.469 (83.498)\tPrec@5 100.000 (99.162)\n",
      "Epoch: [40][50/97], lr: 0.01000\tTime 0.419 (0.411)\tData 0.001 (0.022)\tLoss 2.0488 (2.6451)\tPrec@1 88.281 (83.395)\tPrec@5 98.438 (98.943)\n",
      "Epoch: [40][60/97], lr: 0.01000\tTime 0.384 (0.421)\tData 0.000 (0.021)\tLoss 2.0399 (2.6576)\tPrec@1 88.281 (83.414)\tPrec@5 97.656 (98.835)\n",
      "Epoch: [40][70/97], lr: 0.01000\tTime 0.486 (0.426)\tData 0.000 (0.020)\tLoss 2.3076 (2.6670)\tPrec@1 84.375 (83.297)\tPrec@5 99.219 (98.812)\n",
      "Epoch: [40][80/97], lr: 0.01000\tTime 0.347 (0.419)\tData 0.000 (0.019)\tLoss 2.8676 (2.6839)\tPrec@1 79.688 (83.160)\tPrec@5 99.219 (98.717)\n",
      "Epoch: [40][90/97], lr: 0.01000\tTime 0.337 (0.413)\tData 0.000 (0.019)\tLoss 3.0679 (2.6825)\tPrec@1 79.688 (83.190)\tPrec@5 99.219 (98.695)\n",
      "Epoch: [40][96/97], lr: 0.01000\tTime 0.358 (0.409)\tData 0.000 (0.019)\tLoss 2.5037 (2.6696)\tPrec@1 85.593 (83.298)\tPrec@5 100.000 (98.702)\n",
      "Gated Network Weight Gate= Flip:0.49, Sc:0.51\n",
      "Test: [0/100]\tTime 0.382 (0.382)\tLoss 10.6130 (10.6130)\tPrec@1 46.000 (46.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.074 (0.103)\tLoss 7.4262 (9.2149)\tPrec@1 61.000 (53.455)\tPrec@5 98.000 (96.000)\n",
      "Test: [20/100]\tTime 0.075 (0.090)\tLoss 7.6137 (9.1053)\tPrec@1 59.000 (53.048)\tPrec@5 98.000 (96.048)\n",
      "Test: [30/100]\tTime 0.076 (0.085)\tLoss 8.1555 (9.1024)\tPrec@1 56.000 (52.871)\tPrec@5 97.000 (96.097)\n",
      "Test: [40/100]\tTime 0.075 (0.083)\tLoss 9.7164 (9.1486)\tPrec@1 50.000 (53.049)\tPrec@5 96.000 (95.976)\n",
      "Test: [50/100]\tTime 0.075 (0.082)\tLoss 8.0183 (9.0161)\tPrec@1 61.000 (53.686)\tPrec@5 92.000 (96.000)\n",
      "Test: [60/100]\tTime 0.075 (0.081)\tLoss 7.5464 (8.9888)\tPrec@1 62.000 (53.738)\tPrec@5 96.000 (95.951)\n",
      "Test: [70/100]\tTime 0.078 (0.081)\tLoss 8.7683 (8.9520)\tPrec@1 58.000 (53.859)\tPrec@5 97.000 (96.056)\n",
      "Test: [80/100]\tTime 0.075 (0.082)\tLoss 8.1365 (8.9122)\tPrec@1 59.000 (54.074)\tPrec@5 95.000 (96.123)\n",
      "Test: [90/100]\tTime 0.097 (0.082)\tLoss 8.9051 (8.9808)\tPrec@1 57.000 (53.813)\tPrec@5 97.000 (96.121)\n",
      "val Results: Prec@1 53.930 Prec@5 96.110 Loss 8.99363\n",
      "val Class Accuracy: [0.967,0.977,0.509,0.412,0.679,0.824,0.498,0.353,0.109,0.065]\n",
      "Best Prec@1: 53.930\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [41][0/97], lr: 0.01000\tTime 1.123 (1.123)\tData 0.736 (0.736)\tLoss 2.2823 (2.2823)\tPrec@1 85.156 (85.156)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [41][10/97], lr: 0.01000\tTime 0.385 (0.480)\tData 0.001 (0.079)\tLoss 3.1065 (2.7088)\tPrec@1 82.812 (83.097)\tPrec@5 98.438 (98.935)\n",
      "Epoch: [41][20/97], lr: 0.01000\tTime 0.369 (0.440)\tData 0.000 (0.049)\tLoss 2.0392 (2.6445)\tPrec@1 87.500 (83.594)\tPrec@5 100.000 (98.772)\n",
      "Epoch: [41][30/97], lr: 0.01000\tTime 0.455 (0.438)\tData 0.001 (0.038)\tLoss 2.4226 (2.6808)\tPrec@1 83.594 (83.417)\tPrec@5 100.000 (98.942)\n",
      "Epoch: [41][40/97], lr: 0.01000\tTime 0.384 (0.441)\tData 0.000 (0.032)\tLoss 3.3237 (2.6915)\tPrec@1 74.219 (82.984)\tPrec@5 99.219 (99.066)\n",
      "Epoch: [41][50/97], lr: 0.01000\tTime 0.369 (0.430)\tData 0.000 (0.029)\tLoss 2.4725 (2.6966)\tPrec@1 83.594 (82.981)\tPrec@5 98.438 (98.974)\n",
      "Epoch: [41][60/97], lr: 0.01000\tTime 0.343 (0.422)\tData 0.000 (0.027)\tLoss 3.4128 (2.7209)\tPrec@1 78.906 (82.864)\tPrec@5 96.094 (98.886)\n",
      "Epoch: [41][70/97], lr: 0.01000\tTime 0.360 (0.414)\tData 0.000 (0.025)\tLoss 3.4126 (2.7373)\tPrec@1 78.906 (82.779)\tPrec@5 98.438 (98.889)\n",
      "Epoch: [41][80/97], lr: 0.01000\tTime 0.382 (0.411)\tData 0.000 (0.024)\tLoss 3.3189 (2.7573)\tPrec@1 80.469 (82.581)\tPrec@5 98.438 (98.900)\n",
      "Epoch: [41][90/97], lr: 0.01000\tTime 0.360 (0.407)\tData 0.000 (0.023)\tLoss 3.3780 (2.7312)\tPrec@1 81.250 (82.761)\tPrec@5 96.094 (98.910)\n",
      "Epoch: [41][96/97], lr: 0.01000\tTime 0.334 (0.403)\tData 0.000 (0.023)\tLoss 2.8853 (2.7272)\tPrec@1 83.051 (82.758)\tPrec@5 99.153 (98.896)\n",
      "Gated Network Weight Gate= Flip:0.46, Sc:0.54\n",
      "Test: [0/100]\tTime 0.395 (0.395)\tLoss 10.7210 (10.7210)\tPrec@1 46.000 (46.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.076 (0.104)\tLoss 7.5358 (9.4912)\tPrec@1 60.000 (51.091)\tPrec@5 97.000 (95.545)\n",
      "Test: [20/100]\tTime 0.077 (0.090)\tLoss 7.7749 (9.4391)\tPrec@1 60.000 (51.429)\tPrec@5 95.000 (95.619)\n",
      "Test: [30/100]\tTime 0.076 (0.085)\tLoss 8.5543 (9.4367)\tPrec@1 54.000 (51.355)\tPrec@5 93.000 (95.452)\n",
      "Test: [40/100]\tTime 0.076 (0.083)\tLoss 9.7833 (9.4851)\tPrec@1 50.000 (51.122)\tPrec@5 93.000 (95.317)\n",
      "Test: [50/100]\tTime 0.078 (0.082)\tLoss 8.4753 (9.3345)\tPrec@1 60.000 (52.098)\tPrec@5 96.000 (95.627)\n",
      "Test: [60/100]\tTime 0.075 (0.081)\tLoss 7.8273 (9.2785)\tPrec@1 58.000 (52.279)\tPrec@5 96.000 (95.656)\n",
      "Test: [70/100]\tTime 0.075 (0.080)\tLoss 9.9379 (9.2601)\tPrec@1 46.000 (52.380)\tPrec@5 94.000 (95.634)\n",
      "Test: [80/100]\tTime 0.076 (0.079)\tLoss 8.6542 (9.2303)\tPrec@1 58.000 (52.667)\tPrec@5 95.000 (95.765)\n",
      "Test: [90/100]\tTime 0.075 (0.079)\tLoss 9.4316 (9.2927)\tPrec@1 55.000 (52.330)\tPrec@5 98.000 (95.714)\n",
      "val Results: Prec@1 52.110 Prec@5 95.660 Loss 9.33030\n",
      "val Class Accuracy: [0.975,0.952,0.607,0.664,0.654,0.586,0.549,0.198,0.016,0.010]\n",
      "Best Prec@1: 53.930\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [42][0/97], lr: 0.01000\tTime 1.155 (1.155)\tData 0.697 (0.697)\tLoss 1.7548 (1.7548)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [42][10/97], lr: 0.01000\tTime 0.360 (0.462)\tData 0.000 (0.074)\tLoss 3.0073 (2.5682)\tPrec@1 82.031 (83.523)\tPrec@5 98.438 (98.580)\n",
      "Epoch: [42][20/97], lr: 0.01000\tTime 0.370 (0.417)\tData 0.000 (0.046)\tLoss 2.3907 (2.6911)\tPrec@1 82.812 (82.440)\tPrec@5 97.656 (98.549)\n",
      "Epoch: [42][30/97], lr: 0.01000\tTime 0.357 (0.402)\tData 0.000 (0.036)\tLoss 3.2845 (2.6893)\tPrec@1 77.344 (82.762)\tPrec@5 99.219 (98.690)\n",
      "Epoch: [42][40/97], lr: 0.01000\tTime 0.370 (0.407)\tData 0.000 (0.031)\tLoss 2.4975 (2.5862)\tPrec@1 82.812 (83.556)\tPrec@5 100.000 (98.838)\n",
      "Epoch: [42][50/97], lr: 0.01000\tTime 0.361 (0.400)\tData 0.000 (0.028)\tLoss 2.7246 (2.6160)\tPrec@1 86.719 (83.747)\tPrec@5 98.438 (98.882)\n",
      "Epoch: [42][60/97], lr: 0.01000\tTime 0.350 (0.394)\tData 0.000 (0.026)\tLoss 1.7953 (2.6244)\tPrec@1 89.844 (83.555)\tPrec@5 98.438 (98.847)\n",
      "Epoch: [42][70/97], lr: 0.01000\tTime 0.347 (0.388)\tData 0.000 (0.025)\tLoss 3.2477 (2.6450)\tPrec@1 79.688 (83.429)\tPrec@5 97.656 (98.823)\n",
      "Epoch: [42][80/97], lr: 0.01000\tTime 0.342 (0.383)\tData 0.000 (0.024)\tLoss 2.7051 (2.6361)\tPrec@1 81.250 (83.430)\tPrec@5 99.219 (98.852)\n",
      "Epoch: [42][90/97], lr: 0.01000\tTime 0.335 (0.378)\tData 0.000 (0.023)\tLoss 3.3129 (2.6428)\tPrec@1 77.344 (83.396)\tPrec@5 98.438 (98.893)\n",
      "Epoch: [42][96/97], lr: 0.01000\tTime 0.324 (0.376)\tData 0.000 (0.023)\tLoss 2.8381 (2.6528)\tPrec@1 82.203 (83.323)\tPrec@5 98.305 (98.855)\n",
      "Gated Network Weight Gate= Flip:0.44, Sc:0.56\n",
      "Test: [0/100]\tTime 0.384 (0.384)\tLoss 12.4561 (12.4561)\tPrec@1 38.000 (38.000)\tPrec@5 88.000 (88.000)\n",
      "Test: [10/100]\tTime 0.074 (0.102)\tLoss 8.0426 (10.6866)\tPrec@1 57.000 (46.273)\tPrec@5 94.000 (90.273)\n",
      "Test: [20/100]\tTime 0.074 (0.089)\tLoss 9.5783 (10.4880)\tPrec@1 48.000 (46.571)\tPrec@5 94.000 (90.429)\n",
      "Test: [30/100]\tTime 0.075 (0.084)\tLoss 10.2452 (10.4094)\tPrec@1 47.000 (47.194)\tPrec@5 91.000 (90.355)\n",
      "Test: [40/100]\tTime 0.075 (0.082)\tLoss 10.8133 (10.4489)\tPrec@1 46.000 (47.463)\tPrec@5 87.000 (90.171)\n",
      "Test: [50/100]\tTime 0.075 (0.081)\tLoss 9.6631 (10.2740)\tPrec@1 51.000 (48.275)\tPrec@5 92.000 (90.627)\n",
      "Test: [60/100]\tTime 0.075 (0.080)\tLoss 7.4860 (10.1685)\tPrec@1 62.000 (48.639)\tPrec@5 91.000 (90.525)\n",
      "Test: [70/100]\tTime 0.075 (0.079)\tLoss 9.7354 (10.1650)\tPrec@1 53.000 (48.746)\tPrec@5 90.000 (90.479)\n",
      "Test: [80/100]\tTime 0.075 (0.079)\tLoss 10.1033 (10.1216)\tPrec@1 52.000 (48.988)\tPrec@5 86.000 (90.543)\n",
      "Test: [90/100]\tTime 0.075 (0.079)\tLoss 10.0048 (10.1807)\tPrec@1 51.000 (48.670)\tPrec@5 90.000 (90.538)\n",
      "val Results: Prec@1 48.450 Prec@5 90.490 Loss 10.22851\n",
      "val Class Accuracy: [0.928,0.900,0.679,0.386,0.750,0.777,0.391,0.025,0.008,0.001]\n",
      "Best Prec@1: 53.930\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [43][0/97], lr: 0.01000\tTime 0.850 (0.850)\tData 0.516 (0.516)\tLoss 2.4845 (2.4845)\tPrec@1 84.375 (84.375)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [43][10/97], lr: 0.01000\tTime 0.355 (0.435)\tData 0.000 (0.057)\tLoss 2.0496 (2.4695)\tPrec@1 89.844 (85.298)\tPrec@5 98.438 (98.224)\n",
      "Epoch: [43][20/97], lr: 0.01000\tTime 0.375 (0.404)\tData 0.000 (0.038)\tLoss 2.7713 (2.4156)\tPrec@1 84.375 (85.565)\tPrec@5 99.219 (98.438)\n",
      "Epoch: [43][30/97], lr: 0.01000\tTime 0.351 (0.390)\tData 0.001 (0.031)\tLoss 2.3674 (2.4809)\tPrec@1 85.938 (84.879)\tPrec@5 98.438 (98.412)\n",
      "Epoch: [43][40/97], lr: 0.01000\tTime 0.355 (0.384)\tData 0.000 (0.027)\tLoss 2.9910 (2.5262)\tPrec@1 82.812 (84.451)\tPrec@5 99.219 (98.495)\n",
      "Epoch: [43][50/97], lr: 0.01000\tTime 0.362 (0.382)\tData 0.000 (0.025)\tLoss 2.1548 (2.5648)\tPrec@1 88.281 (84.176)\tPrec@5 99.219 (98.499)\n",
      "Epoch: [43][60/97], lr: 0.01000\tTime 0.340 (0.378)\tData 0.000 (0.023)\tLoss 2.4754 (2.5797)\tPrec@1 82.812 (83.940)\tPrec@5 99.219 (98.591)\n",
      "Epoch: [43][70/97], lr: 0.01000\tTime 0.323 (0.374)\tData 0.000 (0.022)\tLoss 2.9527 (2.5945)\tPrec@1 80.469 (83.803)\tPrec@5 98.438 (98.625)\n",
      "Epoch: [43][80/97], lr: 0.01000\tTime 0.340 (0.371)\tData 0.000 (0.022)\tLoss 3.4575 (2.6122)\tPrec@1 79.688 (83.642)\tPrec@5 97.656 (98.640)\n",
      "Epoch: [43][90/97], lr: 0.01000\tTime 0.358 (0.369)\tData 0.000 (0.021)\tLoss 3.0287 (2.6492)\tPrec@1 78.906 (83.371)\tPrec@5 97.656 (98.626)\n",
      "Epoch: [43][96/97], lr: 0.01000\tTime 0.316 (0.367)\tData 0.000 (0.021)\tLoss 2.1301 (2.6563)\tPrec@1 85.593 (83.266)\tPrec@5 97.458 (98.630)\n",
      "Gated Network Weight Gate= Flip:0.46, Sc:0.54\n",
      "Test: [0/100]\tTime 0.398 (0.398)\tLoss 10.2404 (10.2404)\tPrec@1 51.000 (51.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.103)\tLoss 8.0187 (9.7182)\tPrec@1 60.000 (50.182)\tPrec@5 95.000 (96.455)\n",
      "Test: [20/100]\tTime 0.074 (0.089)\tLoss 8.7702 (9.7266)\tPrec@1 50.000 (49.333)\tPrec@5 98.000 (96.143)\n",
      "Test: [30/100]\tTime 0.074 (0.085)\tLoss 9.1160 (9.6924)\tPrec@1 56.000 (49.581)\tPrec@5 96.000 (95.935)\n",
      "Test: [40/100]\tTime 0.075 (0.082)\tLoss 10.5444 (9.7378)\tPrec@1 46.000 (49.683)\tPrec@5 89.000 (95.439)\n",
      "Test: [50/100]\tTime 0.075 (0.081)\tLoss 9.2417 (9.6338)\tPrec@1 50.000 (50.176)\tPrec@5 98.000 (95.588)\n",
      "Test: [60/100]\tTime 0.075 (0.080)\tLoss 7.8625 (9.6140)\tPrec@1 58.000 (50.131)\tPrec@5 96.000 (95.721)\n",
      "Test: [70/100]\tTime 0.075 (0.079)\tLoss 9.5824 (9.5979)\tPrec@1 51.000 (50.352)\tPrec@5 95.000 (95.620)\n",
      "Test: [80/100]\tTime 0.075 (0.079)\tLoss 9.0116 (9.5686)\tPrec@1 50.000 (50.568)\tPrec@5 94.000 (95.679)\n",
      "Test: [90/100]\tTime 0.075 (0.079)\tLoss 9.7065 (9.6268)\tPrec@1 51.000 (50.341)\tPrec@5 96.000 (95.560)\n",
      "val Results: Prec@1 50.400 Prec@5 95.560 Loss 9.65241\n",
      "val Class Accuracy: [0.820,0.994,0.852,0.578,0.549,0.413,0.388,0.338,0.100,0.008]\n",
      "Best Prec@1: 53.930\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [44][0/97], lr: 0.01000\tTime 0.985 (0.985)\tData 0.583 (0.583)\tLoss 2.9022 (2.9022)\tPrec@1 81.250 (81.250)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [44][10/97], lr: 0.01000\tTime 0.359 (0.456)\tData 0.000 (0.065)\tLoss 3.1595 (2.4582)\tPrec@1 78.906 (84.446)\tPrec@5 98.438 (99.361)\n",
      "Epoch: [44][20/97], lr: 0.01000\tTime 0.367 (0.413)\tData 0.000 (0.041)\tLoss 1.6770 (2.5251)\tPrec@1 89.062 (83.966)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [44][30/97], lr: 0.01000\tTime 0.358 (0.398)\tData 0.000 (0.033)\tLoss 3.5104 (2.5770)\tPrec@1 78.125 (83.669)\tPrec@5 99.219 (99.118)\n",
      "Epoch: [44][40/97], lr: 0.01000\tTime 0.368 (0.391)\tData 0.000 (0.029)\tLoss 2.9271 (2.6080)\tPrec@1 82.812 (83.556)\tPrec@5 100.000 (99.104)\n",
      "Epoch: [44][50/97], lr: 0.01000\tTime 0.351 (0.387)\tData 0.000 (0.026)\tLoss 3.1424 (2.6610)\tPrec@1 79.688 (83.211)\tPrec@5 96.094 (99.035)\n",
      "Epoch: [44][60/97], lr: 0.01000\tTime 0.354 (0.384)\tData 0.000 (0.025)\tLoss 2.5074 (2.6399)\tPrec@1 83.594 (83.376)\tPrec@5 99.219 (99.065)\n",
      "Epoch: [44][70/97], lr: 0.01000\tTime 0.339 (0.379)\tData 0.000 (0.023)\tLoss 2.9520 (2.6386)\tPrec@1 79.688 (83.407)\tPrec@5 97.656 (99.032)\n",
      "Epoch: [44][80/97], lr: 0.01000\tTime 0.322 (0.375)\tData 0.000 (0.023)\tLoss 2.5411 (2.6469)\tPrec@1 84.375 (83.333)\tPrec@5 96.875 (98.987)\n",
      "Epoch: [44][90/97], lr: 0.01000\tTime 0.343 (0.372)\tData 0.000 (0.022)\tLoss 2.8113 (2.6351)\tPrec@1 82.812 (83.474)\tPrec@5 98.438 (98.953)\n",
      "Epoch: [44][96/97], lr: 0.01000\tTime 0.322 (0.370)\tData 0.000 (0.022)\tLoss 2.3957 (2.6282)\tPrec@1 83.051 (83.572)\tPrec@5 99.153 (98.928)\n",
      "Gated Network Weight Gate= Flip:0.50, Sc:0.50\n",
      "Test: [0/100]\tTime 0.318 (0.318)\tLoss 10.9882 (10.9882)\tPrec@1 46.000 (46.000)\tPrec@5 90.000 (90.000)\n",
      "Test: [10/100]\tTime 0.074 (0.096)\tLoss 7.5427 (9.9241)\tPrec@1 63.000 (50.273)\tPrec@5 97.000 (92.182)\n",
      "Test: [20/100]\tTime 0.075 (0.086)\tLoss 9.2748 (9.8259)\tPrec@1 52.000 (49.952)\tPrec@5 97.000 (92.857)\n",
      "Test: [30/100]\tTime 0.074 (0.083)\tLoss 8.5380 (9.7413)\tPrec@1 56.000 (50.484)\tPrec@5 93.000 (92.742)\n",
      "Test: [40/100]\tTime 0.075 (0.081)\tLoss 10.4947 (9.7483)\tPrec@1 50.000 (50.976)\tPrec@5 92.000 (92.512)\n",
      "Test: [50/100]\tTime 0.076 (0.080)\tLoss 9.3688 (9.6659)\tPrec@1 53.000 (51.294)\tPrec@5 92.000 (92.725)\n",
      "Test: [60/100]\tTime 0.076 (0.079)\tLoss 8.3295 (9.6453)\tPrec@1 55.000 (51.148)\tPrec@5 92.000 (92.689)\n",
      "Test: [70/100]\tTime 0.075 (0.079)\tLoss 9.0615 (9.6480)\tPrec@1 53.000 (51.127)\tPrec@5 93.000 (92.704)\n",
      "Test: [80/100]\tTime 0.076 (0.078)\tLoss 9.1702 (9.6076)\tPrec@1 55.000 (51.333)\tPrec@5 89.000 (92.802)\n",
      "Test: [90/100]\tTime 0.075 (0.078)\tLoss 9.2978 (9.6669)\tPrec@1 54.000 (51.121)\tPrec@5 95.000 (92.692)\n",
      "val Results: Prec@1 51.070 Prec@5 92.560 Loss 9.69669\n",
      "val Class Accuracy: [0.946,0.961,0.821,0.340,0.755,0.544,0.460,0.248,0.029,0.003]\n",
      "Best Prec@1: 53.930\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [45][0/97], lr: 0.01000\tTime 0.967 (0.967)\tData 0.590 (0.590)\tLoss 2.8920 (2.8920)\tPrec@1 84.375 (84.375)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [45][10/97], lr: 0.01000\tTime 0.353 (0.446)\tData 0.000 (0.065)\tLoss 2.3404 (2.6459)\tPrec@1 86.719 (84.020)\tPrec@5 100.000 (99.006)\n",
      "Epoch: [45][20/97], lr: 0.01000\tTime 0.358 (0.409)\tData 0.000 (0.042)\tLoss 2.3191 (2.6247)\tPrec@1 85.938 (84.226)\tPrec@5 97.656 (99.107)\n",
      "Epoch: [45][30/97], lr: 0.01000\tTime 0.363 (0.392)\tData 0.000 (0.033)\tLoss 2.7027 (2.5933)\tPrec@1 84.375 (84.375)\tPrec@5 96.875 (98.942)\n",
      "Epoch: [45][40/97], lr: 0.01000\tTime 0.358 (0.388)\tData 0.000 (0.029)\tLoss 3.0814 (2.6072)\tPrec@1 82.031 (84.127)\tPrec@5 97.656 (98.971)\n",
      "Epoch: [45][50/97], lr: 0.01000\tTime 0.366 (0.384)\tData 0.000 (0.026)\tLoss 2.4800 (2.6105)\tPrec@1 84.375 (84.115)\tPrec@5 99.219 (98.912)\n",
      "Epoch: [45][60/97], lr: 0.01000\tTime 0.353 (0.381)\tData 0.000 (0.025)\tLoss 2.5253 (2.5877)\tPrec@1 84.375 (84.132)\tPrec@5 99.219 (98.924)\n",
      "Epoch: [45][70/97], lr: 0.01000\tTime 0.338 (0.376)\tData 0.000 (0.023)\tLoss 2.8544 (2.6002)\tPrec@1 82.812 (83.968)\tPrec@5 98.438 (98.900)\n",
      "Epoch: [45][80/97], lr: 0.01000\tTime 0.344 (0.373)\tData 0.000 (0.023)\tLoss 2.4958 (2.6095)\tPrec@1 84.375 (83.893)\tPrec@5 100.000 (98.958)\n",
      "Epoch: [45][90/97], lr: 0.01000\tTime 0.335 (0.370)\tData 0.000 (0.022)\tLoss 2.7848 (2.6125)\tPrec@1 85.156 (83.989)\tPrec@5 100.000 (98.961)\n",
      "Epoch: [45][96/97], lr: 0.01000\tTime 0.328 (0.368)\tData 0.000 (0.022)\tLoss 2.3715 (2.6113)\tPrec@1 86.441 (83.959)\tPrec@5 96.610 (98.968)\n",
      "Gated Network Weight Gate= Flip:0.48, Sc:0.52\n",
      "Test: [0/100]\tTime 0.362 (0.362)\tLoss 11.8173 (11.8173)\tPrec@1 37.000 (37.000)\tPrec@5 82.000 (82.000)\n",
      "Test: [10/100]\tTime 0.074 (0.100)\tLoss 9.9620 (10.5177)\tPrec@1 48.000 (45.091)\tPrec@5 82.000 (87.818)\n",
      "Test: [20/100]\tTime 0.075 (0.088)\tLoss 9.2810 (10.4076)\tPrec@1 51.000 (45.429)\tPrec@5 87.000 (88.333)\n",
      "Test: [30/100]\tTime 0.075 (0.084)\tLoss 9.3211 (10.4339)\tPrec@1 46.000 (45.065)\tPrec@5 86.000 (88.226)\n",
      "Test: [40/100]\tTime 0.075 (0.082)\tLoss 10.4546 (10.4263)\tPrec@1 46.000 (45.317)\tPrec@5 88.000 (88.512)\n",
      "Test: [50/100]\tTime 0.075 (0.081)\tLoss 9.9080 (10.3084)\tPrec@1 49.000 (45.824)\tPrec@5 89.000 (88.706)\n",
      "Test: [60/100]\tTime 0.075 (0.080)\tLoss 9.5270 (10.3073)\tPrec@1 48.000 (45.738)\tPrec@5 89.000 (88.410)\n",
      "Test: [70/100]\tTime 0.081 (0.079)\tLoss 9.8231 (10.2649)\tPrec@1 49.000 (46.000)\tPrec@5 87.000 (88.535)\n",
      "Test: [80/100]\tTime 0.075 (0.079)\tLoss 8.4878 (10.1993)\tPrec@1 56.000 (46.407)\tPrec@5 92.000 (88.765)\n",
      "Test: [90/100]\tTime 0.075 (0.079)\tLoss 10.5072 (10.3011)\tPrec@1 41.000 (45.978)\tPrec@5 90.000 (88.396)\n",
      "val Results: Prec@1 45.790 Prec@5 88.370 Loss 10.34088\n",
      "val Class Accuracy: [0.949,0.988,0.506,0.565,0.827,0.206,0.008,0.401,0.075,0.054]\n",
      "Best Prec@1: 53.930\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [46][0/97], lr: 0.01000\tTime 0.986 (0.986)\tData 0.595 (0.595)\tLoss 2.7868 (2.7868)\tPrec@1 84.375 (84.375)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [46][10/97], lr: 0.01000\tTime 0.354 (0.444)\tData 0.001 (0.066)\tLoss 2.9171 (2.5011)\tPrec@1 80.469 (84.588)\tPrec@5 97.656 (98.438)\n",
      "Epoch: [46][20/97], lr: 0.01000\tTime 0.366 (0.408)\tData 0.000 (0.042)\tLoss 3.0039 (2.6196)\tPrec@1 83.594 (83.743)\tPrec@5 100.000 (98.735)\n",
      "Epoch: [46][30/97], lr: 0.01000\tTime 0.344 (0.392)\tData 0.000 (0.034)\tLoss 2.3438 (2.6616)\tPrec@1 85.938 (83.417)\tPrec@5 99.219 (98.740)\n",
      "Epoch: [46][40/97], lr: 0.01000\tTime 0.405 (0.387)\tData 0.000 (0.029)\tLoss 2.5132 (2.6259)\tPrec@1 82.812 (83.575)\tPrec@5 99.219 (98.800)\n",
      "Epoch: [46][50/97], lr: 0.01000\tTime 0.365 (0.384)\tData 0.001 (0.027)\tLoss 2.0336 (2.5923)\tPrec@1 86.719 (83.824)\tPrec@5 98.438 (98.882)\n",
      "Epoch: [46][60/97], lr: 0.01000\tTime 0.351 (0.380)\tData 0.000 (0.025)\tLoss 2.4343 (2.5764)\tPrec@1 84.375 (83.824)\tPrec@5 99.219 (98.886)\n",
      "Epoch: [46][70/97], lr: 0.01000\tTime 0.393 (0.376)\tData 0.000 (0.024)\tLoss 2.8812 (2.5564)\tPrec@1 81.250 (84.045)\tPrec@5 99.219 (98.900)\n",
      "Epoch: [46][80/97], lr: 0.01000\tTime 0.341 (0.372)\tData 0.000 (0.023)\tLoss 2.7565 (2.5872)\tPrec@1 79.688 (83.864)\tPrec@5 99.219 (98.872)\n",
      "Epoch: [46][90/97], lr: 0.01000\tTime 0.342 (0.370)\tData 0.000 (0.022)\tLoss 3.0067 (2.5949)\tPrec@1 80.469 (83.817)\tPrec@5 98.438 (98.884)\n",
      "Epoch: [46][96/97], lr: 0.01000\tTime 0.328 (0.368)\tData 0.000 (0.022)\tLoss 3.9032 (2.6245)\tPrec@1 75.424 (83.637)\tPrec@5 96.610 (98.831)\n",
      "Gated Network Weight Gate= Flip:0.43, Sc:0.57\n",
      "Test: [0/100]\tTime 0.300 (0.300)\tLoss 13.0993 (13.0993)\tPrec@1 35.000 (35.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.074 (0.095)\tLoss 10.3938 (11.6257)\tPrec@1 47.000 (41.182)\tPrec@5 90.000 (92.455)\n",
      "Test: [20/100]\tTime 0.075 (0.086)\tLoss 10.1874 (11.4416)\tPrec@1 48.000 (41.476)\tPrec@5 96.000 (92.952)\n",
      "Test: [30/100]\tTime 0.075 (0.082)\tLoss 10.3794 (11.2772)\tPrec@1 47.000 (42.194)\tPrec@5 91.000 (92.774)\n",
      "Test: [40/100]\tTime 0.075 (0.081)\tLoss 11.8218 (11.2737)\tPrec@1 38.000 (42.366)\tPrec@5 90.000 (92.780)\n",
      "Test: [50/100]\tTime 0.075 (0.080)\tLoss 10.9700 (11.1568)\tPrec@1 44.000 (42.961)\tPrec@5 91.000 (93.000)\n",
      "Test: [60/100]\tTime 0.075 (0.079)\tLoss 9.8969 (11.1693)\tPrec@1 47.000 (42.803)\tPrec@5 94.000 (93.049)\n",
      "Test: [70/100]\tTime 0.075 (0.079)\tLoss 10.9233 (11.1346)\tPrec@1 41.000 (42.873)\tPrec@5 91.000 (92.873)\n",
      "Test: [80/100]\tTime 0.076 (0.078)\tLoss 9.7875 (11.0782)\tPrec@1 47.000 (43.012)\tPrec@5 92.000 (92.963)\n",
      "Test: [90/100]\tTime 0.075 (0.078)\tLoss 10.5201 (11.1619)\tPrec@1 43.000 (42.484)\tPrec@5 95.000 (92.890)\n",
      "val Results: Prec@1 42.480 Prec@5 92.770 Loss 11.17763\n",
      "val Class Accuracy: [0.922,0.961,0.874,0.564,0.323,0.312,0.011,0.239,0.041,0.001]\n",
      "Best Prec@1: 53.930\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [47][0/97], lr: 0.01000\tTime 0.924 (0.924)\tData 0.549 (0.549)\tLoss 2.5829 (2.5829)\tPrec@1 82.812 (82.812)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [47][10/97], lr: 0.01000\tTime 0.360 (0.439)\tData 0.000 (0.060)\tLoss 3.1061 (2.5615)\tPrec@1 80.469 (83.807)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [47][20/97], lr: 0.01000\tTime 0.370 (0.404)\tData 0.000 (0.039)\tLoss 2.7178 (2.6217)\tPrec@1 81.250 (83.482)\tPrec@5 99.219 (99.144)\n",
      "Epoch: [47][30/97], lr: 0.01000\tTime 0.361 (0.391)\tData 0.001 (0.032)\tLoss 2.3382 (2.5739)\tPrec@1 85.156 (84.173)\tPrec@5 98.438 (98.992)\n",
      "Epoch: [47][40/97], lr: 0.01000\tTime 0.352 (0.384)\tData 0.000 (0.028)\tLoss 3.0601 (2.6149)\tPrec@1 80.469 (83.803)\tPrec@5 98.438 (99.047)\n",
      "Epoch: [47][50/97], lr: 0.01000\tTime 0.352 (0.381)\tData 0.000 (0.026)\tLoss 2.2004 (2.6541)\tPrec@1 86.719 (83.456)\tPrec@5 99.219 (99.004)\n",
      "Epoch: [47][60/97], lr: 0.01000\tTime 0.360 (0.378)\tData 0.000 (0.024)\tLoss 3.0516 (2.6334)\tPrec@1 79.688 (83.491)\tPrec@5 100.000 (99.014)\n",
      "Epoch: [47][70/97], lr: 0.01000\tTime 0.342 (0.375)\tData 0.000 (0.023)\tLoss 3.3986 (2.6469)\tPrec@1 76.562 (83.385)\tPrec@5 96.875 (98.911)\n",
      "Epoch: [47][80/97], lr: 0.01000\tTime 0.341 (0.370)\tData 0.000 (0.022)\tLoss 1.9473 (2.6150)\tPrec@1 87.500 (83.517)\tPrec@5 100.000 (98.920)\n",
      "Epoch: [47][90/97], lr: 0.01000\tTime 0.349 (0.369)\tData 0.000 (0.021)\tLoss 1.8270 (2.5783)\tPrec@1 89.844 (83.723)\tPrec@5 100.000 (98.970)\n",
      "Epoch: [47][96/97], lr: 0.01000\tTime 0.326 (0.367)\tData 0.000 (0.022)\tLoss 2.6515 (2.5851)\tPrec@1 82.203 (83.693)\tPrec@5 98.305 (98.960)\n",
      "Gated Network Weight Gate= Flip:0.32, Sc:0.68\n",
      "Test: [0/100]\tTime 0.314 (0.314)\tLoss 10.2494 (10.2494)\tPrec@1 43.000 (43.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.074 (0.096)\tLoss 7.8985 (8.7960)\tPrec@1 57.000 (54.545)\tPrec@5 89.000 (94.000)\n",
      "Test: [20/100]\tTime 0.076 (0.086)\tLoss 7.8724 (8.8881)\tPrec@1 57.000 (53.286)\tPrec@5 94.000 (94.238)\n",
      "Test: [30/100]\tTime 0.075 (0.082)\tLoss 7.4000 (8.8226)\tPrec@1 63.000 (53.935)\tPrec@5 97.000 (94.323)\n",
      "Test: [40/100]\tTime 0.075 (0.081)\tLoss 8.7034 (8.7604)\tPrec@1 59.000 (54.512)\tPrec@5 93.000 (94.244)\n",
      "Test: [50/100]\tTime 0.075 (0.080)\tLoss 8.3497 (8.6494)\tPrec@1 60.000 (55.333)\tPrec@5 95.000 (94.431)\n",
      "Test: [60/100]\tTime 0.075 (0.079)\tLoss 7.9382 (8.6142)\tPrec@1 60.000 (55.525)\tPrec@5 94.000 (94.328)\n",
      "Test: [70/100]\tTime 0.075 (0.079)\tLoss 9.3446 (8.6321)\tPrec@1 48.000 (55.338)\tPrec@5 97.000 (94.352)\n",
      "Test: [80/100]\tTime 0.075 (0.078)\tLoss 7.4892 (8.5838)\tPrec@1 61.000 (55.691)\tPrec@5 92.000 (94.494)\n",
      "Test: [90/100]\tTime 0.075 (0.078)\tLoss 8.5992 (8.6600)\tPrec@1 53.000 (55.264)\tPrec@5 96.000 (94.363)\n",
      "val Results: Prec@1 55.240 Prec@5 94.420 Loss 8.68281\n",
      "val Class Accuracy: [0.948,0.978,0.718,0.533,0.543,0.560,0.410,0.739,0.031,0.064]\n",
      "Best Prec@1: 55.240\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [48][0/97], lr: 0.01000\tTime 0.933 (0.933)\tData 0.595 (0.595)\tLoss 1.7768 (1.7768)\tPrec@1 86.719 (86.719)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [48][10/97], lr: 0.01000\tTime 0.368 (0.440)\tData 0.000 (0.066)\tLoss 2.5981 (2.4760)\tPrec@1 80.469 (84.091)\tPrec@5 99.219 (98.793)\n",
      "Epoch: [48][20/97], lr: 0.01000\tTime 0.403 (0.408)\tData 0.000 (0.042)\tLoss 2.3172 (2.4692)\tPrec@1 85.156 (84.524)\tPrec@5 100.000 (98.884)\n",
      "Epoch: [48][30/97], lr: 0.01000\tTime 0.366 (0.396)\tData 0.000 (0.033)\tLoss 1.9668 (2.4849)\tPrec@1 86.719 (84.375)\tPrec@5 98.438 (98.866)\n",
      "Epoch: [48][40/97], lr: 0.01000\tTime 0.381 (0.389)\tData 0.000 (0.029)\tLoss 2.5082 (2.4883)\tPrec@1 85.938 (84.413)\tPrec@5 100.000 (98.914)\n",
      "Epoch: [48][50/97], lr: 0.01000\tTime 0.356 (0.385)\tData 0.000 (0.027)\tLoss 1.9938 (2.4707)\tPrec@1 88.281 (84.651)\tPrec@5 100.000 (99.020)\n",
      "Epoch: [48][60/97], lr: 0.01000\tTime 0.345 (0.382)\tData 0.000 (0.025)\tLoss 2.6836 (2.4632)\tPrec@1 82.812 (84.606)\tPrec@5 99.219 (98.988)\n",
      "Epoch: [48][70/97], lr: 0.01000\tTime 0.365 (0.378)\tData 0.000 (0.024)\tLoss 2.1299 (2.4772)\tPrec@1 85.938 (84.529)\tPrec@5 99.219 (98.966)\n",
      "Epoch: [48][80/97], lr: 0.01000\tTime 0.343 (0.378)\tData 0.001 (0.023)\tLoss 2.0004 (2.4826)\tPrec@1 88.281 (84.578)\tPrec@5 98.438 (98.929)\n",
      "Epoch: [48][90/97], lr: 0.01000\tTime 0.342 (0.376)\tData 0.000 (0.022)\tLoss 2.3273 (2.5230)\tPrec@1 86.719 (84.255)\tPrec@5 98.438 (98.953)\n",
      "Epoch: [48][96/97], lr: 0.01000\tTime 0.328 (0.373)\tData 0.000 (0.022)\tLoss 2.1334 (2.5318)\tPrec@1 88.136 (84.185)\tPrec@5 100.000 (98.944)\n",
      "Gated Network Weight Gate= Flip:0.42, Sc:0.58\n",
      "Test: [0/100]\tTime 0.326 (0.326)\tLoss 9.5293 (9.5293)\tPrec@1 51.000 (51.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.074 (0.097)\tLoss 7.2209 (8.8117)\tPrec@1 62.000 (55.091)\tPrec@5 95.000 (95.636)\n",
      "Test: [20/100]\tTime 0.076 (0.086)\tLoss 7.7681 (8.7551)\tPrec@1 57.000 (54.952)\tPrec@5 95.000 (95.667)\n",
      "Test: [30/100]\tTime 0.075 (0.084)\tLoss 7.4419 (8.6988)\tPrec@1 60.000 (55.000)\tPrec@5 97.000 (95.742)\n",
      "Test: [40/100]\tTime 0.076 (0.082)\tLoss 9.4314 (8.7443)\tPrec@1 53.000 (55.122)\tPrec@5 94.000 (95.488)\n",
      "Test: [50/100]\tTime 0.075 (0.080)\tLoss 8.0292 (8.6219)\tPrec@1 63.000 (56.000)\tPrec@5 97.000 (95.706)\n",
      "Test: [60/100]\tTime 0.078 (0.080)\tLoss 7.1812 (8.5620)\tPrec@1 59.000 (56.148)\tPrec@5 97.000 (95.672)\n",
      "Test: [70/100]\tTime 0.075 (0.080)\tLoss 8.4344 (8.5496)\tPrec@1 56.000 (56.014)\tPrec@5 96.000 (95.718)\n",
      "Test: [80/100]\tTime 0.075 (0.080)\tLoss 8.2669 (8.4861)\tPrec@1 57.000 (56.333)\tPrec@5 97.000 (95.877)\n",
      "Test: [90/100]\tTime 0.085 (0.080)\tLoss 8.8258 (8.5602)\tPrec@1 54.000 (56.088)\tPrec@5 97.000 (95.846)\n",
      "val Results: Prec@1 56.170 Prec@5 95.830 Loss 8.57324\n",
      "val Class Accuracy: [0.969,0.954,0.554,0.504,0.566,0.737,0.603,0.558,0.099,0.073]\n",
      "Best Prec@1: 56.170\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [49][0/97], lr: 0.01000\tTime 1.460 (1.460)\tData 0.889 (0.889)\tLoss 2.4478 (2.4478)\tPrec@1 86.719 (86.719)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [49][10/97], lr: 0.01000\tTime 0.421 (0.568)\tData 0.000 (0.089)\tLoss 1.4704 (2.3439)\tPrec@1 90.625 (84.517)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [49][20/97], lr: 0.01000\tTime 0.448 (0.498)\tData 0.000 (0.052)\tLoss 2.5999 (2.4348)\tPrec@1 82.812 (84.524)\tPrec@5 100.000 (99.256)\n",
      "Epoch: [49][30/97], lr: 0.01000\tTime 0.457 (0.494)\tData 0.001 (0.040)\tLoss 1.8696 (2.4426)\tPrec@1 87.500 (84.627)\tPrec@5 97.656 (99.194)\n",
      "Epoch: [49][40/97], lr: 0.01000\tTime 0.426 (0.478)\tData 0.001 (0.034)\tLoss 2.2204 (2.4422)\tPrec@1 86.719 (84.737)\tPrec@5 97.656 (99.162)\n",
      "Epoch: [49][50/97], lr: 0.01000\tTime 0.399 (0.467)\tData 0.000 (0.030)\tLoss 2.7090 (2.4904)\tPrec@1 82.812 (84.375)\tPrec@5 99.219 (99.004)\n",
      "Epoch: [49][60/97], lr: 0.01000\tTime 0.389 (0.451)\tData 0.001 (0.028)\tLoss 1.9899 (2.4946)\tPrec@1 85.938 (84.183)\tPrec@5 100.000 (99.027)\n",
      "Epoch: [49][70/97], lr: 0.01000\tTime 0.393 (0.443)\tData 0.001 (0.026)\tLoss 2.5521 (2.5062)\tPrec@1 83.594 (84.199)\tPrec@5 99.219 (99.076)\n",
      "Epoch: [49][80/97], lr: 0.01000\tTime 0.370 (0.435)\tData 0.000 (0.025)\tLoss 2.2459 (2.5259)\tPrec@1 85.156 (84.144)\tPrec@5 99.219 (99.045)\n",
      "Epoch: [49][90/97], lr: 0.01000\tTime 0.341 (0.426)\tData 0.000 (0.024)\tLoss 2.9286 (2.5420)\tPrec@1 80.469 (84.049)\tPrec@5 98.438 (99.064)\n",
      "Epoch: [49][96/97], lr: 0.01000\tTime 0.327 (0.421)\tData 0.000 (0.024)\tLoss 2.9008 (2.5331)\tPrec@1 81.356 (84.104)\tPrec@5 99.153 (99.073)\n",
      "Gated Network Weight Gate= Flip:0.47, Sc:0.53\n",
      "Test: [0/100]\tTime 0.435 (0.435)\tLoss 11.1151 (11.1151)\tPrec@1 45.000 (45.000)\tPrec@5 88.000 (88.000)\n",
      "Test: [10/100]\tTime 0.075 (0.107)\tLoss 8.3666 (9.7059)\tPrec@1 59.000 (51.273)\tPrec@5 88.000 (92.182)\n",
      "Test: [20/100]\tTime 0.076 (0.092)\tLoss 8.3589 (9.5479)\tPrec@1 57.000 (51.619)\tPrec@5 96.000 (92.190)\n",
      "Test: [30/100]\tTime 0.075 (0.087)\tLoss 8.1757 (9.5273)\tPrec@1 56.000 (51.355)\tPrec@5 93.000 (92.290)\n",
      "Test: [40/100]\tTime 0.084 (0.085)\tLoss 9.5774 (9.4608)\tPrec@1 56.000 (52.244)\tPrec@5 86.000 (92.268)\n",
      "Test: [50/100]\tTime 0.075 (0.083)\tLoss 8.7473 (9.3798)\tPrec@1 55.000 (52.667)\tPrec@5 94.000 (92.255)\n",
      "Test: [60/100]\tTime 0.075 (0.082)\tLoss 8.3983 (9.3452)\tPrec@1 57.000 (52.541)\tPrec@5 91.000 (92.492)\n",
      "Test: [70/100]\tTime 0.079 (0.081)\tLoss 9.0487 (9.3564)\tPrec@1 54.000 (52.451)\tPrec@5 95.000 (92.507)\n",
      "Test: [80/100]\tTime 0.076 (0.080)\tLoss 8.1768 (9.3081)\tPrec@1 60.000 (52.716)\tPrec@5 94.000 (92.642)\n",
      "Test: [90/100]\tTime 0.076 (0.080)\tLoss 10.1191 (9.3898)\tPrec@1 46.000 (52.242)\tPrec@5 95.000 (92.527)\n",
      "val Results: Prec@1 52.210 Prec@5 92.440 Loss 9.42255\n",
      "val Class Accuracy: [0.954,0.981,0.556,0.746,0.658,0.417,0.104,0.699,0.022,0.084]\n",
      "Best Prec@1: 56.170\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [50][0/97], lr: 0.01000\tTime 1.020 (1.020)\tData 0.615 (0.615)\tLoss 1.9993 (1.9993)\tPrec@1 90.625 (90.625)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [50][10/97], lr: 0.01000\tTime 0.451 (0.506)\tData 0.000 (0.067)\tLoss 2.5713 (2.4319)\tPrec@1 84.375 (85.298)\tPrec@5 99.219 (98.580)\n",
      "Epoch: [50][20/97], lr: 0.01000\tTime 0.450 (0.477)\tData 0.000 (0.042)\tLoss 2.2384 (2.4950)\tPrec@1 86.719 (84.747)\tPrec@5 99.219 (98.958)\n",
      "Epoch: [50][30/97], lr: 0.01000\tTime 0.438 (0.464)\tData 0.000 (0.033)\tLoss 2.9808 (2.5614)\tPrec@1 82.031 (84.199)\tPrec@5 99.219 (98.916)\n",
      "Epoch: [50][40/97], lr: 0.01000\tTime 0.369 (0.446)\tData 0.001 (0.029)\tLoss 1.8919 (2.4830)\tPrec@1 87.500 (84.909)\tPrec@5 99.219 (98.819)\n",
      "Epoch: [50][50/97], lr: 0.01000\tTime 0.383 (0.435)\tData 0.000 (0.026)\tLoss 3.2549 (2.5103)\tPrec@1 79.688 (84.620)\tPrec@5 99.219 (98.851)\n",
      "Epoch: [50][60/97], lr: 0.01000\tTime 0.361 (0.429)\tData 0.000 (0.024)\tLoss 2.4126 (2.5695)\tPrec@1 83.594 (84.208)\tPrec@5 100.000 (98.809)\n",
      "Epoch: [50][70/97], lr: 0.01000\tTime 0.421 (0.423)\tData 0.000 (0.023)\tLoss 1.9812 (2.5767)\tPrec@1 89.062 (84.210)\tPrec@5 99.219 (98.812)\n",
      "Epoch: [50][80/97], lr: 0.01000\tTime 0.413 (0.427)\tData 0.000 (0.022)\tLoss 2.4243 (2.5414)\tPrec@1 86.719 (84.404)\tPrec@5 98.438 (98.852)\n",
      "Epoch: [50][90/97], lr: 0.01000\tTime 0.469 (0.434)\tData 0.000 (0.022)\tLoss 2.4342 (2.5280)\tPrec@1 83.594 (84.487)\tPrec@5 98.438 (98.918)\n",
      "Epoch: [50][96/97], lr: 0.01000\tTime 0.370 (0.433)\tData 0.000 (0.022)\tLoss 2.2710 (2.5415)\tPrec@1 87.288 (84.322)\tPrec@5 97.458 (98.888)\n",
      "Gated Network Weight Gate= Flip:0.54, Sc:0.46\n",
      "Test: [0/100]\tTime 0.381 (0.381)\tLoss 9.7638 (9.7638)\tPrec@1 53.000 (53.000)\tPrec@5 89.000 (89.000)\n",
      "Test: [10/100]\tTime 0.083 (0.103)\tLoss 7.2905 (9.8852)\tPrec@1 63.000 (50.545)\tPrec@5 93.000 (89.545)\n",
      "Test: [20/100]\tTime 0.074 (0.090)\tLoss 9.0978 (9.9193)\tPrec@1 55.000 (49.905)\tPrec@5 90.000 (89.667)\n",
      "Test: [30/100]\tTime 0.075 (0.085)\tLoss 10.1953 (10.0818)\tPrec@1 49.000 (49.387)\tPrec@5 86.000 (88.968)\n",
      "Test: [40/100]\tTime 0.078 (0.083)\tLoss 10.7993 (10.1491)\tPrec@1 48.000 (49.463)\tPrec@5 84.000 (88.927)\n",
      "Test: [50/100]\tTime 0.079 (0.082)\tLoss 9.4715 (10.0438)\tPrec@1 50.000 (50.118)\tPrec@5 87.000 (89.039)\n",
      "Test: [60/100]\tTime 0.075 (0.082)\tLoss 9.5748 (10.0614)\tPrec@1 52.000 (49.934)\tPrec@5 90.000 (88.852)\n",
      "Test: [70/100]\tTime 0.074 (0.082)\tLoss 9.8144 (10.0633)\tPrec@1 53.000 (49.915)\tPrec@5 86.000 (88.718)\n",
      "Test: [80/100]\tTime 0.078 (0.082)\tLoss 9.9863 (10.0258)\tPrec@1 50.000 (50.272)\tPrec@5 85.000 (88.840)\n",
      "Test: [90/100]\tTime 0.084 (0.083)\tLoss 10.4054 (10.0768)\tPrec@1 52.000 (50.143)\tPrec@5 90.000 (88.703)\n",
      "val Results: Prec@1 50.120 Prec@5 88.690 Loss 10.09380\n",
      "val Class Accuracy: [0.843,0.967,0.601,0.810,0.361,0.375,0.830,0.176,0.044,0.005]\n",
      "Best Prec@1: 56.170\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [51][0/97], lr: 0.01000\tTime 1.572 (1.572)\tData 0.916 (0.916)\tLoss 2.9605 (2.9605)\tPrec@1 80.469 (80.469)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [51][10/97], lr: 0.01000\tTime 0.410 (0.584)\tData 0.000 (0.091)\tLoss 1.9456 (2.5148)\tPrec@1 87.500 (83.807)\tPrec@5 99.219 (98.864)\n",
      "Epoch: [51][20/97], lr: 0.01000\tTime 0.429 (0.523)\tData 0.001 (0.055)\tLoss 1.9574 (2.5122)\tPrec@1 88.281 (83.966)\tPrec@5 100.000 (99.070)\n",
      "Epoch: [51][30/97], lr: 0.01000\tTime 0.354 (0.487)\tData 0.001 (0.041)\tLoss 2.5262 (2.4428)\tPrec@1 83.594 (84.526)\tPrec@5 99.219 (99.042)\n",
      "Epoch: [51][40/97], lr: 0.01000\tTime 0.444 (0.469)\tData 0.000 (0.035)\tLoss 1.8567 (2.4393)\tPrec@1 87.500 (84.718)\tPrec@5 100.000 (99.104)\n",
      "Epoch: [51][50/97], lr: 0.01000\tTime 0.453 (0.467)\tData 0.001 (0.031)\tLoss 2.3884 (2.4791)\tPrec@1 85.156 (84.544)\tPrec@5 100.000 (99.157)\n",
      "Epoch: [51][60/97], lr: 0.01000\tTime 0.466 (0.469)\tData 0.001 (0.028)\tLoss 3.0089 (2.4937)\tPrec@1 79.688 (84.413)\tPrec@5 99.219 (99.129)\n",
      "Epoch: [51][70/97], lr: 0.01000\tTime 0.413 (0.466)\tData 0.000 (0.026)\tLoss 2.6440 (2.4736)\tPrec@1 84.375 (84.595)\tPrec@5 97.656 (99.043)\n",
      "Epoch: [51][80/97], lr: 0.01000\tTime 0.409 (0.460)\tData 0.000 (0.025)\tLoss 2.2854 (2.5082)\tPrec@1 86.719 (84.375)\tPrec@5 98.438 (99.016)\n",
      "Epoch: [51][90/97], lr: 0.01000\tTime 0.379 (0.456)\tData 0.000 (0.024)\tLoss 3.1251 (2.5081)\tPrec@1 79.688 (84.384)\tPrec@5 96.875 (98.953)\n",
      "Epoch: [51][96/97], lr: 0.01000\tTime 0.332 (0.449)\tData 0.000 (0.024)\tLoss 2.3897 (2.5088)\tPrec@1 87.288 (84.467)\tPrec@5 99.153 (98.928)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.420 (0.420)\tLoss 11.7202 (11.7202)\tPrec@1 43.000 (43.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.074 (0.106)\tLoss 8.4719 (10.6104)\tPrec@1 59.000 (47.364)\tPrec@5 91.000 (92.455)\n",
      "Test: [20/100]\tTime 0.084 (0.092)\tLoss 10.0527 (10.6563)\tPrec@1 43.000 (46.476)\tPrec@5 94.000 (92.286)\n",
      "Test: [30/100]\tTime 0.075 (0.086)\tLoss 9.3921 (10.5744)\tPrec@1 52.000 (46.968)\tPrec@5 96.000 (92.323)\n",
      "Test: [40/100]\tTime 0.076 (0.084)\tLoss 11.1946 (10.5329)\tPrec@1 46.000 (47.415)\tPrec@5 87.000 (92.024)\n",
      "Test: [50/100]\tTime 0.076 (0.083)\tLoss 10.2928 (10.4236)\tPrec@1 49.000 (47.941)\tPrec@5 90.000 (92.157)\n",
      "Test: [60/100]\tTime 0.077 (0.082)\tLoss 9.0279 (10.4279)\tPrec@1 55.000 (47.787)\tPrec@5 91.000 (92.197)\n",
      "Test: [70/100]\tTime 0.095 (0.084)\tLoss 10.3018 (10.4089)\tPrec@1 48.000 (47.789)\tPrec@5 92.000 (92.366)\n",
      "Test: [80/100]\tTime 0.075 (0.083)\tLoss 8.8730 (10.3440)\tPrec@1 59.000 (47.951)\tPrec@5 92.000 (92.556)\n",
      "Test: [90/100]\tTime 0.075 (0.083)\tLoss 10.3041 (10.4220)\tPrec@1 46.000 (47.593)\tPrec@5 93.000 (92.505)\n",
      "val Results: Prec@1 47.430 Prec@5 92.460 Loss 10.45456\n",
      "val Class Accuracy: [0.979,0.942,0.865,0.562,0.457,0.240,0.268,0.267,0.111,0.052]\n",
      "Best Prec@1: 56.170\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [52][0/97], lr: 0.01000\tTime 1.093 (1.093)\tData 0.685 (0.685)\tLoss 2.2994 (2.2994)\tPrec@1 86.719 (86.719)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [52][10/97], lr: 0.01000\tTime 0.375 (0.448)\tData 0.000 (0.073)\tLoss 2.6562 (2.5597)\tPrec@1 85.938 (83.949)\tPrec@5 98.438 (98.722)\n",
      "Epoch: [52][20/97], lr: 0.01000\tTime 0.357 (0.408)\tData 0.000 (0.045)\tLoss 2.4279 (2.5460)\tPrec@1 85.156 (84.115)\tPrec@5 98.438 (98.847)\n",
      "Epoch: [52][30/97], lr: 0.01000\tTime 0.348 (0.394)\tData 0.000 (0.036)\tLoss 2.7182 (2.5278)\tPrec@1 85.156 (84.274)\tPrec@5 99.219 (98.891)\n",
      "Epoch: [52][40/97], lr: 0.01000\tTime 0.347 (0.386)\tData 0.000 (0.031)\tLoss 2.4557 (2.5165)\tPrec@1 84.375 (84.337)\tPrec@5 100.000 (98.952)\n",
      "Epoch: [52][50/97], lr: 0.01000\tTime 0.374 (0.381)\tData 0.000 (0.028)\tLoss 2.4942 (2.5233)\tPrec@1 85.156 (84.329)\tPrec@5 99.219 (98.958)\n",
      "Epoch: [52][60/97], lr: 0.01000\tTime 0.348 (0.377)\tData 0.000 (0.026)\tLoss 2.7036 (2.5130)\tPrec@1 84.375 (84.426)\tPrec@5 99.219 (98.975)\n",
      "Epoch: [52][70/97], lr: 0.01000\tTime 0.347 (0.374)\tData 0.000 (0.024)\tLoss 3.0401 (2.5398)\tPrec@1 82.031 (84.210)\tPrec@5 96.875 (98.889)\n",
      "Epoch: [52][80/97], lr: 0.01000\tTime 0.345 (0.371)\tData 0.000 (0.023)\tLoss 1.8470 (2.5383)\tPrec@1 89.062 (84.240)\tPrec@5 100.000 (98.881)\n",
      "Epoch: [52][90/97], lr: 0.01000\tTime 0.332 (0.369)\tData 0.000 (0.023)\tLoss 2.2886 (2.5337)\tPrec@1 84.375 (84.246)\tPrec@5 99.219 (98.781)\n",
      "Epoch: [52][96/97], lr: 0.01000\tTime 0.332 (0.367)\tData 0.000 (0.023)\tLoss 1.7731 (2.5175)\tPrec@1 90.678 (84.370)\tPrec@5 98.305 (98.791)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.341 (0.341)\tLoss 9.5707 (9.5707)\tPrec@1 50.000 (50.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.074 (0.099)\tLoss 6.8857 (8.5105)\tPrec@1 62.000 (57.000)\tPrec@5 98.000 (96.091)\n",
      "Test: [20/100]\tTime 0.075 (0.088)\tLoss 6.6584 (8.5061)\tPrec@1 62.000 (56.714)\tPrec@5 98.000 (96.429)\n",
      "Test: [30/100]\tTime 0.077 (0.084)\tLoss 7.9153 (8.5886)\tPrec@1 61.000 (55.710)\tPrec@5 97.000 (96.097)\n",
      "Test: [40/100]\tTime 0.075 (0.082)\tLoss 9.1265 (8.5976)\tPrec@1 54.000 (55.854)\tPrec@5 95.000 (95.927)\n",
      "Test: [50/100]\tTime 0.075 (0.081)\tLoss 7.7116 (8.4867)\tPrec@1 66.000 (56.549)\tPrec@5 96.000 (96.059)\n",
      "Test: [60/100]\tTime 0.076 (0.080)\tLoss 7.5557 (8.4511)\tPrec@1 60.000 (56.607)\tPrec@5 96.000 (96.098)\n",
      "Test: [70/100]\tTime 0.075 (0.079)\tLoss 8.2881 (8.4166)\tPrec@1 58.000 (56.761)\tPrec@5 96.000 (96.070)\n",
      "Test: [80/100]\tTime 0.076 (0.079)\tLoss 7.7570 (8.3679)\tPrec@1 60.000 (56.988)\tPrec@5 92.000 (96.062)\n",
      "Test: [90/100]\tTime 0.076 (0.079)\tLoss 8.6979 (8.4256)\tPrec@1 55.000 (56.780)\tPrec@5 96.000 (95.923)\n",
      "val Results: Prec@1 56.710 Prec@5 95.850 Loss 8.44473\n",
      "val Class Accuracy: [0.961,0.976,0.624,0.563,0.531,0.738,0.711,0.442,0.086,0.039]\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [53][0/97], lr: 0.01000\tTime 0.915 (0.915)\tData 0.546 (0.546)\tLoss 1.7648 (1.7648)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [53][10/97], lr: 0.01000\tTime 0.361 (0.433)\tData 0.000 (0.062)\tLoss 2.6677 (2.3127)\tPrec@1 82.812 (85.582)\tPrec@5 99.219 (99.077)\n",
      "Epoch: [53][20/97], lr: 0.01000\tTime 0.404 (0.401)\tData 0.000 (0.040)\tLoss 1.2562 (2.2750)\tPrec@1 93.750 (86.124)\tPrec@5 99.219 (99.070)\n",
      "Epoch: [53][30/97], lr: 0.01000\tTime 0.354 (0.389)\tData 0.000 (0.032)\tLoss 1.6098 (2.3377)\tPrec@1 89.062 (85.761)\tPrec@5 100.000 (99.068)\n",
      "Epoch: [53][40/97], lr: 0.01000\tTime 0.350 (0.383)\tData 0.000 (0.028)\tLoss 2.8908 (2.4538)\tPrec@1 80.469 (84.661)\tPrec@5 99.219 (99.047)\n",
      "Epoch: [53][50/97], lr: 0.01000\tTime 0.376 (0.380)\tData 0.000 (0.026)\tLoss 2.0777 (2.5082)\tPrec@1 86.719 (84.482)\tPrec@5 97.656 (98.958)\n",
      "Epoch: [53][60/97], lr: 0.01000\tTime 0.355 (0.377)\tData 0.000 (0.024)\tLoss 2.4651 (2.4873)\tPrec@1 85.156 (84.593)\tPrec@5 98.438 (98.873)\n",
      "Epoch: [53][70/97], lr: 0.01000\tTime 0.353 (0.374)\tData 0.000 (0.023)\tLoss 1.8513 (2.4980)\tPrec@1 89.062 (84.463)\tPrec@5 100.000 (98.966)\n",
      "Epoch: [53][80/97], lr: 0.01000\tTime 0.346 (0.371)\tData 0.000 (0.022)\tLoss 2.7914 (2.5296)\tPrec@1 83.594 (84.230)\tPrec@5 98.438 (98.968)\n",
      "Epoch: [53][90/97], lr: 0.01000\tTime 0.334 (0.368)\tData 0.000 (0.022)\tLoss 3.0289 (2.5328)\tPrec@1 81.250 (84.220)\tPrec@5 96.875 (98.918)\n",
      "Epoch: [53][96/97], lr: 0.01000\tTime 0.332 (0.366)\tData 0.000 (0.022)\tLoss 2.5346 (2.5091)\tPrec@1 84.746 (84.362)\tPrec@5 98.305 (98.960)\n",
      "Gated Network Weight Gate= Flip:0.51, Sc:0.49\n",
      "Test: [0/100]\tTime 0.356 (0.356)\tLoss 12.4577 (12.4577)\tPrec@1 38.000 (38.000)\tPrec@5 88.000 (88.000)\n",
      "Test: [10/100]\tTime 0.076 (0.101)\tLoss 8.9003 (11.7456)\tPrec@1 56.000 (41.909)\tPrec@5 97.000 (92.364)\n",
      "Test: [20/100]\tTime 0.075 (0.089)\tLoss 10.9854 (11.9005)\tPrec@1 44.000 (41.000)\tPrec@5 89.000 (91.476)\n",
      "Test: [30/100]\tTime 0.075 (0.085)\tLoss 10.7696 (11.8578)\tPrec@1 47.000 (41.097)\tPrec@5 90.000 (91.129)\n",
      "Test: [40/100]\tTime 0.075 (0.082)\tLoss 12.4922 (11.9522)\tPrec@1 38.000 (40.732)\tPrec@5 84.000 (90.634)\n",
      "Test: [50/100]\tTime 0.075 (0.081)\tLoss 11.0609 (11.8516)\tPrec@1 44.000 (41.137)\tPrec@5 93.000 (90.824)\n",
      "Test: [60/100]\tTime 0.076 (0.080)\tLoss 10.4084 (11.8193)\tPrec@1 44.000 (41.262)\tPrec@5 94.000 (90.574)\n",
      "Test: [70/100]\tTime 0.076 (0.080)\tLoss 11.4299 (11.7968)\tPrec@1 43.000 (41.465)\tPrec@5 87.000 (90.366)\n",
      "Test: [80/100]\tTime 0.075 (0.079)\tLoss 10.4860 (11.7407)\tPrec@1 48.000 (41.704)\tPrec@5 92.000 (90.494)\n",
      "Test: [90/100]\tTime 0.075 (0.079)\tLoss 12.2431 (11.7724)\tPrec@1 42.000 (41.604)\tPrec@5 88.000 (90.407)\n",
      "val Results: Prec@1 41.660 Prec@5 90.380 Loss 11.78502\n",
      "val Class Accuracy: [0.800,0.999,0.479,0.527,0.263,0.099,0.724,0.255,0.020,0.000]\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [54][0/97], lr: 0.01000\tTime 0.895 (0.895)\tData 0.527 (0.527)\tLoss 2.1674 (2.1674)\tPrec@1 86.719 (86.719)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [54][10/97], lr: 0.01000\tTime 0.371 (0.442)\tData 0.000 (0.059)\tLoss 2.3972 (2.3452)\tPrec@1 84.375 (85.156)\tPrec@5 100.000 (99.148)\n",
      "Epoch: [54][20/97], lr: 0.01000\tTime 0.387 (0.410)\tData 0.000 (0.038)\tLoss 3.1338 (2.3899)\tPrec@1 77.344 (84.561)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [54][30/97], lr: 0.01000\tTime 0.354 (0.397)\tData 0.000 (0.030)\tLoss 3.0051 (2.4501)\tPrec@1 79.688 (84.350)\tPrec@5 98.438 (99.118)\n",
      "Epoch: [54][40/97], lr: 0.01000\tTime 0.353 (0.390)\tData 0.001 (0.027)\tLoss 2.3043 (2.4689)\tPrec@1 87.500 (84.489)\tPrec@5 98.438 (99.047)\n",
      "Epoch: [54][50/97], lr: 0.01000\tTime 0.354 (0.385)\tData 0.000 (0.025)\tLoss 2.5065 (2.4899)\tPrec@1 86.719 (84.482)\tPrec@5 97.656 (98.912)\n",
      "Epoch: [54][60/97], lr: 0.01000\tTime 0.350 (0.381)\tData 0.000 (0.023)\tLoss 2.8290 (2.4699)\tPrec@1 81.250 (84.606)\tPrec@5 99.219 (98.975)\n",
      "Epoch: [54][70/97], lr: 0.01000\tTime 0.357 (0.377)\tData 0.000 (0.022)\tLoss 2.7941 (2.4770)\tPrec@1 81.250 (84.507)\tPrec@5 98.438 (98.988)\n",
      "Epoch: [54][80/97], lr: 0.01000\tTime 0.340 (0.374)\tData 0.000 (0.022)\tLoss 3.6130 (2.4828)\tPrec@1 78.125 (84.558)\tPrec@5 98.438 (98.997)\n",
      "Epoch: [54][90/97], lr: 0.01000\tTime 0.331 (0.372)\tData 0.000 (0.021)\tLoss 1.3769 (2.4817)\tPrec@1 92.188 (84.590)\tPrec@5 99.219 (99.004)\n",
      "Epoch: [54][96/97], lr: 0.01000\tTime 0.331 (0.370)\tData 0.000 (0.021)\tLoss 2.8100 (2.4963)\tPrec@1 81.356 (84.459)\tPrec@5 98.305 (98.976)\n",
      "Gated Network Weight Gate= Flip:0.54, Sc:0.46\n",
      "Test: [0/100]\tTime 0.325 (0.325)\tLoss 11.1403 (11.1403)\tPrec@1 46.000 (46.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.074 (0.097)\tLoss 7.9656 (10.5817)\tPrec@1 57.000 (46.909)\tPrec@5 94.000 (90.455)\n",
      "Test: [20/100]\tTime 0.076 (0.086)\tLoss 9.7095 (10.5643)\tPrec@1 48.000 (47.476)\tPrec@5 95.000 (90.952)\n",
      "Test: [30/100]\tTime 0.076 (0.084)\tLoss 9.3490 (10.5100)\tPrec@1 50.000 (47.839)\tPrec@5 93.000 (90.806)\n",
      "Test: [40/100]\tTime 0.075 (0.082)\tLoss 11.6029 (10.5655)\tPrec@1 41.000 (47.634)\tPrec@5 86.000 (90.610)\n",
      "Test: [50/100]\tTime 0.075 (0.080)\tLoss 10.5683 (10.4796)\tPrec@1 47.000 (48.314)\tPrec@5 95.000 (90.882)\n",
      "Test: [60/100]\tTime 0.075 (0.080)\tLoss 8.4585 (10.4399)\tPrec@1 58.000 (48.361)\tPrec@5 94.000 (91.033)\n",
      "Test: [70/100]\tTime 0.075 (0.079)\tLoss 9.8259 (10.4051)\tPrec@1 49.000 (48.535)\tPrec@5 91.000 (90.746)\n",
      "Test: [80/100]\tTime 0.075 (0.079)\tLoss 9.9210 (10.3633)\tPrec@1 50.000 (48.691)\tPrec@5 88.000 (90.864)\n",
      "Test: [90/100]\tTime 0.077 (0.078)\tLoss 10.3566 (10.4331)\tPrec@1 51.000 (48.275)\tPrec@5 95.000 (90.890)\n",
      "val Results: Prec@1 48.230 Prec@5 90.820 Loss 10.45742\n",
      "val Class Accuracy: [0.880,0.963,0.898,0.409,0.354,0.384,0.593,0.324,0.018,0.000]\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [55][0/97], lr: 0.01000\tTime 1.035 (1.035)\tData 0.597 (0.597)\tLoss 3.5594 (3.5594)\tPrec@1 75.000 (75.000)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [55][10/97], lr: 0.01000\tTime 0.347 (0.457)\tData 0.000 (0.064)\tLoss 2.2558 (2.4107)\tPrec@1 85.938 (84.020)\tPrec@5 98.438 (99.361)\n",
      "Epoch: [55][20/97], lr: 0.01000\tTime 0.372 (0.415)\tData 0.000 (0.041)\tLoss 1.7645 (2.4168)\tPrec@1 90.625 (84.412)\tPrec@5 100.000 (99.293)\n",
      "Epoch: [55][30/97], lr: 0.01000\tTime 0.367 (0.399)\tData 0.000 (0.033)\tLoss 2.4343 (2.4278)\tPrec@1 84.375 (84.627)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [55][40/97], lr: 0.01000\tTime 0.358 (0.389)\tData 0.000 (0.029)\tLoss 2.3875 (2.4287)\tPrec@1 85.156 (84.642)\tPrec@5 98.438 (99.143)\n",
      "Epoch: [55][50/97], lr: 0.01000\tTime 0.369 (0.386)\tData 0.000 (0.026)\tLoss 2.1071 (2.3828)\tPrec@1 88.281 (85.018)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [55][60/97], lr: 0.01000\tTime 0.349 (0.382)\tData 0.001 (0.024)\tLoss 2.5016 (2.4038)\tPrec@1 83.594 (84.951)\tPrec@5 98.438 (99.103)\n",
      "Epoch: [55][70/97], lr: 0.01000\tTime 0.377 (0.378)\tData 0.000 (0.023)\tLoss 1.7646 (2.4125)\tPrec@1 88.281 (84.793)\tPrec@5 100.000 (99.131)\n",
      "Epoch: [55][80/97], lr: 0.01000\tTime 0.361 (0.377)\tData 0.000 (0.022)\tLoss 2.2087 (2.4269)\tPrec@1 88.281 (84.857)\tPrec@5 98.438 (99.074)\n",
      "Epoch: [55][90/97], lr: 0.01000\tTime 0.353 (0.375)\tData 0.000 (0.021)\tLoss 2.7815 (2.4528)\tPrec@1 81.250 (84.718)\tPrec@5 100.000 (99.047)\n",
      "Epoch: [55][96/97], lr: 0.01000\tTime 0.330 (0.373)\tData 0.000 (0.022)\tLoss 2.7965 (2.4645)\tPrec@1 83.051 (84.653)\tPrec@5 98.305 (99.041)\n",
      "Gated Network Weight Gate= Flip:0.38, Sc:0.62\n",
      "Test: [0/100]\tTime 0.369 (0.369)\tLoss 10.8476 (10.8476)\tPrec@1 47.000 (47.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.074 (0.101)\tLoss 7.9097 (9.8848)\tPrec@1 61.000 (50.818)\tPrec@5 95.000 (92.273)\n",
      "Test: [20/100]\tTime 0.075 (0.088)\tLoss 9.3495 (10.0395)\tPrec@1 52.000 (49.857)\tPrec@5 95.000 (91.667)\n",
      "Test: [30/100]\tTime 0.076 (0.084)\tLoss 8.9572 (10.1111)\tPrec@1 52.000 (49.226)\tPrec@5 88.000 (91.613)\n",
      "Test: [40/100]\tTime 0.075 (0.082)\tLoss 11.8467 (10.1391)\tPrec@1 39.000 (49.341)\tPrec@5 86.000 (91.659)\n",
      "Test: [50/100]\tTime 0.077 (0.081)\tLoss 10.4039 (10.0477)\tPrec@1 43.000 (49.667)\tPrec@5 93.000 (92.196)\n",
      "Test: [60/100]\tTime 0.075 (0.080)\tLoss 8.4989 (10.0154)\tPrec@1 54.000 (49.525)\tPrec@5 90.000 (92.000)\n",
      "Test: [70/100]\tTime 0.076 (0.080)\tLoss 9.6489 (10.0012)\tPrec@1 51.000 (49.732)\tPrec@5 93.000 (91.986)\n",
      "Test: [80/100]\tTime 0.076 (0.079)\tLoss 8.7777 (9.9572)\tPrec@1 59.000 (49.877)\tPrec@5 93.000 (92.025)\n",
      "Test: [90/100]\tTime 0.076 (0.079)\tLoss 10.6110 (10.0355)\tPrec@1 45.000 (49.407)\tPrec@5 94.000 (92.000)\n",
      "val Results: Prec@1 49.300 Prec@5 91.820 Loss 10.07370\n",
      "val Class Accuracy: [0.965,0.949,0.782,0.580,0.578,0.199,0.593,0.248,0.006,0.030]\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [56][0/97], lr: 0.01000\tTime 0.933 (0.933)\tData 0.581 (0.581)\tLoss 2.4487 (2.4487)\tPrec@1 84.375 (84.375)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [56][10/97], lr: 0.01000\tTime 0.362 (0.445)\tData 0.000 (0.065)\tLoss 2.6045 (2.4640)\tPrec@1 81.250 (84.730)\tPrec@5 98.438 (98.935)\n",
      "Epoch: [56][20/97], lr: 0.01000\tTime 0.367 (0.410)\tData 0.000 (0.042)\tLoss 2.3249 (2.4935)\tPrec@1 84.375 (84.784)\tPrec@5 99.219 (98.847)\n",
      "Epoch: [56][30/97], lr: 0.01000\tTime 0.352 (0.394)\tData 0.001 (0.033)\tLoss 2.3773 (2.3817)\tPrec@1 85.156 (85.232)\tPrec@5 99.219 (98.942)\n",
      "Epoch: [56][40/97], lr: 0.01000\tTime 0.363 (0.387)\tData 0.001 (0.029)\tLoss 1.9392 (2.3633)\tPrec@1 89.062 (85.271)\tPrec@5 100.000 (99.009)\n",
      "Epoch: [56][50/97], lr: 0.01000\tTime 0.352 (0.384)\tData 0.000 (0.026)\tLoss 3.0374 (2.4182)\tPrec@1 82.031 (84.896)\tPrec@5 100.000 (99.050)\n",
      "Epoch: [56][60/97], lr: 0.01000\tTime 0.358 (0.380)\tData 0.000 (0.025)\tLoss 3.0661 (2.4276)\tPrec@1 82.812 (84.836)\tPrec@5 99.219 (99.039)\n",
      "Epoch: [56][70/97], lr: 0.01000\tTime 0.341 (0.376)\tData 0.000 (0.023)\tLoss 3.2051 (2.4471)\tPrec@1 79.688 (84.771)\tPrec@5 100.000 (98.999)\n",
      "Epoch: [56][80/97], lr: 0.01000\tTime 0.340 (0.372)\tData 0.000 (0.022)\tLoss 1.9757 (2.4367)\tPrec@1 86.719 (84.751)\tPrec@5 100.000 (98.987)\n",
      "Epoch: [56][90/97], lr: 0.01000\tTime 0.356 (0.370)\tData 0.000 (0.022)\tLoss 2.3772 (2.4308)\tPrec@1 86.719 (84.830)\tPrec@5 98.438 (98.970)\n",
      "Epoch: [56][96/97], lr: 0.01000\tTime 0.332 (0.369)\tData 0.000 (0.022)\tLoss 2.4787 (2.4376)\tPrec@1 83.051 (84.782)\tPrec@5 98.305 (98.952)\n",
      "Gated Network Weight Gate= Flip:0.52, Sc:0.48\n",
      "Test: [0/100]\tTime 0.454 (0.454)\tLoss 9.9088 (9.9088)\tPrec@1 50.000 (50.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.074 (0.109)\tLoss 6.8943 (8.6247)\tPrec@1 64.000 (55.455)\tPrec@5 95.000 (95.727)\n",
      "Test: [20/100]\tTime 0.074 (0.093)\tLoss 7.1457 (8.6127)\tPrec@1 64.000 (55.952)\tPrec@5 97.000 (95.905)\n",
      "Test: [30/100]\tTime 0.075 (0.087)\tLoss 7.1094 (8.5210)\tPrec@1 60.000 (56.387)\tPrec@5 98.000 (95.806)\n",
      "Test: [40/100]\tTime 0.075 (0.084)\tLoss 9.2294 (8.5647)\tPrec@1 55.000 (56.488)\tPrec@5 94.000 (95.561)\n",
      "Test: [50/100]\tTime 0.075 (0.082)\tLoss 8.6556 (8.4780)\tPrec@1 57.000 (56.882)\tPrec@5 98.000 (95.922)\n",
      "Test: [60/100]\tTime 0.076 (0.081)\tLoss 7.3623 (8.4518)\tPrec@1 60.000 (56.918)\tPrec@5 95.000 (95.967)\n",
      "Test: [70/100]\tTime 0.078 (0.080)\tLoss 8.1591 (8.4471)\tPrec@1 60.000 (57.000)\tPrec@5 96.000 (95.803)\n",
      "Test: [80/100]\tTime 0.075 (0.080)\tLoss 7.6188 (8.3971)\tPrec@1 65.000 (57.346)\tPrec@5 95.000 (95.901)\n",
      "Test: [90/100]\tTime 0.075 (0.079)\tLoss 8.5858 (8.4890)\tPrec@1 54.000 (56.868)\tPrec@5 96.000 (95.758)\n",
      "val Results: Prec@1 56.780 Prec@5 95.790 Loss 8.51947\n",
      "val Class Accuracy: [0.919,0.948,0.853,0.751,0.509,0.473,0.491,0.618,0.072,0.044]\n",
      "Best Prec@1: 56.780\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [57][0/97], lr: 0.01000\tTime 0.958 (0.958)\tData 0.603 (0.603)\tLoss 2.3647 (2.3647)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [57][10/97], lr: 0.01000\tTime 0.368 (0.449)\tData 0.000 (0.066)\tLoss 2.6692 (2.3214)\tPrec@1 82.812 (85.440)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [57][20/97], lr: 0.01000\tTime 0.366 (0.407)\tData 0.000 (0.042)\tLoss 2.2436 (2.3011)\tPrec@1 85.156 (85.789)\tPrec@5 98.438 (99.070)\n",
      "Epoch: [57][30/97], lr: 0.01000\tTime 0.340 (0.394)\tData 0.000 (0.033)\tLoss 1.8559 (2.3438)\tPrec@1 89.062 (85.307)\tPrec@5 100.000 (99.143)\n",
      "Epoch: [57][40/97], lr: 0.01000\tTime 0.350 (0.387)\tData 0.000 (0.029)\tLoss 3.0536 (2.3243)\tPrec@1 82.812 (85.461)\tPrec@5 97.656 (99.104)\n",
      "Epoch: [57][50/97], lr: 0.01000\tTime 0.358 (0.383)\tData 0.000 (0.027)\tLoss 2.7693 (2.3145)\tPrec@1 81.250 (85.677)\tPrec@5 100.000 (99.173)\n",
      "Epoch: [57][60/97], lr: 0.01000\tTime 0.381 (0.380)\tData 0.000 (0.025)\tLoss 2.1160 (2.3069)\tPrec@1 86.719 (85.720)\tPrec@5 100.000 (99.180)\n",
      "Epoch: [57][70/97], lr: 0.01000\tTime 0.347 (0.376)\tData 0.000 (0.024)\tLoss 2.3804 (2.3330)\tPrec@1 85.156 (85.596)\tPrec@5 99.219 (99.153)\n",
      "Epoch: [57][80/97], lr: 0.01000\tTime 0.344 (0.373)\tData 0.000 (0.023)\tLoss 2.9264 (2.3680)\tPrec@1 82.031 (85.417)\tPrec@5 98.438 (99.122)\n",
      "Epoch: [57][90/97], lr: 0.01000\tTime 0.345 (0.371)\tData 0.000 (0.022)\tLoss 1.8713 (2.3691)\tPrec@1 87.500 (85.345)\tPrec@5 98.438 (99.081)\n",
      "Epoch: [57][96/97], lr: 0.01000\tTime 0.347 (0.369)\tData 0.000 (0.022)\tLoss 2.3457 (2.3839)\tPrec@1 85.593 (85.281)\tPrec@5 99.153 (99.073)\n",
      "Gated Network Weight Gate= Flip:0.52, Sc:0.48\n",
      "Test: [0/100]\tTime 0.492 (0.492)\tLoss 12.1457 (12.1457)\tPrec@1 40.000 (40.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.074 (0.112)\tLoss 8.7254 (10.3572)\tPrec@1 56.000 (49.273)\tPrec@5 91.000 (90.909)\n",
      "Test: [20/100]\tTime 0.075 (0.094)\tLoss 7.5996 (10.1730)\tPrec@1 60.000 (49.143)\tPrec@5 95.000 (91.762)\n",
      "Test: [30/100]\tTime 0.075 (0.088)\tLoss 9.8855 (10.1953)\tPrec@1 50.000 (49.452)\tPrec@5 94.000 (91.516)\n",
      "Test: [40/100]\tTime 0.075 (0.085)\tLoss 10.5020 (10.2164)\tPrec@1 48.000 (49.610)\tPrec@5 93.000 (91.341)\n",
      "Test: [50/100]\tTime 0.075 (0.083)\tLoss 9.6715 (10.0599)\tPrec@1 48.000 (50.157)\tPrec@5 95.000 (91.510)\n",
      "Test: [60/100]\tTime 0.075 (0.082)\tLoss 7.8368 (10.0329)\tPrec@1 61.000 (50.410)\tPrec@5 92.000 (91.590)\n",
      "Test: [70/100]\tTime 0.075 (0.081)\tLoss 9.1409 (9.9870)\tPrec@1 55.000 (50.634)\tPrec@5 95.000 (91.465)\n",
      "Test: [80/100]\tTime 0.074 (0.080)\tLoss 9.1313 (9.9234)\tPrec@1 56.000 (50.914)\tPrec@5 93.000 (91.630)\n",
      "Test: [90/100]\tTime 0.076 (0.080)\tLoss 9.7858 (10.0143)\tPrec@1 54.000 (50.385)\tPrec@5 93.000 (91.648)\n",
      "val Results: Prec@1 50.480 Prec@5 91.670 Loss 10.01072\n",
      "val Class Accuracy: [0.883,0.904,0.638,0.603,0.449,0.848,0.296,0.363,0.052,0.012]\n",
      "Best Prec@1: 56.780\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [58][0/97], lr: 0.01000\tTime 0.884 (0.884)\tData 0.556 (0.556)\tLoss 2.4856 (2.4856)\tPrec@1 82.812 (82.812)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [58][10/97], lr: 0.01000\tTime 0.366 (0.434)\tData 0.000 (0.062)\tLoss 2.4335 (2.4375)\tPrec@1 84.375 (84.375)\tPrec@5 99.219 (98.864)\n",
      "Epoch: [58][20/97], lr: 0.01000\tTime 0.359 (0.402)\tData 0.000 (0.040)\tLoss 3.0274 (2.5103)\tPrec@1 79.688 (83.705)\tPrec@5 99.219 (99.033)\n",
      "Epoch: [58][30/97], lr: 0.01000\tTime 0.351 (0.390)\tData 0.000 (0.032)\tLoss 1.8944 (2.4615)\tPrec@1 89.062 (84.375)\tPrec@5 99.219 (99.118)\n",
      "Epoch: [58][40/97], lr: 0.01000\tTime 0.351 (0.384)\tData 0.000 (0.028)\tLoss 2.4983 (2.3626)\tPrec@1 83.594 (84.947)\tPrec@5 97.656 (99.085)\n",
      "Epoch: [58][50/97], lr: 0.01000\tTime 0.376 (0.382)\tData 0.000 (0.026)\tLoss 2.5936 (2.3896)\tPrec@1 85.938 (84.896)\tPrec@5 96.875 (99.081)\n",
      "Epoch: [58][60/97], lr: 0.01000\tTime 0.353 (0.379)\tData 0.000 (0.024)\tLoss 2.8695 (2.3956)\tPrec@1 82.812 (84.939)\tPrec@5 100.000 (99.065)\n",
      "Epoch: [58][70/97], lr: 0.01000\tTime 0.336 (0.374)\tData 0.000 (0.023)\tLoss 2.5237 (2.4089)\tPrec@1 85.156 (84.793)\tPrec@5 100.000 (99.087)\n",
      "Epoch: [58][80/97], lr: 0.01000\tTime 0.341 (0.371)\tData 0.000 (0.022)\tLoss 1.5877 (2.3999)\tPrec@1 91.406 (84.925)\tPrec@5 100.000 (99.074)\n",
      "Epoch: [58][90/97], lr: 0.01000\tTime 0.345 (0.369)\tData 0.000 (0.022)\tLoss 2.0105 (2.4149)\tPrec@1 89.062 (84.907)\tPrec@5 100.000 (99.030)\n",
      "Epoch: [58][96/97], lr: 0.01000\tTime 0.338 (0.367)\tData 0.000 (0.022)\tLoss 2.5505 (2.4187)\tPrec@1 85.593 (84.935)\tPrec@5 98.305 (99.033)\n",
      "Gated Network Weight Gate= Flip:0.41, Sc:0.59\n",
      "Test: [0/100]\tTime 0.380 (0.380)\tLoss 11.1344 (11.1344)\tPrec@1 42.000 (42.000)\tPrec@5 90.000 (90.000)\n",
      "Test: [10/100]\tTime 0.074 (0.103)\tLoss 8.6959 (9.8658)\tPrec@1 57.000 (51.182)\tPrec@5 94.000 (93.000)\n",
      "Test: [20/100]\tTime 0.075 (0.090)\tLoss 8.9920 (9.8585)\tPrec@1 53.000 (50.810)\tPrec@5 92.000 (92.857)\n",
      "Test: [30/100]\tTime 0.074 (0.086)\tLoss 8.9291 (9.8054)\tPrec@1 53.000 (51.419)\tPrec@5 90.000 (92.806)\n",
      "Test: [40/100]\tTime 0.075 (0.083)\tLoss 9.8456 (9.8169)\tPrec@1 54.000 (51.585)\tPrec@5 91.000 (92.829)\n",
      "Test: [50/100]\tTime 0.075 (0.082)\tLoss 9.4431 (9.7130)\tPrec@1 51.000 (52.059)\tPrec@5 93.000 (92.980)\n",
      "Test: [60/100]\tTime 0.075 (0.081)\tLoss 8.5099 (9.6893)\tPrec@1 58.000 (51.918)\tPrec@5 95.000 (92.918)\n",
      "Test: [70/100]\tTime 0.075 (0.080)\tLoss 9.2425 (9.6697)\tPrec@1 56.000 (52.028)\tPrec@5 97.000 (93.014)\n",
      "Test: [80/100]\tTime 0.075 (0.080)\tLoss 8.5203 (9.6030)\tPrec@1 55.000 (52.222)\tPrec@5 95.000 (93.198)\n",
      "Test: [90/100]\tTime 0.075 (0.079)\tLoss 9.7822 (9.6856)\tPrec@1 49.000 (51.714)\tPrec@5 97.000 (93.011)\n",
      "val Results: Prec@1 51.650 Prec@5 93.070 Loss 9.69000\n",
      "val Class Accuracy: [0.798,0.999,0.656,0.356,0.569,0.511,0.430,0.616,0.217,0.013]\n",
      "Best Prec@1: 56.780\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [59][0/97], lr: 0.01000\tTime 0.930 (0.930)\tData 0.577 (0.577)\tLoss 2.4077 (2.4077)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [59][10/97], lr: 0.01000\tTime 0.395 (0.447)\tData 0.000 (0.064)\tLoss 2.2596 (2.2472)\tPrec@1 83.594 (85.724)\tPrec@5 99.219 (99.148)\n",
      "Epoch: [59][20/97], lr: 0.01000\tTime 0.351 (0.417)\tData 0.000 (0.041)\tLoss 2.7388 (2.3506)\tPrec@1 84.375 (85.379)\tPrec@5 99.219 (99.144)\n",
      "Epoch: [59][30/97], lr: 0.01000\tTime 0.362 (0.399)\tData 0.000 (0.033)\tLoss 2.5813 (2.4270)\tPrec@1 85.156 (84.803)\tPrec@5 99.219 (99.168)\n",
      "Epoch: [59][40/97], lr: 0.01000\tTime 0.360 (0.392)\tData 0.000 (0.029)\tLoss 2.6959 (2.4128)\tPrec@1 85.156 (85.118)\tPrec@5 100.000 (99.009)\n",
      "Epoch: [59][50/97], lr: 0.01000\tTime 0.354 (0.387)\tData 0.000 (0.026)\tLoss 2.3730 (2.4149)\tPrec@1 85.156 (85.141)\tPrec@5 99.219 (99.081)\n",
      "Epoch: [59][60/97], lr: 0.01000\tTime 0.347 (0.383)\tData 0.000 (0.024)\tLoss 1.6988 (2.3672)\tPrec@1 89.062 (85.400)\tPrec@5 100.000 (98.975)\n",
      "Epoch: [59][70/97], lr: 0.01000\tTime 0.348 (0.379)\tData 0.001 (0.023)\tLoss 3.2595 (2.3826)\tPrec@1 78.906 (85.376)\tPrec@5 96.094 (98.966)\n",
      "Epoch: [59][80/97], lr: 0.01000\tTime 0.344 (0.375)\tData 0.000 (0.022)\tLoss 3.3512 (2.4117)\tPrec@1 78.906 (85.031)\tPrec@5 100.000 (98.997)\n",
      "Epoch: [59][90/97], lr: 0.01000\tTime 0.349 (0.373)\tData 0.000 (0.022)\tLoss 1.7181 (2.4102)\tPrec@1 92.969 (85.070)\tPrec@5 99.219 (98.987)\n",
      "Epoch: [59][96/97], lr: 0.01000\tTime 0.343 (0.371)\tData 0.000 (0.022)\tLoss 2.7296 (2.4109)\tPrec@1 80.508 (85.007)\tPrec@5 98.305 (98.984)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.389 (0.389)\tLoss 9.8315 (9.8315)\tPrec@1 53.000 (53.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.074 (0.103)\tLoss 7.1401 (9.4775)\tPrec@1 63.000 (51.909)\tPrec@5 99.000 (95.909)\n",
      "Test: [20/100]\tTime 0.076 (0.089)\tLoss 9.2889 (9.5829)\tPrec@1 47.000 (51.048)\tPrec@5 96.000 (95.810)\n",
      "Test: [30/100]\tTime 0.075 (0.085)\tLoss 8.5288 (9.5325)\tPrec@1 56.000 (51.290)\tPrec@5 96.000 (95.742)\n",
      "Test: [40/100]\tTime 0.074 (0.083)\tLoss 10.5374 (9.5831)\tPrec@1 48.000 (51.146)\tPrec@5 92.000 (95.439)\n",
      "Test: [50/100]\tTime 0.075 (0.081)\tLoss 9.7135 (9.5036)\tPrec@1 49.000 (51.314)\tPrec@5 96.000 (95.549)\n",
      "Test: [60/100]\tTime 0.075 (0.081)\tLoss 8.1193 (9.4889)\tPrec@1 55.000 (51.180)\tPrec@5 93.000 (95.393)\n",
      "Test: [70/100]\tTime 0.076 (0.080)\tLoss 9.0025 (9.4768)\tPrec@1 53.000 (51.169)\tPrec@5 96.000 (95.366)\n",
      "Test: [80/100]\tTime 0.075 (0.079)\tLoss 8.1931 (9.4413)\tPrec@1 61.000 (51.272)\tPrec@5 94.000 (95.333)\n",
      "Test: [90/100]\tTime 0.075 (0.079)\tLoss 9.5688 (9.4947)\tPrec@1 48.000 (51.000)\tPrec@5 98.000 (95.341)\n",
      "val Results: Prec@1 50.920 Prec@5 95.170 Loss 9.52928\n",
      "val Class Accuracy: [0.981,0.951,0.706,0.473,0.484,0.224,0.755,0.458,0.043,0.017]\n",
      "Best Prec@1: 56.780\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [60][0/97], lr: 0.01000\tTime 0.862 (0.862)\tData 0.533 (0.533)\tLoss 2.4619 (2.4619)\tPrec@1 84.375 (84.375)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [60][10/97], lr: 0.01000\tTime 0.381 (0.440)\tData 0.000 (0.060)\tLoss 2.0479 (2.4233)\tPrec@1 88.281 (85.014)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [60][20/97], lr: 0.01000\tTime 0.361 (0.406)\tData 0.000 (0.039)\tLoss 1.6937 (2.2963)\tPrec@1 92.969 (85.677)\tPrec@5 100.000 (99.293)\n",
      "Epoch: [60][30/97], lr: 0.01000\tTime 0.417 (0.398)\tData 0.000 (0.032)\tLoss 2.0418 (2.3070)\tPrec@1 90.625 (85.736)\tPrec@5 99.219 (99.294)\n",
      "Epoch: [60][40/97], lr: 0.01000\tTime 0.355 (0.389)\tData 0.000 (0.028)\tLoss 3.0417 (2.3248)\tPrec@1 81.250 (85.652)\tPrec@5 99.219 (99.295)\n",
      "Epoch: [60][50/97], lr: 0.01000\tTime 0.352 (0.384)\tData 0.000 (0.025)\tLoss 2.8973 (2.3289)\tPrec@1 83.594 (85.800)\tPrec@5 97.656 (99.066)\n",
      "Epoch: [60][60/97], lr: 0.01000\tTime 0.352 (0.381)\tData 0.000 (0.024)\tLoss 2.8952 (2.3776)\tPrec@1 82.812 (85.553)\tPrec@5 97.656 (99.014)\n",
      "Epoch: [60][70/97], lr: 0.01000\tTime 0.340 (0.377)\tData 0.000 (0.023)\tLoss 2.2704 (2.3541)\tPrec@1 85.156 (85.607)\tPrec@5 98.438 (98.977)\n",
      "Epoch: [60][80/97], lr: 0.01000\tTime 0.357 (0.373)\tData 0.000 (0.022)\tLoss 1.7312 (2.3846)\tPrec@1 89.844 (85.446)\tPrec@5 98.438 (99.007)\n",
      "Epoch: [60][90/97], lr: 0.01000\tTime 0.343 (0.371)\tData 0.000 (0.021)\tLoss 2.9439 (2.3934)\tPrec@1 78.906 (85.302)\tPrec@5 98.438 (99.030)\n",
      "Epoch: [60][96/97], lr: 0.01000\tTime 0.324 (0.369)\tData 0.000 (0.022)\tLoss 1.5836 (2.3823)\tPrec@1 90.678 (85.410)\tPrec@5 99.153 (99.057)\n",
      "Gated Network Weight Gate= Flip:0.49, Sc:0.51\n",
      "Test: [0/100]\tTime 0.385 (0.385)\tLoss 10.4222 (10.4222)\tPrec@1 53.000 (53.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.074 (0.102)\tLoss 7.2328 (9.4680)\tPrec@1 66.000 (54.636)\tPrec@5 97.000 (93.182)\n",
      "Test: [20/100]\tTime 0.075 (0.089)\tLoss 8.3398 (9.4081)\tPrec@1 53.000 (53.952)\tPrec@5 95.000 (93.571)\n",
      "Test: [30/100]\tTime 0.074 (0.084)\tLoss 9.1181 (9.4315)\tPrec@1 54.000 (53.387)\tPrec@5 92.000 (93.806)\n",
      "Test: [40/100]\tTime 0.075 (0.082)\tLoss 10.4391 (9.5288)\tPrec@1 46.000 (52.902)\tPrec@5 90.000 (93.756)\n",
      "Test: [50/100]\tTime 0.075 (0.081)\tLoss 9.2396 (9.3957)\tPrec@1 52.000 (53.627)\tPrec@5 95.000 (93.843)\n",
      "Test: [60/100]\tTime 0.079 (0.080)\tLoss 7.9854 (9.3551)\tPrec@1 58.000 (53.607)\tPrec@5 91.000 (93.705)\n",
      "Test: [70/100]\tTime 0.075 (0.079)\tLoss 8.7203 (9.3275)\tPrec@1 57.000 (53.634)\tPrec@5 95.000 (93.577)\n",
      "Test: [80/100]\tTime 0.076 (0.079)\tLoss 8.8247 (9.2835)\tPrec@1 55.000 (53.877)\tPrec@5 94.000 (93.630)\n",
      "Test: [90/100]\tTime 0.075 (0.079)\tLoss 9.9181 (9.3641)\tPrec@1 52.000 (53.582)\tPrec@5 92.000 (93.495)\n",
      "val Results: Prec@1 53.600 Prec@5 93.380 Loss 9.38534\n",
      "val Class Accuracy: [0.846,0.982,0.757,0.882,0.478,0.502,0.507,0.308,0.098,0.000]\n",
      "Best Prec@1: 56.780\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [61][0/97], lr: 0.01000\tTime 0.928 (0.928)\tData 0.565 (0.565)\tLoss 1.4649 (1.4649)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [61][10/97], lr: 0.01000\tTime 0.343 (0.436)\tData 0.000 (0.063)\tLoss 2.2301 (2.3248)\tPrec@1 87.500 (86.151)\tPrec@5 97.656 (98.864)\n",
      "Epoch: [61][20/97], lr: 0.01000\tTime 0.350 (0.402)\tData 0.000 (0.041)\tLoss 2.5769 (2.2808)\tPrec@1 83.594 (86.124)\tPrec@5 99.219 (98.921)\n",
      "Epoch: [61][30/97], lr: 0.01000\tTime 0.357 (0.392)\tData 0.000 (0.033)\tLoss 2.7157 (2.3900)\tPrec@1 82.812 (85.408)\tPrec@5 100.000 (99.068)\n",
      "Epoch: [61][40/97], lr: 0.01000\tTime 0.363 (0.385)\tData 0.000 (0.029)\tLoss 2.2422 (2.4286)\tPrec@1 85.938 (85.232)\tPrec@5 100.000 (99.047)\n",
      "Epoch: [61][50/97], lr: 0.01000\tTime 0.360 (0.381)\tData 0.000 (0.026)\tLoss 3.1747 (2.4852)\tPrec@1 78.906 (84.651)\tPrec@5 97.656 (99.112)\n",
      "Epoch: [61][60/97], lr: 0.01000\tTime 0.345 (0.378)\tData 0.000 (0.024)\tLoss 2.6627 (2.4698)\tPrec@1 84.375 (84.798)\tPrec@5 99.219 (99.142)\n",
      "Epoch: [61][70/97], lr: 0.01000\tTime 0.349 (0.374)\tData 0.000 (0.023)\tLoss 2.5922 (2.4711)\tPrec@1 84.375 (84.705)\tPrec@5 99.219 (99.087)\n",
      "Epoch: [61][80/97], lr: 0.01000\tTime 0.352 (0.372)\tData 0.000 (0.022)\tLoss 2.0393 (2.4358)\tPrec@1 91.406 (85.079)\tPrec@5 100.000 (99.074)\n",
      "Epoch: [61][90/97], lr: 0.01000\tTime 0.336 (0.370)\tData 0.000 (0.022)\tLoss 2.0580 (2.4209)\tPrec@1 88.281 (85.113)\tPrec@5 99.219 (99.038)\n",
      "Epoch: [61][96/97], lr: 0.01000\tTime 0.390 (0.369)\tData 0.000 (0.022)\tLoss 2.3884 (2.4171)\tPrec@1 85.593 (85.128)\tPrec@5 99.153 (99.049)\n",
      "Gated Network Weight Gate= Flip:0.60, Sc:0.40\n",
      "Test: [0/100]\tTime 0.384 (0.384)\tLoss 10.8731 (10.8731)\tPrec@1 52.000 (52.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.074 (0.102)\tLoss 7.9197 (10.4504)\tPrec@1 61.000 (51.182)\tPrec@5 91.000 (93.545)\n",
      "Test: [20/100]\tTime 0.075 (0.089)\tLoss 8.5492 (10.4449)\tPrec@1 57.000 (50.571)\tPrec@5 94.000 (93.143)\n",
      "Test: [30/100]\tTime 0.074 (0.085)\tLoss 10.1125 (10.3882)\tPrec@1 53.000 (50.774)\tPrec@5 95.000 (93.258)\n",
      "Test: [40/100]\tTime 0.075 (0.082)\tLoss 11.0421 (10.4613)\tPrec@1 49.000 (50.610)\tPrec@5 90.000 (93.220)\n",
      "Test: [50/100]\tTime 0.075 (0.081)\tLoss 10.2455 (10.3647)\tPrec@1 52.000 (50.902)\tPrec@5 97.000 (93.294)\n",
      "Test: [60/100]\tTime 0.075 (0.080)\tLoss 8.6409 (10.3595)\tPrec@1 55.000 (50.689)\tPrec@5 94.000 (93.328)\n",
      "Test: [70/100]\tTime 0.079 (0.079)\tLoss 9.5444 (10.3199)\tPrec@1 53.000 (50.887)\tPrec@5 92.000 (93.155)\n",
      "Test: [80/100]\tTime 0.075 (0.079)\tLoss 9.9351 (10.2785)\tPrec@1 52.000 (51.025)\tPrec@5 90.000 (93.272)\n",
      "Test: [90/100]\tTime 0.075 (0.079)\tLoss 11.1237 (10.3567)\tPrec@1 48.000 (50.670)\tPrec@5 94.000 (93.264)\n",
      "val Results: Prec@1 50.650 Prec@5 93.130 Loss 10.36299\n",
      "val Class Accuracy: [0.928,0.986,0.686,0.912,0.367,0.370,0.326,0.364,0.120,0.006]\n",
      "Best Prec@1: 56.780\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [62][0/97], lr: 0.01000\tTime 0.881 (0.881)\tData 0.525 (0.525)\tLoss 2.0095 (2.0095)\tPrec@1 86.719 (86.719)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [62][10/97], lr: 0.01000\tTime 0.353 (0.446)\tData 0.000 (0.060)\tLoss 2.2064 (2.4454)\tPrec@1 85.938 (84.659)\tPrec@5 97.656 (99.290)\n",
      "Epoch: [62][20/97], lr: 0.01000\tTime 0.370 (0.408)\tData 0.000 (0.039)\tLoss 2.7179 (2.3858)\tPrec@1 84.375 (85.082)\tPrec@5 98.438 (99.293)\n",
      "Epoch: [62][30/97], lr: 0.01000\tTime 0.356 (0.391)\tData 0.001 (0.032)\tLoss 1.9558 (2.3978)\tPrec@1 85.156 (85.307)\tPrec@5 99.219 (99.093)\n",
      "Epoch: [62][40/97], lr: 0.01000\tTime 0.362 (0.386)\tData 0.000 (0.028)\tLoss 2.2224 (2.3813)\tPrec@1 85.938 (85.366)\tPrec@5 99.219 (99.104)\n",
      "Epoch: [62][50/97], lr: 0.01000\tTime 0.393 (0.386)\tData 0.000 (0.025)\tLoss 2.6065 (2.3681)\tPrec@1 83.594 (85.509)\tPrec@5 99.219 (99.004)\n",
      "Epoch: [62][60/97], lr: 0.01000\tTime 0.374 (0.383)\tData 0.000 (0.023)\tLoss 2.1043 (2.3274)\tPrec@1 85.156 (85.835)\tPrec@5 98.438 (99.014)\n",
      "Epoch: [62][70/97], lr: 0.01000\tTime 0.338 (0.379)\tData 0.000 (0.022)\tLoss 2.8488 (2.3253)\tPrec@1 82.812 (85.827)\tPrec@5 96.875 (99.043)\n",
      "Epoch: [62][80/97], lr: 0.01000\tTime 0.327 (0.376)\tData 0.000 (0.021)\tLoss 2.7683 (2.3585)\tPrec@1 81.250 (85.523)\tPrec@5 99.219 (99.064)\n",
      "Epoch: [62][90/97], lr: 0.01000\tTime 0.343 (0.373)\tData 0.000 (0.021)\tLoss 2.9391 (2.3586)\tPrec@1 82.812 (85.534)\tPrec@5 98.438 (99.081)\n",
      "Epoch: [62][96/97], lr: 0.01000\tTime 0.336 (0.371)\tData 0.000 (0.021)\tLoss 1.6766 (2.3633)\tPrec@1 88.136 (85.483)\tPrec@5 100.000 (99.113)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.354 (0.354)\tLoss 10.2671 (10.2671)\tPrec@1 51.000 (51.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.074 (0.099)\tLoss 7.3778 (9.2260)\tPrec@1 64.000 (54.000)\tPrec@5 98.000 (96.182)\n",
      "Test: [20/100]\tTime 0.075 (0.088)\tLoss 8.2556 (9.2683)\tPrec@1 59.000 (53.667)\tPrec@5 97.000 (95.762)\n",
      "Test: [30/100]\tTime 0.076 (0.084)\tLoss 8.8144 (9.3495)\tPrec@1 58.000 (53.065)\tPrec@5 97.000 (95.677)\n",
      "Test: [40/100]\tTime 0.075 (0.082)\tLoss 9.8695 (9.4184)\tPrec@1 52.000 (53.220)\tPrec@5 93.000 (95.195)\n",
      "Test: [50/100]\tTime 0.075 (0.081)\tLoss 8.8002 (9.3022)\tPrec@1 59.000 (53.706)\tPrec@5 95.000 (95.078)\n",
      "Test: [60/100]\tTime 0.075 (0.080)\tLoss 7.8053 (9.2913)\tPrec@1 61.000 (53.754)\tPrec@5 97.000 (95.016)\n",
      "Test: [70/100]\tTime 0.077 (0.080)\tLoss 8.8468 (9.2588)\tPrec@1 55.000 (53.845)\tPrec@5 95.000 (95.028)\n",
      "Test: [80/100]\tTime 0.075 (0.079)\tLoss 8.8412 (9.2097)\tPrec@1 56.000 (53.914)\tPrec@5 96.000 (95.111)\n",
      "Test: [90/100]\tTime 0.075 (0.079)\tLoss 9.4405 (9.2660)\tPrec@1 52.000 (53.670)\tPrec@5 97.000 (95.132)\n",
      "val Results: Prec@1 53.640 Prec@5 95.050 Loss 9.27720\n",
      "val Class Accuracy: [0.919,0.987,0.571,0.900,0.666,0.332,0.482,0.278,0.189,0.040]\n",
      "Best Prec@1: 56.780\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [63][0/97], lr: 0.01000\tTime 0.849 (0.849)\tData 0.535 (0.535)\tLoss 1.9045 (1.9045)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [63][10/97], lr: 0.01000\tTime 0.358 (0.445)\tData 0.001 (0.061)\tLoss 2.3220 (2.4378)\tPrec@1 85.156 (84.304)\tPrec@5 98.438 (99.077)\n",
      "Epoch: [63][20/97], lr: 0.01000\tTime 0.358 (0.406)\tData 0.000 (0.039)\tLoss 1.8168 (2.3474)\tPrec@1 90.625 (85.231)\tPrec@5 99.219 (98.772)\n",
      "Epoch: [63][30/97], lr: 0.01000\tTime 0.373 (0.393)\tData 0.000 (0.032)\tLoss 2.1796 (2.3631)\tPrec@1 85.938 (85.333)\tPrec@5 100.000 (98.765)\n",
      "Epoch: [63][40/97], lr: 0.01000\tTime 0.344 (0.387)\tData 0.000 (0.028)\tLoss 1.8966 (2.3199)\tPrec@1 89.062 (85.671)\tPrec@5 100.000 (98.971)\n",
      "Epoch: [63][50/97], lr: 0.01000\tTime 0.354 (0.383)\tData 0.000 (0.026)\tLoss 1.7936 (2.3004)\tPrec@1 89.844 (85.922)\tPrec@5 99.219 (98.943)\n",
      "Epoch: [63][60/97], lr: 0.01000\tTime 0.365 (0.381)\tData 0.000 (0.024)\tLoss 2.0303 (2.3097)\tPrec@1 88.281 (85.707)\tPrec@5 100.000 (98.937)\n",
      "Epoch: [63][70/97], lr: 0.01000\tTime 0.339 (0.376)\tData 0.000 (0.023)\tLoss 2.5696 (2.3507)\tPrec@1 84.375 (85.387)\tPrec@5 100.000 (98.922)\n",
      "Epoch: [63][80/97], lr: 0.01000\tTime 0.340 (0.373)\tData 0.000 (0.022)\tLoss 2.5007 (2.3532)\tPrec@1 85.156 (85.378)\tPrec@5 100.000 (98.968)\n",
      "Epoch: [63][90/97], lr: 0.01000\tTime 0.339 (0.371)\tData 0.000 (0.022)\tLoss 2.6698 (2.4010)\tPrec@1 84.375 (85.079)\tPrec@5 100.000 (98.910)\n",
      "Epoch: [63][96/97], lr: 0.01000\tTime 0.329 (0.369)\tData 0.000 (0.022)\tLoss 2.6698 (2.4129)\tPrec@1 83.898 (85.056)\tPrec@5 99.153 (98.896)\n",
      "Gated Network Weight Gate= Flip:0.37, Sc:0.63\n",
      "Test: [0/100]\tTime 0.340 (0.340)\tLoss 12.1459 (12.1459)\tPrec@1 38.000 (38.000)\tPrec@5 85.000 (85.000)\n",
      "Test: [10/100]\tTime 0.073 (0.098)\tLoss 8.8669 (10.4760)\tPrec@1 57.000 (46.727)\tPrec@5 91.000 (91.182)\n",
      "Test: [20/100]\tTime 0.075 (0.087)\tLoss 9.2791 (10.4237)\tPrec@1 51.000 (46.000)\tPrec@5 91.000 (91.571)\n",
      "Test: [30/100]\tTime 0.076 (0.083)\tLoss 9.4234 (10.3777)\tPrec@1 51.000 (46.161)\tPrec@5 92.000 (91.645)\n",
      "Test: [40/100]\tTime 0.075 (0.081)\tLoss 11.2639 (10.3562)\tPrec@1 44.000 (46.610)\tPrec@5 91.000 (91.512)\n",
      "Test: [50/100]\tTime 0.075 (0.080)\tLoss 10.6055 (10.2950)\tPrec@1 50.000 (47.020)\tPrec@5 89.000 (91.647)\n",
      "Test: [60/100]\tTime 0.074 (0.079)\tLoss 8.4612 (10.3121)\tPrec@1 59.000 (46.951)\tPrec@5 92.000 (91.443)\n",
      "Test: [70/100]\tTime 0.075 (0.078)\tLoss 9.7289 (10.2881)\tPrec@1 52.000 (47.028)\tPrec@5 91.000 (91.549)\n",
      "Test: [80/100]\tTime 0.075 (0.078)\tLoss 8.9337 (10.2354)\tPrec@1 53.000 (47.210)\tPrec@5 94.000 (91.778)\n",
      "Test: [90/100]\tTime 0.075 (0.078)\tLoss 10.2915 (10.3210)\tPrec@1 45.000 (46.758)\tPrec@5 95.000 (91.604)\n",
      "val Results: Prec@1 46.490 Prec@5 91.670 Loss 10.35796\n",
      "val Class Accuracy: [0.977,0.980,0.729,0.261,0.620,0.254,0.207,0.345,0.263,0.013]\n",
      "Best Prec@1: 56.780\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [64][0/97], lr: 0.01000\tTime 0.966 (0.966)\tData 0.563 (0.563)\tLoss 2.9563 (2.9563)\tPrec@1 83.594 (83.594)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [64][10/97], lr: 0.01000\tTime 0.366 (0.445)\tData 0.000 (0.062)\tLoss 2.4384 (2.3972)\tPrec@1 83.594 (85.795)\tPrec@5 99.219 (98.935)\n",
      "Epoch: [64][20/97], lr: 0.01000\tTime 0.352 (0.408)\tData 0.000 (0.040)\tLoss 2.5342 (2.3915)\tPrec@1 84.375 (85.454)\tPrec@5 98.438 (99.033)\n",
      "Epoch: [64][30/97], lr: 0.01000\tTime 0.368 (0.395)\tData 0.000 (0.032)\tLoss 1.7741 (2.3469)\tPrec@1 88.281 (85.912)\tPrec@5 98.438 (98.942)\n",
      "Epoch: [64][40/97], lr: 0.01000\tTime 0.349 (0.386)\tData 0.000 (0.028)\tLoss 1.5009 (2.3704)\tPrec@1 89.844 (85.614)\tPrec@5 99.219 (99.123)\n",
      "Epoch: [64][50/97], lr: 0.01000\tTime 0.354 (0.383)\tData 0.000 (0.026)\tLoss 2.3135 (2.3414)\tPrec@1 85.938 (85.784)\tPrec@5 100.000 (99.188)\n",
      "Epoch: [64][60/97], lr: 0.01000\tTime 0.347 (0.380)\tData 0.000 (0.024)\tLoss 2.2461 (2.3518)\tPrec@1 87.500 (85.669)\tPrec@5 96.094 (99.103)\n",
      "Epoch: [64][70/97], lr: 0.01000\tTime 0.339 (0.375)\tData 0.000 (0.023)\tLoss 1.4253 (2.3026)\tPrec@1 92.969 (86.048)\tPrec@5 100.000 (99.142)\n",
      "Epoch: [64][80/97], lr: 0.01000\tTime 0.349 (0.373)\tData 0.000 (0.022)\tLoss 2.5911 (2.2911)\tPrec@1 81.250 (86.082)\tPrec@5 98.438 (99.103)\n",
      "Epoch: [64][90/97], lr: 0.01000\tTime 0.337 (0.370)\tData 0.000 (0.022)\tLoss 2.8489 (2.3035)\tPrec@1 82.812 (85.998)\tPrec@5 98.438 (99.047)\n",
      "Epoch: [64][96/97], lr: 0.01000\tTime 0.333 (0.368)\tData 0.000 (0.022)\tLoss 2.8861 (2.3223)\tPrec@1 83.051 (85.846)\tPrec@5 98.305 (99.017)\n",
      "Gated Network Weight Gate= Flip:0.51, Sc:0.49\n",
      "Test: [0/100]\tTime 0.369 (0.369)\tLoss 9.7285 (9.7285)\tPrec@1 53.000 (53.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.074 (0.101)\tLoss 7.1642 (8.6973)\tPrec@1 65.000 (57.727)\tPrec@5 90.000 (90.727)\n",
      "Test: [20/100]\tTime 0.075 (0.088)\tLoss 7.3256 (8.7338)\tPrec@1 59.000 (57.381)\tPrec@5 90.000 (90.190)\n",
      "Test: [30/100]\tTime 0.075 (0.084)\tLoss 7.7570 (8.7409)\tPrec@1 66.000 (57.645)\tPrec@5 90.000 (90.355)\n",
      "Test: [40/100]\tTime 0.074 (0.082)\tLoss 9.1656 (8.7439)\tPrec@1 56.000 (57.463)\tPrec@5 86.000 (90.537)\n",
      "Test: [50/100]\tTime 0.075 (0.081)\tLoss 7.7041 (8.6392)\tPrec@1 63.000 (57.961)\tPrec@5 94.000 (90.667)\n",
      "Test: [60/100]\tTime 0.076 (0.080)\tLoss 7.7623 (8.5502)\tPrec@1 64.000 (58.393)\tPrec@5 89.000 (90.869)\n",
      "Test: [70/100]\tTime 0.076 (0.080)\tLoss 8.1988 (8.5698)\tPrec@1 63.000 (58.423)\tPrec@5 95.000 (90.732)\n",
      "Test: [80/100]\tTime 0.076 (0.079)\tLoss 7.5174 (8.5052)\tPrec@1 66.000 (58.864)\tPrec@5 91.000 (90.827)\n",
      "Test: [90/100]\tTime 0.075 (0.079)\tLoss 8.6936 (8.6063)\tPrec@1 58.000 (58.341)\tPrec@5 91.000 (90.516)\n",
      "val Results: Prec@1 58.200 Prec@5 90.550 Loss 8.64097\n",
      "val Class Accuracy: [0.940,0.968,0.695,0.593,0.634,0.724,0.519,0.612,0.013,0.122]\n",
      "Best Prec@1: 58.200\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [65][0/97], lr: 0.01000\tTime 0.920 (0.920)\tData 0.570 (0.570)\tLoss 2.6960 (2.6960)\tPrec@1 83.594 (83.594)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [65][10/97], lr: 0.01000\tTime 0.368 (0.449)\tData 0.000 (0.064)\tLoss 2.4188 (2.2449)\tPrec@1 85.156 (86.009)\tPrec@5 98.438 (98.935)\n",
      "Epoch: [65][20/97], lr: 0.01000\tTime 0.351 (0.412)\tData 0.000 (0.041)\tLoss 1.6835 (2.1359)\tPrec@1 88.281 (86.756)\tPrec@5 99.219 (98.884)\n",
      "Epoch: [65][30/97], lr: 0.01000\tTime 0.357 (0.396)\tData 0.000 (0.033)\tLoss 3.4766 (2.2415)\tPrec@1 77.344 (86.013)\tPrec@5 97.656 (98.891)\n",
      "Epoch: [65][40/97], lr: 0.01000\tTime 0.367 (0.391)\tData 0.000 (0.029)\tLoss 2.5109 (2.2640)\tPrec@1 87.500 (85.899)\tPrec@5 100.000 (99.028)\n",
      "Epoch: [65][50/97], lr: 0.01000\tTime 0.349 (0.387)\tData 0.000 (0.026)\tLoss 2.4223 (2.2620)\tPrec@1 84.375 (85.983)\tPrec@5 100.000 (99.050)\n",
      "Epoch: [65][60/97], lr: 0.01000\tTime 0.361 (0.384)\tData 0.001 (0.025)\tLoss 1.7778 (2.2641)\tPrec@1 89.844 (86.014)\tPrec@5 100.000 (99.039)\n",
      "Epoch: [65][70/97], lr: 0.01000\tTime 0.349 (0.379)\tData 0.000 (0.023)\tLoss 2.5323 (2.2994)\tPrec@1 82.812 (85.783)\tPrec@5 100.000 (99.021)\n",
      "Epoch: [65][80/97], lr: 0.01000\tTime 0.338 (0.376)\tData 0.000 (0.022)\tLoss 2.7018 (2.3217)\tPrec@1 85.156 (85.610)\tPrec@5 100.000 (99.093)\n",
      "Epoch: [65][90/97], lr: 0.01000\tTime 0.361 (0.374)\tData 0.000 (0.022)\tLoss 2.8141 (2.3345)\tPrec@1 80.469 (85.474)\tPrec@5 100.000 (99.124)\n",
      "Epoch: [65][96/97], lr: 0.01000\tTime 0.321 (0.372)\tData 0.000 (0.022)\tLoss 2.3577 (2.3339)\tPrec@1 86.441 (85.434)\tPrec@5 100.000 (99.162)\n",
      "Gated Network Weight Gate= Flip:0.48, Sc:0.52\n",
      "Test: [0/100]\tTime 0.359 (0.359)\tLoss 10.6558 (10.6558)\tPrec@1 47.000 (47.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.073 (0.100)\tLoss 8.4522 (9.4666)\tPrec@1 56.000 (51.818)\tPrec@5 88.000 (92.545)\n",
      "Test: [20/100]\tTime 0.075 (0.088)\tLoss 8.3527 (9.2178)\tPrec@1 56.000 (53.857)\tPrec@5 95.000 (92.571)\n",
      "Test: [30/100]\tTime 0.075 (0.084)\tLoss 8.0365 (9.1899)\tPrec@1 59.000 (54.097)\tPrec@5 92.000 (92.774)\n",
      "Test: [40/100]\tTime 0.076 (0.082)\tLoss 9.1010 (9.2135)\tPrec@1 55.000 (54.220)\tPrec@5 91.000 (92.756)\n",
      "Test: [50/100]\tTime 0.077 (0.081)\tLoss 8.1375 (9.1077)\tPrec@1 62.000 (54.765)\tPrec@5 96.000 (92.961)\n",
      "Test: [60/100]\tTime 0.078 (0.080)\tLoss 8.0257 (9.0986)\tPrec@1 63.000 (54.770)\tPrec@5 90.000 (92.787)\n",
      "Test: [70/100]\tTime 0.075 (0.080)\tLoss 8.0128 (9.0906)\tPrec@1 58.000 (54.831)\tPrec@5 96.000 (92.620)\n",
      "Test: [80/100]\tTime 0.075 (0.079)\tLoss 8.4828 (9.0352)\tPrec@1 59.000 (55.111)\tPrec@5 92.000 (92.741)\n",
      "Test: [90/100]\tTime 0.075 (0.079)\tLoss 9.9872 (9.1434)\tPrec@1 49.000 (54.626)\tPrec@5 92.000 (92.538)\n",
      "val Results: Prec@1 54.590 Prec@5 92.620 Loss 9.16375\n",
      "val Class Accuracy: [0.943,0.982,0.519,0.849,0.643,0.463,0.122,0.678,0.168,0.092]\n",
      "Best Prec@1: 58.200\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [66][0/97], lr: 0.01000\tTime 0.911 (0.911)\tData 0.554 (0.554)\tLoss 2.5777 (2.5777)\tPrec@1 86.719 (86.719)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [66][10/97], lr: 0.01000\tTime 0.363 (0.448)\tData 0.000 (0.063)\tLoss 2.2762 (2.1913)\tPrec@1 86.719 (86.790)\tPrec@5 99.219 (99.077)\n",
      "Epoch: [66][20/97], lr: 0.01000\tTime 0.347 (0.408)\tData 0.001 (0.041)\tLoss 1.9975 (2.2575)\tPrec@1 85.938 (86.272)\tPrec@5 98.438 (99.070)\n",
      "Epoch: [66][30/97], lr: 0.01000\tTime 0.383 (0.394)\tData 0.000 (0.033)\tLoss 1.9432 (2.2380)\tPrec@1 88.281 (86.467)\tPrec@5 100.000 (99.294)\n",
      "Epoch: [66][40/97], lr: 0.01000\tTime 0.356 (0.388)\tData 0.000 (0.029)\tLoss 2.8862 (2.2567)\tPrec@1 83.594 (86.242)\tPrec@5 99.219 (99.352)\n",
      "Epoch: [66][50/97], lr: 0.01000\tTime 0.355 (0.384)\tData 0.000 (0.026)\tLoss 2.0583 (2.2957)\tPrec@1 88.281 (85.983)\tPrec@5 100.000 (99.249)\n",
      "Epoch: [66][60/97], lr: 0.01000\tTime 0.359 (0.382)\tData 0.000 (0.024)\tLoss 2.0138 (2.3170)\tPrec@1 89.844 (85.925)\tPrec@5 99.219 (99.270)\n",
      "Epoch: [66][70/97], lr: 0.01000\tTime 0.339 (0.378)\tData 0.000 (0.023)\tLoss 2.2614 (2.3193)\tPrec@1 85.156 (85.904)\tPrec@5 98.438 (99.186)\n",
      "Epoch: [66][80/97], lr: 0.01000\tTime 0.349 (0.374)\tData 0.000 (0.022)\tLoss 2.8937 (2.2970)\tPrec@1 78.906 (85.947)\tPrec@5 99.219 (99.190)\n",
      "Epoch: [66][90/97], lr: 0.01000\tTime 0.350 (0.373)\tData 0.000 (0.022)\tLoss 2.7253 (2.3332)\tPrec@1 81.250 (85.740)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [66][96/97], lr: 0.01000\tTime 0.328 (0.371)\tData 0.000 (0.022)\tLoss 2.7181 (2.3437)\tPrec@1 86.441 (85.709)\tPrec@5 98.305 (99.210)\n",
      "Gated Network Weight Gate= Flip:0.38, Sc:0.62\n",
      "Test: [0/100]\tTime 0.349 (0.349)\tLoss 9.8154 (9.8154)\tPrec@1 54.000 (54.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.074 (0.099)\tLoss 7.3970 (8.9806)\tPrec@1 64.000 (54.364)\tPrec@5 95.000 (95.182)\n",
      "Test: [20/100]\tTime 0.075 (0.087)\tLoss 8.4140 (9.1083)\tPrec@1 55.000 (53.286)\tPrec@5 96.000 (95.381)\n",
      "Test: [30/100]\tTime 0.075 (0.083)\tLoss 7.9447 (9.0873)\tPrec@1 58.000 (52.871)\tPrec@5 95.000 (95.323)\n",
      "Test: [40/100]\tTime 0.075 (0.081)\tLoss 10.0977 (9.1258)\tPrec@1 47.000 (53.024)\tPrec@5 93.000 (95.122)\n",
      "Test: [50/100]\tTime 0.075 (0.080)\tLoss 8.7323 (9.0804)\tPrec@1 58.000 (53.392)\tPrec@5 94.000 (95.157)\n",
      "Test: [60/100]\tTime 0.076 (0.079)\tLoss 7.9518 (9.0619)\tPrec@1 59.000 (53.311)\tPrec@5 95.000 (95.016)\n",
      "Test: [70/100]\tTime 0.074 (0.079)\tLoss 9.3457 (9.0552)\tPrec@1 50.000 (53.324)\tPrec@5 96.000 (95.042)\n",
      "Test: [80/100]\tTime 0.077 (0.078)\tLoss 8.1927 (8.9989)\tPrec@1 58.000 (53.642)\tPrec@5 95.000 (95.198)\n",
      "Test: [90/100]\tTime 0.075 (0.078)\tLoss 9.1568 (9.0598)\tPrec@1 55.000 (53.451)\tPrec@5 96.000 (95.198)\n",
      "val Results: Prec@1 53.520 Prec@5 95.210 Loss 9.06325\n",
      "val Class Accuracy: [0.953,0.994,0.693,0.589,0.525,0.347,0.512,0.518,0.173,0.048]\n",
      "Best Prec@1: 58.200\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [67][0/97], lr: 0.01000\tTime 0.868 (0.868)\tData 0.471 (0.471)\tLoss 2.5234 (2.5234)\tPrec@1 84.375 (84.375)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [67][10/97], lr: 0.01000\tTime 0.370 (0.440)\tData 0.000 (0.054)\tLoss 2.7830 (2.4020)\tPrec@1 84.375 (85.156)\tPrec@5 98.438 (99.148)\n",
      "Epoch: [67][20/97], lr: 0.01000\tTime 0.361 (0.405)\tData 0.000 (0.036)\tLoss 2.5289 (2.3146)\tPrec@1 85.156 (85.863)\tPrec@5 99.219 (99.144)\n",
      "Epoch: [67][30/97], lr: 0.01000\tTime 0.382 (0.392)\tData 0.000 (0.029)\tLoss 2.8462 (2.2613)\tPrec@1 84.375 (86.492)\tPrec@5 97.656 (99.118)\n",
      "Epoch: [67][40/97], lr: 0.01000\tTime 0.353 (0.386)\tData 0.000 (0.026)\tLoss 2.0627 (2.2486)\tPrec@1 89.844 (86.547)\tPrec@5 99.219 (98.914)\n",
      "Epoch: [67][50/97], lr: 0.01000\tTime 0.409 (0.383)\tData 0.000 (0.024)\tLoss 2.7143 (2.2735)\tPrec@1 82.812 (86.183)\tPrec@5 100.000 (98.974)\n",
      "Epoch: [67][60/97], lr: 0.01000\tTime 0.365 (0.380)\tData 0.000 (0.022)\tLoss 2.2476 (2.2876)\tPrec@1 85.938 (86.142)\tPrec@5 100.000 (99.001)\n",
      "Epoch: [67][70/97], lr: 0.01000\tTime 0.337 (0.376)\tData 0.000 (0.021)\tLoss 3.1758 (2.2950)\tPrec@1 80.469 (86.103)\tPrec@5 99.219 (98.977)\n",
      "Epoch: [67][80/97], lr: 0.01000\tTime 0.346 (0.373)\tData 0.000 (0.021)\tLoss 2.6089 (2.3160)\tPrec@1 85.156 (85.938)\tPrec@5 99.219 (98.997)\n",
      "Epoch: [67][90/97], lr: 0.01000\tTime 0.374 (0.370)\tData 0.000 (0.020)\tLoss 2.3847 (2.3108)\tPrec@1 87.500 (86.032)\tPrec@5 98.438 (99.013)\n",
      "Epoch: [67][96/97], lr: 0.01000\tTime 0.335 (0.369)\tData 0.000 (0.021)\tLoss 2.0075 (2.3122)\tPrec@1 89.831 (86.055)\tPrec@5 99.153 (99.025)\n",
      "Gated Network Weight Gate= Flip:0.45, Sc:0.55\n",
      "Test: [0/100]\tTime 0.354 (0.354)\tLoss 8.1673 (8.1673)\tPrec@1 59.000 (59.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.074 (0.099)\tLoss 6.1600 (7.6033)\tPrec@1 68.000 (62.909)\tPrec@5 97.000 (96.636)\n",
      "Test: [20/100]\tTime 0.076 (0.088)\tLoss 7.1463 (7.5624)\tPrec@1 61.000 (62.810)\tPrec@5 97.000 (96.571)\n",
      "Test: [30/100]\tTime 0.075 (0.084)\tLoss 6.7617 (7.6600)\tPrec@1 65.000 (62.290)\tPrec@5 97.000 (96.710)\n",
      "Test: [40/100]\tTime 0.074 (0.082)\tLoss 7.9374 (7.6646)\tPrec@1 62.000 (62.537)\tPrec@5 98.000 (96.805)\n",
      "Test: [50/100]\tTime 0.077 (0.080)\tLoss 7.1744 (7.5960)\tPrec@1 65.000 (62.725)\tPrec@5 96.000 (96.784)\n",
      "Test: [60/100]\tTime 0.075 (0.080)\tLoss 6.7437 (7.5630)\tPrec@1 67.000 (62.738)\tPrec@5 94.000 (96.836)\n",
      "Test: [70/100]\tTime 0.076 (0.079)\tLoss 7.4658 (7.5417)\tPrec@1 62.000 (63.099)\tPrec@5 97.000 (96.817)\n",
      "Test: [80/100]\tTime 0.077 (0.079)\tLoss 6.9348 (7.5140)\tPrec@1 67.000 (63.210)\tPrec@5 98.000 (96.926)\n",
      "Test: [90/100]\tTime 0.076 (0.079)\tLoss 8.1054 (7.5865)\tPrec@1 58.000 (62.791)\tPrec@5 98.000 (96.813)\n",
      "val Results: Prec@1 62.650 Prec@5 96.770 Loss 7.61640\n",
      "val Class Accuracy: [0.939,0.983,0.715,0.680,0.801,0.496,0.676,0.508,0.292,0.175]\n",
      "Best Prec@1: 62.650\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [68][0/97], lr: 0.01000\tTime 0.851 (0.851)\tData 0.520 (0.520)\tLoss 3.3797 (3.3797)\tPrec@1 78.906 (78.906)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [68][10/97], lr: 0.01000\tTime 0.360 (0.437)\tData 0.000 (0.059)\tLoss 2.5531 (2.3739)\tPrec@1 85.938 (85.298)\tPrec@5 98.438 (99.290)\n",
      "Epoch: [68][20/97], lr: 0.01000\tTime 0.358 (0.404)\tData 0.000 (0.038)\tLoss 1.5943 (2.3246)\tPrec@1 89.062 (85.751)\tPrec@5 99.219 (98.921)\n",
      "Epoch: [68][30/97], lr: 0.01000\tTime 0.415 (0.391)\tData 0.000 (0.031)\tLoss 2.5808 (2.3877)\tPrec@1 83.594 (85.307)\tPrec@5 100.000 (98.841)\n",
      "Epoch: [68][40/97], lr: 0.01000\tTime 0.349 (0.385)\tData 0.000 (0.027)\tLoss 3.0640 (2.4008)\tPrec@1 78.906 (85.328)\tPrec@5 98.438 (98.895)\n",
      "Epoch: [68][50/97], lr: 0.01000\tTime 0.361 (0.382)\tData 0.000 (0.025)\tLoss 2.5868 (2.3391)\tPrec@1 82.812 (85.677)\tPrec@5 99.219 (98.912)\n",
      "Epoch: [68][60/97], lr: 0.01000\tTime 0.353 (0.379)\tData 0.000 (0.024)\tLoss 1.8737 (2.3413)\tPrec@1 89.062 (85.617)\tPrec@5 99.219 (98.911)\n",
      "Epoch: [68][70/97], lr: 0.01000\tTime 0.342 (0.376)\tData 0.000 (0.022)\tLoss 2.5724 (2.3770)\tPrec@1 85.156 (85.321)\tPrec@5 99.219 (98.933)\n",
      "Epoch: [68][80/97], lr: 0.01000\tTime 0.343 (0.373)\tData 0.000 (0.022)\tLoss 2.5794 (2.3711)\tPrec@1 85.938 (85.446)\tPrec@5 97.656 (98.910)\n",
      "Epoch: [68][90/97], lr: 0.01000\tTime 0.345 (0.371)\tData 0.000 (0.021)\tLoss 1.8346 (2.3693)\tPrec@1 88.281 (85.379)\tPrec@5 100.000 (98.987)\n",
      "Epoch: [68][96/97], lr: 0.01000\tTime 0.327 (0.369)\tData 0.000 (0.021)\tLoss 2.4583 (2.3597)\tPrec@1 84.746 (85.434)\tPrec@5 99.153 (99.009)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.369 (0.369)\tLoss 10.5034 (10.5034)\tPrec@1 48.000 (48.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.101)\tLoss 7.7862 (9.8098)\tPrec@1 62.000 (51.727)\tPrec@5 98.000 (94.455)\n",
      "Test: [20/100]\tTime 0.074 (0.088)\tLoss 8.3924 (9.7608)\tPrec@1 57.000 (51.667)\tPrec@5 98.000 (94.143)\n",
      "Test: [30/100]\tTime 0.075 (0.084)\tLoss 8.9558 (9.7308)\tPrec@1 53.000 (52.000)\tPrec@5 93.000 (93.677)\n",
      "Test: [40/100]\tTime 0.075 (0.082)\tLoss 10.1732 (9.7425)\tPrec@1 53.000 (52.341)\tPrec@5 91.000 (93.463)\n",
      "Test: [50/100]\tTime 0.074 (0.081)\tLoss 9.3016 (9.6100)\tPrec@1 56.000 (53.098)\tPrec@5 92.000 (93.569)\n",
      "Test: [60/100]\tTime 0.075 (0.080)\tLoss 7.8314 (9.5721)\tPrec@1 66.000 (53.098)\tPrec@5 91.000 (93.557)\n",
      "Test: [70/100]\tTime 0.075 (0.079)\tLoss 9.7762 (9.5561)\tPrec@1 49.000 (53.014)\tPrec@5 93.000 (93.563)\n",
      "Test: [80/100]\tTime 0.075 (0.079)\tLoss 9.0306 (9.5129)\tPrec@1 57.000 (53.136)\tPrec@5 92.000 (93.617)\n",
      "Test: [90/100]\tTime 0.076 (0.078)\tLoss 9.4996 (9.5766)\tPrec@1 55.000 (52.890)\tPrec@5 94.000 (93.527)\n",
      "val Results: Prec@1 52.630 Prec@5 93.560 Loss 9.61739\n",
      "val Class Accuracy: [0.964,0.985,0.790,0.709,0.634,0.387,0.557,0.171,0.056,0.010]\n",
      "Best Prec@1: 62.650\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [69][0/97], lr: 0.01000\tTime 0.889 (0.889)\tData 0.535 (0.535)\tLoss 2.2850 (2.2850)\tPrec@1 85.156 (85.156)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [69][10/97], lr: 0.01000\tTime 0.370 (0.445)\tData 0.000 (0.060)\tLoss 1.8712 (2.1259)\tPrec@1 89.062 (86.861)\tPrec@5 98.438 (99.077)\n",
      "Epoch: [69][20/97], lr: 0.01000\tTime 0.360 (0.408)\tData 0.000 (0.039)\tLoss 1.6940 (2.1211)\tPrec@1 89.062 (87.240)\tPrec@5 97.656 (99.182)\n",
      "Epoch: [69][30/97], lr: 0.01000\tTime 0.379 (0.401)\tData 0.001 (0.032)\tLoss 1.4618 (2.1755)\tPrec@1 89.062 (86.492)\tPrec@5 99.219 (99.118)\n",
      "Epoch: [69][40/97], lr: 0.01000\tTime 0.346 (0.392)\tData 0.000 (0.028)\tLoss 3.5095 (2.2467)\tPrec@1 77.344 (85.976)\tPrec@5 96.875 (99.028)\n",
      "Epoch: [69][50/97], lr: 0.01000\tTime 0.354 (0.388)\tData 0.000 (0.025)\tLoss 1.8279 (2.2300)\tPrec@1 89.844 (86.029)\tPrec@5 96.875 (99.066)\n",
      "Epoch: [69][60/97], lr: 0.01000\tTime 0.359 (0.384)\tData 0.000 (0.024)\tLoss 1.2465 (2.2917)\tPrec@1 94.531 (85.771)\tPrec@5 99.219 (99.091)\n",
      "Epoch: [69][70/97], lr: 0.01000\tTime 0.336 (0.379)\tData 0.000 (0.023)\tLoss 2.6403 (2.2620)\tPrec@1 83.594 (86.048)\tPrec@5 100.000 (99.120)\n",
      "Epoch: [69][80/97], lr: 0.01000\tTime 0.357 (0.375)\tData 0.000 (0.022)\tLoss 2.2024 (2.2654)\tPrec@1 87.500 (85.947)\tPrec@5 97.656 (99.103)\n",
      "Epoch: [69][90/97], lr: 0.01000\tTime 0.340 (0.372)\tData 0.000 (0.021)\tLoss 1.7650 (2.2765)\tPrec@1 89.844 (85.998)\tPrec@5 99.219 (99.064)\n",
      "Epoch: [69][96/97], lr: 0.01000\tTime 0.336 (0.371)\tData 0.000 (0.022)\tLoss 2.6548 (2.2870)\tPrec@1 83.051 (85.918)\tPrec@5 97.458 (99.033)\n",
      "Gated Network Weight Gate= Flip:0.53, Sc:0.47\n",
      "Test: [0/100]\tTime 0.360 (0.360)\tLoss 8.9667 (8.9667)\tPrec@1 55.000 (55.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.074 (0.100)\tLoss 6.6983 (8.3118)\tPrec@1 66.000 (58.182)\tPrec@5 98.000 (95.636)\n",
      "Test: [20/100]\tTime 0.075 (0.088)\tLoss 7.5547 (8.3579)\tPrec@1 57.000 (57.762)\tPrec@5 95.000 (95.476)\n",
      "Test: [30/100]\tTime 0.075 (0.083)\tLoss 7.8002 (8.3525)\tPrec@1 58.000 (57.613)\tPrec@5 94.000 (95.355)\n",
      "Test: [40/100]\tTime 0.075 (0.081)\tLoss 8.8465 (8.4010)\tPrec@1 55.000 (57.341)\tPrec@5 93.000 (95.317)\n",
      "Test: [50/100]\tTime 0.075 (0.081)\tLoss 8.1081 (8.3305)\tPrec@1 62.000 (57.824)\tPrec@5 95.000 (95.353)\n",
      "Test: [60/100]\tTime 0.077 (0.081)\tLoss 7.1125 (8.3377)\tPrec@1 62.000 (57.525)\tPrec@5 92.000 (95.115)\n",
      "Test: [70/100]\tTime 0.076 (0.081)\tLoss 8.6541 (8.3303)\tPrec@1 56.000 (57.718)\tPrec@5 97.000 (95.070)\n",
      "Test: [80/100]\tTime 0.075 (0.080)\tLoss 7.6380 (8.2815)\tPrec@1 62.000 (57.889)\tPrec@5 94.000 (95.198)\n",
      "Test: [90/100]\tTime 0.076 (0.080)\tLoss 8.1509 (8.3377)\tPrec@1 61.000 (57.549)\tPrec@5 97.000 (95.209)\n",
      "val Results: Prec@1 57.580 Prec@5 95.230 Loss 8.34868\n",
      "val Class Accuracy: [0.977,0.970,0.642,0.536,0.549,0.594,0.796,0.425,0.232,0.037]\n",
      "Best Prec@1: 62.650\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [70][0/97], lr: 0.01000\tTime 0.926 (0.926)\tData 0.547 (0.547)\tLoss 2.0074 (2.0074)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [70][10/97], lr: 0.01000\tTime 0.348 (0.440)\tData 0.000 (0.062)\tLoss 2.4110 (2.3100)\tPrec@1 82.812 (86.009)\tPrec@5 98.438 (99.148)\n",
      "Epoch: [70][20/97], lr: 0.01000\tTime 0.404 (0.414)\tData 0.000 (0.040)\tLoss 2.2627 (2.3849)\tPrec@1 88.281 (85.231)\tPrec@5 96.875 (98.958)\n",
      "Epoch: [70][30/97], lr: 0.01000\tTime 0.417 (0.417)\tData 0.000 (0.032)\tLoss 2.3856 (2.2419)\tPrec@1 84.375 (86.240)\tPrec@5 99.219 (98.967)\n",
      "Epoch: [70][40/97], lr: 0.01000\tTime 0.357 (0.412)\tData 0.001 (0.028)\tLoss 2.3512 (2.2637)\tPrec@1 85.938 (86.033)\tPrec@5 98.438 (98.990)\n",
      "Epoch: [70][50/97], lr: 0.01000\tTime 0.358 (0.404)\tData 0.000 (0.026)\tLoss 2.7464 (2.2608)\tPrec@1 81.250 (86.014)\tPrec@5 99.219 (98.989)\n",
      "Epoch: [70][60/97], lr: 0.01000\tTime 0.458 (0.411)\tData 0.001 (0.024)\tLoss 1.7805 (2.2577)\tPrec@1 89.844 (86.130)\tPrec@5 98.438 (99.027)\n",
      "Epoch: [70][70/97], lr: 0.01000\tTime 0.458 (0.413)\tData 0.000 (0.022)\tLoss 2.3113 (2.2789)\tPrec@1 86.719 (86.213)\tPrec@5 97.656 (99.043)\n",
      "Epoch: [70][80/97], lr: 0.01000\tTime 0.389 (0.413)\tData 0.000 (0.021)\tLoss 2.1717 (2.2670)\tPrec@1 87.500 (86.256)\tPrec@5 99.219 (99.093)\n",
      "Epoch: [70][90/97], lr: 0.01000\tTime 0.333 (0.414)\tData 0.000 (0.020)\tLoss 3.0410 (2.3061)\tPrec@1 82.812 (85.955)\tPrec@5 97.656 (99.073)\n",
      "Epoch: [70][96/97], lr: 0.01000\tTime 0.490 (0.416)\tData 0.000 (0.021)\tLoss 2.8817 (2.3063)\tPrec@1 77.966 (85.902)\tPrec@5 99.153 (99.073)\n",
      "Gated Network Weight Gate= Flip:0.52, Sc:0.48\n",
      "Test: [0/100]\tTime 0.822 (0.822)\tLoss 9.5999 (9.5999)\tPrec@1 55.000 (55.000)\tPrec@5 89.000 (89.000)\n",
      "Test: [10/100]\tTime 0.074 (0.153)\tLoss 6.8861 (8.4505)\tPrec@1 67.000 (59.000)\tPrec@5 92.000 (90.545)\n",
      "Test: [20/100]\tTime 0.078 (0.118)\tLoss 8.4688 (8.4772)\tPrec@1 54.000 (58.286)\tPrec@5 95.000 (91.190)\n",
      "Test: [30/100]\tTime 0.075 (0.105)\tLoss 8.0068 (8.4814)\tPrec@1 61.000 (58.194)\tPrec@5 92.000 (91.484)\n",
      "Test: [40/100]\tTime 0.074 (0.099)\tLoss 8.7206 (8.4752)\tPrec@1 54.000 (58.098)\tPrec@5 87.000 (91.341)\n",
      "Test: [50/100]\tTime 0.074 (0.096)\tLoss 8.7257 (8.3956)\tPrec@1 57.000 (58.608)\tPrec@5 91.000 (91.490)\n",
      "Test: [60/100]\tTime 0.094 (0.093)\tLoss 7.7612 (8.3782)\tPrec@1 65.000 (58.820)\tPrec@5 91.000 (91.689)\n",
      "Test: [70/100]\tTime 0.093 (0.093)\tLoss 7.7898 (8.4195)\tPrec@1 60.000 (58.577)\tPrec@5 96.000 (91.690)\n",
      "Test: [80/100]\tTime 0.076 (0.092)\tLoss 8.1339 (8.3978)\tPrec@1 64.000 (58.630)\tPrec@5 87.000 (91.691)\n",
      "Test: [90/100]\tTime 0.090 (0.092)\tLoss 7.5806 (8.4681)\tPrec@1 63.000 (58.308)\tPrec@5 93.000 (91.549)\n",
      "val Results: Prec@1 58.240 Prec@5 91.510 Loss 8.50101\n",
      "val Class Accuracy: [0.865,0.927,0.827,0.548,0.817,0.391,0.656,0.516,0.157,0.120]\n",
      "Best Prec@1: 62.650\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [71][0/97], lr: 0.01000\tTime 1.167 (1.167)\tData 0.703 (0.703)\tLoss 1.8472 (1.8472)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [71][10/97], lr: 0.01000\tTime 0.373 (0.490)\tData 0.000 (0.072)\tLoss 2.6638 (2.1704)\tPrec@1 82.031 (86.790)\tPrec@5 96.875 (98.864)\n",
      "Epoch: [71][20/97], lr: 0.01000\tTime 0.377 (0.432)\tData 0.001 (0.045)\tLoss 2.0038 (2.2552)\tPrec@1 89.844 (86.198)\tPrec@5 100.000 (98.921)\n",
      "Epoch: [71][30/97], lr: 0.01000\tTime 0.352 (0.413)\tData 0.000 (0.036)\tLoss 2.0738 (2.3173)\tPrec@1 86.719 (85.736)\tPrec@5 100.000 (98.916)\n",
      "Epoch: [71][40/97], lr: 0.01000\tTime 0.364 (0.401)\tData 0.000 (0.031)\tLoss 2.5899 (2.2839)\tPrec@1 82.031 (85.804)\tPrec@5 98.438 (98.933)\n",
      "Epoch: [71][50/97], lr: 0.01000\tTime 0.364 (0.395)\tData 0.000 (0.028)\tLoss 2.2901 (2.2784)\tPrec@1 86.719 (85.846)\tPrec@5 99.219 (98.851)\n",
      "Epoch: [71][60/97], lr: 0.01000\tTime 0.502 (0.395)\tData 0.000 (0.026)\tLoss 2.3897 (2.2849)\tPrec@1 85.938 (85.925)\tPrec@5 100.000 (98.911)\n",
      "Epoch: [71][70/97], lr: 0.01000\tTime 0.429 (0.408)\tData 0.001 (0.024)\tLoss 2.4319 (2.2901)\tPrec@1 86.719 (85.960)\tPrec@5 100.000 (98.999)\n",
      "Epoch: [71][80/97], lr: 0.01000\tTime 0.600 (0.425)\tData 0.001 (0.023)\tLoss 2.0616 (2.2844)\tPrec@1 87.500 (86.024)\tPrec@5 99.219 (99.016)\n",
      "Epoch: [71][90/97], lr: 0.01000\tTime 0.490 (0.439)\tData 0.001 (0.021)\tLoss 2.2363 (2.2859)\tPrec@1 89.062 (86.075)\tPrec@5 99.219 (99.038)\n",
      "Epoch: [71][96/97], lr: 0.01000\tTime 0.400 (0.440)\tData 0.000 (0.021)\tLoss 1.8423 (2.3069)\tPrec@1 88.136 (85.950)\tPrec@5 100.000 (99.033)\n",
      "Gated Network Weight Gate= Flip:0.49, Sc:0.51\n",
      "Test: [0/100]\tTime 0.814 (0.814)\tLoss 9.7266 (9.7266)\tPrec@1 56.000 (56.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.074 (0.149)\tLoss 7.4771 (8.7534)\tPrec@1 64.000 (58.182)\tPrec@5 96.000 (94.364)\n",
      "Test: [20/100]\tTime 0.079 (0.118)\tLoss 7.9510 (8.7236)\tPrec@1 57.000 (56.571)\tPrec@5 94.000 (94.476)\n",
      "Test: [30/100]\tTime 0.135 (0.110)\tLoss 8.8727 (8.7787)\tPrec@1 53.000 (56.581)\tPrec@5 99.000 (94.548)\n",
      "Test: [40/100]\tTime 0.079 (0.104)\tLoss 9.3079 (8.8338)\tPrec@1 54.000 (56.122)\tPrec@5 98.000 (94.537)\n",
      "Test: [50/100]\tTime 0.076 (0.101)\tLoss 7.9088 (8.7540)\tPrec@1 56.000 (56.294)\tPrec@5 91.000 (94.373)\n",
      "Test: [60/100]\tTime 0.104 (0.100)\tLoss 7.9663 (8.7120)\tPrec@1 57.000 (56.574)\tPrec@5 95.000 (94.426)\n",
      "Test: [70/100]\tTime 0.097 (0.099)\tLoss 8.6263 (8.7031)\tPrec@1 57.000 (56.634)\tPrec@5 95.000 (94.394)\n",
      "Test: [80/100]\tTime 0.074 (0.099)\tLoss 7.9677 (8.6401)\tPrec@1 62.000 (57.136)\tPrec@5 94.000 (94.605)\n",
      "Test: [90/100]\tTime 0.165 (0.100)\tLoss 8.7264 (8.6915)\tPrec@1 60.000 (57.066)\tPrec@5 93.000 (94.582)\n",
      "val Results: Prec@1 56.830 Prec@5 94.390 Loss 8.72919\n",
      "val Class Accuracy: [0.869,0.993,0.578,0.515,0.757,0.667,0.610,0.218,0.287,0.189]\n",
      "Best Prec@1: 62.650\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [72][0/97], lr: 0.01000\tTime 1.842 (1.842)\tData 1.114 (1.114)\tLoss 2.1992 (2.1992)\tPrec@1 87.500 (87.500)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [72][10/97], lr: 0.01000\tTime 0.536 (0.669)\tData 0.001 (0.109)\tLoss 2.7248 (2.2601)\tPrec@1 82.812 (86.293)\tPrec@5 98.438 (99.148)\n",
      "Epoch: [72][20/97], lr: 0.01000\tTime 0.677 (0.651)\tData 0.002 (0.062)\tLoss 2.1476 (2.1710)\tPrec@1 89.062 (87.016)\tPrec@5 99.219 (99.144)\n",
      "Epoch: [72][30/97], lr: 0.01000\tTime 0.527 (0.632)\tData 0.001 (0.046)\tLoss 2.3677 (2.1870)\tPrec@1 87.500 (86.845)\tPrec@5 98.438 (99.244)\n",
      "Epoch: [72][40/97], lr: 0.01000\tTime 0.395 (0.603)\tData 0.001 (0.037)\tLoss 1.9417 (2.1807)\tPrec@1 87.500 (86.719)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [72][50/97], lr: 0.01000\tTime 0.506 (0.586)\tData 0.001 (0.033)\tLoss 2.2659 (2.2311)\tPrec@1 88.281 (86.412)\tPrec@5 98.438 (99.203)\n",
      "Epoch: [72][60/97], lr: 0.01000\tTime 0.465 (0.579)\tData 0.001 (0.030)\tLoss 2.2458 (2.2589)\tPrec@1 86.719 (86.309)\tPrec@5 99.219 (99.065)\n",
      "Epoch: [72][70/97], lr: 0.01000\tTime 0.390 (0.568)\tData 0.000 (0.027)\tLoss 2.1451 (2.2535)\tPrec@1 86.719 (86.279)\tPrec@5 99.219 (99.109)\n",
      "Epoch: [72][80/97], lr: 0.01000\tTime 0.381 (0.548)\tData 0.000 (0.025)\tLoss 1.8990 (2.2658)\tPrec@1 88.281 (86.169)\tPrec@5 100.000 (99.122)\n",
      "Epoch: [72][90/97], lr: 0.01000\tTime 0.481 (0.546)\tData 0.001 (0.024)\tLoss 1.7674 (2.2639)\tPrec@1 89.062 (86.229)\tPrec@5 98.438 (99.124)\n",
      "Epoch: [72][96/97], lr: 0.01000\tTime 0.531 (0.543)\tData 0.001 (0.024)\tLoss 3.6395 (2.2862)\tPrec@1 82.203 (86.120)\tPrec@5 97.458 (99.089)\n",
      "Gated Network Weight Gate= Flip:0.43, Sc:0.57\n",
      "Test: [0/100]\tTime 0.786 (0.786)\tLoss 8.9379 (8.9379)\tPrec@1 54.000 (54.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.105 (0.154)\tLoss 6.8823 (8.2050)\tPrec@1 63.000 (59.455)\tPrec@5 96.000 (96.636)\n",
      "Test: [20/100]\tTime 0.077 (0.122)\tLoss 6.9521 (8.0117)\tPrec@1 61.000 (59.810)\tPrec@5 99.000 (96.476)\n",
      "Test: [30/100]\tTime 0.074 (0.110)\tLoss 7.2899 (7.9391)\tPrec@1 65.000 (60.613)\tPrec@5 95.000 (96.290)\n",
      "Test: [40/100]\tTime 0.074 (0.106)\tLoss 8.0413 (7.9100)\tPrec@1 60.000 (60.902)\tPrec@5 96.000 (96.000)\n",
      "Test: [50/100]\tTime 0.077 (0.102)\tLoss 7.3500 (7.8099)\tPrec@1 61.000 (61.412)\tPrec@5 98.000 (96.235)\n",
      "Test: [60/100]\tTime 0.074 (0.098)\tLoss 6.3563 (7.8466)\tPrec@1 64.000 (60.836)\tPrec@5 94.000 (96.328)\n",
      "Test: [70/100]\tTime 0.073 (0.095)\tLoss 7.8784 (7.8476)\tPrec@1 57.000 (60.789)\tPrec@5 97.000 (96.239)\n",
      "Test: [80/100]\tTime 0.101 (0.093)\tLoss 7.1376 (7.7952)\tPrec@1 65.000 (61.123)\tPrec@5 96.000 (96.370)\n",
      "Test: [90/100]\tTime 0.080 (0.092)\tLoss 7.0059 (7.8799)\tPrec@1 66.000 (60.725)\tPrec@5 100.000 (96.341)\n",
      "val Results: Prec@1 60.570 Prec@5 96.370 Loss 7.89895\n",
      "val Class Accuracy: [0.943,0.987,0.835,0.539,0.663,0.563,0.351,0.597,0.478,0.101]\n",
      "Best Prec@1: 62.650\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [73][0/97], lr: 0.01000\tTime 1.360 (1.360)\tData 0.776 (0.776)\tLoss 2.2515 (2.2515)\tPrec@1 89.062 (89.062)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [73][10/97], lr: 0.01000\tTime 0.529 (0.637)\tData 0.001 (0.080)\tLoss 2.0796 (2.2982)\tPrec@1 85.938 (86.222)\tPrec@5 98.438 (98.864)\n",
      "Epoch: [73][20/97], lr: 0.01000\tTime 0.539 (0.591)\tData 0.001 (0.047)\tLoss 2.2555 (2.2391)\tPrec@1 83.594 (86.124)\tPrec@5 98.438 (98.958)\n",
      "Epoch: [73][30/97], lr: 0.01000\tTime 0.411 (0.557)\tData 0.000 (0.035)\tLoss 1.9817 (2.2267)\tPrec@1 87.500 (86.089)\tPrec@5 100.000 (99.093)\n",
      "Epoch: [73][40/97], lr: 0.01000\tTime 0.474 (0.541)\tData 0.001 (0.030)\tLoss 1.7690 (2.2094)\tPrec@1 89.062 (86.376)\tPrec@5 99.219 (99.200)\n",
      "Epoch: [73][50/97], lr: 0.01000\tTime 0.428 (0.524)\tData 0.001 (0.027)\tLoss 2.4746 (2.2261)\tPrec@1 85.156 (86.244)\tPrec@5 100.000 (99.127)\n",
      "Epoch: [73][60/97], lr: 0.01000\tTime 0.339 (0.507)\tData 0.000 (0.025)\tLoss 2.3276 (2.2457)\tPrec@1 85.156 (86.104)\tPrec@5 100.000 (99.091)\n",
      "Epoch: [73][70/97], lr: 0.01000\tTime 0.359 (0.489)\tData 0.000 (0.023)\tLoss 2.3438 (2.2425)\tPrec@1 85.938 (86.191)\tPrec@5 99.219 (99.098)\n",
      "Epoch: [73][80/97], lr: 0.01000\tTime 0.437 (0.482)\tData 0.000 (0.022)\tLoss 1.9565 (2.2635)\tPrec@1 90.625 (85.986)\tPrec@5 100.000 (99.074)\n",
      "Epoch: [73][90/97], lr: 0.01000\tTime 0.357 (0.471)\tData 0.000 (0.021)\tLoss 2.2088 (2.2486)\tPrec@1 83.594 (86.109)\tPrec@5 97.656 (99.116)\n",
      "Epoch: [73][96/97], lr: 0.01000\tTime 0.365 (0.465)\tData 0.000 (0.022)\tLoss 1.6923 (2.2543)\tPrec@1 88.983 (86.087)\tPrec@5 100.000 (99.121)\n",
      "Gated Network Weight Gate= Flip:0.47, Sc:0.53\n",
      "Test: [0/100]\tTime 0.483 (0.483)\tLoss 9.7374 (9.7374)\tPrec@1 53.000 (53.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.074 (0.117)\tLoss 7.0600 (8.9500)\tPrec@1 63.000 (56.455)\tPrec@5 97.000 (94.364)\n",
      "Test: [20/100]\tTime 0.076 (0.097)\tLoss 7.9117 (8.9546)\tPrec@1 56.000 (55.524)\tPrec@5 92.000 (93.810)\n",
      "Test: [30/100]\tTime 0.074 (0.090)\tLoss 8.3267 (9.0248)\tPrec@1 59.000 (55.161)\tPrec@5 93.000 (93.484)\n",
      "Test: [40/100]\tTime 0.075 (0.087)\tLoss 9.3398 (9.0935)\tPrec@1 58.000 (54.902)\tPrec@5 90.000 (93.366)\n",
      "Test: [50/100]\tTime 0.079 (0.086)\tLoss 8.7690 (9.0088)\tPrec@1 57.000 (55.353)\tPrec@5 93.000 (93.392)\n",
      "Test: [60/100]\tTime 0.074 (0.089)\tLoss 7.3599 (8.9983)\tPrec@1 62.000 (55.197)\tPrec@5 93.000 (93.131)\n",
      "Test: [70/100]\tTime 0.078 (0.088)\tLoss 8.7204 (8.9630)\tPrec@1 59.000 (55.507)\tPrec@5 92.000 (93.197)\n",
      "Test: [80/100]\tTime 0.075 (0.086)\tLoss 8.7123 (8.9204)\tPrec@1 57.000 (55.765)\tPrec@5 92.000 (93.333)\n",
      "Test: [90/100]\tTime 0.077 (0.085)\tLoss 9.5937 (9.0054)\tPrec@1 54.000 (55.330)\tPrec@5 93.000 (93.297)\n",
      "val Results: Prec@1 55.190 Prec@5 93.250 Loss 9.03155\n",
      "val Class Accuracy: [0.951,0.993,0.442,0.772,0.750,0.401,0.614,0.318,0.263,0.015]\n",
      "Best Prec@1: 62.650\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [74][0/97], lr: 0.01000\tTime 1.019 (1.019)\tData 0.599 (0.599)\tLoss 2.2003 (2.2003)\tPrec@1 87.500 (87.500)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [74][10/97], lr: 0.01000\tTime 0.372 (0.480)\tData 0.000 (0.066)\tLoss 3.1317 (2.3123)\tPrec@1 80.469 (85.653)\tPrec@5 98.438 (98.935)\n",
      "Epoch: [74][20/97], lr: 0.01000\tTime 0.381 (0.425)\tData 0.001 (0.042)\tLoss 2.6808 (2.1600)\tPrec@1 80.469 (86.496)\tPrec@5 100.000 (99.144)\n",
      "Epoch: [74][30/97], lr: 0.01000\tTime 0.437 (0.416)\tData 0.000 (0.033)\tLoss 1.4900 (2.2448)\tPrec@1 89.844 (86.038)\tPrec@5 99.219 (99.042)\n",
      "Epoch: [74][40/97], lr: 0.01000\tTime 0.694 (0.441)\tData 0.001 (0.029)\tLoss 2.5469 (2.2981)\tPrec@1 84.375 (85.785)\tPrec@5 99.219 (99.066)\n",
      "Epoch: [74][50/97], lr: 0.01000\tTime 0.598 (0.472)\tData 0.016 (0.025)\tLoss 1.8745 (2.2526)\tPrec@1 89.844 (86.091)\tPrec@5 98.438 (99.096)\n",
      "Epoch: [74][60/97], lr: 0.01000\tTime 0.464 (0.483)\tData 0.001 (0.022)\tLoss 2.8607 (2.2927)\tPrec@1 84.375 (85.963)\tPrec@5 100.000 (99.052)\n",
      "Epoch: [74][70/97], lr: 0.01000\tTime 0.555 (0.489)\tData 0.001 (0.021)\tLoss 2.4267 (2.3175)\tPrec@1 85.156 (85.750)\tPrec@5 99.219 (99.032)\n",
      "Epoch: [74][80/97], lr: 0.01000\tTime 0.463 (0.494)\tData 0.000 (0.020)\tLoss 1.4896 (2.3170)\tPrec@1 89.062 (85.696)\tPrec@5 100.000 (99.026)\n",
      "Epoch: [74][90/97], lr: 0.01000\tTime 0.461 (0.493)\tData 0.000 (0.019)\tLoss 2.0336 (2.3077)\tPrec@1 88.281 (85.723)\tPrec@5 100.000 (99.038)\n",
      "Epoch: [74][96/97], lr: 0.01000\tTime 0.405 (0.489)\tData 0.000 (0.019)\tLoss 2.6406 (2.3197)\tPrec@1 82.203 (85.652)\tPrec@5 99.153 (99.065)\n",
      "Gated Network Weight Gate= Flip:0.47, Sc:0.53\n",
      "Test: [0/100]\tTime 0.704 (0.704)\tLoss 7.5414 (7.5414)\tPrec@1 64.000 (64.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.086 (0.146)\tLoss 5.6162 (7.2622)\tPrec@1 71.000 (64.727)\tPrec@5 98.000 (96.364)\n",
      "Test: [20/100]\tTime 0.075 (0.120)\tLoss 5.4485 (7.1790)\tPrec@1 72.000 (64.571)\tPrec@5 97.000 (96.333)\n",
      "Test: [30/100]\tTime 0.074 (0.109)\tLoss 6.8898 (7.2176)\tPrec@1 66.000 (64.452)\tPrec@5 97.000 (96.000)\n",
      "Test: [40/100]\tTime 0.074 (0.100)\tLoss 7.3810 (7.2970)\tPrec@1 65.000 (63.927)\tPrec@5 96.000 (95.976)\n",
      "Test: [50/100]\tTime 0.074 (0.096)\tLoss 6.7639 (7.2126)\tPrec@1 70.000 (64.451)\tPrec@5 97.000 (96.039)\n",
      "Test: [60/100]\tTime 0.074 (0.093)\tLoss 5.9693 (7.1591)\tPrec@1 72.000 (64.787)\tPrec@5 98.000 (95.967)\n",
      "Test: [70/100]\tTime 0.074 (0.090)\tLoss 6.8427 (7.1781)\tPrec@1 67.000 (64.761)\tPrec@5 98.000 (95.958)\n",
      "Test: [80/100]\tTime 0.075 (0.088)\tLoss 6.5818 (7.1254)\tPrec@1 70.000 (65.074)\tPrec@5 98.000 (96.062)\n",
      "Test: [90/100]\tTime 0.077 (0.088)\tLoss 7.6965 (7.1888)\tPrec@1 60.000 (64.714)\tPrec@5 96.000 (96.099)\n",
      "val Results: Prec@1 64.720 Prec@5 96.030 Loss 7.20701\n",
      "val Class Accuracy: [0.947,0.959,0.730,0.623,0.592,0.721,0.846,0.572,0.331,0.151]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [75][0/97], lr: 0.01000\tTime 1.404 (1.404)\tData 0.866 (0.866)\tLoss 2.2051 (2.2051)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [75][10/97], lr: 0.01000\tTime 0.514 (0.568)\tData 0.001 (0.086)\tLoss 2.3073 (2.1025)\tPrec@1 85.156 (87.358)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [75][20/97], lr: 0.01000\tTime 0.409 (0.522)\tData 0.001 (0.050)\tLoss 2.9796 (2.1796)\tPrec@1 82.812 (86.905)\tPrec@5 96.094 (99.033)\n",
      "Epoch: [75][30/97], lr: 0.01000\tTime 0.457 (0.487)\tData 0.001 (0.038)\tLoss 2.4830 (2.2158)\tPrec@1 85.938 (86.668)\tPrec@5 100.000 (99.118)\n",
      "Epoch: [75][40/97], lr: 0.01000\tTime 0.449 (0.485)\tData 0.001 (0.032)\tLoss 1.7848 (2.1997)\tPrec@1 89.062 (86.623)\tPrec@5 100.000 (99.181)\n",
      "Epoch: [75][50/97], lr: 0.01000\tTime 0.378 (0.477)\tData 0.000 (0.028)\tLoss 2.2007 (2.2072)\tPrec@1 88.281 (86.673)\tPrec@5 99.219 (99.142)\n",
      "Epoch: [75][60/97], lr: 0.01000\tTime 0.370 (0.462)\tData 0.001 (0.026)\tLoss 1.8954 (2.2223)\tPrec@1 86.719 (86.603)\tPrec@5 99.219 (99.193)\n",
      "Epoch: [75][70/97], lr: 0.01000\tTime 0.371 (0.451)\tData 0.000 (0.025)\tLoss 2.2653 (2.1865)\tPrec@1 86.719 (86.906)\tPrec@5 98.438 (99.153)\n",
      "Epoch: [75][80/97], lr: 0.01000\tTime 0.362 (0.441)\tData 0.000 (0.024)\tLoss 2.4241 (2.1953)\tPrec@1 86.719 (86.844)\tPrec@5 100.000 (99.171)\n",
      "Epoch: [75][90/97], lr: 0.01000\tTime 0.367 (0.433)\tData 0.000 (0.023)\tLoss 2.5047 (2.2246)\tPrec@1 85.156 (86.599)\tPrec@5 99.219 (99.124)\n",
      "Epoch: [75][96/97], lr: 0.01000\tTime 0.361 (0.430)\tData 0.000 (0.023)\tLoss 2.6366 (2.2345)\tPrec@1 83.051 (86.498)\tPrec@5 100.000 (99.146)\n",
      "Gated Network Weight Gate= Flip:0.60, Sc:0.40\n",
      "Test: [0/100]\tTime 0.462 (0.462)\tLoss 9.2675 (9.2675)\tPrec@1 58.000 (58.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.109)\tLoss 6.4502 (8.7377)\tPrec@1 67.000 (56.909)\tPrec@5 94.000 (93.818)\n",
      "Test: [20/100]\tTime 0.075 (0.093)\tLoss 7.3916 (8.8666)\tPrec@1 61.000 (55.286)\tPrec@5 97.000 (93.524)\n",
      "Test: [30/100]\tTime 0.074 (0.087)\tLoss 8.7306 (8.8813)\tPrec@1 54.000 (55.097)\tPrec@5 93.000 (93.323)\n",
      "Test: [40/100]\tTime 0.089 (0.085)\tLoss 8.9878 (8.9363)\tPrec@1 55.000 (55.024)\tPrec@5 92.000 (93.073)\n",
      "Test: [50/100]\tTime 0.075 (0.084)\tLoss 8.0632 (8.8400)\tPrec@1 60.000 (55.706)\tPrec@5 94.000 (93.275)\n",
      "Test: [60/100]\tTime 0.075 (0.082)\tLoss 7.2365 (8.7981)\tPrec@1 64.000 (55.951)\tPrec@5 93.000 (93.180)\n",
      "Test: [70/100]\tTime 0.074 (0.081)\tLoss 9.1106 (8.8007)\tPrec@1 55.000 (55.873)\tPrec@5 94.000 (93.169)\n",
      "Test: [80/100]\tTime 0.076 (0.081)\tLoss 8.8705 (8.7659)\tPrec@1 57.000 (56.025)\tPrec@5 91.000 (93.284)\n",
      "Test: [90/100]\tTime 0.077 (0.080)\tLoss 8.5363 (8.8080)\tPrec@1 60.000 (55.857)\tPrec@5 94.000 (93.286)\n",
      "val Results: Prec@1 55.900 Prec@5 93.310 Loss 8.82631\n",
      "val Class Accuracy: [0.967,0.989,0.565,0.642,0.492,0.590,0.780,0.267,0.274,0.024]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [76][0/97], lr: 0.01000\tTime 1.602 (1.602)\tData 0.975 (0.975)\tLoss 2.1645 (2.1645)\tPrec@1 84.375 (84.375)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [76][10/97], lr: 0.01000\tTime 0.370 (0.505)\tData 0.001 (0.099)\tLoss 2.5942 (2.2880)\tPrec@1 85.156 (86.151)\tPrec@5 97.656 (98.935)\n",
      "Epoch: [76][20/97], lr: 0.01000\tTime 0.469 (0.474)\tData 0.001 (0.059)\tLoss 2.7764 (2.2842)\tPrec@1 82.031 (86.272)\tPrec@5 98.438 (98.958)\n",
      "Epoch: [76][30/97], lr: 0.01000\tTime 0.426 (0.460)\tData 0.001 (0.044)\tLoss 2.1776 (2.3103)\tPrec@1 85.938 (85.912)\tPrec@5 98.438 (99.017)\n",
      "Epoch: [76][40/97], lr: 0.01000\tTime 0.399 (0.451)\tData 0.001 (0.037)\tLoss 2.6886 (2.3070)\tPrec@1 80.469 (85.842)\tPrec@5 99.219 (98.952)\n",
      "Epoch: [76][50/97], lr: 0.01000\tTime 0.416 (0.444)\tData 0.000 (0.033)\tLoss 1.9118 (2.2999)\tPrec@1 86.719 (85.754)\tPrec@5 97.656 (98.989)\n",
      "Epoch: [76][60/97], lr: 0.01000\tTime 0.395 (0.442)\tData 0.000 (0.029)\tLoss 2.4613 (2.3025)\tPrec@1 85.156 (85.745)\tPrec@5 99.219 (99.039)\n",
      "Epoch: [76][70/97], lr: 0.01000\tTime 0.375 (0.435)\tData 0.000 (0.027)\tLoss 3.2393 (2.3204)\tPrec@1 78.906 (85.651)\tPrec@5 99.219 (99.021)\n",
      "Epoch: [76][80/97], lr: 0.01000\tTime 0.393 (0.431)\tData 0.001 (0.026)\tLoss 1.8614 (2.2848)\tPrec@1 89.062 (85.957)\tPrec@5 99.219 (99.055)\n",
      "Epoch: [76][90/97], lr: 0.01000\tTime 0.425 (0.431)\tData 0.000 (0.025)\tLoss 2.5074 (2.2959)\tPrec@1 83.594 (85.860)\tPrec@5 100.000 (99.047)\n",
      "Epoch: [76][96/97], lr: 0.01000\tTime 0.343 (0.427)\tData 0.000 (0.024)\tLoss 1.7612 (2.2942)\tPrec@1 88.136 (85.821)\tPrec@5 99.153 (99.073)\n",
      "Gated Network Weight Gate= Flip:0.54, Sc:0.46\n",
      "Test: [0/100]\tTime 0.502 (0.502)\tLoss 10.2230 (10.2230)\tPrec@1 51.000 (51.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.079 (0.114)\tLoss 7.7066 (9.0955)\tPrec@1 65.000 (57.182)\tPrec@5 89.000 (93.818)\n",
      "Test: [20/100]\tTime 0.074 (0.097)\tLoss 7.5283 (9.1197)\tPrec@1 62.000 (56.429)\tPrec@5 93.000 (94.143)\n",
      "Test: [30/100]\tTime 0.075 (0.093)\tLoss 8.2473 (9.0867)\tPrec@1 59.000 (56.290)\tPrec@5 94.000 (94.290)\n",
      "Test: [40/100]\tTime 0.104 (0.090)\tLoss 9.0186 (9.0673)\tPrec@1 55.000 (56.317)\tPrec@5 93.000 (94.146)\n",
      "Test: [50/100]\tTime 0.076 (0.088)\tLoss 8.2924 (9.0200)\tPrec@1 61.000 (56.588)\tPrec@5 95.000 (94.216)\n",
      "Test: [60/100]\tTime 0.074 (0.087)\tLoss 7.6247 (9.0325)\tPrec@1 65.000 (56.459)\tPrec@5 93.000 (94.082)\n",
      "Test: [70/100]\tTime 0.075 (0.086)\tLoss 9.5307 (9.0297)\tPrec@1 53.000 (56.380)\tPrec@5 95.000 (94.042)\n",
      "Test: [80/100]\tTime 0.085 (0.085)\tLoss 8.6756 (8.9770)\tPrec@1 56.000 (56.679)\tPrec@5 95.000 (94.160)\n",
      "Test: [90/100]\tTime 0.075 (0.084)\tLoss 9.7470 (9.0560)\tPrec@1 53.000 (56.429)\tPrec@5 91.000 (94.055)\n",
      "val Results: Prec@1 56.310 Prec@5 94.130 Loss 9.08627\n",
      "val Class Accuracy: [0.921,0.994,0.678,0.834,0.675,0.387,0.447,0.402,0.218,0.075]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [77][0/97], lr: 0.01000\tTime 1.123 (1.123)\tData 0.667 (0.667)\tLoss 2.2401 (2.2401)\tPrec@1 86.719 (86.719)\tPrec@5 100.000 (100.000)\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"cifar_train_lorot-E.py\", line 656, in <module>\n",
      "    main()\n",
      "  File \"cifar_train_lorot-E.py\", line 181, in main\n",
      "    main_worker(args.gpu, ngpus_per_node, args)\n",
      "  File \"cifar_train_lorot-E.py\", line 361, in main_worker\n",
      "    train(\n",
      "  File \"cifar_train_lorot-E.py\", line 509, in train\n",
      "    loss.backward()\n",
      "  File \"/home/aristo/miniconda3/envs/pytorch-test/lib/python3.8/site-packages/torch/_tensor.py\", line 396, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/home/aristo/miniconda3/envs/pytorch-test/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 173, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python cifar_train_lorot-E.py --gpu 0 --imb_type exp --exp_str flipscLdamDrw --imb_factor 0.01 --loss_type LDAM --train_rule DRW -g -m \"fliplr sc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar_train_lorot-E.py:175: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.\n",
      "  warnings.warn(\n",
      "Use GPU: 0 for training\n",
      "=> creating model 'resnet32'\n",
      "num_trans : 16\n",
      "num_flipped : 4\n",
      "num shuffled channel : 24\n",
      "=> loading checkpoint '/media/aristo/Data A/documents/kuliah/Project/Localizable-Rotation/Imbalanced/checkpoint/cifar10_resnet32_LDAM_DRW_exp_0.01_flipscLdamDrw/ckpt.pth.tar'\n",
      "=> loaded checkpoint '/media/aristo/Data A/documents/kuliah/Project/Localizable-Rotation/Imbalanced/checkpoint/cifar10_resnet32_LDAM_DRW_exp_0.01_flipscLdamDrw/ckpt.pth.tar' (epoch 77)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[5000, 2997, 1796, 1077, 645, 387, 232, 139, 83, 50]\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "/media/aristo/Data A/documents/kuliah/Project/Localizable-Rotation/Imbalanced/losses.py:49: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/TensorCompare.cpp:402.)\n",
      "  output = torch.where(index, x_m, x)\n",
      "Epoch: [77][0/97], lr: 0.01000\tTime 4.333 (4.333)\tData 0.174 (0.174)\tLoss 1.8926 (1.8926)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [77][10/97], lr: 0.01000\tTime 0.345 (0.682)\tData 0.000 (0.016)\tLoss 1.7144 (2.1847)\tPrec@1 89.844 (87.216)\tPrec@5 100.000 (99.077)\n",
      "Epoch: [77][20/97], lr: 0.01000\tTime 0.341 (0.522)\tData 0.000 (0.016)\tLoss 2.4127 (2.2918)\tPrec@1 82.812 (86.124)\tPrec@5 99.219 (98.884)\n",
      "Epoch: [77][30/97], lr: 0.01000\tTime 0.346 (0.468)\tData 0.000 (0.016)\tLoss 2.7837 (2.2653)\tPrec@1 83.594 (86.190)\tPrec@5 97.656 (99.093)\n",
      "Epoch: [77][40/97], lr: 0.01000\tTime 0.348 (0.439)\tData 0.001 (0.016)\tLoss 1.8919 (2.2158)\tPrec@1 87.500 (86.414)\tPrec@5 100.000 (99.085)\n",
      "Epoch: [77][50/97], lr: 0.01000\tTime 0.343 (0.421)\tData 0.000 (0.016)\tLoss 2.3357 (2.2469)\tPrec@1 82.812 (86.275)\tPrec@5 99.219 (99.004)\n",
      "Epoch: [77][60/97], lr: 0.01000\tTime 0.344 (0.410)\tData 0.000 (0.016)\tLoss 2.1187 (2.2203)\tPrec@1 88.281 (86.373)\tPrec@5 99.219 (99.039)\n",
      "Epoch: [77][70/97], lr: 0.01000\tTime 0.349 (0.402)\tData 0.000 (0.016)\tLoss 3.0355 (2.2611)\tPrec@1 82.031 (86.202)\tPrec@5 98.438 (99.054)\n",
      "Epoch: [77][80/97], lr: 0.01000\tTime 0.347 (0.396)\tData 0.000 (0.016)\tLoss 1.9215 (2.2612)\tPrec@1 88.281 (86.208)\tPrec@5 100.000 (99.084)\n",
      "Epoch: [77][90/97], lr: 0.01000\tTime 0.340 (0.391)\tData 0.000 (0.016)\tLoss 1.9105 (2.2479)\tPrec@1 86.719 (86.221)\tPrec@5 100.000 (99.099)\n",
      "Epoch: [77][96/97], lr: 0.01000\tTime 0.932 (0.394)\tData 0.000 (0.017)\tLoss 3.0273 (2.2338)\tPrec@1 80.508 (86.265)\tPrec@5 98.305 (99.105)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.366 (0.366)\tLoss 9.5603 (9.5603)\tPrec@1 54.000 (54.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.100)\tLoss 6.7419 (8.4486)\tPrec@1 67.000 (58.545)\tPrec@5 96.000 (93.636)\n",
      "Test: [20/100]\tTime 0.073 (0.087)\tLoss 6.6362 (8.2160)\tPrec@1 68.000 (60.238)\tPrec@5 96.000 (94.000)\n",
      "Test: [30/100]\tTime 0.074 (0.083)\tLoss 7.3126 (8.2561)\tPrec@1 62.000 (59.968)\tPrec@5 94.000 (94.097)\n",
      "Test: [40/100]\tTime 0.074 (0.081)\tLoss 8.0677 (8.2587)\tPrec@1 62.000 (60.098)\tPrec@5 94.000 (93.976)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 7.6205 (8.1691)\tPrec@1 64.000 (60.745)\tPrec@5 95.000 (93.922)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 6.9374 (8.1222)\tPrec@1 66.000 (60.918)\tPrec@5 93.000 (93.770)\n",
      "Test: [70/100]\tTime 0.073 (0.078)\tLoss 7.9029 (8.0978)\tPrec@1 65.000 (61.085)\tPrec@5 93.000 (93.634)\n",
      "Test: [80/100]\tTime 0.073 (0.077)\tLoss 7.3231 (8.0505)\tPrec@1 65.000 (61.148)\tPrec@5 92.000 (93.642)\n",
      "Test: [90/100]\tTime 0.073 (0.077)\tLoss 7.9838 (8.1047)\tPrec@1 64.000 (61.066)\tPrec@5 95.000 (93.560)\n",
      "val Results: Prec@1 60.790 Prec@5 93.610 Loss 8.15094\n",
      "val Class Accuracy: [0.801,0.996,0.759,0.571,0.759,0.695,0.746,0.467,0.268,0.017]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [78][0/97], lr: 0.01000\tTime 0.678 (0.678)\tData 0.381 (0.381)\tLoss 1.9123 (1.9123)\tPrec@1 88.281 (88.281)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [78][10/97], lr: 0.01000\tTime 0.351 (0.396)\tData 0.000 (0.047)\tLoss 3.0058 (2.2987)\tPrec@1 82.812 (85.866)\tPrec@5 97.656 (99.219)\n",
      "Epoch: [78][20/97], lr: 0.01000\tTime 0.361 (0.378)\tData 0.000 (0.032)\tLoss 2.4935 (2.1801)\tPrec@1 86.719 (87.277)\tPrec@5 98.438 (99.256)\n",
      "Epoch: [78][30/97], lr: 0.01000\tTime 0.348 (0.370)\tData 0.000 (0.027)\tLoss 2.2902 (2.1577)\tPrec@1 85.156 (87.046)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [78][40/97], lr: 0.01000\tTime 0.345 (0.366)\tData 0.000 (0.024)\tLoss 2.2689 (2.1615)\tPrec@1 85.156 (86.947)\tPrec@5 100.000 (99.162)\n",
      "Epoch: [78][50/97], lr: 0.01000\tTime 0.343 (0.363)\tData 0.000 (0.023)\tLoss 1.5825 (2.1604)\tPrec@1 92.188 (86.933)\tPrec@5 98.438 (99.188)\n",
      "Epoch: [78][60/97], lr: 0.01000\tTime 0.343 (0.361)\tData 0.000 (0.022)\tLoss 2.0835 (2.1242)\tPrec@1 88.281 (87.154)\tPrec@5 100.000 (99.232)\n",
      "Epoch: [78][70/97], lr: 0.01000\tTime 0.347 (0.360)\tData 0.000 (0.021)\tLoss 2.7223 (2.1576)\tPrec@1 83.594 (86.983)\tPrec@5 98.438 (99.131)\n",
      "Epoch: [78][80/97], lr: 0.01000\tTime 0.362 (0.359)\tData 0.000 (0.020)\tLoss 2.6091 (2.1815)\tPrec@1 82.031 (86.757)\tPrec@5 99.219 (99.132)\n",
      "Epoch: [78][90/97], lr: 0.01000\tTime 0.341 (0.358)\tData 0.000 (0.020)\tLoss 2.7601 (2.2030)\tPrec@1 82.812 (86.710)\tPrec@5 100.000 (99.141)\n",
      "Epoch: [78][96/97], lr: 0.01000\tTime 0.333 (0.357)\tData 0.000 (0.020)\tLoss 2.3141 (2.2181)\tPrec@1 86.441 (86.660)\tPrec@5 100.000 (99.113)\n",
      "Gated Network Weight Gate= Flip:0.61, Sc:0.39\n",
      "Test: [0/100]\tTime 0.318 (0.318)\tLoss 8.7959 (8.7959)\tPrec@1 56.000 (56.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.074 (0.096)\tLoss 6.9108 (8.5351)\tPrec@1 64.000 (56.636)\tPrec@5 96.000 (95.000)\n",
      "Test: [20/100]\tTime 0.074 (0.085)\tLoss 8.3355 (8.5139)\tPrec@1 56.000 (57.048)\tPrec@5 97.000 (94.810)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 7.2827 (8.5100)\tPrec@1 65.000 (57.452)\tPrec@5 95.000 (94.871)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 9.4723 (8.5189)\tPrec@1 53.000 (57.537)\tPrec@5 92.000 (94.634)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 8.3464 (8.4829)\tPrec@1 57.000 (57.569)\tPrec@5 93.000 (94.667)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 6.7217 (8.4723)\tPrec@1 65.000 (57.393)\tPrec@5 95.000 (94.459)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 8.6160 (8.4555)\tPrec@1 56.000 (57.352)\tPrec@5 93.000 (94.535)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 7.7501 (8.4213)\tPrec@1 60.000 (57.444)\tPrec@5 96.000 (94.716)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 8.2373 (8.4781)\tPrec@1 62.000 (57.154)\tPrec@5 98.000 (94.604)\n",
      "val Results: Prec@1 57.110 Prec@5 94.660 Loss 8.50226\n",
      "val Class Accuracy: [0.963,0.973,0.748,0.543,0.558,0.342,0.568,0.395,0.590,0.031]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [79][0/97], lr: 0.01000\tTime 0.688 (0.688)\tData 0.374 (0.374)\tLoss 1.7140 (1.7140)\tPrec@1 92.188 (92.188)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [79][10/97], lr: 0.01000\tTime 0.353 (0.400)\tData 0.000 (0.047)\tLoss 2.8202 (2.5358)\tPrec@1 81.250 (84.659)\tPrec@5 99.219 (98.722)\n",
      "Epoch: [79][20/97], lr: 0.01000\tTime 0.348 (0.378)\tData 0.000 (0.032)\tLoss 2.1715 (2.3950)\tPrec@1 87.500 (85.305)\tPrec@5 98.438 (98.958)\n",
      "Epoch: [79][30/97], lr: 0.01000\tTime 0.363 (0.374)\tData 0.000 (0.027)\tLoss 3.2299 (2.3426)\tPrec@1 80.469 (85.736)\tPrec@5 97.656 (98.967)\n",
      "Epoch: [79][40/97], lr: 0.01000\tTime 0.360 (0.372)\tData 0.000 (0.024)\tLoss 1.7001 (2.2961)\tPrec@1 90.625 (86.223)\tPrec@5 100.000 (99.104)\n",
      "Epoch: [79][50/97], lr: 0.01000\tTime 0.354 (0.370)\tData 0.000 (0.022)\tLoss 2.1180 (2.2635)\tPrec@1 86.719 (86.397)\tPrec@5 100.000 (99.188)\n",
      "Epoch: [79][60/97], lr: 0.01000\tTime 0.346 (0.367)\tData 0.000 (0.021)\tLoss 2.4293 (2.2592)\tPrec@1 83.594 (86.386)\tPrec@5 100.000 (99.270)\n",
      "Epoch: [79][70/97], lr: 0.01000\tTime 0.364 (0.367)\tData 0.000 (0.021)\tLoss 2.0372 (2.2296)\tPrec@1 88.281 (86.554)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [79][80/97], lr: 0.01000\tTime 0.349 (0.366)\tData 0.000 (0.020)\tLoss 2.7383 (2.2465)\tPrec@1 81.250 (86.362)\tPrec@5 100.000 (99.315)\n",
      "Epoch: [79][90/97], lr: 0.01000\tTime 0.363 (0.364)\tData 0.000 (0.020)\tLoss 2.0092 (2.2575)\tPrec@1 89.062 (86.289)\tPrec@5 100.000 (99.305)\n",
      "Epoch: [79][96/97], lr: 0.01000\tTime 0.327 (0.363)\tData 0.000 (0.020)\tLoss 2.5384 (2.2803)\tPrec@1 84.746 (86.087)\tPrec@5 98.305 (99.266)\n",
      "Gated Network Weight Gate= Flip:0.51, Sc:0.49\n",
      "Test: [0/100]\tTime 0.320 (0.320)\tLoss 8.5030 (8.5030)\tPrec@1 60.000 (60.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.074 (0.096)\tLoss 6.3605 (7.9091)\tPrec@1 67.000 (62.727)\tPrec@5 95.000 (94.636)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 7.4259 (7.8074)\tPrec@1 64.000 (62.333)\tPrec@5 96.000 (95.143)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 7.0566 (7.8368)\tPrec@1 65.000 (61.581)\tPrec@5 94.000 (95.387)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 8.0089 (7.8566)\tPrec@1 61.000 (61.927)\tPrec@5 92.000 (95.146)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 7.3236 (7.7657)\tPrec@1 63.000 (62.196)\tPrec@5 95.000 (95.235)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 7.0182 (7.7537)\tPrec@1 65.000 (62.131)\tPrec@5 95.000 (95.230)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 7.5426 (7.7701)\tPrec@1 63.000 (62.113)\tPrec@5 96.000 (95.099)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 7.6528 (7.7208)\tPrec@1 63.000 (62.296)\tPrec@5 93.000 (95.160)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 8.3232 (7.8094)\tPrec@1 59.000 (61.747)\tPrec@5 96.000 (94.857)\n",
      "val Results: Prec@1 61.710 Prec@5 94.890 Loss 7.82769\n",
      "val Class Accuracy: [0.841,0.948,0.611,0.770,0.860,0.600,0.531,0.412,0.287,0.311]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [80][0/97], lr: 0.01000\tTime 0.707 (0.707)\tData 0.402 (0.402)\tLoss 2.4386 (2.4386)\tPrec@1 83.594 (83.594)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [80][10/97], lr: 0.01000\tTime 0.354 (0.413)\tData 0.000 (0.049)\tLoss 2.5256 (2.3325)\tPrec@1 82.812 (85.511)\tPrec@5 97.656 (98.935)\n",
      "Epoch: [80][20/97], lr: 0.01000\tTime 0.361 (0.393)\tData 0.000 (0.033)\tLoss 2.0184 (2.1634)\tPrec@1 87.500 (86.570)\tPrec@5 100.000 (99.144)\n",
      "Epoch: [80][30/97], lr: 0.01000\tTime 0.350 (0.380)\tData 0.000 (0.028)\tLoss 2.6194 (2.2677)\tPrec@1 85.156 (86.064)\tPrec@5 97.656 (99.219)\n",
      "Epoch: [80][40/97], lr: 0.01000\tTime 0.354 (0.376)\tData 0.000 (0.025)\tLoss 2.5839 (2.2147)\tPrec@1 84.375 (86.566)\tPrec@5 99.219 (99.238)\n",
      "Epoch: [80][50/97], lr: 0.01000\tTime 0.347 (0.373)\tData 0.000 (0.023)\tLoss 1.4797 (2.1995)\tPrec@1 92.188 (86.765)\tPrec@5 100.000 (99.157)\n",
      "Epoch: [80][60/97], lr: 0.01000\tTime 0.350 (0.370)\tData 0.000 (0.022)\tLoss 1.4249 (2.1533)\tPrec@1 92.188 (87.052)\tPrec@5 100.000 (99.129)\n",
      "Epoch: [80][70/97], lr: 0.01000\tTime 0.347 (0.369)\tData 0.000 (0.021)\tLoss 3.0810 (2.1893)\tPrec@1 79.688 (86.752)\tPrec@5 97.656 (99.142)\n",
      "Epoch: [80][80/97], lr: 0.01000\tTime 0.360 (0.369)\tData 0.000 (0.020)\tLoss 2.2531 (2.1726)\tPrec@1 86.719 (86.777)\tPrec@5 100.000 (99.171)\n",
      "Epoch: [80][90/97], lr: 0.01000\tTime 0.341 (0.366)\tData 0.000 (0.020)\tLoss 2.4233 (2.1804)\tPrec@1 86.719 (86.693)\tPrec@5 98.438 (99.150)\n",
      "Epoch: [80][96/97], lr: 0.01000\tTime 0.327 (0.365)\tData 0.000 (0.020)\tLoss 2.2432 (2.1834)\tPrec@1 84.746 (86.700)\tPrec@5 98.305 (99.138)\n",
      "Gated Network Weight Gate= Flip:0.60, Sc:0.40\n",
      "Test: [0/100]\tTime 0.338 (0.338)\tLoss 9.4013 (9.4013)\tPrec@1 53.000 (53.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.074 (0.098)\tLoss 7.1117 (8.5157)\tPrec@1 62.000 (57.273)\tPrec@5 96.000 (95.091)\n",
      "Test: [20/100]\tTime 0.074 (0.086)\tLoss 6.6841 (8.5034)\tPrec@1 65.000 (57.524)\tPrec@5 96.000 (94.952)\n",
      "Test: [30/100]\tTime 0.073 (0.082)\tLoss 8.5014 (8.5246)\tPrec@1 54.000 (57.323)\tPrec@5 95.000 (95.226)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 8.1378 (8.5257)\tPrec@1 60.000 (57.512)\tPrec@5 94.000 (94.951)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 7.8941 (8.4657)\tPrec@1 64.000 (57.804)\tPrec@5 94.000 (95.020)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 6.9634 (8.4460)\tPrec@1 64.000 (57.623)\tPrec@5 96.000 (95.148)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 8.7514 (8.4404)\tPrec@1 54.000 (57.606)\tPrec@5 95.000 (95.211)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 7.5848 (8.4072)\tPrec@1 63.000 (57.716)\tPrec@5 97.000 (95.370)\n",
      "Test: [90/100]\tTime 0.073 (0.077)\tLoss 8.4605 (8.4786)\tPrec@1 58.000 (57.242)\tPrec@5 96.000 (95.209)\n",
      "val Results: Prec@1 57.260 Prec@5 95.190 Loss 8.48594\n",
      "val Class Accuracy: [0.981,0.961,0.738,0.392,0.644,0.567,0.534,0.501,0.395,0.013]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [81][0/97], lr: 0.01000\tTime 0.769 (0.769)\tData 0.451 (0.451)\tLoss 1.6029 (1.6029)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [81][10/97], lr: 0.01000\tTime 0.361 (0.438)\tData 0.000 (0.053)\tLoss 1.8639 (2.0376)\tPrec@1 86.719 (87.003)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [81][20/97], lr: 0.01000\tTime 0.364 (0.402)\tData 0.000 (0.035)\tLoss 3.0325 (2.2122)\tPrec@1 82.812 (85.938)\tPrec@5 100.000 (99.554)\n",
      "Epoch: [81][30/97], lr: 0.01000\tTime 0.359 (0.389)\tData 0.000 (0.029)\tLoss 2.2055 (2.2141)\tPrec@1 87.500 (86.089)\tPrec@5 99.219 (99.420)\n",
      "Epoch: [81][40/97], lr: 0.01000\tTime 0.355 (0.383)\tData 0.001 (0.026)\tLoss 2.0229 (2.2373)\tPrec@1 89.844 (86.185)\tPrec@5 98.438 (99.371)\n",
      "Epoch: [81][50/97], lr: 0.01000\tTime 0.369 (0.380)\tData 0.000 (0.024)\tLoss 3.1308 (2.2598)\tPrec@1 81.250 (86.167)\tPrec@5 100.000 (99.372)\n",
      "Epoch: [81][60/97], lr: 0.01000\tTime 0.348 (0.377)\tData 0.000 (0.022)\tLoss 2.9032 (2.3020)\tPrec@1 82.812 (86.014)\tPrec@5 98.438 (99.334)\n",
      "Epoch: [81][70/97], lr: 0.01000\tTime 0.339 (0.374)\tData 0.000 (0.022)\tLoss 2.3379 (2.2814)\tPrec@1 82.812 (86.059)\tPrec@5 99.219 (99.285)\n",
      "Epoch: [81][80/97], lr: 0.01000\tTime 0.340 (0.371)\tData 0.000 (0.021)\tLoss 1.4929 (2.2711)\tPrec@1 90.625 (86.111)\tPrec@5 99.219 (99.180)\n",
      "Epoch: [81][90/97], lr: 0.01000\tTime 0.340 (0.368)\tData 0.000 (0.020)\tLoss 1.7550 (2.2710)\tPrec@1 90.625 (86.135)\tPrec@5 99.219 (99.167)\n",
      "Epoch: [81][96/97], lr: 0.01000\tTime 0.331 (0.367)\tData 0.000 (0.021)\tLoss 2.2659 (2.2868)\tPrec@1 87.288 (86.039)\tPrec@5 98.305 (99.170)\n",
      "Gated Network Weight Gate= Flip:0.50, Sc:0.50\n",
      "Test: [0/100]\tTime 0.332 (0.332)\tLoss 9.0362 (9.0362)\tPrec@1 58.000 (58.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.097)\tLoss 7.1165 (7.9153)\tPrec@1 63.000 (60.273)\tPrec@5 92.000 (95.182)\n",
      "Test: [20/100]\tTime 0.074 (0.086)\tLoss 6.8564 (7.7735)\tPrec@1 61.000 (61.286)\tPrec@5 98.000 (95.762)\n",
      "Test: [30/100]\tTime 0.075 (0.082)\tLoss 7.1544 (7.8229)\tPrec@1 63.000 (60.774)\tPrec@5 96.000 (95.839)\n",
      "Test: [40/100]\tTime 0.074 (0.080)\tLoss 8.3173 (7.8253)\tPrec@1 59.000 (61.024)\tPrec@5 93.000 (95.561)\n",
      "Test: [50/100]\tTime 0.075 (0.079)\tLoss 7.6190 (7.7496)\tPrec@1 66.000 (61.353)\tPrec@5 95.000 (95.706)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 6.0389 (7.7476)\tPrec@1 73.000 (61.262)\tPrec@5 94.000 (95.721)\n",
      "Test: [70/100]\tTime 0.074 (0.078)\tLoss 7.9166 (7.7550)\tPrec@1 56.000 (61.296)\tPrec@5 98.000 (95.746)\n",
      "Test: [80/100]\tTime 0.075 (0.077)\tLoss 7.5892 (7.6931)\tPrec@1 62.000 (61.531)\tPrec@5 95.000 (95.778)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 8.5987 (7.7716)\tPrec@1 60.000 (61.187)\tPrec@5 99.000 (95.648)\n",
      "val Results: Prec@1 61.080 Prec@5 95.650 Loss 7.80129\n",
      "val Class Accuracy: [0.967,0.968,0.710,0.725,0.703,0.617,0.459,0.493,0.428,0.038]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [82][0/97], lr: 0.01000\tTime 0.887 (0.887)\tData 0.457 (0.457)\tLoss 2.0200 (2.0200)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [82][10/97], lr: 0.01000\tTime 0.355 (0.440)\tData 0.000 (0.053)\tLoss 1.8738 (1.9509)\tPrec@1 85.938 (88.281)\tPrec@5 99.219 (99.503)\n",
      "Epoch: [82][20/97], lr: 0.01000\tTime 0.355 (0.403)\tData 0.000 (0.035)\tLoss 1.6780 (2.0557)\tPrec@1 89.844 (87.574)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [82][30/97], lr: 0.01000\tTime 0.365 (0.390)\tData 0.000 (0.029)\tLoss 2.0549 (2.1040)\tPrec@1 85.938 (87.046)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [82][40/97], lr: 0.01000\tTime 0.354 (0.384)\tData 0.000 (0.026)\tLoss 2.5753 (2.1605)\tPrec@1 85.156 (86.757)\tPrec@5 98.438 (99.371)\n",
      "Epoch: [82][50/97], lr: 0.01000\tTime 0.357 (0.379)\tData 0.000 (0.024)\tLoss 1.6028 (2.1583)\tPrec@1 89.844 (86.826)\tPrec@5 100.000 (99.387)\n",
      "Epoch: [82][60/97], lr: 0.01000\tTime 0.378 (0.378)\tData 0.000 (0.022)\tLoss 2.4718 (2.1635)\tPrec@1 85.156 (86.860)\tPrec@5 99.219 (99.347)\n",
      "Epoch: [82][70/97], lr: 0.01000\tTime 0.347 (0.375)\tData 0.000 (0.021)\tLoss 1.6964 (2.1659)\tPrec@1 89.844 (86.851)\tPrec@5 100.000 (99.351)\n",
      "Epoch: [82][80/97], lr: 0.01000\tTime 0.352 (0.374)\tData 0.000 (0.021)\tLoss 2.5376 (2.1692)\tPrec@1 84.375 (86.786)\tPrec@5 99.219 (99.334)\n",
      "Epoch: [82][90/97], lr: 0.01000\tTime 0.352 (0.373)\tData 0.000 (0.020)\tLoss 1.6450 (2.1533)\tPrec@1 91.406 (86.848)\tPrec@5 100.000 (99.287)\n",
      "Epoch: [82][96/97], lr: 0.01000\tTime 0.329 (0.371)\tData 0.000 (0.021)\tLoss 2.2990 (2.1519)\tPrec@1 86.441 (86.877)\tPrec@5 99.153 (99.258)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.317 (0.317)\tLoss 10.1222 (10.1222)\tPrec@1 52.000 (52.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.075 (0.096)\tLoss 7.2808 (9.1663)\tPrec@1 66.000 (55.455)\tPrec@5 92.000 (91.636)\n",
      "Test: [20/100]\tTime 0.075 (0.085)\tLoss 8.1981 (9.1035)\tPrec@1 60.000 (55.952)\tPrec@5 92.000 (91.571)\n",
      "Test: [30/100]\tTime 0.073 (0.082)\tLoss 8.7578 (9.1428)\tPrec@1 55.000 (55.903)\tPrec@5 95.000 (91.452)\n",
      "Test: [40/100]\tTime 0.074 (0.080)\tLoss 9.1232 (9.1361)\tPrec@1 52.000 (56.122)\tPrec@5 93.000 (91.220)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 8.7374 (9.0319)\tPrec@1 57.000 (56.647)\tPrec@5 94.000 (91.373)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 6.7061 (9.0012)\tPrec@1 70.000 (56.656)\tPrec@5 94.000 (91.557)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 8.2637 (8.9969)\tPrec@1 60.000 (56.803)\tPrec@5 94.000 (91.437)\n",
      "Test: [80/100]\tTime 0.075 (0.077)\tLoss 8.9649 (8.9383)\tPrec@1 58.000 (57.247)\tPrec@5 89.000 (91.506)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 9.2205 (9.0169)\tPrec@1 55.000 (56.912)\tPrec@5 94.000 (91.385)\n",
      "val Results: Prec@1 56.930 Prec@5 91.370 Loss 9.03518\n",
      "val Class Accuracy: [0.861,0.951,0.788,0.866,0.688,0.550,0.465,0.377,0.101,0.046]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [83][0/97], lr: 0.01000\tTime 0.813 (0.813)\tData 0.489 (0.489)\tLoss 2.1767 (2.1767)\tPrec@1 89.062 (89.062)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [83][10/97], lr: 0.01000\tTime 0.365 (0.433)\tData 0.000 (0.055)\tLoss 2.1041 (2.1834)\tPrec@1 84.375 (86.435)\tPrec@5 98.438 (99.006)\n",
      "Epoch: [83][20/97], lr: 0.01000\tTime 0.362 (0.403)\tData 0.000 (0.036)\tLoss 1.8905 (2.0504)\tPrec@1 89.844 (87.835)\tPrec@5 99.219 (99.182)\n",
      "Epoch: [83][30/97], lr: 0.01000\tTime 0.354 (0.392)\tData 0.000 (0.029)\tLoss 2.0031 (2.0389)\tPrec@1 89.062 (87.802)\tPrec@5 100.000 (99.320)\n",
      "Epoch: [83][40/97], lr: 0.01000\tTime 0.362 (0.387)\tData 0.000 (0.026)\tLoss 1.6301 (2.0592)\tPrec@1 90.625 (87.671)\tPrec@5 99.219 (99.333)\n",
      "Epoch: [83][50/97], lr: 0.01000\tTime 0.370 (0.384)\tData 0.000 (0.024)\tLoss 2.5529 (2.1232)\tPrec@1 83.594 (87.240)\tPrec@5 96.875 (99.280)\n",
      "Epoch: [83][60/97], lr: 0.01000\tTime 0.416 (0.382)\tData 0.000 (0.023)\tLoss 2.1604 (2.1113)\tPrec@1 88.281 (87.398)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [83][70/97], lr: 0.01000\tTime 0.368 (0.380)\tData 0.000 (0.022)\tLoss 2.6650 (2.1291)\tPrec@1 82.031 (87.236)\tPrec@5 99.219 (99.241)\n",
      "Epoch: [83][80/97], lr: 0.01000\tTime 0.370 (0.379)\tData 0.000 (0.021)\tLoss 2.7108 (2.1587)\tPrec@1 82.812 (86.815)\tPrec@5 99.219 (99.199)\n",
      "Epoch: [83][90/97], lr: 0.01000\tTime 0.363 (0.377)\tData 0.000 (0.020)\tLoss 2.3871 (2.1631)\tPrec@1 84.375 (86.753)\tPrec@5 98.438 (99.227)\n",
      "Epoch: [83][96/97], lr: 0.01000\tTime 0.330 (0.375)\tData 0.000 (0.021)\tLoss 2.5911 (2.1632)\tPrec@1 85.593 (86.821)\tPrec@5 100.000 (99.242)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.360 (0.360)\tLoss 10.0576 (10.0576)\tPrec@1 52.000 (52.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.099)\tLoss 6.8851 (8.5074)\tPrec@1 66.000 (59.545)\tPrec@5 95.000 (93.455)\n",
      "Test: [20/100]\tTime 0.073 (0.087)\tLoss 7.1897 (8.3809)\tPrec@1 67.000 (59.333)\tPrec@5 95.000 (93.810)\n",
      "Test: [30/100]\tTime 0.074 (0.083)\tLoss 7.8412 (8.4287)\tPrec@1 59.000 (58.871)\tPrec@5 94.000 (93.226)\n",
      "Test: [40/100]\tTime 0.074 (0.081)\tLoss 8.1682 (8.4874)\tPrec@1 63.000 (58.488)\tPrec@5 97.000 (93.146)\n",
      "Test: [50/100]\tTime 0.074 (0.080)\tLoss 7.2562 (8.3483)\tPrec@1 67.000 (59.353)\tPrec@5 94.000 (93.157)\n",
      "Test: [60/100]\tTime 0.074 (0.079)\tLoss 6.4600 (8.2710)\tPrec@1 71.000 (59.689)\tPrec@5 94.000 (93.230)\n",
      "Test: [70/100]\tTime 0.073 (0.078)\tLoss 7.7238 (8.2528)\tPrec@1 58.000 (59.746)\tPrec@5 95.000 (93.141)\n",
      "Test: [80/100]\tTime 0.074 (0.078)\tLoss 8.1014 (8.1963)\tPrec@1 58.000 (60.136)\tPrec@5 93.000 (93.247)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 8.1375 (8.2602)\tPrec@1 62.000 (59.857)\tPrec@5 94.000 (93.143)\n",
      "val Results: Prec@1 60.010 Prec@5 93.150 Loss 8.26462\n",
      "val Class Accuracy: [0.911,0.952,0.694,0.665,0.699,0.752,0.739,0.354,0.200,0.035]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [84][0/97], lr: 0.01000\tTime 0.774 (0.774)\tData 0.441 (0.441)\tLoss 2.2266 (2.2266)\tPrec@1 84.375 (84.375)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [84][10/97], lr: 0.01000\tTime 0.360 (0.416)\tData 0.000 (0.052)\tLoss 2.8387 (2.1523)\tPrec@1 80.469 (87.145)\tPrec@5 97.656 (98.722)\n",
      "Epoch: [84][20/97], lr: 0.01000\tTime 0.358 (0.391)\tData 0.000 (0.035)\tLoss 2.7309 (2.1616)\tPrec@1 84.375 (87.016)\tPrec@5 100.000 (98.958)\n",
      "Epoch: [84][30/97], lr: 0.01000\tTime 0.354 (0.381)\tData 0.000 (0.029)\tLoss 2.0239 (2.1647)\tPrec@1 89.844 (87.122)\tPrec@5 100.000 (99.068)\n",
      "Epoch: [84][40/97], lr: 0.01000\tTime 0.356 (0.378)\tData 0.001 (0.026)\tLoss 1.6223 (2.1225)\tPrec@1 92.188 (87.481)\tPrec@5 100.000 (99.123)\n",
      "Epoch: [84][50/97], lr: 0.01000\tTime 0.351 (0.374)\tData 0.000 (0.024)\tLoss 2.2570 (2.1611)\tPrec@1 85.156 (87.255)\tPrec@5 99.219 (99.188)\n",
      "Epoch: [84][60/97], lr: 0.01000\tTime 0.359 (0.371)\tData 0.000 (0.022)\tLoss 2.2598 (2.1734)\tPrec@1 85.156 (87.116)\tPrec@5 99.219 (99.142)\n",
      "Epoch: [84][70/97], lr: 0.01000\tTime 0.349 (0.368)\tData 0.000 (0.021)\tLoss 2.7143 (2.1822)\tPrec@1 82.031 (86.917)\tPrec@5 99.219 (99.164)\n",
      "Epoch: [84][80/97], lr: 0.01000\tTime 0.343 (0.367)\tData 0.000 (0.021)\tLoss 2.0695 (2.1763)\tPrec@1 86.719 (86.921)\tPrec@5 100.000 (99.209)\n",
      "Epoch: [84][90/97], lr: 0.01000\tTime 0.334 (0.364)\tData 0.000 (0.020)\tLoss 1.9970 (2.1685)\tPrec@1 85.938 (86.933)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [84][96/97], lr: 0.01000\tTime 0.321 (0.363)\tData 0.000 (0.021)\tLoss 2.6719 (2.1626)\tPrec@1 83.051 (86.974)\tPrec@5 98.305 (99.218)\n",
      "Gated Network Weight Gate= Flip:0.53, Sc:0.47\n",
      "Test: [0/100]\tTime 0.302 (0.302)\tLoss 9.5060 (9.5060)\tPrec@1 53.000 (53.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.074 (0.094)\tLoss 6.9085 (8.5571)\tPrec@1 65.000 (58.000)\tPrec@5 97.000 (95.182)\n",
      "Test: [20/100]\tTime 0.074 (0.085)\tLoss 7.0934 (8.3373)\tPrec@1 67.000 (59.333)\tPrec@5 98.000 (95.381)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 8.4242 (8.3876)\tPrec@1 59.000 (59.097)\tPrec@5 93.000 (95.387)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 8.5346 (8.3590)\tPrec@1 60.000 (59.341)\tPrec@5 95.000 (95.415)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 7.5921 (8.2481)\tPrec@1 64.000 (59.902)\tPrec@5 98.000 (95.451)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 7.0126 (8.2661)\tPrec@1 70.000 (59.574)\tPrec@5 97.000 (95.492)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 7.9212 (8.2303)\tPrec@1 62.000 (59.803)\tPrec@5 98.000 (95.507)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 7.6896 (8.1643)\tPrec@1 62.000 (60.259)\tPrec@5 95.000 (95.593)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 9.0845 (8.2578)\tPrec@1 56.000 (59.692)\tPrec@5 99.000 (95.516)\n",
      "val Results: Prec@1 59.620 Prec@5 95.560 Loss 8.28152\n",
      "val Class Accuracy: [0.920,0.986,0.661,0.794,0.789,0.658,0.280,0.436,0.329,0.109]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [85][0/97], lr: 0.01000\tTime 0.781 (0.781)\tData 0.445 (0.445)\tLoss 2.2040 (2.2040)\tPrec@1 85.938 (85.938)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [85][10/97], lr: 0.01000\tTime 0.358 (0.417)\tData 0.001 (0.052)\tLoss 2.2274 (2.2524)\tPrec@1 83.594 (86.080)\tPrec@5 99.219 (99.077)\n",
      "Epoch: [85][20/97], lr: 0.01000\tTime 0.375 (0.391)\tData 0.000 (0.035)\tLoss 2.6600 (2.1605)\tPrec@1 84.375 (86.868)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [85][30/97], lr: 0.01000\tTime 0.344 (0.381)\tData 0.000 (0.028)\tLoss 1.9624 (2.1827)\tPrec@1 88.281 (86.668)\tPrec@5 98.438 (99.194)\n",
      "Epoch: [85][40/97], lr: 0.01000\tTime 0.364 (0.376)\tData 0.000 (0.025)\tLoss 2.2075 (2.2236)\tPrec@1 85.938 (86.414)\tPrec@5 100.000 (99.257)\n",
      "Epoch: [85][50/97], lr: 0.01000\tTime 0.355 (0.374)\tData 0.000 (0.023)\tLoss 2.3343 (2.2202)\tPrec@1 87.500 (86.474)\tPrec@5 99.219 (99.234)\n",
      "Epoch: [85][60/97], lr: 0.01000\tTime 0.378 (0.371)\tData 0.000 (0.022)\tLoss 2.0379 (2.2259)\tPrec@1 86.719 (86.245)\tPrec@5 99.219 (99.206)\n",
      "Epoch: [85][70/97], lr: 0.01000\tTime 0.360 (0.371)\tData 0.001 (0.021)\tLoss 1.9055 (2.2283)\tPrec@1 87.500 (86.224)\tPrec@5 100.000 (99.186)\n",
      "Epoch: [85][80/97], lr: 0.01000\tTime 0.378 (0.370)\tData 0.001 (0.020)\tLoss 2.2156 (2.2206)\tPrec@1 86.719 (86.372)\tPrec@5 100.000 (99.199)\n",
      "Epoch: [85][90/97], lr: 0.01000\tTime 0.389 (0.375)\tData 0.000 (0.020)\tLoss 2.0931 (2.2130)\tPrec@1 89.844 (86.435)\tPrec@5 99.219 (99.236)\n",
      "Epoch: [85][96/97], lr: 0.01000\tTime 0.364 (0.376)\tData 0.000 (0.020)\tLoss 2.2458 (2.2199)\tPrec@1 84.746 (86.321)\tPrec@5 100.000 (99.250)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.414 (0.414)\tLoss 10.2413 (10.2413)\tPrec@1 50.000 (50.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.073 (0.105)\tLoss 8.0244 (9.6526)\tPrec@1 58.000 (52.909)\tPrec@5 96.000 (92.182)\n",
      "Test: [20/100]\tTime 0.073 (0.091)\tLoss 8.8827 (9.5369)\tPrec@1 53.000 (52.810)\tPrec@5 96.000 (92.381)\n",
      "Test: [30/100]\tTime 0.075 (0.085)\tLoss 9.6398 (9.5910)\tPrec@1 47.000 (52.226)\tPrec@5 95.000 (92.645)\n",
      "Test: [40/100]\tTime 0.074 (0.083)\tLoss 10.3946 (9.6019)\tPrec@1 47.000 (52.512)\tPrec@5 90.000 (92.512)\n",
      "Test: [50/100]\tTime 0.078 (0.081)\tLoss 9.5928 (9.5056)\tPrec@1 53.000 (53.137)\tPrec@5 94.000 (92.784)\n",
      "Test: [60/100]\tTime 0.074 (0.081)\tLoss 7.4681 (9.4883)\tPrec@1 62.000 (53.197)\tPrec@5 94.000 (92.770)\n",
      "Test: [70/100]\tTime 0.076 (0.080)\tLoss 9.1272 (9.4752)\tPrec@1 55.000 (53.268)\tPrec@5 93.000 (92.648)\n",
      "Test: [80/100]\tTime 0.073 (0.079)\tLoss 8.9981 (9.4111)\tPrec@1 57.000 (53.519)\tPrec@5 89.000 (92.679)\n",
      "Test: [90/100]\tTime 0.074 (0.079)\tLoss 9.4239 (9.4826)\tPrec@1 56.000 (53.011)\tPrec@5 95.000 (92.648)\n",
      "val Results: Prec@1 52.960 Prec@5 92.700 Loss 9.49921\n",
      "val Class Accuracy: [0.893,0.938,0.824,0.789,0.633,0.391,0.422,0.065,0.277,0.064]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [86][0/97], lr: 0.01000\tTime 0.935 (0.935)\tData 0.603 (0.603)\tLoss 2.7674 (2.7674)\tPrec@1 84.375 (84.375)\tPrec@5 96.875 (96.875)\n",
      "Epoch: [86][10/97], lr: 0.01000\tTime 0.347 (0.432)\tData 0.001 (0.065)\tLoss 2.4035 (2.2834)\tPrec@1 85.156 (86.151)\tPrec@5 98.438 (98.864)\n",
      "Epoch: [86][20/97], lr: 0.01000\tTime 0.321 (0.383)\tData 0.000 (0.042)\tLoss 2.0423 (2.1842)\tPrec@1 86.719 (86.533)\tPrec@5 99.219 (99.033)\n",
      "Epoch: [86][30/97], lr: 0.01000\tTime 0.320 (0.364)\tData 0.000 (0.034)\tLoss 2.4548 (2.1188)\tPrec@1 84.375 (86.971)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [86][40/97], lr: 0.01000\tTime 0.322 (0.354)\tData 0.000 (0.030)\tLoss 2.0225 (2.0533)\tPrec@1 89.062 (87.481)\tPrec@5 99.219 (99.295)\n",
      "Epoch: [86][50/97], lr: 0.01000\tTime 0.335 (0.348)\tData 0.000 (0.027)\tLoss 2.4574 (2.0801)\tPrec@1 84.375 (87.240)\tPrec@5 98.438 (99.249)\n",
      "Epoch: [86][60/97], lr: 0.01000\tTime 0.317 (0.344)\tData 0.000 (0.026)\tLoss 2.2745 (2.1114)\tPrec@1 88.281 (86.898)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [86][70/97], lr: 0.01000\tTime 0.318 (0.341)\tData 0.000 (0.024)\tLoss 2.0095 (2.1305)\tPrec@1 87.500 (86.840)\tPrec@5 100.000 (99.186)\n",
      "Epoch: [86][80/97], lr: 0.01000\tTime 0.320 (0.339)\tData 0.000 (0.023)\tLoss 2.4508 (2.1742)\tPrec@1 85.938 (86.526)\tPrec@5 99.219 (99.103)\n",
      "Epoch: [86][90/97], lr: 0.01000\tTime 0.319 (0.337)\tData 0.000 (0.023)\tLoss 2.2596 (2.1560)\tPrec@1 84.375 (86.624)\tPrec@5 98.438 (99.090)\n",
      "Epoch: [86][96/97], lr: 0.01000\tTime 0.312 (0.336)\tData 0.000 (0.023)\tLoss 2.9160 (2.1692)\tPrec@1 80.508 (86.571)\tPrec@5 98.305 (99.097)\n",
      "Gated Network Weight Gate= Flip:0.47, Sc:0.53\n",
      "Test: [0/100]\tTime 0.283 (0.283)\tLoss 7.8448 (7.8448)\tPrec@1 61.000 (61.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 6.5498 (7.2545)\tPrec@1 67.000 (64.182)\tPrec@5 96.000 (96.364)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.9762 (7.3826)\tPrec@1 66.000 (63.714)\tPrec@5 97.000 (95.952)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.4242 (7.4042)\tPrec@1 69.000 (63.645)\tPrec@5 94.000 (96.194)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 6.0239 (7.3520)\tPrec@1 74.000 (64.049)\tPrec@5 96.000 (96.195)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.2714 (7.2689)\tPrec@1 68.000 (64.569)\tPrec@5 98.000 (96.235)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 6.9646 (7.2621)\tPrec@1 65.000 (64.475)\tPrec@5 98.000 (96.311)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 6.7322 (7.2754)\tPrec@1 70.000 (64.507)\tPrec@5 98.000 (96.408)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.9317 (7.2367)\tPrec@1 64.000 (64.679)\tPrec@5 98.000 (96.556)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.5154 (7.3161)\tPrec@1 62.000 (64.396)\tPrec@5 99.000 (96.429)\n",
      "val Results: Prec@1 64.250 Prec@5 96.410 Loss 7.34528\n",
      "val Class Accuracy: [0.950,0.996,0.575,0.666,0.795,0.512,0.639,0.674,0.382,0.236]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [87][0/97], lr: 0.01000\tTime 0.461 (0.461)\tData 0.222 (0.222)\tLoss 2.2427 (2.2427)\tPrec@1 85.156 (85.156)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [87][10/97], lr: 0.01000\tTime 0.320 (0.338)\tData 0.000 (0.033)\tLoss 2.6225 (2.3700)\tPrec@1 85.156 (84.943)\tPrec@5 98.438 (99.361)\n",
      "Epoch: [87][20/97], lr: 0.01000\tTime 0.336 (0.331)\tData 0.000 (0.026)\tLoss 2.2366 (2.3149)\tPrec@1 86.719 (85.193)\tPrec@5 100.000 (99.182)\n",
      "Epoch: [87][30/97], lr: 0.01000\tTime 0.320 (0.328)\tData 0.000 (0.022)\tLoss 2.0445 (2.2394)\tPrec@1 85.938 (85.963)\tPrec@5 99.219 (99.143)\n",
      "Epoch: [87][40/97], lr: 0.01000\tTime 0.320 (0.327)\tData 0.000 (0.021)\tLoss 3.1256 (2.2933)\tPrec@1 81.250 (85.842)\tPrec@5 98.438 (99.162)\n",
      "Epoch: [87][50/97], lr: 0.01000\tTime 0.324 (0.326)\tData 0.000 (0.020)\tLoss 2.4463 (2.2384)\tPrec@1 82.812 (86.183)\tPrec@5 100.000 (99.203)\n",
      "Epoch: [87][60/97], lr: 0.01000\tTime 0.321 (0.326)\tData 0.000 (0.020)\tLoss 2.5558 (2.2188)\tPrec@1 85.156 (86.347)\tPrec@5 100.000 (99.193)\n",
      "Epoch: [87][70/97], lr: 0.01000\tTime 0.322 (0.325)\tData 0.000 (0.019)\tLoss 1.8235 (2.1879)\tPrec@1 87.500 (86.576)\tPrec@5 99.219 (99.186)\n",
      "Epoch: [87][80/97], lr: 0.01000\tTime 0.321 (0.325)\tData 0.000 (0.019)\tLoss 2.7026 (2.1990)\tPrec@1 83.594 (86.516)\tPrec@5 99.219 (99.171)\n",
      "Epoch: [87][90/97], lr: 0.01000\tTime 0.316 (0.324)\tData 0.000 (0.019)\tLoss 1.6736 (2.1819)\tPrec@1 92.188 (86.650)\tPrec@5 100.000 (99.202)\n",
      "Epoch: [87][96/97], lr: 0.01000\tTime 0.305 (0.324)\tData 0.000 (0.019)\tLoss 2.1336 (2.1589)\tPrec@1 86.441 (86.740)\tPrec@5 100.000 (99.226)\n",
      "Gated Network Weight Gate= Flip:0.48, Sc:0.52\n",
      "Test: [0/100]\tTime 0.260 (0.260)\tLoss 10.5216 (10.5216)\tPrec@1 47.000 (47.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 7.8549 (8.9899)\tPrec@1 59.000 (55.273)\tPrec@5 96.000 (94.818)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 8.6499 (8.8002)\tPrec@1 56.000 (56.667)\tPrec@5 92.000 (94.714)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.2082 (8.8636)\tPrec@1 56.000 (56.000)\tPrec@5 88.000 (94.548)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 8.4060 (8.8012)\tPrec@1 59.000 (56.390)\tPrec@5 96.000 (94.634)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.8467 (8.7309)\tPrec@1 63.000 (56.510)\tPrec@5 97.000 (94.784)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.5311 (8.7040)\tPrec@1 64.000 (56.607)\tPrec@5 93.000 (94.639)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.7533 (8.7025)\tPrec@1 55.000 (56.648)\tPrec@5 97.000 (94.690)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.6408 (8.6525)\tPrec@1 63.000 (56.790)\tPrec@5 98.000 (94.802)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.7739 (8.7438)\tPrec@1 56.000 (56.297)\tPrec@5 99.000 (94.703)\n",
      "val Results: Prec@1 56.160 Prec@5 94.720 Loss 8.77489\n",
      "val Class Accuracy: [0.960,0.991,0.522,0.350,0.891,0.639,0.421,0.451,0.355,0.036]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [88][0/97], lr: 0.01000\tTime 0.472 (0.472)\tData 0.239 (0.239)\tLoss 2.0904 (2.0904)\tPrec@1 85.156 (85.156)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [88][10/97], lr: 0.01000\tTime 0.319 (0.339)\tData 0.000 (0.035)\tLoss 2.6964 (2.1661)\tPrec@1 84.375 (86.293)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [88][20/97], lr: 0.01000\tTime 0.321 (0.332)\tData 0.000 (0.027)\tLoss 2.2669 (2.1917)\tPrec@1 89.844 (86.644)\tPrec@5 98.438 (99.405)\n",
      "Epoch: [88][30/97], lr: 0.01000\tTime 0.320 (0.330)\tData 0.000 (0.024)\tLoss 1.8797 (2.1594)\tPrec@1 89.844 (87.097)\tPrec@5 99.219 (99.370)\n",
      "Epoch: [88][40/97], lr: 0.01000\tTime 0.321 (0.328)\tData 0.000 (0.022)\tLoss 2.3532 (2.1261)\tPrec@1 85.938 (87.233)\tPrec@5 99.219 (99.371)\n",
      "Epoch: [88][50/97], lr: 0.01000\tTime 0.318 (0.328)\tData 0.000 (0.021)\tLoss 1.8394 (2.1313)\tPrec@1 87.500 (87.178)\tPrec@5 99.219 (99.311)\n",
      "Epoch: [88][60/97], lr: 0.01000\tTime 0.322 (0.327)\tData 0.000 (0.020)\tLoss 2.3706 (2.1421)\tPrec@1 87.500 (87.167)\tPrec@5 98.438 (99.206)\n",
      "Epoch: [88][70/97], lr: 0.01000\tTime 0.321 (0.327)\tData 0.000 (0.020)\tLoss 1.6663 (2.1225)\tPrec@1 88.281 (87.148)\tPrec@5 100.000 (99.252)\n",
      "Epoch: [88][80/97], lr: 0.01000\tTime 0.320 (0.326)\tData 0.000 (0.019)\tLoss 2.2311 (2.1353)\tPrec@1 85.156 (86.998)\tPrec@5 99.219 (99.238)\n",
      "Epoch: [88][90/97], lr: 0.01000\tTime 0.319 (0.326)\tData 0.000 (0.019)\tLoss 2.5748 (2.1396)\tPrec@1 85.156 (86.925)\tPrec@5 98.438 (99.245)\n",
      "Epoch: [88][96/97], lr: 0.01000\tTime 0.309 (0.326)\tData 0.000 (0.020)\tLoss 2.0882 (2.1599)\tPrec@1 89.831 (86.837)\tPrec@5 98.305 (99.218)\n",
      "Gated Network Weight Gate= Flip:0.44, Sc:0.56\n",
      "Test: [0/100]\tTime 0.266 (0.266)\tLoss 9.4031 (9.4031)\tPrec@1 53.000 (53.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 7.1042 (8.4818)\tPrec@1 65.000 (58.000)\tPrec@5 99.000 (97.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.9088 (8.4268)\tPrec@1 56.000 (58.476)\tPrec@5 97.000 (97.143)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.4055 (8.4185)\tPrec@1 60.000 (58.548)\tPrec@5 96.000 (96.839)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 8.4881 (8.3864)\tPrec@1 60.000 (59.024)\tPrec@5 99.000 (96.780)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.8469 (8.3008)\tPrec@1 61.000 (59.627)\tPrec@5 97.000 (96.686)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.5174 (8.2853)\tPrec@1 70.000 (59.787)\tPrec@5 96.000 (96.705)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.3121 (8.3053)\tPrec@1 62.000 (59.817)\tPrec@5 97.000 (96.577)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 7.5438 (8.2602)\tPrec@1 62.000 (59.926)\tPrec@5 96.000 (96.630)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.6168 (8.3624)\tPrec@1 63.000 (59.560)\tPrec@5 95.000 (96.495)\n",
      "val Results: Prec@1 59.460 Prec@5 96.480 Loss 8.38214\n",
      "val Class Accuracy: [0.973,0.978,0.745,0.683,0.615,0.616,0.311,0.476,0.345,0.204]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [89][0/97], lr: 0.01000\tTime 0.551 (0.551)\tData 0.317 (0.317)\tLoss 1.1758 (1.1758)\tPrec@1 94.531 (94.531)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [89][10/97], lr: 0.01000\tTime 0.320 (0.347)\tData 0.000 (0.043)\tLoss 2.3699 (1.8144)\tPrec@1 85.156 (88.565)\tPrec@5 96.875 (99.361)\n",
      "Epoch: [89][20/97], lr: 0.01000\tTime 0.341 (0.336)\tData 0.000 (0.030)\tLoss 2.0094 (2.0353)\tPrec@1 85.156 (87.016)\tPrec@5 99.219 (99.144)\n",
      "Epoch: [89][30/97], lr: 0.01000\tTime 0.320 (0.332)\tData 0.000 (0.026)\tLoss 2.5029 (2.1280)\tPrec@1 85.156 (86.643)\tPrec@5 100.000 (99.068)\n",
      "Epoch: [89][40/97], lr: 0.01000\tTime 0.319 (0.329)\tData 0.000 (0.024)\tLoss 2.0514 (2.1695)\tPrec@1 89.844 (86.662)\tPrec@5 99.219 (99.162)\n",
      "Epoch: [89][50/97], lr: 0.01000\tTime 0.320 (0.328)\tData 0.000 (0.022)\tLoss 2.8783 (2.1810)\tPrec@1 82.812 (86.443)\tPrec@5 96.094 (99.081)\n",
      "Epoch: [89][60/97], lr: 0.01000\tTime 0.319 (0.328)\tData 0.000 (0.022)\tLoss 2.0536 (2.1616)\tPrec@1 88.281 (86.603)\tPrec@5 100.000 (99.103)\n",
      "Epoch: [89][70/97], lr: 0.01000\tTime 0.324 (0.327)\tData 0.000 (0.021)\tLoss 2.0821 (2.1484)\tPrec@1 86.719 (86.686)\tPrec@5 100.000 (99.153)\n",
      "Epoch: [89][80/97], lr: 0.01000\tTime 0.319 (0.327)\tData 0.000 (0.020)\tLoss 2.0074 (2.1457)\tPrec@1 86.719 (86.748)\tPrec@5 98.438 (99.151)\n",
      "Epoch: [89][90/97], lr: 0.01000\tTime 0.319 (0.326)\tData 0.000 (0.020)\tLoss 2.0208 (2.1699)\tPrec@1 88.281 (86.676)\tPrec@5 99.219 (99.141)\n",
      "Epoch: [89][96/97], lr: 0.01000\tTime 0.307 (0.326)\tData 0.000 (0.021)\tLoss 2.5098 (2.1730)\tPrec@1 84.746 (86.627)\tPrec@5 98.305 (99.138)\n",
      "Gated Network Weight Gate= Flip:0.54, Sc:0.46\n",
      "Test: [0/100]\tTime 0.264 (0.264)\tLoss 12.4480 (12.4480)\tPrec@1 42.000 (42.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 8.4581 (10.7173)\tPrec@1 60.000 (49.091)\tPrec@5 95.000 (92.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 8.8206 (10.5663)\tPrec@1 58.000 (49.286)\tPrec@5 96.000 (92.476)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 9.7723 (10.4705)\tPrec@1 49.000 (49.742)\tPrec@5 93.000 (92.871)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 10.4935 (10.4291)\tPrec@1 47.000 (49.878)\tPrec@5 95.000 (92.854)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 10.1133 (10.3732)\tPrec@1 53.000 (50.373)\tPrec@5 95.000 (92.784)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.8941 (10.3725)\tPrec@1 57.000 (50.082)\tPrec@5 94.000 (92.885)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 10.0727 (10.3727)\tPrec@1 53.000 (50.254)\tPrec@5 97.000 (92.690)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 9.6349 (10.2951)\tPrec@1 52.000 (50.765)\tPrec@5 95.000 (92.753)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.8809 (10.3948)\tPrec@1 54.000 (50.374)\tPrec@5 96.000 (92.659)\n",
      "val Results: Prec@1 50.370 Prec@5 92.660 Loss 10.40710\n",
      "val Class Accuracy: [0.945,0.985,0.795,0.766,0.344,0.528,0.241,0.380,0.052,0.001]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [90][0/97], lr: 0.01000\tTime 0.465 (0.465)\tData 0.250 (0.250)\tLoss 2.3417 (2.3417)\tPrec@1 85.938 (85.938)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [90][10/97], lr: 0.01000\tTime 0.318 (0.339)\tData 0.000 (0.037)\tLoss 1.5276 (2.2046)\tPrec@1 89.844 (86.506)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [90][20/97], lr: 0.01000\tTime 0.319 (0.331)\tData 0.000 (0.028)\tLoss 2.3451 (2.1023)\tPrec@1 85.156 (87.165)\tPrec@5 99.219 (99.368)\n",
      "Epoch: [90][30/97], lr: 0.01000\tTime 0.321 (0.331)\tData 0.000 (0.024)\tLoss 2.3719 (2.2001)\tPrec@1 86.719 (86.517)\tPrec@5 99.219 (99.194)\n",
      "Epoch: [90][40/97], lr: 0.01000\tTime 0.320 (0.329)\tData 0.000 (0.022)\tLoss 1.7799 (2.1625)\tPrec@1 88.281 (86.795)\tPrec@5 99.219 (99.143)\n",
      "Epoch: [90][50/97], lr: 0.01000\tTime 0.322 (0.328)\tData 0.000 (0.021)\tLoss 1.4927 (2.1121)\tPrec@1 92.188 (87.056)\tPrec@5 99.219 (99.234)\n",
      "Epoch: [90][60/97], lr: 0.01000\tTime 0.324 (0.327)\tData 0.000 (0.021)\tLoss 2.0895 (2.1034)\tPrec@1 88.281 (87.193)\tPrec@5 98.438 (99.206)\n",
      "Epoch: [90][70/97], lr: 0.01000\tTime 0.333 (0.327)\tData 0.000 (0.020)\tLoss 1.9629 (2.1089)\tPrec@1 89.062 (87.071)\tPrec@5 99.219 (99.241)\n",
      "Epoch: [90][80/97], lr: 0.01000\tTime 0.344 (0.329)\tData 0.000 (0.020)\tLoss 1.8651 (2.1415)\tPrec@1 87.500 (86.941)\tPrec@5 99.219 (99.199)\n",
      "Epoch: [90][90/97], lr: 0.01000\tTime 0.326 (0.329)\tData 0.000 (0.019)\tLoss 3.1562 (2.1646)\tPrec@1 81.250 (86.779)\tPrec@5 97.656 (99.193)\n",
      "Epoch: [90][96/97], lr: 0.01000\tTime 0.311 (0.328)\tData 0.000 (0.020)\tLoss 1.7338 (2.1762)\tPrec@1 88.983 (86.660)\tPrec@5 99.153 (99.218)\n",
      "Gated Network Weight Gate= Flip:0.54, Sc:0.46\n",
      "Test: [0/100]\tTime 0.261 (0.261)\tLoss 9.5416 (9.5416)\tPrec@1 52.000 (52.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.4333 (8.5900)\tPrec@1 67.000 (57.091)\tPrec@5 96.000 (94.273)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.6008 (8.5080)\tPrec@1 64.000 (58.000)\tPrec@5 96.000 (94.286)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.0699 (8.5729)\tPrec@1 63.000 (58.129)\tPrec@5 92.000 (94.097)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 8.8579 (8.5643)\tPrec@1 57.000 (58.341)\tPrec@5 96.000 (94.195)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 8.1637 (8.4876)\tPrec@1 61.000 (59.039)\tPrec@5 93.000 (94.078)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.0466 (8.4533)\tPrec@1 61.000 (59.197)\tPrec@5 95.000 (94.033)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.2737 (8.4374)\tPrec@1 61.000 (59.268)\tPrec@5 93.000 (93.944)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.8780 (8.3823)\tPrec@1 65.000 (59.605)\tPrec@5 94.000 (94.099)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.8659 (8.4410)\tPrec@1 58.000 (59.220)\tPrec@5 96.000 (94.099)\n",
      "val Results: Prec@1 59.220 Prec@5 94.110 Loss 8.45940\n",
      "val Class Accuracy: [0.819,0.990,0.676,0.403,0.558,0.866,0.884,0.384,0.189,0.153]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [91][0/97], lr: 0.01000\tTime 0.467 (0.467)\tData 0.264 (0.264)\tLoss 2.7690 (2.7690)\tPrec@1 83.594 (83.594)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [91][10/97], lr: 0.01000\tTime 0.322 (0.340)\tData 0.000 (0.039)\tLoss 2.6436 (2.2830)\tPrec@1 83.594 (86.151)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [91][20/97], lr: 0.01000\tTime 0.325 (0.332)\tData 0.000 (0.028)\tLoss 2.3694 (2.1286)\tPrec@1 85.156 (87.500)\tPrec@5 100.000 (99.628)\n",
      "Epoch: [91][30/97], lr: 0.01000\tTime 0.321 (0.329)\tData 0.000 (0.025)\tLoss 1.5539 (2.0960)\tPrec@1 89.844 (87.399)\tPrec@5 97.656 (99.521)\n",
      "Epoch: [91][40/97], lr: 0.01000\tTime 0.322 (0.328)\tData 0.000 (0.023)\tLoss 2.7130 (2.0503)\tPrec@1 82.812 (87.614)\tPrec@5 99.219 (99.447)\n",
      "Epoch: [91][50/97], lr: 0.01000\tTime 0.319 (0.327)\tData 0.000 (0.022)\tLoss 2.0191 (2.0820)\tPrec@1 89.844 (87.485)\tPrec@5 100.000 (99.311)\n",
      "Epoch: [91][60/97], lr: 0.01000\tTime 0.320 (0.326)\tData 0.000 (0.021)\tLoss 2.4001 (2.1170)\tPrec@1 83.594 (87.167)\tPrec@5 100.000 (99.232)\n",
      "Epoch: [91][70/97], lr: 0.01000\tTime 0.321 (0.326)\tData 0.000 (0.020)\tLoss 2.9790 (2.1426)\tPrec@1 81.250 (86.972)\tPrec@5 99.219 (99.263)\n",
      "Epoch: [91][80/97], lr: 0.01000\tTime 0.320 (0.325)\tData 0.000 (0.020)\tLoss 1.9651 (2.1385)\tPrec@1 89.062 (87.105)\tPrec@5 99.219 (99.248)\n",
      "Epoch: [91][90/97], lr: 0.01000\tTime 0.318 (0.325)\tData 0.000 (0.020)\tLoss 3.2366 (2.1642)\tPrec@1 81.250 (86.899)\tPrec@5 99.219 (99.193)\n",
      "Epoch: [91][96/97], lr: 0.01000\tTime 0.309 (0.325)\tData 0.000 (0.020)\tLoss 1.9043 (2.1559)\tPrec@1 88.983 (86.974)\tPrec@5 99.153 (99.194)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.256 (0.256)\tLoss 11.0718 (11.0718)\tPrec@1 48.000 (48.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 8.4152 (9.6057)\tPrec@1 57.000 (52.273)\tPrec@5 91.000 (92.545)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 8.2265 (9.6358)\tPrec@1 62.000 (51.857)\tPrec@5 95.000 (92.429)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.9499 (9.5734)\tPrec@1 57.000 (52.194)\tPrec@5 92.000 (92.387)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 9.3863 (9.4958)\tPrec@1 54.000 (52.927)\tPrec@5 97.000 (92.488)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 9.0160 (9.4397)\tPrec@1 54.000 (52.941)\tPrec@5 94.000 (92.686)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.0033 (9.4347)\tPrec@1 59.000 (52.869)\tPrec@5 92.000 (92.557)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 9.1335 (9.4171)\tPrec@1 58.000 (53.113)\tPrec@5 95.000 (92.465)\n",
      "Test: [80/100]\tTime 0.074 (0.075)\tLoss 7.9461 (9.3688)\tPrec@1 61.000 (53.506)\tPrec@5 90.000 (92.531)\n",
      "Test: [90/100]\tTime 0.074 (0.075)\tLoss 9.2236 (9.4583)\tPrec@1 54.000 (53.022)\tPrec@5 93.000 (92.330)\n",
      "val Results: Prec@1 52.910 Prec@5 92.420 Loss 9.49291\n",
      "val Class Accuracy: [0.957,0.984,0.754,0.574,0.690,0.345,0.180,0.510,0.146,0.151]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [92][0/97], lr: 0.01000\tTime 0.908 (0.908)\tData 0.534 (0.534)\tLoss 2.3904 (2.3904)\tPrec@1 87.500 (87.500)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [92][10/97], lr: 0.01000\tTime 0.323 (0.404)\tData 0.000 (0.058)\tLoss 1.3862 (2.0329)\tPrec@1 90.625 (87.855)\tPrec@5 100.000 (99.077)\n",
      "Epoch: [92][20/97], lr: 0.01000\tTime 0.325 (0.369)\tData 0.000 (0.039)\tLoss 2.2708 (2.1261)\tPrec@1 85.156 (87.202)\tPrec@5 99.219 (99.182)\n",
      "Epoch: [92][30/97], lr: 0.01000\tTime 0.322 (0.357)\tData 0.000 (0.032)\tLoss 2.1963 (2.0783)\tPrec@1 85.938 (87.550)\tPrec@5 98.438 (99.194)\n",
      "Epoch: [92][40/97], lr: 0.01000\tTime 0.321 (0.350)\tData 0.000 (0.028)\tLoss 3.0819 (2.1372)\tPrec@1 81.250 (87.157)\tPrec@5 98.438 (99.162)\n",
      "Epoch: [92][50/97], lr: 0.01000\tTime 0.324 (0.347)\tData 0.000 (0.026)\tLoss 2.0913 (2.1319)\tPrec@1 85.938 (87.086)\tPrec@5 100.000 (99.157)\n",
      "Epoch: [92][60/97], lr: 0.01000\tTime 0.323 (0.344)\tData 0.000 (0.024)\tLoss 2.5290 (2.1124)\tPrec@1 84.375 (87.141)\tPrec@5 100.000 (99.206)\n",
      "Epoch: [92][70/97], lr: 0.01000\tTime 0.323 (0.342)\tData 0.000 (0.023)\tLoss 1.4164 (2.1193)\tPrec@1 89.844 (87.005)\tPrec@5 100.000 (99.252)\n",
      "Epoch: [92][80/97], lr: 0.01000\tTime 0.330 (0.341)\tData 0.000 (0.023)\tLoss 2.3583 (2.1366)\tPrec@1 86.719 (86.931)\tPrec@5 98.438 (99.228)\n",
      "Epoch: [92][90/97], lr: 0.01000\tTime 0.320 (0.340)\tData 0.000 (0.022)\tLoss 2.7574 (2.1575)\tPrec@1 82.812 (86.770)\tPrec@5 99.219 (99.227)\n",
      "Epoch: [92][96/97], lr: 0.01000\tTime 0.317 (0.339)\tData 0.000 (0.022)\tLoss 1.2836 (2.1604)\tPrec@1 91.525 (86.724)\tPrec@5 100.000 (99.242)\n",
      "Gated Network Weight Gate= Flip:0.53, Sc:0.47\n",
      "Test: [0/100]\tTime 0.277 (0.277)\tLoss 9.6570 (9.6570)\tPrec@1 50.000 (50.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 6.9460 (8.5946)\tPrec@1 64.000 (56.909)\tPrec@5 97.000 (96.455)\n",
      "Test: [20/100]\tTime 0.074 (0.083)\tLoss 7.4262 (8.7661)\tPrec@1 62.000 (56.190)\tPrec@5 98.000 (95.952)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 8.6242 (8.7847)\tPrec@1 54.000 (56.097)\tPrec@5 93.000 (95.806)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 8.3472 (8.7371)\tPrec@1 57.000 (56.488)\tPrec@5 97.000 (95.780)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 8.7307 (8.6991)\tPrec@1 57.000 (56.725)\tPrec@5 95.000 (95.882)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 7.1140 (8.6964)\tPrec@1 66.000 (56.590)\tPrec@5 95.000 (95.836)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.4278 (8.6944)\tPrec@1 59.000 (56.676)\tPrec@5 99.000 (95.789)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 8.0804 (8.6553)\tPrec@1 60.000 (56.889)\tPrec@5 97.000 (95.815)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 9.3221 (8.7135)\tPrec@1 52.000 (56.615)\tPrec@5 97.000 (95.813)\n",
      "val Results: Prec@1 56.450 Prec@5 95.780 Loss 8.74155\n",
      "val Class Accuracy: [0.969,0.989,0.691,0.837,0.448,0.331,0.456,0.627,0.275,0.022]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [93][0/97], lr: 0.01000\tTime 0.581 (0.581)\tData 0.322 (0.322)\tLoss 2.5697 (2.5697)\tPrec@1 84.375 (84.375)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [93][10/97], lr: 0.01000\tTime 0.321 (0.361)\tData 0.000 (0.042)\tLoss 1.7734 (2.0113)\tPrec@1 89.062 (87.216)\tPrec@5 100.000 (99.148)\n",
      "Epoch: [93][20/97], lr: 0.01000\tTime 0.323 (0.347)\tData 0.000 (0.030)\tLoss 1.7389 (2.0518)\tPrec@1 88.281 (87.426)\tPrec@5 100.000 (99.182)\n",
      "Epoch: [93][30/97], lr: 0.01000\tTime 0.321 (0.341)\tData 0.000 (0.026)\tLoss 2.1340 (2.0240)\tPrec@1 86.719 (87.576)\tPrec@5 100.000 (99.320)\n",
      "Epoch: [93][40/97], lr: 0.01000\tTime 0.319 (0.338)\tData 0.000 (0.024)\tLoss 1.5140 (2.0697)\tPrec@1 90.625 (87.233)\tPrec@5 100.000 (99.333)\n",
      "Epoch: [93][50/97], lr: 0.01000\tTime 0.321 (0.337)\tData 0.000 (0.022)\tLoss 1.8395 (2.0589)\tPrec@1 90.625 (87.224)\tPrec@5 99.219 (99.372)\n",
      "Epoch: [93][60/97], lr: 0.01000\tTime 0.322 (0.335)\tData 0.000 (0.022)\tLoss 2.0987 (2.0714)\tPrec@1 86.719 (87.103)\tPrec@5 100.000 (99.360)\n",
      "Epoch: [93][70/97], lr: 0.01000\tTime 0.320 (0.334)\tData 0.000 (0.021)\tLoss 2.6235 (2.0755)\tPrec@1 82.812 (87.082)\tPrec@5 99.219 (99.318)\n",
      "Epoch: [93][80/97], lr: 0.01000\tTime 0.323 (0.334)\tData 0.000 (0.020)\tLoss 2.1715 (2.1215)\tPrec@1 86.719 (86.912)\tPrec@5 100.000 (99.315)\n",
      "Epoch: [93][90/97], lr: 0.01000\tTime 0.322 (0.334)\tData 0.000 (0.020)\tLoss 1.6967 (2.1267)\tPrec@1 89.844 (86.882)\tPrec@5 99.219 (99.245)\n",
      "Epoch: [93][96/97], lr: 0.01000\tTime 0.317 (0.333)\tData 0.000 (0.021)\tLoss 2.4888 (2.1494)\tPrec@1 79.661 (86.692)\tPrec@5 99.153 (99.242)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.282 (0.282)\tLoss 9.4703 (9.4703)\tPrec@1 53.000 (53.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 7.3844 (8.4629)\tPrec@1 62.000 (58.091)\tPrec@5 92.000 (93.727)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 7.2665 (8.6146)\tPrec@1 61.000 (56.952)\tPrec@5 97.000 (94.286)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 7.6142 (8.5551)\tPrec@1 65.000 (57.323)\tPrec@5 92.000 (93.935)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 7.5885 (8.4934)\tPrec@1 60.000 (57.854)\tPrec@5 95.000 (93.951)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 7.9204 (8.4097)\tPrec@1 58.000 (58.392)\tPrec@5 93.000 (94.039)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 6.7144 (8.3615)\tPrec@1 68.000 (58.607)\tPrec@5 95.000 (94.164)\n",
      "Test: [70/100]\tTime 0.075 (0.076)\tLoss 8.3915 (8.3681)\tPrec@1 59.000 (58.732)\tPrec@5 96.000 (94.085)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 7.6939 (8.3234)\tPrec@1 63.000 (58.988)\tPrec@5 96.000 (94.370)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 8.1981 (8.3828)\tPrec@1 59.000 (58.659)\tPrec@5 96.000 (94.242)\n",
      "val Results: Prec@1 58.580 Prec@5 94.330 Loss 8.41054\n",
      "val Class Accuracy: [0.969,0.992,0.771,0.600,0.589,0.656,0.300,0.682,0.236,0.063]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [94][0/97], lr: 0.01000\tTime 0.529 (0.529)\tData 0.312 (0.312)\tLoss 2.1257 (2.1257)\tPrec@1 86.719 (86.719)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [94][10/97], lr: 0.01000\tTime 0.322 (0.353)\tData 0.000 (0.042)\tLoss 1.9092 (2.1488)\tPrec@1 88.281 (86.577)\tPrec@5 99.219 (99.574)\n",
      "Epoch: [94][20/97], lr: 0.01000\tTime 0.321 (0.341)\tData 0.000 (0.030)\tLoss 1.6954 (2.0561)\tPrec@1 91.406 (87.426)\tPrec@5 99.219 (99.405)\n",
      "Epoch: [94][30/97], lr: 0.01000\tTime 0.322 (0.337)\tData 0.000 (0.026)\tLoss 2.3982 (2.0662)\tPrec@1 87.500 (87.576)\tPrec@5 100.000 (99.294)\n",
      "Epoch: [94][40/97], lr: 0.01000\tTime 0.323 (0.336)\tData 0.000 (0.024)\tLoss 2.5672 (2.1096)\tPrec@1 82.031 (87.233)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [94][50/97], lr: 0.01000\tTime 0.321 (0.335)\tData 0.000 (0.022)\tLoss 2.3321 (2.1410)\tPrec@1 85.156 (86.872)\tPrec@5 100.000 (99.249)\n",
      "Epoch: [94][60/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.021)\tLoss 1.8055 (2.1451)\tPrec@1 89.844 (86.808)\tPrec@5 98.438 (99.180)\n",
      "Epoch: [94][70/97], lr: 0.01000\tTime 0.323 (0.333)\tData 0.000 (0.021)\tLoss 1.5428 (2.1377)\tPrec@1 90.625 (86.818)\tPrec@5 100.000 (99.230)\n",
      "Epoch: [94][80/97], lr: 0.01000\tTime 0.319 (0.333)\tData 0.000 (0.020)\tLoss 1.7912 (2.1063)\tPrec@1 89.062 (87.018)\tPrec@5 99.219 (99.248)\n",
      "Epoch: [94][90/97], lr: 0.01000\tTime 0.320 (0.332)\tData 0.000 (0.020)\tLoss 2.1836 (2.1393)\tPrec@1 86.719 (86.830)\tPrec@5 98.438 (99.227)\n",
      "Epoch: [94][96/97], lr: 0.01000\tTime 0.316 (0.332)\tData 0.000 (0.020)\tLoss 2.3557 (2.1682)\tPrec@1 83.898 (86.595)\tPrec@5 99.153 (99.226)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.287 (0.287)\tLoss 11.1627 (11.1627)\tPrec@1 45.000 (45.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 8.7873 (10.2691)\tPrec@1 56.000 (49.455)\tPrec@5 88.000 (91.727)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 7.9728 (10.2357)\tPrec@1 58.000 (49.810)\tPrec@5 96.000 (91.048)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 9.9634 (10.2896)\tPrec@1 46.000 (49.613)\tPrec@5 92.000 (91.032)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 10.4922 (10.2834)\tPrec@1 49.000 (49.902)\tPrec@5 91.000 (91.000)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 9.3885 (10.1925)\tPrec@1 55.000 (50.314)\tPrec@5 91.000 (91.039)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.7486 (10.1381)\tPrec@1 56.000 (50.410)\tPrec@5 91.000 (90.984)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 10.1745 (10.1283)\tPrec@1 50.000 (50.437)\tPrec@5 96.000 (91.099)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 8.9977 (10.0639)\tPrec@1 56.000 (50.580)\tPrec@5 91.000 (91.309)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 10.2428 (10.1207)\tPrec@1 48.000 (50.418)\tPrec@5 96.000 (91.253)\n",
      "val Results: Prec@1 50.270 Prec@5 91.200 Loss 10.15952\n",
      "val Class Accuracy: [0.987,0.950,0.516,0.407,0.418,0.740,0.609,0.274,0.102,0.024]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [95][0/97], lr: 0.01000\tTime 0.528 (0.528)\tData 0.284 (0.284)\tLoss 2.4205 (2.4205)\tPrec@1 82.812 (82.812)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [95][10/97], lr: 0.01000\tTime 0.321 (0.354)\tData 0.000 (0.038)\tLoss 2.9652 (2.1038)\tPrec@1 78.906 (86.506)\tPrec@5 100.000 (99.716)\n",
      "Epoch: [95][20/97], lr: 0.01000\tTime 0.326 (0.343)\tData 0.000 (0.028)\tLoss 2.2945 (2.1757)\tPrec@1 85.156 (85.938)\tPrec@5 97.656 (99.405)\n",
      "Epoch: [95][30/97], lr: 0.01000\tTime 0.323 (0.339)\tData 0.000 (0.024)\tLoss 1.7827 (2.1514)\tPrec@1 89.844 (86.164)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [95][40/97], lr: 0.01000\tTime 0.321 (0.337)\tData 0.000 (0.023)\tLoss 2.5345 (2.1693)\tPrec@1 85.938 (86.223)\tPrec@5 100.000 (99.409)\n",
      "Epoch: [95][50/97], lr: 0.01000\tTime 0.325 (0.336)\tData 0.000 (0.021)\tLoss 1.7127 (2.1768)\tPrec@1 89.844 (86.244)\tPrec@5 99.219 (99.341)\n",
      "Epoch: [95][60/97], lr: 0.01000\tTime 0.325 (0.335)\tData 0.000 (0.021)\tLoss 2.7214 (2.1869)\tPrec@1 84.375 (86.245)\tPrec@5 100.000 (99.321)\n",
      "Epoch: [95][70/97], lr: 0.01000\tTime 0.321 (0.335)\tData 0.000 (0.020)\tLoss 1.6349 (2.1780)\tPrec@1 89.844 (86.356)\tPrec@5 100.000 (99.318)\n",
      "Epoch: [95][80/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.020)\tLoss 1.9011 (2.1679)\tPrec@1 91.406 (86.526)\tPrec@5 100.000 (99.334)\n",
      "Epoch: [95][90/97], lr: 0.01000\tTime 0.318 (0.334)\tData 0.000 (0.019)\tLoss 1.7180 (2.1710)\tPrec@1 90.625 (86.556)\tPrec@5 98.438 (99.305)\n",
      "Epoch: [95][96/97], lr: 0.01000\tTime 0.314 (0.334)\tData 0.000 (0.020)\tLoss 2.2694 (2.1673)\tPrec@1 86.441 (86.611)\tPrec@5 98.305 (99.299)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.278 (0.278)\tLoss 9.6050 (9.6050)\tPrec@1 52.000 (52.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 6.5875 (8.7554)\tPrec@1 70.000 (57.182)\tPrec@5 98.000 (95.727)\n",
      "Test: [20/100]\tTime 0.074 (0.083)\tLoss 6.9484 (8.7971)\tPrec@1 64.000 (56.190)\tPrec@5 98.000 (95.667)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 8.3478 (8.7909)\tPrec@1 58.000 (56.258)\tPrec@5 97.000 (95.613)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 9.0583 (8.7659)\tPrec@1 56.000 (56.634)\tPrec@5 93.000 (95.366)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 8.2174 (8.6708)\tPrec@1 59.000 (57.039)\tPrec@5 94.000 (95.373)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 6.5872 (8.6330)\tPrec@1 68.000 (57.230)\tPrec@5 96.000 (95.295)\n",
      "Test: [70/100]\tTime 0.075 (0.077)\tLoss 8.4126 (8.6250)\tPrec@1 59.000 (57.296)\tPrec@5 98.000 (95.225)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 8.5199 (8.5879)\tPrec@1 56.000 (57.395)\tPrec@5 96.000 (95.321)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 9.0969 (8.6592)\tPrec@1 54.000 (57.165)\tPrec@5 98.000 (95.429)\n",
      "val Results: Prec@1 57.240 Prec@5 95.400 Loss 8.66609\n",
      "val Class Accuracy: [0.971,0.974,0.759,0.800,0.541,0.583,0.393,0.482,0.167,0.054]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [96][0/97], lr: 0.01000\tTime 0.524 (0.524)\tData 0.283 (0.283)\tLoss 2.5775 (2.5775)\tPrec@1 85.156 (85.156)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [96][10/97], lr: 0.01000\tTime 0.323 (0.355)\tData 0.000 (0.040)\tLoss 2.5128 (2.0617)\tPrec@1 82.031 (87.642)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [96][20/97], lr: 0.01000\tTime 0.323 (0.344)\tData 0.000 (0.029)\tLoss 2.7329 (2.0034)\tPrec@1 81.250 (87.723)\tPrec@5 100.000 (99.554)\n",
      "Epoch: [96][30/97], lr: 0.01000\tTime 0.323 (0.339)\tData 0.000 (0.025)\tLoss 1.6959 (2.0183)\tPrec@1 89.062 (87.550)\tPrec@5 99.219 (99.471)\n",
      "Epoch: [96][40/97], lr: 0.01000\tTime 0.321 (0.337)\tData 0.000 (0.023)\tLoss 2.9608 (1.9850)\tPrec@1 83.594 (87.881)\tPrec@5 97.656 (99.371)\n",
      "Epoch: [96][50/97], lr: 0.01000\tTime 0.324 (0.335)\tData 0.000 (0.022)\tLoss 2.1323 (1.9978)\tPrec@1 88.281 (87.837)\tPrec@5 100.000 (99.418)\n",
      "Epoch: [96][60/97], lr: 0.01000\tTime 0.327 (0.335)\tData 0.000 (0.021)\tLoss 2.9589 (2.0176)\tPrec@1 83.594 (87.769)\tPrec@5 97.656 (99.372)\n",
      "Epoch: [96][70/97], lr: 0.01000\tTime 0.322 (0.334)\tData 0.000 (0.021)\tLoss 2.2732 (2.0702)\tPrec@1 85.938 (87.357)\tPrec@5 100.000 (99.362)\n",
      "Epoch: [96][80/97], lr: 0.01000\tTime 0.321 (0.334)\tData 0.000 (0.020)\tLoss 2.2608 (2.0902)\tPrec@1 83.594 (87.249)\tPrec@5 99.219 (99.334)\n",
      "Epoch: [96][90/97], lr: 0.01000\tTime 0.320 (0.333)\tData 0.000 (0.020)\tLoss 2.0467 (2.0856)\tPrec@1 88.281 (87.208)\tPrec@5 100.000 (99.390)\n",
      "Epoch: [96][96/97], lr: 0.01000\tTime 0.317 (0.333)\tData 0.000 (0.020)\tLoss 1.6037 (2.0790)\tPrec@1 91.525 (87.200)\tPrec@5 100.000 (99.371)\n",
      "Gated Network Weight Gate= Flip:0.52, Sc:0.48\n",
      "Test: [0/100]\tTime 0.292 (0.292)\tLoss 8.1654 (8.1654)\tPrec@1 59.000 (59.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 6.5602 (7.9465)\tPrec@1 67.000 (60.091)\tPrec@5 100.000 (97.091)\n",
      "Test: [20/100]\tTime 0.074 (0.084)\tLoss 6.2640 (8.0035)\tPrec@1 65.000 (59.762)\tPrec@5 98.000 (96.571)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 7.5269 (8.0296)\tPrec@1 60.000 (59.548)\tPrec@5 96.000 (96.355)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 7.9643 (8.0193)\tPrec@1 67.000 (60.171)\tPrec@5 96.000 (96.122)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 7.7173 (7.9433)\tPrec@1 67.000 (60.647)\tPrec@5 98.000 (96.216)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 6.8072 (7.9501)\tPrec@1 63.000 (60.443)\tPrec@5 98.000 (96.213)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 8.3043 (7.9594)\tPrec@1 57.000 (60.366)\tPrec@5 95.000 (96.211)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 8.0832 (7.9186)\tPrec@1 60.000 (60.580)\tPrec@5 95.000 (96.284)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 7.7310 (7.9788)\tPrec@1 65.000 (60.319)\tPrec@5 98.000 (96.297)\n",
      "val Results: Prec@1 60.370 Prec@5 96.270 Loss 7.99023\n",
      "val Class Accuracy: [0.971,0.988,0.702,0.736,0.519,0.615,0.553,0.554,0.343,0.056]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [97][0/97], lr: 0.01000\tTime 0.497 (0.497)\tData 0.281 (0.281)\tLoss 1.2605 (1.2605)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [97][10/97], lr: 0.01000\tTime 0.324 (0.355)\tData 0.000 (0.039)\tLoss 1.6752 (1.9972)\tPrec@1 90.625 (88.281)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [97][20/97], lr: 0.01000\tTime 0.322 (0.343)\tData 0.000 (0.029)\tLoss 2.3707 (2.0528)\tPrec@1 87.500 (87.351)\tPrec@5 98.438 (99.368)\n",
      "Epoch: [97][30/97], lr: 0.01000\tTime 0.324 (0.339)\tData 0.000 (0.025)\tLoss 1.9054 (2.0721)\tPrec@1 89.844 (87.298)\tPrec@5 98.438 (99.269)\n",
      "Epoch: [97][40/97], lr: 0.01000\tTime 0.322 (0.337)\tData 0.000 (0.023)\tLoss 1.8237 (2.1255)\tPrec@1 89.062 (86.947)\tPrec@5 99.219 (99.352)\n",
      "Epoch: [97][50/97], lr: 0.01000\tTime 0.323 (0.335)\tData 0.000 (0.022)\tLoss 2.5774 (2.1870)\tPrec@1 82.812 (86.443)\tPrec@5 98.438 (99.295)\n",
      "Epoch: [97][60/97], lr: 0.01000\tTime 0.319 (0.334)\tData 0.000 (0.021)\tLoss 1.5389 (2.1770)\tPrec@1 90.625 (86.655)\tPrec@5 100.000 (99.347)\n",
      "Epoch: [97][70/97], lr: 0.01000\tTime 0.322 (0.333)\tData 0.000 (0.020)\tLoss 2.2778 (2.1745)\tPrec@1 87.500 (86.796)\tPrec@5 100.000 (99.384)\n",
      "Epoch: [97][80/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.020)\tLoss 2.2157 (2.1652)\tPrec@1 85.156 (86.777)\tPrec@5 99.219 (99.363)\n",
      "Epoch: [97][90/97], lr: 0.01000\tTime 0.320 (0.332)\tData 0.000 (0.020)\tLoss 1.8360 (2.1533)\tPrec@1 86.719 (86.813)\tPrec@5 100.000 (99.373)\n",
      "Epoch: [97][96/97], lr: 0.01000\tTime 0.319 (0.332)\tData 0.000 (0.020)\tLoss 2.7700 (2.1503)\tPrec@1 82.203 (86.853)\tPrec@5 98.305 (99.347)\n",
      "Gated Network Weight Gate= Flip:0.52, Sc:0.48\n",
      "Test: [0/100]\tTime 0.279 (0.279)\tLoss 8.9020 (8.9020)\tPrec@1 58.000 (58.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 6.8352 (8.4304)\tPrec@1 65.000 (60.909)\tPrec@5 96.000 (95.909)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 7.5552 (8.4429)\tPrec@1 62.000 (60.095)\tPrec@5 96.000 (95.190)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 7.8289 (8.4953)\tPrec@1 62.000 (59.935)\tPrec@5 93.000 (95.258)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 8.7749 (8.5258)\tPrec@1 60.000 (59.878)\tPrec@5 91.000 (94.732)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 8.1675 (8.4255)\tPrec@1 65.000 (60.392)\tPrec@5 94.000 (94.765)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 6.5663 (8.3668)\tPrec@1 70.000 (60.410)\tPrec@5 94.000 (94.623)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.5513 (8.3613)\tPrec@1 57.000 (60.465)\tPrec@5 95.000 (94.592)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 7.9820 (8.3340)\tPrec@1 64.000 (60.580)\tPrec@5 92.000 (94.630)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 8.9603 (8.4166)\tPrec@1 57.000 (60.099)\tPrec@5 95.000 (94.527)\n",
      "val Results: Prec@1 59.900 Prec@5 94.500 Loss 8.45603\n",
      "val Class Accuracy: [0.942,0.991,0.776,0.658,0.792,0.545,0.739,0.380,0.129,0.038]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [98][0/97], lr: 0.01000\tTime 0.650 (0.650)\tData 0.366 (0.366)\tLoss 1.7921 (1.7921)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [98][10/97], lr: 0.01000\tTime 0.323 (0.370)\tData 0.000 (0.046)\tLoss 1.8600 (1.9759)\tPrec@1 88.281 (87.784)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [98][20/97], lr: 0.01000\tTime 0.320 (0.350)\tData 0.000 (0.032)\tLoss 1.5927 (2.0283)\tPrec@1 89.062 (87.277)\tPrec@5 99.219 (99.405)\n",
      "Epoch: [98][30/97], lr: 0.01000\tTime 0.317 (0.344)\tData 0.000 (0.027)\tLoss 2.4095 (2.0330)\tPrec@1 84.375 (87.248)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [98][40/97], lr: 0.01000\tTime 0.321 (0.341)\tData 0.000 (0.025)\tLoss 1.4694 (2.0547)\tPrec@1 92.188 (87.176)\tPrec@5 99.219 (99.390)\n",
      "Epoch: [98][50/97], lr: 0.01000\tTime 0.321 (0.339)\tData 0.000 (0.023)\tLoss 2.2981 (2.0701)\tPrec@1 86.719 (87.117)\tPrec@5 99.219 (99.387)\n",
      "Epoch: [98][60/97], lr: 0.01000\tTime 0.321 (0.337)\tData 0.000 (0.022)\tLoss 2.2807 (2.0670)\tPrec@1 85.156 (87.154)\tPrec@5 99.219 (99.372)\n",
      "Epoch: [98][70/97], lr: 0.01000\tTime 0.324 (0.337)\tData 0.000 (0.022)\tLoss 2.0947 (2.0907)\tPrec@1 85.156 (87.038)\tPrec@5 100.000 (99.384)\n",
      "Epoch: [98][80/97], lr: 0.01000\tTime 0.323 (0.336)\tData 0.000 (0.021)\tLoss 2.1298 (2.1182)\tPrec@1 87.500 (86.806)\tPrec@5 98.438 (99.354)\n",
      "Epoch: [98][90/97], lr: 0.01000\tTime 0.320 (0.335)\tData 0.000 (0.021)\tLoss 2.4331 (2.1577)\tPrec@1 82.812 (86.633)\tPrec@5 98.438 (99.348)\n",
      "Epoch: [98][96/97], lr: 0.01000\tTime 0.320 (0.335)\tData 0.000 (0.021)\tLoss 2.3084 (2.1712)\tPrec@1 88.136 (86.507)\tPrec@5 100.000 (99.363)\n",
      "Gated Network Weight Gate= Flip:0.49, Sc:0.51\n",
      "Test: [0/100]\tTime 0.285 (0.285)\tLoss 9.0871 (9.0871)\tPrec@1 59.000 (59.000)\tPrec@5 89.000 (89.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 6.5938 (8.1918)\tPrec@1 69.000 (61.000)\tPrec@5 95.000 (89.727)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.6377 (8.0468)\tPrec@1 68.000 (61.143)\tPrec@5 93.000 (90.333)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 8.3996 (8.0689)\tPrec@1 57.000 (61.097)\tPrec@5 90.000 (89.968)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 8.2247 (8.1200)\tPrec@1 60.000 (60.902)\tPrec@5 90.000 (89.756)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 7.8566 (8.0262)\tPrec@1 64.000 (61.745)\tPrec@5 89.000 (89.843)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 6.6994 (7.9871)\tPrec@1 70.000 (61.770)\tPrec@5 91.000 (89.934)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 7.9228 (8.0251)\tPrec@1 60.000 (61.563)\tPrec@5 91.000 (89.634)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 8.0948 (7.9845)\tPrec@1 66.000 (61.938)\tPrec@5 88.000 (89.667)\n",
      "Test: [90/100]\tTime 0.075 (0.076)\tLoss 8.2254 (8.0308)\tPrec@1 62.000 (61.736)\tPrec@5 88.000 (89.659)\n",
      "val Results: Prec@1 61.470 Prec@5 89.650 Loss 8.07059\n",
      "val Class Accuracy: [0.801,0.955,0.837,0.615,0.769,0.613,0.702,0.470,0.358,0.027]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [99][0/97], lr: 0.01000\tTime 0.544 (0.544)\tData 0.327 (0.327)\tLoss 2.1031 (2.1031)\tPrec@1 87.500 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [99][10/97], lr: 0.01000\tTime 0.324 (0.361)\tData 0.000 (0.043)\tLoss 1.8642 (2.0266)\tPrec@1 89.062 (87.855)\tPrec@5 100.000 (99.148)\n",
      "Epoch: [99][20/97], lr: 0.01000\tTime 0.320 (0.347)\tData 0.000 (0.031)\tLoss 2.2626 (2.0832)\tPrec@1 83.594 (87.574)\tPrec@5 98.438 (99.293)\n",
      "Epoch: [99][30/97], lr: 0.01000\tTime 0.322 (0.342)\tData 0.000 (0.026)\tLoss 2.0720 (2.1609)\tPrec@1 88.281 (87.097)\tPrec@5 99.219 (99.345)\n",
      "Epoch: [99][40/97], lr: 0.01000\tTime 0.322 (0.339)\tData 0.000 (0.024)\tLoss 2.0876 (2.1589)\tPrec@1 84.375 (86.814)\tPrec@5 100.000 (99.276)\n",
      "Epoch: [99][50/97], lr: 0.01000\tTime 0.343 (0.338)\tData 0.000 (0.023)\tLoss 1.4978 (2.1400)\tPrec@1 89.844 (86.949)\tPrec@5 99.219 (99.203)\n",
      "Epoch: [99][60/97], lr: 0.01000\tTime 0.323 (0.337)\tData 0.000 (0.022)\tLoss 2.4564 (2.1547)\tPrec@1 85.156 (86.847)\tPrec@5 99.219 (99.232)\n",
      "Epoch: [99][70/97], lr: 0.01000\tTime 0.320 (0.336)\tData 0.000 (0.021)\tLoss 2.8114 (2.1917)\tPrec@1 83.594 (86.675)\tPrec@5 98.438 (99.252)\n",
      "Epoch: [99][80/97], lr: 0.01000\tTime 0.320 (0.335)\tData 0.000 (0.021)\tLoss 1.9419 (2.1722)\tPrec@1 86.719 (86.796)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [99][90/97], lr: 0.01000\tTime 0.317 (0.334)\tData 0.000 (0.020)\tLoss 2.2758 (2.1866)\tPrec@1 85.156 (86.719)\tPrec@5 98.438 (99.296)\n",
      "Epoch: [99][96/97], lr: 0.01000\tTime 0.316 (0.333)\tData 0.000 (0.021)\tLoss 1.7376 (2.1701)\tPrec@1 89.831 (86.837)\tPrec@5 100.000 (99.307)\n",
      "Gated Network Weight Gate= Flip:0.51, Sc:0.49\n",
      "Test: [0/100]\tTime 0.261 (0.261)\tLoss 8.4104 (8.4104)\tPrec@1 61.000 (61.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.7364 (8.0875)\tPrec@1 69.000 (60.909)\tPrec@5 95.000 (94.818)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.4698 (8.0096)\tPrec@1 60.000 (60.952)\tPrec@5 96.000 (94.429)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.7510 (8.0258)\tPrec@1 62.000 (60.742)\tPrec@5 94.000 (94.323)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 7.4742 (7.9916)\tPrec@1 64.000 (61.390)\tPrec@5 97.000 (94.366)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 8.2803 (7.9750)\tPrec@1 58.000 (61.333)\tPrec@5 94.000 (94.235)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.0424 (8.0260)\tPrec@1 69.000 (61.164)\tPrec@5 92.000 (94.148)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 7.0607 (8.0346)\tPrec@1 66.000 (61.099)\tPrec@5 97.000 (94.099)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 7.7861 (8.0230)\tPrec@1 62.000 (61.099)\tPrec@5 96.000 (94.321)\n",
      "Test: [90/100]\tTime 0.074 (0.075)\tLoss 7.3416 (8.1021)\tPrec@1 66.000 (60.780)\tPrec@5 96.000 (94.187)\n",
      "val Results: Prec@1 60.760 Prec@5 94.200 Loss 8.13020\n",
      "val Class Accuracy: [0.859,0.976,0.821,0.776,0.806,0.318,0.267,0.520,0.484,0.249]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [100][0/97], lr: 0.01000\tTime 0.494 (0.494)\tData 0.276 (0.276)\tLoss 2.1011 (2.1011)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [100][10/97], lr: 0.01000\tTime 0.319 (0.350)\tData 0.000 (0.038)\tLoss 1.8339 (1.9614)\tPrec@1 88.281 (88.423)\tPrec@5 100.000 (99.148)\n",
      "Epoch: [100][20/97], lr: 0.01000\tTime 0.323 (0.340)\tData 0.000 (0.028)\tLoss 2.2624 (2.0930)\tPrec@1 85.156 (87.165)\tPrec@5 100.000 (99.368)\n",
      "Epoch: [100][30/97], lr: 0.01000\tTime 0.321 (0.336)\tData 0.000 (0.024)\tLoss 2.8793 (2.1013)\tPrec@1 82.812 (87.198)\tPrec@5 99.219 (99.370)\n",
      "Epoch: [100][40/97], lr: 0.01000\tTime 0.318 (0.334)\tData 0.000 (0.023)\tLoss 1.8422 (2.0617)\tPrec@1 89.062 (87.271)\tPrec@5 100.000 (99.276)\n",
      "Epoch: [100][50/97], lr: 0.01000\tTime 0.320 (0.332)\tData 0.000 (0.021)\tLoss 2.7490 (2.0465)\tPrec@1 82.031 (87.347)\tPrec@5 99.219 (99.295)\n",
      "Epoch: [100][60/97], lr: 0.01000\tTime 0.320 (0.331)\tData 0.000 (0.021)\tLoss 1.8892 (2.0625)\tPrec@1 87.500 (87.129)\tPrec@5 99.219 (99.283)\n",
      "Epoch: [100][70/97], lr: 0.01000\tTime 0.319 (0.330)\tData 0.000 (0.020)\tLoss 2.0127 (2.0910)\tPrec@1 88.281 (86.928)\tPrec@5 99.219 (99.329)\n",
      "Epoch: [100][80/97], lr: 0.01000\tTime 0.320 (0.330)\tData 0.000 (0.020)\tLoss 1.7095 (2.0901)\tPrec@1 89.844 (86.912)\tPrec@5 99.219 (99.315)\n",
      "Epoch: [100][90/97], lr: 0.01000\tTime 0.317 (0.329)\tData 0.000 (0.020)\tLoss 1.8907 (2.1083)\tPrec@1 87.500 (86.830)\tPrec@5 100.000 (99.279)\n",
      "Epoch: [100][96/97], lr: 0.01000\tTime 0.313 (0.329)\tData 0.000 (0.020)\tLoss 1.7441 (2.1201)\tPrec@1 89.831 (86.748)\tPrec@5 98.305 (99.291)\n",
      "Gated Network Weight Gate= Flip:0.59, Sc:0.41\n",
      "Test: [0/100]\tTime 0.275 (0.275)\tLoss 8.4320 (8.4320)\tPrec@1 58.000 (58.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 5.9184 (7.6058)\tPrec@1 70.000 (62.455)\tPrec@5 99.000 (97.545)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.8743 (7.6764)\tPrec@1 63.000 (61.762)\tPrec@5 98.000 (97.429)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.6460 (7.6437)\tPrec@1 67.000 (62.484)\tPrec@5 95.000 (97.226)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 7.1928 (7.7093)\tPrec@1 66.000 (62.341)\tPrec@5 96.000 (97.146)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 7.6499 (7.6746)\tPrec@1 65.000 (62.510)\tPrec@5 99.000 (97.196)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 6.2661 (7.6371)\tPrec@1 70.000 (62.525)\tPrec@5 97.000 (97.180)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 7.3694 (7.6506)\tPrec@1 66.000 (62.549)\tPrec@5 97.000 (97.085)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 7.4568 (7.6108)\tPrec@1 63.000 (62.654)\tPrec@5 98.000 (97.148)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 8.0137 (7.6728)\tPrec@1 65.000 (62.396)\tPrec@5 100.000 (97.231)\n",
      "val Results: Prec@1 62.280 Prec@5 97.190 Loss 7.70594\n",
      "val Class Accuracy: [0.968,0.987,0.723,0.490,0.781,0.482,0.811,0.651,0.301,0.034]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [101][0/97], lr: 0.01000\tTime 0.685 (0.685)\tData 0.374 (0.374)\tLoss 2.2930 (2.2930)\tPrec@1 86.719 (86.719)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [101][10/97], lr: 0.01000\tTime 0.376 (0.414)\tData 0.000 (0.045)\tLoss 1.6314 (1.9893)\tPrec@1 92.188 (87.855)\tPrec@5 99.219 (99.574)\n",
      "Epoch: [101][20/97], lr: 0.01000\tTime 0.329 (0.382)\tData 0.000 (0.031)\tLoss 1.0929 (1.9374)\tPrec@1 92.969 (87.723)\tPrec@5 100.000 (99.628)\n",
      "Epoch: [101][30/97], lr: 0.01000\tTime 0.320 (0.364)\tData 0.000 (0.026)\tLoss 1.7003 (1.9672)\tPrec@1 90.625 (87.853)\tPrec@5 100.000 (99.521)\n",
      "Epoch: [101][40/97], lr: 0.01000\tTime 0.319 (0.355)\tData 0.000 (0.024)\tLoss 2.1369 (1.9800)\tPrec@1 87.500 (87.786)\tPrec@5 98.438 (99.390)\n",
      "Epoch: [101][50/97], lr: 0.01000\tTime 0.319 (0.349)\tData 0.000 (0.023)\tLoss 2.3857 (1.9997)\tPrec@1 85.156 (87.699)\tPrec@5 100.000 (99.387)\n",
      "Epoch: [101][60/97], lr: 0.01000\tTime 0.319 (0.345)\tData 0.000 (0.022)\tLoss 2.0295 (2.0454)\tPrec@1 89.062 (87.500)\tPrec@5 98.438 (99.321)\n",
      "Epoch: [101][70/97], lr: 0.01000\tTime 0.322 (0.342)\tData 0.000 (0.021)\tLoss 2.4330 (2.0459)\tPrec@1 85.938 (87.566)\tPrec@5 96.875 (99.241)\n",
      "Epoch: [101][80/97], lr: 0.01000\tTime 0.319 (0.339)\tData 0.000 (0.021)\tLoss 2.7189 (2.0912)\tPrec@1 84.375 (87.288)\tPrec@5 100.000 (99.257)\n",
      "Epoch: [101][90/97], lr: 0.01000\tTime 0.319 (0.337)\tData 0.000 (0.020)\tLoss 2.4315 (2.1233)\tPrec@1 85.938 (87.165)\tPrec@5 98.438 (99.262)\n",
      "Epoch: [101][96/97], lr: 0.01000\tTime 0.308 (0.336)\tData 0.000 (0.021)\tLoss 3.1173 (2.1404)\tPrec@1 79.661 (86.990)\tPrec@5 97.458 (99.226)\n",
      "Gated Network Weight Gate= Flip:0.61, Sc:0.39\n",
      "Test: [0/100]\tTime 0.247 (0.247)\tLoss 7.9436 (7.9436)\tPrec@1 65.000 (65.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.074 (0.089)\tLoss 5.7203 (7.4393)\tPrec@1 71.000 (64.364)\tPrec@5 95.000 (95.000)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.2686 (7.3889)\tPrec@1 66.000 (64.238)\tPrec@5 98.000 (95.333)\n",
      "Test: [30/100]\tTime 0.074 (0.079)\tLoss 7.2242 (7.3915)\tPrec@1 66.000 (64.097)\tPrec@5 97.000 (95.194)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 6.8267 (7.4089)\tPrec@1 68.000 (64.122)\tPrec@5 97.000 (95.098)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.7486 (7.3507)\tPrec@1 65.000 (64.216)\tPrec@5 99.000 (95.294)\n",
      "Test: [60/100]\tTime 0.074 (0.076)\tLoss 5.6634 (7.2778)\tPrec@1 72.000 (64.525)\tPrec@5 95.000 (95.262)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 6.8003 (7.2865)\tPrec@1 63.000 (64.493)\tPrec@5 98.000 (95.239)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 7.2120 (7.2350)\tPrec@1 69.000 (64.877)\tPrec@5 92.000 (95.222)\n",
      "Test: [90/100]\tTime 0.074 (0.075)\tLoss 6.8776 (7.2927)\tPrec@1 69.000 (64.615)\tPrec@5 99.000 (95.253)\n",
      "val Results: Prec@1 64.610 Prec@5 95.260 Loss 7.31326\n",
      "val Class Accuracy: [0.940,0.970,0.700,0.581,0.593,0.816,0.594,0.695,0.424,0.148]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [102][0/97], lr: 0.01000\tTime 0.469 (0.469)\tData 0.272 (0.272)\tLoss 2.2567 (2.2567)\tPrec@1 87.500 (87.500)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [102][10/97], lr: 0.01000\tTime 0.334 (0.344)\tData 0.000 (0.039)\tLoss 1.9092 (2.0977)\tPrec@1 89.844 (87.145)\tPrec@5 98.438 (99.077)\n",
      "Epoch: [102][20/97], lr: 0.01000\tTime 0.320 (0.335)\tData 0.000 (0.028)\tLoss 2.1323 (2.0285)\tPrec@1 85.156 (87.686)\tPrec@5 98.438 (99.107)\n",
      "Epoch: [102][30/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.025)\tLoss 1.9801 (2.0717)\tPrec@1 86.719 (87.273)\tPrec@5 99.219 (99.093)\n",
      "Epoch: [102][40/97], lr: 0.01000\tTime 0.329 (0.330)\tData 0.000 (0.023)\tLoss 2.0042 (2.1048)\tPrec@1 87.500 (86.966)\tPrec@5 100.000 (99.162)\n",
      "Epoch: [102][50/97], lr: 0.01000\tTime 0.322 (0.329)\tData 0.000 (0.022)\tLoss 1.7840 (2.0958)\tPrec@1 89.062 (87.010)\tPrec@5 99.219 (99.188)\n",
      "Epoch: [102][60/97], lr: 0.01000\tTime 0.322 (0.328)\tData 0.000 (0.021)\tLoss 2.4149 (2.1066)\tPrec@1 85.156 (86.949)\tPrec@5 99.219 (99.257)\n",
      "Epoch: [102][70/97], lr: 0.01000\tTime 0.323 (0.328)\tData 0.000 (0.020)\tLoss 1.3868 (2.0894)\tPrec@1 89.062 (87.104)\tPrec@5 100.000 (99.252)\n",
      "Epoch: [102][80/97], lr: 0.01000\tTime 0.322 (0.327)\tData 0.000 (0.020)\tLoss 2.8584 (2.1159)\tPrec@1 80.469 (86.950)\tPrec@5 97.656 (99.171)\n",
      "Epoch: [102][90/97], lr: 0.01000\tTime 0.319 (0.327)\tData 0.000 (0.020)\tLoss 1.7749 (2.0986)\tPrec@1 89.062 (87.062)\tPrec@5 99.219 (99.210)\n",
      "Epoch: [102][96/97], lr: 0.01000\tTime 0.309 (0.326)\tData 0.000 (0.020)\tLoss 2.5564 (2.1104)\tPrec@1 83.898 (86.998)\tPrec@5 99.153 (99.218)\n",
      "Gated Network Weight Gate= Flip:0.66, Sc:0.34\n",
      "Test: [0/100]\tTime 0.260 (0.260)\tLoss 8.7598 (8.7598)\tPrec@1 60.000 (60.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 5.6674 (7.6918)\tPrec@1 71.000 (64.273)\tPrec@5 98.000 (96.091)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.7604 (7.5717)\tPrec@1 63.000 (64.238)\tPrec@5 100.000 (96.190)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.5044 (7.5240)\tPrec@1 68.000 (64.323)\tPrec@5 99.000 (96.419)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 8.1631 (7.5876)\tPrec@1 62.000 (64.195)\tPrec@5 94.000 (96.098)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.3217 (7.5493)\tPrec@1 66.000 (64.176)\tPrec@5 96.000 (96.118)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 5.9211 (7.4953)\tPrec@1 72.000 (64.377)\tPrec@5 97.000 (96.066)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 7.8351 (7.5018)\tPrec@1 59.000 (64.113)\tPrec@5 99.000 (96.183)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 7.3621 (7.4196)\tPrec@1 65.000 (64.630)\tPrec@5 96.000 (96.333)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.8131 (7.4639)\tPrec@1 70.000 (64.385)\tPrec@5 98.000 (96.341)\n",
      "val Results: Prec@1 64.250 Prec@5 96.360 Loss 7.49547\n",
      "val Class Accuracy: [0.905,0.989,0.830,0.432,0.775,0.782,0.678,0.544,0.371,0.119]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [103][0/97], lr: 0.01000\tTime 0.490 (0.490)\tData 0.271 (0.271)\tLoss 1.9082 (1.9082)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [103][10/97], lr: 0.01000\tTime 0.322 (0.340)\tData 0.000 (0.039)\tLoss 2.0974 (2.1477)\tPrec@1 87.500 (87.145)\tPrec@5 99.219 (99.361)\n",
      "Epoch: [103][20/97], lr: 0.01000\tTime 0.329 (0.332)\tData 0.000 (0.028)\tLoss 2.4194 (2.0866)\tPrec@1 84.375 (87.277)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [103][30/97], lr: 0.01000\tTime 0.319 (0.329)\tData 0.000 (0.024)\tLoss 1.7731 (2.0782)\tPrec@1 87.500 (87.374)\tPrec@5 98.438 (99.446)\n",
      "Epoch: [103][40/97], lr: 0.01000\tTime 0.321 (0.328)\tData 0.000 (0.023)\tLoss 1.4975 (2.0824)\tPrec@1 90.625 (87.329)\tPrec@5 99.219 (99.409)\n",
      "Epoch: [103][50/97], lr: 0.01000\tTime 0.319 (0.327)\tData 0.000 (0.022)\tLoss 1.6906 (2.0418)\tPrec@1 88.281 (87.454)\tPrec@5 100.000 (99.464)\n",
      "Epoch: [103][60/97], lr: 0.01000\tTime 0.320 (0.326)\tData 0.000 (0.021)\tLoss 2.3189 (2.0280)\tPrec@1 85.156 (87.577)\tPrec@5 100.000 (99.475)\n",
      "Epoch: [103][70/97], lr: 0.01000\tTime 0.319 (0.326)\tData 0.000 (0.020)\tLoss 2.2709 (2.0365)\tPrec@1 84.375 (87.511)\tPrec@5 98.438 (99.428)\n",
      "Epoch: [103][80/97], lr: 0.01000\tTime 0.317 (0.326)\tData 0.000 (0.020)\tLoss 1.2134 (2.0386)\tPrec@1 94.531 (87.519)\tPrec@5 100.000 (99.325)\n",
      "Epoch: [103][90/97], lr: 0.01000\tTime 0.319 (0.325)\tData 0.000 (0.020)\tLoss 1.6135 (2.0275)\tPrec@1 91.406 (87.629)\tPrec@5 100.000 (99.339)\n",
      "Epoch: [103][96/97], lr: 0.01000\tTime 0.308 (0.325)\tData 0.000 (0.020)\tLoss 2.1015 (2.0311)\tPrec@1 87.288 (87.619)\tPrec@5 98.305 (99.299)\n",
      "Gated Network Weight Gate= Flip:0.60, Sc:0.40\n",
      "Test: [0/100]\tTime 0.249 (0.249)\tLoss 9.5748 (9.5748)\tPrec@1 55.000 (55.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.074 (0.090)\tLoss 6.1925 (8.1801)\tPrec@1 71.000 (60.455)\tPrec@5 96.000 (95.818)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.8420 (8.1446)\tPrec@1 66.000 (59.667)\tPrec@5 97.000 (95.952)\n",
      "Test: [30/100]\tTime 0.074 (0.079)\tLoss 7.7061 (8.2204)\tPrec@1 61.000 (59.419)\tPrec@5 95.000 (95.677)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 8.2588 (8.2524)\tPrec@1 60.000 (59.220)\tPrec@5 94.000 (95.561)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 7.7587 (8.1839)\tPrec@1 63.000 (59.529)\tPrec@5 96.000 (95.686)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 7.1109 (8.1455)\tPrec@1 65.000 (59.656)\tPrec@5 97.000 (95.803)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 8.3098 (8.1232)\tPrec@1 62.000 (59.859)\tPrec@5 95.000 (95.972)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 8.1218 (8.0740)\tPrec@1 60.000 (60.086)\tPrec@5 95.000 (96.148)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 8.2082 (8.1314)\tPrec@1 62.000 (59.912)\tPrec@5 98.000 (96.099)\n",
      "val Results: Prec@1 59.960 Prec@5 96.080 Loss 8.13986\n",
      "val Class Accuracy: [0.941,0.983,0.651,0.710,0.574,0.666,0.730,0.482,0.215,0.044]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [104][0/97], lr: 0.01000\tTime 0.542 (0.542)\tData 0.308 (0.308)\tLoss 2.1168 (2.1168)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [104][10/97], lr: 0.01000\tTime 0.325 (0.353)\tData 0.000 (0.043)\tLoss 1.5547 (1.9730)\tPrec@1 89.844 (87.855)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [104][20/97], lr: 0.01000\tTime 0.320 (0.339)\tData 0.000 (0.030)\tLoss 1.8527 (2.0045)\tPrec@1 87.500 (87.946)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [104][30/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.026)\tLoss 2.2073 (2.0716)\tPrec@1 86.719 (87.374)\tPrec@5 98.438 (99.370)\n",
      "Epoch: [104][40/97], lr: 0.01000\tTime 0.329 (0.332)\tData 0.000 (0.024)\tLoss 1.9303 (2.0760)\tPrec@1 88.281 (87.386)\tPrec@5 100.000 (99.314)\n",
      "Epoch: [104][50/97], lr: 0.01000\tTime 0.326 (0.330)\tData 0.000 (0.022)\tLoss 2.3618 (2.0558)\tPrec@1 85.156 (87.531)\tPrec@5 96.875 (99.265)\n",
      "Epoch: [104][60/97], lr: 0.01000\tTime 0.319 (0.329)\tData 0.000 (0.022)\tLoss 2.2223 (2.0620)\tPrec@1 88.281 (87.590)\tPrec@5 100.000 (99.283)\n",
      "Epoch: [104][70/97], lr: 0.01000\tTime 0.321 (0.328)\tData 0.000 (0.021)\tLoss 2.1155 (2.1029)\tPrec@1 86.719 (87.280)\tPrec@5 99.219 (99.241)\n",
      "Epoch: [104][80/97], lr: 0.01000\tTime 0.320 (0.328)\tData 0.000 (0.020)\tLoss 2.8545 (2.1177)\tPrec@1 81.250 (87.162)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [104][90/97], lr: 0.01000\tTime 0.323 (0.327)\tData 0.000 (0.020)\tLoss 1.6550 (2.1169)\tPrec@1 89.844 (87.139)\tPrec@5 100.000 (99.227)\n",
      "Epoch: [104][96/97], lr: 0.01000\tTime 0.309 (0.327)\tData 0.000 (0.021)\tLoss 2.0990 (2.1030)\tPrec@1 85.593 (87.176)\tPrec@5 99.153 (99.250)\n",
      "Gated Network Weight Gate= Flip:0.52, Sc:0.48\n",
      "Test: [0/100]\tTime 0.265 (0.265)\tLoss 9.4092 (9.4092)\tPrec@1 57.000 (57.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 6.9480 (9.1249)\tPrec@1 69.000 (55.455)\tPrec@5 98.000 (97.636)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.9424 (8.9737)\tPrec@1 59.000 (55.905)\tPrec@5 98.000 (97.238)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 8.6385 (8.9229)\tPrec@1 55.000 (56.355)\tPrec@5 97.000 (97.129)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 9.7833 (8.9645)\tPrec@1 49.000 (56.293)\tPrec@5 93.000 (96.732)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 9.2445 (8.8963)\tPrec@1 54.000 (56.490)\tPrec@5 96.000 (96.824)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.6968 (8.9413)\tPrec@1 59.000 (55.967)\tPrec@5 95.000 (96.721)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.8088 (8.9083)\tPrec@1 59.000 (56.197)\tPrec@5 99.000 (96.817)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 8.6932 (8.8540)\tPrec@1 56.000 (56.444)\tPrec@5 97.000 (96.988)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.9249 (8.9161)\tPrec@1 54.000 (56.165)\tPrec@5 99.000 (96.934)\n",
      "val Results: Prec@1 56.100 Prec@5 96.960 Loss 8.94404\n",
      "val Class Accuracy: [0.853,0.992,0.906,0.617,0.423,0.631,0.328,0.383,0.419,0.058]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [105][0/97], lr: 0.01000\tTime 0.494 (0.494)\tData 0.270 (0.270)\tLoss 2.8165 (2.8165)\tPrec@1 82.812 (82.812)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [105][10/97], lr: 0.01000\tTime 0.334 (0.345)\tData 0.000 (0.039)\tLoss 2.1083 (2.3540)\tPrec@1 86.719 (86.080)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [105][20/97], lr: 0.01000\tTime 0.332 (0.343)\tData 0.000 (0.028)\tLoss 2.3505 (2.3123)\tPrec@1 86.719 (85.826)\tPrec@5 98.438 (99.256)\n",
      "Epoch: [105][30/97], lr: 0.01000\tTime 0.321 (0.336)\tData 0.000 (0.024)\tLoss 1.8473 (2.1613)\tPrec@1 90.625 (87.072)\tPrec@5 99.219 (99.168)\n",
      "Epoch: [105][40/97], lr: 0.01000\tTime 0.323 (0.333)\tData 0.000 (0.023)\tLoss 1.9600 (2.1263)\tPrec@1 87.500 (87.271)\tPrec@5 98.438 (99.257)\n",
      "Epoch: [105][50/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.021)\tLoss 1.9898 (2.1329)\tPrec@1 89.062 (87.240)\tPrec@5 99.219 (99.311)\n",
      "Epoch: [105][60/97], lr: 0.01000\tTime 0.321 (0.329)\tData 0.000 (0.021)\tLoss 1.8103 (2.1179)\tPrec@1 89.062 (87.295)\tPrec@5 100.000 (99.347)\n",
      "Epoch: [105][70/97], lr: 0.01000\tTime 0.323 (0.329)\tData 0.000 (0.020)\tLoss 2.3466 (2.1117)\tPrec@1 85.156 (87.346)\tPrec@5 96.875 (99.351)\n",
      "Epoch: [105][80/97], lr: 0.01000\tTime 0.320 (0.328)\tData 0.000 (0.020)\tLoss 2.5482 (2.0936)\tPrec@1 82.812 (87.452)\tPrec@5 99.219 (99.383)\n",
      "Epoch: [105][90/97], lr: 0.01000\tTime 0.321 (0.327)\tData 0.000 (0.019)\tLoss 2.2066 (2.0879)\tPrec@1 86.719 (87.629)\tPrec@5 99.219 (99.390)\n",
      "Epoch: [105][96/97], lr: 0.01000\tTime 0.311 (0.327)\tData 0.000 (0.020)\tLoss 1.9381 (2.0722)\tPrec@1 87.288 (87.740)\tPrec@5 100.000 (99.379)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.303 (0.303)\tLoss 8.8275 (8.8275)\tPrec@1 60.000 (60.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.094)\tLoss 6.5976 (8.1711)\tPrec@1 68.000 (61.182)\tPrec@5 96.000 (96.455)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 7.2391 (8.1256)\tPrec@1 64.000 (61.143)\tPrec@5 94.000 (95.429)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 7.6727 (8.1659)\tPrec@1 60.000 (60.903)\tPrec@5 93.000 (95.290)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 8.8888 (8.1802)\tPrec@1 57.000 (60.780)\tPrec@5 92.000 (95.220)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 7.6965 (8.0721)\tPrec@1 65.000 (61.216)\tPrec@5 94.000 (95.412)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 7.2675 (8.0815)\tPrec@1 63.000 (60.918)\tPrec@5 92.000 (95.344)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 8.2429 (8.0749)\tPrec@1 63.000 (61.014)\tPrec@5 98.000 (95.423)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 7.1103 (8.0297)\tPrec@1 66.000 (61.309)\tPrec@5 98.000 (95.556)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 7.9864 (8.0852)\tPrec@1 65.000 (61.033)\tPrec@5 98.000 (95.549)\n",
      "val Results: Prec@1 60.930 Prec@5 95.510 Loss 8.11746\n",
      "val Class Accuracy: [0.972,0.995,0.682,0.554,0.818,0.504,0.685,0.321,0.346,0.216]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [106][0/97], lr: 0.01000\tTime 0.522 (0.522)\tData 0.303 (0.303)\tLoss 1.4880 (1.4880)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [106][10/97], lr: 0.01000\tTime 0.326 (0.346)\tData 0.000 (0.041)\tLoss 2.1651 (1.9809)\tPrec@1 89.062 (87.784)\tPrec@5 99.219 (99.361)\n",
      "Epoch: [106][20/97], lr: 0.01000\tTime 0.322 (0.335)\tData 0.000 (0.030)\tLoss 2.1100 (2.0329)\tPrec@1 86.719 (87.723)\tPrec@5 100.000 (99.033)\n",
      "Epoch: [106][30/97], lr: 0.01000\tTime 0.320 (0.332)\tData 0.000 (0.026)\tLoss 1.6958 (2.0071)\tPrec@1 90.625 (88.105)\tPrec@5 100.000 (99.118)\n",
      "Epoch: [106][40/97], lr: 0.01000\tTime 0.327 (0.331)\tData 0.000 (0.023)\tLoss 1.1812 (2.0406)\tPrec@1 91.406 (87.652)\tPrec@5 100.000 (99.200)\n",
      "Epoch: [106][50/97], lr: 0.01000\tTime 0.322 (0.329)\tData 0.000 (0.022)\tLoss 1.7810 (2.0409)\tPrec@1 87.500 (87.531)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [106][60/97], lr: 0.01000\tTime 0.322 (0.328)\tData 0.000 (0.021)\tLoss 2.4273 (2.0651)\tPrec@1 82.031 (87.333)\tPrec@5 98.438 (99.232)\n",
      "Epoch: [106][70/97], lr: 0.01000\tTime 0.322 (0.328)\tData 0.000 (0.021)\tLoss 1.5567 (2.0411)\tPrec@1 91.406 (87.401)\tPrec@5 100.000 (99.263)\n",
      "Epoch: [106][80/97], lr: 0.01000\tTime 0.319 (0.327)\tData 0.000 (0.020)\tLoss 2.6228 (2.0766)\tPrec@1 80.469 (87.066)\tPrec@5 99.219 (99.209)\n",
      "Epoch: [106][90/97], lr: 0.01000\tTime 0.325 (0.327)\tData 0.000 (0.020)\tLoss 2.1430 (2.0884)\tPrec@1 87.500 (86.959)\tPrec@5 99.219 (99.245)\n",
      "Epoch: [106][96/97], lr: 0.01000\tTime 0.314 (0.327)\tData 0.000 (0.020)\tLoss 2.9066 (2.1024)\tPrec@1 80.508 (86.821)\tPrec@5 99.153 (99.258)\n",
      "Gated Network Weight Gate= Flip:0.46, Sc:0.54\n",
      "Test: [0/100]\tTime 0.273 (0.273)\tLoss 9.2633 (9.2633)\tPrec@1 54.000 (54.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 7.6717 (8.4796)\tPrec@1 62.000 (58.364)\tPrec@5 95.000 (96.091)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 7.7199 (8.4822)\tPrec@1 66.000 (59.619)\tPrec@5 96.000 (95.333)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 7.9532 (8.4734)\tPrec@1 61.000 (59.516)\tPrec@5 92.000 (95.000)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 8.1128 (8.4537)\tPrec@1 64.000 (59.683)\tPrec@5 95.000 (94.854)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.4811 (8.3289)\tPrec@1 66.000 (60.333)\tPrec@5 92.000 (94.922)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 7.6392 (8.3367)\tPrec@1 63.000 (60.197)\tPrec@5 95.000 (94.721)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.9293 (8.3362)\tPrec@1 57.000 (60.183)\tPrec@5 96.000 (94.873)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 7.6783 (8.2871)\tPrec@1 61.000 (60.383)\tPrec@5 98.000 (94.988)\n",
      "Test: [90/100]\tTime 0.074 (0.075)\tLoss 7.1266 (8.3788)\tPrec@1 65.000 (59.901)\tPrec@5 96.000 (94.901)\n",
      "val Results: Prec@1 59.760 Prec@5 94.770 Loss 8.42714\n",
      "val Class Accuracy: [0.854,0.999,0.652,0.458,0.760,0.602,0.601,0.442,0.376,0.232]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [107][0/97], lr: 0.01000\tTime 0.426 (0.426)\tData 0.235 (0.235)\tLoss 1.9831 (1.9831)\tPrec@1 86.719 (86.719)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [107][10/97], lr: 0.01000\tTime 0.327 (0.339)\tData 0.000 (0.036)\tLoss 1.7841 (2.2133)\tPrec@1 89.062 (85.795)\tPrec@5 99.219 (99.574)\n",
      "Epoch: [107][20/97], lr: 0.01000\tTime 0.355 (0.333)\tData 0.000 (0.027)\tLoss 1.9783 (2.2060)\tPrec@1 89.844 (85.975)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [107][30/97], lr: 0.01000\tTime 0.322 (0.330)\tData 0.000 (0.024)\tLoss 1.8043 (2.2221)\tPrec@1 88.281 (85.862)\tPrec@5 100.000 (99.244)\n",
      "Epoch: [107][40/97], lr: 0.01000\tTime 0.322 (0.329)\tData 0.000 (0.022)\tLoss 1.8076 (2.1732)\tPrec@1 89.844 (86.433)\tPrec@5 100.000 (99.333)\n",
      "Epoch: [107][50/97], lr: 0.01000\tTime 0.322 (0.328)\tData 0.000 (0.021)\tLoss 2.0997 (2.1292)\tPrec@1 85.938 (86.749)\tPrec@5 100.000 (99.311)\n",
      "Epoch: [107][60/97], lr: 0.01000\tTime 0.323 (0.327)\tData 0.000 (0.020)\tLoss 2.3778 (2.1080)\tPrec@1 84.375 (86.796)\tPrec@5 100.000 (99.372)\n",
      "Epoch: [107][70/97], lr: 0.01000\tTime 0.326 (0.327)\tData 0.000 (0.020)\tLoss 1.6258 (2.0702)\tPrec@1 89.062 (87.027)\tPrec@5 100.000 (99.351)\n",
      "Epoch: [107][80/97], lr: 0.01000\tTime 0.323 (0.326)\tData 0.000 (0.020)\tLoss 2.2089 (2.1096)\tPrec@1 87.500 (86.854)\tPrec@5 100.000 (99.334)\n",
      "Epoch: [107][90/97], lr: 0.01000\tTime 0.320 (0.327)\tData 0.000 (0.019)\tLoss 2.5024 (2.1244)\tPrec@1 84.375 (86.787)\tPrec@5 98.438 (99.348)\n",
      "Epoch: [107][96/97], lr: 0.01000\tTime 0.308 (0.326)\tData 0.000 (0.020)\tLoss 2.4709 (2.1410)\tPrec@1 83.898 (86.676)\tPrec@5 98.305 (99.283)\n",
      "Gated Network Weight Gate= Flip:0.59, Sc:0.41\n",
      "Test: [0/100]\tTime 0.296 (0.296)\tLoss 10.2298 (10.2298)\tPrec@1 52.000 (52.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.094)\tLoss 7.2390 (9.3848)\tPrec@1 66.000 (57.182)\tPrec@5 97.000 (92.091)\n",
      "Test: [20/100]\tTime 0.074 (0.084)\tLoss 9.4001 (9.2047)\tPrec@1 54.000 (57.762)\tPrec@5 95.000 (92.524)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 8.7213 (9.2354)\tPrec@1 58.000 (57.290)\tPrec@5 92.000 (92.194)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 9.8336 (9.3597)\tPrec@1 54.000 (56.659)\tPrec@5 92.000 (92.098)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 9.7703 (9.3091)\tPrec@1 52.000 (56.745)\tPrec@5 93.000 (92.039)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 7.0407 (9.2665)\tPrec@1 65.000 (56.836)\tPrec@5 96.000 (92.164)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 8.6713 (9.2324)\tPrec@1 62.000 (57.127)\tPrec@5 94.000 (92.000)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 8.8027 (9.1943)\tPrec@1 61.000 (57.247)\tPrec@5 89.000 (91.975)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 9.2683 (9.2591)\tPrec@1 59.000 (56.879)\tPrec@5 94.000 (91.923)\n",
      "val Results: Prec@1 56.810 Prec@5 91.930 Loss 9.28083\n",
      "val Class Accuracy: [0.861,0.939,0.852,0.574,0.854,0.588,0.549,0.105,0.319,0.040]\n",
      "Best Prec@1: 64.720\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [108][0/97], lr: 0.01000\tTime 0.474 (0.474)\tData 0.240 (0.240)\tLoss 2.4774 (2.4774)\tPrec@1 84.375 (84.375)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [108][10/97], lr: 0.01000\tTime 0.327 (0.342)\tData 0.000 (0.035)\tLoss 2.5415 (2.3346)\tPrec@1 85.938 (85.511)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [108][20/97], lr: 0.01000\tTime 0.320 (0.333)\tData 0.000 (0.026)\tLoss 2.1702 (2.2529)\tPrec@1 85.938 (86.049)\tPrec@5 99.219 (99.144)\n",
      "Epoch: [108][30/97], lr: 0.01000\tTime 0.323 (0.330)\tData 0.000 (0.023)\tLoss 1.9624 (2.1909)\tPrec@1 86.719 (86.442)\tPrec@5 99.219 (99.269)\n",
      "Epoch: [108][40/97], lr: 0.01000\tTime 0.333 (0.329)\tData 0.000 (0.022)\tLoss 2.4223 (2.1700)\tPrec@1 85.938 (86.643)\tPrec@5 99.219 (99.295)\n",
      "Epoch: [108][50/97], lr: 0.01000\tTime 0.321 (0.328)\tData 0.000 (0.021)\tLoss 2.4304 (2.1403)\tPrec@1 83.594 (86.657)\tPrec@5 98.438 (99.311)\n",
      "Epoch: [108][60/97], lr: 0.01000\tTime 0.324 (0.328)\tData 0.000 (0.020)\tLoss 1.8739 (2.0968)\tPrec@1 87.500 (87.065)\tPrec@5 100.000 (99.360)\n",
      "Epoch: [108][70/97], lr: 0.01000\tTime 0.321 (0.327)\tData 0.000 (0.020)\tLoss 2.5821 (2.0957)\tPrec@1 82.812 (87.060)\tPrec@5 98.438 (99.384)\n",
      "Epoch: [108][80/97], lr: 0.01000\tTime 0.319 (0.327)\tData 0.000 (0.019)\tLoss 1.9490 (2.0805)\tPrec@1 87.500 (87.133)\tPrec@5 98.438 (99.383)\n",
      "Epoch: [108][90/97], lr: 0.01000\tTime 0.323 (0.327)\tData 0.000 (0.019)\tLoss 2.2406 (2.0796)\tPrec@1 85.156 (87.148)\tPrec@5 100.000 (99.365)\n",
      "Epoch: [108][96/97], lr: 0.01000\tTime 0.314 (0.327)\tData 0.000 (0.020)\tLoss 1.5158 (2.0666)\tPrec@1 92.373 (87.232)\tPrec@5 100.000 (99.347)\n",
      "Gated Network Weight Gate= Flip:0.47, Sc:0.53\n",
      "Test: [0/100]\tTime 0.261 (0.261)\tLoss 6.6639 (6.6639)\tPrec@1 72.000 (72.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.074 (0.090)\tLoss 5.4758 (6.9655)\tPrec@1 70.000 (66.909)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.074 (0.082)\tLoss 6.3706 (6.9326)\tPrec@1 65.000 (66.571)\tPrec@5 98.000 (97.810)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 6.3152 (6.9448)\tPrec@1 67.000 (66.581)\tPrec@5 99.000 (97.484)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 7.1423 (6.9878)\tPrec@1 67.000 (66.439)\tPrec@5 98.000 (97.146)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 7.7814 (6.9642)\tPrec@1 61.000 (66.549)\tPrec@5 96.000 (97.137)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 6.4123 (6.9822)\tPrec@1 66.000 (66.344)\tPrec@5 98.000 (97.131)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 7.0294 (6.9822)\tPrec@1 66.000 (66.408)\tPrec@5 99.000 (97.268)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 7.2945 (6.9373)\tPrec@1 63.000 (66.506)\tPrec@5 98.000 (97.333)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 6.9328 (6.9938)\tPrec@1 66.000 (66.198)\tPrec@5 99.000 (97.264)\n",
      "val Results: Prec@1 66.250 Prec@5 97.300 Loss 6.99605\n",
      "val Class Accuracy: [0.912,0.994,0.819,0.786,0.571,0.538,0.692,0.548,0.526,0.239]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [109][0/97], lr: 0.01000\tTime 0.485 (0.485)\tData 0.275 (0.275)\tLoss 2.3602 (2.3602)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [109][10/97], lr: 0.01000\tTime 0.326 (0.340)\tData 0.000 (0.039)\tLoss 1.2697 (2.0044)\tPrec@1 93.750 (88.281)\tPrec@5 100.000 (99.148)\n",
      "Epoch: [109][20/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.028)\tLoss 1.9512 (1.9988)\tPrec@1 87.500 (88.058)\tPrec@5 100.000 (99.107)\n",
      "Epoch: [109][30/97], lr: 0.01000\tTime 0.324 (0.329)\tData 0.000 (0.025)\tLoss 2.1669 (2.0766)\tPrec@1 88.281 (87.626)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [109][40/97], lr: 0.01000\tTime 0.321 (0.328)\tData 0.000 (0.023)\tLoss 1.8151 (2.0578)\tPrec@1 89.844 (87.786)\tPrec@5 100.000 (99.257)\n",
      "Epoch: [109][50/97], lr: 0.01000\tTime 0.320 (0.327)\tData 0.000 (0.022)\tLoss 1.8032 (2.0426)\tPrec@1 89.062 (87.837)\tPrec@5 99.219 (99.265)\n",
      "Epoch: [109][60/97], lr: 0.01000\tTime 0.319 (0.326)\tData 0.000 (0.021)\tLoss 2.3196 (2.0696)\tPrec@1 85.938 (87.692)\tPrec@5 100.000 (99.283)\n",
      "Epoch: [109][70/97], lr: 0.01000\tTime 0.327 (0.326)\tData 0.000 (0.020)\tLoss 1.6128 (2.0570)\tPrec@1 90.625 (87.720)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [109][80/97], lr: 0.01000\tTime 0.320 (0.326)\tData 0.000 (0.020)\tLoss 1.3654 (2.0493)\tPrec@1 92.188 (87.741)\tPrec@5 100.000 (99.315)\n",
      "Epoch: [109][90/97], lr: 0.01000\tTime 0.317 (0.325)\tData 0.000 (0.020)\tLoss 2.3785 (2.0387)\tPrec@1 85.156 (87.792)\tPrec@5 100.000 (99.348)\n",
      "Epoch: [109][96/97], lr: 0.01000\tTime 0.310 (0.325)\tData 0.000 (0.020)\tLoss 1.6680 (2.0333)\tPrec@1 91.525 (87.812)\tPrec@5 97.458 (99.315)\n",
      "Gated Network Weight Gate= Flip:0.63, Sc:0.37\n",
      "Test: [0/100]\tTime 0.245 (0.245)\tLoss 10.5859 (10.5859)\tPrec@1 51.000 (51.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 8.0914 (9.2349)\tPrec@1 60.000 (57.636)\tPrec@5 93.000 (92.364)\n",
      "Test: [20/100]\tTime 0.074 (0.082)\tLoss 7.9245 (9.1251)\tPrec@1 57.000 (57.143)\tPrec@5 97.000 (92.810)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.1729 (9.1375)\tPrec@1 62.000 (57.129)\tPrec@5 92.000 (92.548)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 9.3407 (9.1167)\tPrec@1 54.000 (57.000)\tPrec@5 92.000 (92.610)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 8.6216 (8.9991)\tPrec@1 61.000 (57.588)\tPrec@5 91.000 (92.490)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.4957 (8.9676)\tPrec@1 64.000 (57.508)\tPrec@5 92.000 (92.361)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.1152 (8.9337)\tPrec@1 63.000 (57.859)\tPrec@5 95.000 (92.352)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 7.6036 (8.8904)\tPrec@1 64.000 (58.049)\tPrec@5 93.000 (92.432)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 9.5286 (8.9904)\tPrec@1 55.000 (57.637)\tPrec@5 92.000 (92.198)\n",
      "val Results: Prec@1 57.600 Prec@5 92.380 Loss 9.01377\n",
      "val Class Accuracy: [0.918,0.997,0.602,0.800,0.761,0.513,0.228,0.610,0.099,0.232]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [110][0/97], lr: 0.01000\tTime 0.418 (0.418)\tData 0.222 (0.222)\tLoss 2.0172 (2.0172)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [110][10/97], lr: 0.01000\tTime 0.327 (0.336)\tData 0.000 (0.034)\tLoss 2.1540 (2.2191)\tPrec@1 87.500 (86.861)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [110][20/97], lr: 0.01000\tTime 0.318 (0.330)\tData 0.000 (0.026)\tLoss 1.8120 (2.0086)\tPrec@1 92.188 (88.170)\tPrec@5 99.219 (99.368)\n",
      "Epoch: [110][30/97], lr: 0.01000\tTime 0.321 (0.328)\tData 0.000 (0.023)\tLoss 2.4329 (2.0645)\tPrec@1 86.719 (87.903)\tPrec@5 99.219 (99.294)\n",
      "Epoch: [110][40/97], lr: 0.01000\tTime 0.323 (0.327)\tData 0.000 (0.022)\tLoss 1.8905 (2.0948)\tPrec@1 89.844 (87.576)\tPrec@5 99.219 (99.333)\n",
      "Epoch: [110][50/97], lr: 0.01000\tTime 0.321 (0.327)\tData 0.000 (0.021)\tLoss 1.6970 (2.1107)\tPrec@1 90.625 (87.286)\tPrec@5 98.438 (99.188)\n",
      "Epoch: [110][60/97], lr: 0.01000\tTime 0.320 (0.327)\tData 0.000 (0.020)\tLoss 1.9723 (2.1034)\tPrec@1 88.281 (87.218)\tPrec@5 99.219 (99.193)\n",
      "Epoch: [110][70/97], lr: 0.01000\tTime 0.321 (0.326)\tData 0.000 (0.020)\tLoss 1.8746 (2.0781)\tPrec@1 88.281 (87.412)\tPrec@5 100.000 (99.241)\n",
      "Epoch: [110][80/97], lr: 0.01000\tTime 0.321 (0.325)\tData 0.000 (0.019)\tLoss 2.1533 (2.0709)\tPrec@1 85.938 (87.548)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [110][90/97], lr: 0.01000\tTime 0.323 (0.325)\tData 0.000 (0.019)\tLoss 1.4458 (2.0491)\tPrec@1 91.406 (87.655)\tPrec@5 100.000 (99.236)\n",
      "Epoch: [110][96/97], lr: 0.01000\tTime 0.310 (0.325)\tData 0.000 (0.020)\tLoss 2.0313 (2.0477)\tPrec@1 88.983 (87.700)\tPrec@5 99.153 (99.258)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.283 (0.283)\tLoss 8.3906 (8.3906)\tPrec@1 61.000 (61.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 7.2409 (8.6151)\tPrec@1 65.000 (57.909)\tPrec@5 94.000 (94.818)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 8.2915 (8.4999)\tPrec@1 59.000 (58.524)\tPrec@5 97.000 (95.476)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 8.6860 (8.6452)\tPrec@1 58.000 (58.097)\tPrec@5 95.000 (95.452)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 9.5433 (8.7101)\tPrec@1 56.000 (58.000)\tPrec@5 95.000 (95.171)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 8.9496 (8.6470)\tPrec@1 56.000 (58.373)\tPrec@5 94.000 (95.176)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 7.9372 (8.6781)\tPrec@1 63.000 (58.180)\tPrec@5 98.000 (95.230)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 7.7308 (8.6616)\tPrec@1 66.000 (58.296)\tPrec@5 95.000 (95.141)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 8.3434 (8.6246)\tPrec@1 63.000 (58.457)\tPrec@5 92.000 (95.259)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 9.5665 (8.7174)\tPrec@1 54.000 (57.956)\tPrec@5 99.000 (95.253)\n",
      "val Results: Prec@1 57.940 Prec@5 95.260 Loss 8.73775\n",
      "val Class Accuracy: [0.901,0.977,0.629,0.654,0.946,0.391,0.428,0.238,0.546,0.084]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [111][0/97], lr: 0.01000\tTime 0.467 (0.467)\tData 0.243 (0.243)\tLoss 1.7322 (1.7322)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [111][10/97], lr: 0.01000\tTime 0.320 (0.341)\tData 0.000 (0.036)\tLoss 2.4980 (2.3488)\tPrec@1 87.500 (85.440)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [111][20/97], lr: 0.01000\tTime 0.336 (0.333)\tData 0.000 (0.027)\tLoss 1.4278 (2.1284)\tPrec@1 92.188 (87.054)\tPrec@5 99.219 (99.330)\n",
      "Epoch: [111][30/97], lr: 0.01000\tTime 0.323 (0.330)\tData 0.000 (0.024)\tLoss 1.9751 (2.0547)\tPrec@1 89.844 (87.651)\tPrec@5 100.000 (99.370)\n",
      "Epoch: [111][40/97], lr: 0.01000\tTime 0.322 (0.329)\tData 0.000 (0.022)\tLoss 1.8241 (2.0142)\tPrec@1 88.281 (88.014)\tPrec@5 98.438 (99.371)\n",
      "Epoch: [111][50/97], lr: 0.01000\tTime 0.321 (0.327)\tData 0.000 (0.021)\tLoss 2.5530 (2.0419)\tPrec@1 85.938 (87.714)\tPrec@5 99.219 (99.357)\n",
      "Epoch: [111][60/97], lr: 0.01000\tTime 0.322 (0.327)\tData 0.000 (0.020)\tLoss 1.8977 (2.0702)\tPrec@1 88.281 (87.538)\tPrec@5 99.219 (99.334)\n",
      "Epoch: [111][70/97], lr: 0.01000\tTime 0.322 (0.326)\tData 0.000 (0.020)\tLoss 1.9760 (2.0800)\tPrec@1 88.281 (87.489)\tPrec@5 99.219 (99.340)\n",
      "Epoch: [111][80/97], lr: 0.01000\tTime 0.321 (0.326)\tData 0.000 (0.020)\tLoss 1.9066 (2.0949)\tPrec@1 89.844 (87.326)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [111][90/97], lr: 0.01000\tTime 0.318 (0.326)\tData 0.000 (0.019)\tLoss 2.0055 (2.0838)\tPrec@1 88.281 (87.371)\tPrec@5 100.000 (99.305)\n",
      "Epoch: [111][96/97], lr: 0.01000\tTime 0.309 (0.325)\tData 0.000 (0.020)\tLoss 1.5820 (2.0933)\tPrec@1 89.831 (87.248)\tPrec@5 99.153 (99.331)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.246 (0.246)\tLoss 8.5555 (8.5555)\tPrec@1 60.000 (60.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 6.3501 (8.1515)\tPrec@1 69.000 (60.636)\tPrec@5 98.000 (97.091)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 6.7866 (8.1883)\tPrec@1 65.000 (59.619)\tPrec@5 98.000 (96.571)\n",
      "Test: [30/100]\tTime 0.074 (0.079)\tLoss 8.0864 (8.2453)\tPrec@1 59.000 (59.323)\tPrec@5 97.000 (96.355)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 8.6579 (8.2784)\tPrec@1 57.000 (59.561)\tPrec@5 96.000 (96.146)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 8.3943 (8.2096)\tPrec@1 57.000 (59.824)\tPrec@5 97.000 (96.078)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.4897 (8.1573)\tPrec@1 67.000 (59.951)\tPrec@5 99.000 (96.131)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 7.4710 (8.1410)\tPrec@1 64.000 (60.042)\tPrec@5 96.000 (96.056)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 8.2807 (8.0858)\tPrec@1 59.000 (60.420)\tPrec@5 97.000 (96.222)\n",
      "Test: [90/100]\tTime 0.074 (0.075)\tLoss 8.0236 (8.1257)\tPrec@1 61.000 (60.220)\tPrec@5 97.000 (96.165)\n",
      "val Results: Prec@1 60.190 Prec@5 96.220 Loss 8.13581\n",
      "val Class Accuracy: [0.887,0.994,0.732,0.822,0.508,0.546,0.629,0.621,0.232,0.048]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [112][0/97], lr: 0.01000\tTime 0.518 (0.518)\tData 0.286 (0.286)\tLoss 2.1039 (2.1039)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [112][10/97], lr: 0.01000\tTime 0.326 (0.346)\tData 0.000 (0.040)\tLoss 1.7588 (1.9230)\tPrec@1 87.500 (88.423)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [112][20/97], lr: 0.01000\tTime 0.320 (0.335)\tData 0.000 (0.029)\tLoss 1.8917 (1.9977)\tPrec@1 90.625 (87.872)\tPrec@5 99.219 (99.144)\n",
      "Epoch: [112][30/97], lr: 0.01000\tTime 0.321 (0.331)\tData 0.000 (0.025)\tLoss 1.7495 (2.0275)\tPrec@1 89.062 (87.828)\tPrec@5 99.219 (99.168)\n",
      "Epoch: [112][40/97], lr: 0.01000\tTime 0.323 (0.329)\tData 0.000 (0.023)\tLoss 2.4536 (2.0461)\tPrec@1 85.938 (87.919)\tPrec@5 98.438 (99.143)\n",
      "Epoch: [112][50/97], lr: 0.01000\tTime 0.320 (0.328)\tData 0.000 (0.022)\tLoss 1.5821 (2.0280)\tPrec@1 92.188 (88.051)\tPrec@5 100.000 (99.249)\n",
      "Epoch: [112][60/97], lr: 0.01000\tTime 0.322 (0.327)\tData 0.000 (0.021)\tLoss 2.1262 (2.0154)\tPrec@1 85.938 (88.025)\tPrec@5 99.219 (99.283)\n",
      "Epoch: [112][70/97], lr: 0.01000\tTime 0.320 (0.327)\tData 0.000 (0.021)\tLoss 1.8929 (2.0316)\tPrec@1 89.062 (87.929)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [112][80/97], lr: 0.01000\tTime 0.321 (0.326)\tData 0.000 (0.020)\tLoss 1.9009 (2.0377)\tPrec@1 90.625 (87.741)\tPrec@5 99.219 (99.315)\n",
      "Epoch: [112][90/97], lr: 0.01000\tTime 0.321 (0.326)\tData 0.000 (0.020)\tLoss 2.2337 (2.0364)\tPrec@1 85.938 (87.749)\tPrec@5 100.000 (99.322)\n",
      "Epoch: [112][96/97], lr: 0.01000\tTime 0.310 (0.325)\tData 0.000 (0.020)\tLoss 1.7143 (2.0336)\tPrec@1 91.525 (87.780)\tPrec@5 99.153 (99.315)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.305 (0.305)\tLoss 9.2288 (9.2288)\tPrec@1 57.000 (57.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.094)\tLoss 6.4499 (8.3460)\tPrec@1 67.000 (60.909)\tPrec@5 96.000 (95.636)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 7.5816 (8.3006)\tPrec@1 61.000 (60.619)\tPrec@5 99.000 (95.524)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 7.9564 (8.3253)\tPrec@1 63.000 (60.774)\tPrec@5 97.000 (95.581)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 8.4383 (8.3518)\tPrec@1 63.000 (60.976)\tPrec@5 94.000 (95.537)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 7.7099 (8.2600)\tPrec@1 64.000 (61.529)\tPrec@5 96.000 (95.549)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 6.4853 (8.1863)\tPrec@1 72.000 (61.820)\tPrec@5 96.000 (95.475)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 8.9251 (8.1845)\tPrec@1 56.000 (61.873)\tPrec@5 95.000 (95.380)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 7.4174 (8.1358)\tPrec@1 65.000 (62.099)\tPrec@5 96.000 (95.593)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 8.1475 (8.2026)\tPrec@1 60.000 (61.648)\tPrec@5 99.000 (95.571)\n",
      "val Results: Prec@1 61.340 Prec@5 95.560 Loss 8.25605\n",
      "val Class Accuracy: [0.930,0.995,0.847,0.684,0.758,0.553,0.625,0.422,0.218,0.102]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [113][0/97], lr: 0.01000\tTime 0.641 (0.641)\tData 0.355 (0.355)\tLoss 2.1165 (2.1165)\tPrec@1 84.375 (84.375)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [113][10/97], lr: 0.01000\tTime 0.326 (0.370)\tData 0.000 (0.046)\tLoss 2.4897 (1.9622)\tPrec@1 85.938 (87.145)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [113][20/97], lr: 0.01000\tTime 0.342 (0.351)\tData 0.000 (0.032)\tLoss 2.2854 (2.0221)\tPrec@1 85.156 (87.314)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [113][30/97], lr: 0.01000\tTime 0.325 (0.343)\tData 0.000 (0.027)\tLoss 1.6882 (1.9674)\tPrec@1 88.281 (87.802)\tPrec@5 99.219 (99.420)\n",
      "Epoch: [113][40/97], lr: 0.01000\tTime 0.322 (0.338)\tData 0.000 (0.025)\tLoss 2.2673 (1.9868)\tPrec@1 85.156 (87.652)\tPrec@5 98.438 (99.333)\n",
      "Epoch: [113][50/97], lr: 0.01000\tTime 0.320 (0.335)\tData 0.000 (0.023)\tLoss 1.8611 (1.9891)\tPrec@1 88.281 (87.699)\tPrec@5 100.000 (99.341)\n",
      "Epoch: [113][60/97], lr: 0.01000\tTime 0.319 (0.333)\tData 0.000 (0.022)\tLoss 2.4885 (2.0259)\tPrec@1 82.812 (87.551)\tPrec@5 99.219 (99.270)\n",
      "Epoch: [113][70/97], lr: 0.01000\tTime 0.321 (0.331)\tData 0.000 (0.021)\tLoss 2.7166 (2.0486)\tPrec@1 82.812 (87.434)\tPrec@5 100.000 (99.241)\n",
      "Epoch: [113][80/97], lr: 0.01000\tTime 0.320 (0.330)\tData 0.000 (0.021)\tLoss 2.8983 (2.0806)\tPrec@1 82.031 (87.162)\tPrec@5 99.219 (99.277)\n",
      "Epoch: [113][90/97], lr: 0.01000\tTime 0.317 (0.330)\tData 0.000 (0.020)\tLoss 2.0142 (2.0794)\tPrec@1 89.844 (87.139)\tPrec@5 99.219 (99.270)\n",
      "Epoch: [113][96/97], lr: 0.01000\tTime 0.308 (0.329)\tData 0.000 (0.021)\tLoss 2.7337 (2.0759)\tPrec@1 82.203 (87.167)\tPrec@5 100.000 (99.283)\n",
      "Gated Network Weight Gate= Flip:0.53, Sc:0.47\n",
      "Test: [0/100]\tTime 0.302 (0.302)\tLoss 6.9633 (6.9633)\tPrec@1 69.000 (69.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.094)\tLoss 5.7004 (7.3114)\tPrec@1 72.000 (64.182)\tPrec@5 98.000 (96.273)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 7.0209 (7.2100)\tPrec@1 65.000 (64.952)\tPrec@5 98.000 (95.857)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.6716 (7.3135)\tPrec@1 68.000 (64.290)\tPrec@5 98.000 (95.419)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 7.7235 (7.3345)\tPrec@1 58.000 (64.073)\tPrec@5 96.000 (95.415)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 7.4706 (7.3085)\tPrec@1 63.000 (64.078)\tPrec@5 96.000 (95.490)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 6.4349 (7.2829)\tPrec@1 64.000 (64.033)\tPrec@5 95.000 (95.508)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 6.8546 (7.2779)\tPrec@1 63.000 (64.085)\tPrec@5 97.000 (95.507)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 7.2207 (7.2182)\tPrec@1 64.000 (64.420)\tPrec@5 94.000 (95.716)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 6.8680 (7.2407)\tPrec@1 66.000 (64.275)\tPrec@5 98.000 (95.769)\n",
      "val Results: Prec@1 64.020 Prec@5 95.780 Loss 7.26648\n",
      "val Class Accuracy: [0.901,0.972,0.876,0.459,0.584,0.642,0.758,0.578,0.551,0.081]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [114][0/97], lr: 0.01000\tTime 0.488 (0.488)\tData 0.245 (0.245)\tLoss 2.2611 (2.2611)\tPrec@1 89.844 (89.844)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [114][10/97], lr: 0.01000\tTime 0.322 (0.346)\tData 0.000 (0.036)\tLoss 1.1923 (1.7454)\tPrec@1 91.406 (90.199)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [114][20/97], lr: 0.01000\tTime 0.325 (0.337)\tData 0.000 (0.027)\tLoss 1.7597 (1.9417)\tPrec@1 89.844 (88.467)\tPrec@5 99.219 (99.070)\n",
      "Epoch: [114][30/97], lr: 0.01000\tTime 0.322 (0.333)\tData 0.000 (0.024)\tLoss 2.6885 (1.9826)\tPrec@1 82.812 (88.155)\tPrec@5 99.219 (99.143)\n",
      "Epoch: [114][40/97], lr: 0.01000\tTime 0.341 (0.331)\tData 0.000 (0.022)\tLoss 1.5855 (2.0022)\tPrec@1 87.500 (88.034)\tPrec@5 99.219 (99.066)\n",
      "Epoch: [114][50/97], lr: 0.01000\tTime 0.321 (0.329)\tData 0.000 (0.021)\tLoss 3.0853 (1.9974)\tPrec@1 79.688 (87.929)\tPrec@5 98.438 (99.142)\n",
      "Epoch: [114][60/97], lr: 0.01000\tTime 0.320 (0.329)\tData 0.000 (0.020)\tLoss 2.6782 (2.0570)\tPrec@1 83.594 (87.513)\tPrec@5 100.000 (99.193)\n",
      "Epoch: [114][70/97], lr: 0.01000\tTime 0.322 (0.328)\tData 0.000 (0.020)\tLoss 1.6158 (2.0654)\tPrec@1 91.406 (87.522)\tPrec@5 100.000 (99.208)\n",
      "Epoch: [114][80/97], lr: 0.01000\tTime 0.323 (0.327)\tData 0.000 (0.020)\tLoss 2.6760 (2.0751)\tPrec@1 81.250 (87.442)\tPrec@5 100.000 (99.228)\n",
      "Epoch: [114][90/97], lr: 0.01000\tTime 0.319 (0.327)\tData 0.000 (0.019)\tLoss 1.9043 (2.0681)\tPrec@1 87.500 (87.509)\tPrec@5 98.438 (99.193)\n",
      "Epoch: [114][96/97], lr: 0.01000\tTime 0.313 (0.326)\tData 0.000 (0.020)\tLoss 1.9138 (2.0487)\tPrec@1 87.288 (87.659)\tPrec@5 100.000 (99.234)\n",
      "Gated Network Weight Gate= Flip:0.59, Sc:0.41\n",
      "Test: [0/100]\tTime 0.287 (0.287)\tLoss 7.2693 (7.2693)\tPrec@1 64.000 (64.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 5.9876 (7.2975)\tPrec@1 68.000 (64.909)\tPrec@5 97.000 (97.545)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.5675 (7.2241)\tPrec@1 67.000 (64.810)\tPrec@5 97.000 (97.429)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 7.1559 (7.3089)\tPrec@1 67.000 (64.452)\tPrec@5 98.000 (97.323)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 6.6525 (7.2744)\tPrec@1 66.000 (65.024)\tPrec@5 95.000 (96.927)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 7.2732 (7.2220)\tPrec@1 64.000 (65.157)\tPrec@5 98.000 (96.980)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 5.8575 (7.1838)\tPrec@1 75.000 (65.279)\tPrec@5 97.000 (96.902)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 6.6412 (7.1517)\tPrec@1 68.000 (65.535)\tPrec@5 99.000 (96.972)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.8204 (7.1083)\tPrec@1 70.000 (65.741)\tPrec@5 96.000 (97.049)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 7.4427 (7.1546)\tPrec@1 64.000 (65.670)\tPrec@5 98.000 (97.011)\n",
      "val Results: Prec@1 65.440 Prec@5 96.920 Loss 7.20344\n",
      "val Class Accuracy: [0.963,0.984,0.749,0.677,0.778,0.570,0.756,0.507,0.342,0.218]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [115][0/97], lr: 0.01000\tTime 0.428 (0.428)\tData 0.233 (0.233)\tLoss 2.3057 (2.3057)\tPrec@1 83.594 (83.594)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [115][10/97], lr: 0.01000\tTime 0.319 (0.336)\tData 0.000 (0.036)\tLoss 1.6844 (2.0072)\tPrec@1 89.844 (87.287)\tPrec@5 100.000 (99.148)\n",
      "Epoch: [115][20/97], lr: 0.01000\tTime 0.327 (0.330)\tData 0.000 (0.027)\tLoss 1.3047 (2.0015)\tPrec@1 89.844 (87.537)\tPrec@5 100.000 (99.256)\n",
      "Epoch: [115][30/97], lr: 0.01000\tTime 0.320 (0.328)\tData 0.000 (0.024)\tLoss 1.4211 (1.9916)\tPrec@1 92.188 (87.676)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [115][40/97], lr: 0.01000\tTime 0.325 (0.327)\tData 0.000 (0.022)\tLoss 2.2422 (2.0602)\tPrec@1 80.469 (87.195)\tPrec@5 98.438 (99.085)\n",
      "Epoch: [115][50/97], lr: 0.01000\tTime 0.320 (0.326)\tData 0.000 (0.021)\tLoss 2.2061 (2.0350)\tPrec@1 89.844 (87.423)\tPrec@5 99.219 (99.127)\n",
      "Epoch: [115][60/97], lr: 0.01000\tTime 0.319 (0.326)\tData 0.000 (0.020)\tLoss 1.9768 (2.0499)\tPrec@1 87.500 (87.398)\tPrec@5 99.219 (99.129)\n",
      "Epoch: [115][70/97], lr: 0.01000\tTime 0.320 (0.325)\tData 0.000 (0.020)\tLoss 2.5099 (2.0726)\tPrec@1 83.594 (87.269)\tPrec@5 97.656 (99.120)\n",
      "Epoch: [115][80/97], lr: 0.01000\tTime 0.321 (0.325)\tData 0.000 (0.020)\tLoss 1.8192 (2.0808)\tPrec@1 88.281 (87.230)\tPrec@5 98.438 (99.103)\n",
      "Epoch: [115][90/97], lr: 0.01000\tTime 0.318 (0.325)\tData 0.000 (0.019)\tLoss 1.4263 (2.0823)\tPrec@1 91.406 (87.242)\tPrec@5 99.219 (99.107)\n",
      "Epoch: [115][96/97], lr: 0.01000\tTime 0.311 (0.324)\tData 0.000 (0.020)\tLoss 2.6520 (2.0849)\tPrec@1 82.203 (87.256)\tPrec@5 99.153 (99.097)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.324 (0.324)\tLoss 9.7012 (9.7012)\tPrec@1 54.000 (54.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.096)\tLoss 6.6132 (8.6438)\tPrec@1 68.000 (57.636)\tPrec@5 96.000 (95.455)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 7.3389 (8.4871)\tPrec@1 62.000 (58.381)\tPrec@5 98.000 (95.619)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 7.9484 (8.5057)\tPrec@1 59.000 (58.323)\tPrec@5 94.000 (95.839)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 8.0634 (8.4587)\tPrec@1 63.000 (58.878)\tPrec@5 97.000 (95.927)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 7.8632 (8.3853)\tPrec@1 61.000 (59.216)\tPrec@5 94.000 (95.824)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 7.3900 (8.4163)\tPrec@1 63.000 (58.934)\tPrec@5 96.000 (95.852)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 8.2033 (8.4197)\tPrec@1 58.000 (58.789)\tPrec@5 98.000 (95.930)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 7.5969 (8.3794)\tPrec@1 64.000 (58.951)\tPrec@5 94.000 (95.963)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 8.6278 (8.4483)\tPrec@1 58.000 (58.670)\tPrec@5 98.000 (95.901)\n",
      "val Results: Prec@1 58.780 Prec@5 95.900 Loss 8.44853\n",
      "val Class Accuracy: [0.941,0.991,0.835,0.648,0.557,0.496,0.440,0.572,0.381,0.017]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [116][0/97], lr: 0.01000\tTime 0.465 (0.465)\tData 0.267 (0.267)\tLoss 2.3393 (2.3393)\tPrec@1 84.375 (84.375)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [116][10/97], lr: 0.01000\tTime 0.323 (0.341)\tData 0.000 (0.039)\tLoss 1.8454 (1.9260)\tPrec@1 88.281 (88.352)\tPrec@5 99.219 (99.148)\n",
      "Epoch: [116][20/97], lr: 0.01000\tTime 0.319 (0.333)\tData 0.000 (0.029)\tLoss 1.6658 (1.9797)\tPrec@1 91.406 (87.686)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [116][30/97], lr: 0.01000\tTime 0.320 (0.331)\tData 0.000 (0.025)\tLoss 2.6458 (1.9529)\tPrec@1 85.156 (87.802)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [116][40/97], lr: 0.01000\tTime 0.354 (0.330)\tData 0.000 (0.023)\tLoss 1.6880 (1.9769)\tPrec@1 89.062 (87.900)\tPrec@5 100.000 (99.352)\n",
      "Epoch: [116][50/97], lr: 0.01000\tTime 0.326 (0.330)\tData 0.000 (0.021)\tLoss 1.7850 (2.0196)\tPrec@1 87.500 (87.638)\tPrec@5 100.000 (99.403)\n",
      "Epoch: [116][60/97], lr: 0.01000\tTime 0.325 (0.330)\tData 0.000 (0.021)\tLoss 1.8631 (2.0330)\tPrec@1 88.281 (87.602)\tPrec@5 99.219 (99.385)\n",
      "Epoch: [116][70/97], lr: 0.01000\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 2.3320 (2.0476)\tPrec@1 85.156 (87.588)\tPrec@5 97.656 (99.329)\n",
      "Epoch: [116][80/97], lr: 0.01000\tTime 0.324 (0.329)\tData 0.000 (0.020)\tLoss 2.3331 (2.0495)\tPrec@1 85.156 (87.519)\tPrec@5 99.219 (99.334)\n",
      "Epoch: [116][90/97], lr: 0.01000\tTime 0.322 (0.328)\tData 0.000 (0.019)\tLoss 2.4491 (2.0664)\tPrec@1 85.156 (87.431)\tPrec@5 99.219 (99.279)\n",
      "Epoch: [116][96/97], lr: 0.01000\tTime 0.312 (0.328)\tData 0.000 (0.020)\tLoss 1.9184 (2.0631)\tPrec@1 88.136 (87.442)\tPrec@5 100.000 (99.283)\n",
      "Gated Network Weight Gate= Flip:0.48, Sc:0.52\n",
      "Test: [0/100]\tTime 0.318 (0.318)\tLoss 7.8626 (7.8626)\tPrec@1 61.000 (61.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.096)\tLoss 5.5896 (7.4647)\tPrec@1 71.000 (62.727)\tPrec@5 98.000 (97.455)\n",
      "Test: [20/100]\tTime 0.074 (0.085)\tLoss 6.7145 (7.4221)\tPrec@1 65.000 (62.810)\tPrec@5 98.000 (97.190)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 6.6118 (7.4725)\tPrec@1 69.000 (62.645)\tPrec@5 98.000 (97.032)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 7.0094 (7.4520)\tPrec@1 65.000 (62.976)\tPrec@5 97.000 (96.976)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 7.6991 (7.3794)\tPrec@1 62.000 (63.333)\tPrec@5 96.000 (96.961)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 7.2588 (7.4324)\tPrec@1 62.000 (63.066)\tPrec@5 99.000 (96.934)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 7.8363 (7.4254)\tPrec@1 63.000 (63.127)\tPrec@5 97.000 (96.915)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 7.1112 (7.4083)\tPrec@1 66.000 (63.173)\tPrec@5 97.000 (96.914)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 8.1851 (7.4622)\tPrec@1 62.000 (62.934)\tPrec@5 95.000 (96.813)\n",
      "val Results: Prec@1 63.100 Prec@5 96.850 Loss 7.45776\n",
      "val Class Accuracy: [0.974,0.985,0.642,0.783,0.596,0.388,0.764,0.546,0.459,0.173]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [117][0/97], lr: 0.01000\tTime 0.519 (0.519)\tData 0.277 (0.277)\tLoss 2.5309 (2.5309)\tPrec@1 83.594 (83.594)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [117][10/97], lr: 0.01000\tTime 0.320 (0.344)\tData 0.000 (0.039)\tLoss 2.2337 (2.0128)\tPrec@1 87.500 (87.784)\tPrec@5 99.219 (99.006)\n",
      "Epoch: [117][20/97], lr: 0.01000\tTime 0.325 (0.334)\tData 0.000 (0.028)\tLoss 1.3893 (1.9343)\tPrec@1 92.969 (88.318)\tPrec@5 100.000 (99.293)\n",
      "Epoch: [117][30/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.025)\tLoss 2.0520 (1.9767)\tPrec@1 86.719 (88.080)\tPrec@5 100.000 (99.244)\n",
      "Epoch: [117][40/97], lr: 0.01000\tTime 0.333 (0.333)\tData 0.000 (0.023)\tLoss 2.3776 (1.9638)\tPrec@1 85.156 (88.053)\tPrec@5 96.875 (99.295)\n",
      "Epoch: [117][50/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.022)\tLoss 2.3676 (1.9625)\tPrec@1 83.594 (88.021)\tPrec@5 99.219 (99.326)\n",
      "Epoch: [117][60/97], lr: 0.01000\tTime 0.350 (0.333)\tData 0.000 (0.021)\tLoss 1.4470 (1.9624)\tPrec@1 91.406 (87.974)\tPrec@5 100.000 (99.385)\n",
      "Epoch: [117][70/97], lr: 0.01000\tTime 0.374 (0.343)\tData 0.000 (0.020)\tLoss 1.8687 (1.9588)\tPrec@1 91.406 (88.160)\tPrec@5 100.000 (99.406)\n",
      "Epoch: [117][80/97], lr: 0.01000\tTime 0.350 (0.351)\tData 0.000 (0.020)\tLoss 1.6415 (1.9584)\tPrec@1 91.406 (88.185)\tPrec@5 99.219 (99.392)\n",
      "Epoch: [117][90/97], lr: 0.01000\tTime 0.332 (0.353)\tData 0.000 (0.019)\tLoss 2.1164 (1.9740)\tPrec@1 87.500 (88.118)\tPrec@5 100.000 (99.330)\n",
      "Epoch: [117][96/97], lr: 0.01000\tTime 0.324 (0.353)\tData 0.000 (0.020)\tLoss 1.4079 (1.9792)\tPrec@1 93.220 (88.054)\tPrec@5 100.000 (99.355)\n",
      "Gated Network Weight Gate= Flip:0.65, Sc:0.35\n",
      "Test: [0/100]\tTime 0.327 (0.327)\tLoss 8.0716 (8.0716)\tPrec@1 60.000 (60.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.096)\tLoss 6.2855 (7.8865)\tPrec@1 65.000 (60.545)\tPrec@5 95.000 (97.636)\n",
      "Test: [20/100]\tTime 0.074 (0.086)\tLoss 6.1347 (7.6709)\tPrec@1 65.000 (61.381)\tPrec@5 97.000 (97.381)\n",
      "Test: [30/100]\tTime 0.074 (0.082)\tLoss 7.3879 (7.6181)\tPrec@1 64.000 (62.065)\tPrec@5 96.000 (97.161)\n",
      "Test: [40/100]\tTime 0.074 (0.080)\tLoss 6.7480 (7.6317)\tPrec@1 66.000 (61.927)\tPrec@5 96.000 (96.951)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 7.9168 (7.5715)\tPrec@1 60.000 (62.412)\tPrec@5 97.000 (97.039)\n",
      "Test: [60/100]\tTime 0.074 (0.079)\tLoss 6.2572 (7.5977)\tPrec@1 68.000 (62.197)\tPrec@5 97.000 (96.984)\n",
      "Test: [70/100]\tTime 0.076 (0.079)\tLoss 7.0587 (7.6006)\tPrec@1 67.000 (62.254)\tPrec@5 100.000 (97.028)\n",
      "Test: [80/100]\tTime 0.084 (0.079)\tLoss 7.4989 (7.5577)\tPrec@1 63.000 (62.556)\tPrec@5 99.000 (97.148)\n",
      "Test: [90/100]\tTime 0.074 (0.079)\tLoss 7.3945 (7.6509)\tPrec@1 64.000 (62.044)\tPrec@5 98.000 (96.989)\n",
      "val Results: Prec@1 62.080 Prec@5 96.890 Loss 7.66661\n",
      "val Class Accuracy: [0.946,0.983,0.682,0.659,0.611,0.696,0.441,0.551,0.472,0.167]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [118][0/97], lr: 0.01000\tTime 1.289 (1.289)\tData 0.761 (0.761)\tLoss 1.9058 (1.9058)\tPrec@1 85.938 (85.938)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [118][10/97], lr: 0.01000\tTime 0.396 (0.504)\tData 0.001 (0.079)\tLoss 2.3768 (2.0765)\tPrec@1 84.375 (87.003)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [118][20/97], lr: 0.01000\tTime 0.321 (0.442)\tData 0.000 (0.049)\tLoss 2.3357 (2.0995)\tPrec@1 84.375 (86.830)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [118][30/97], lr: 0.01000\tTime 0.323 (0.406)\tData 0.000 (0.038)\tLoss 2.1945 (2.0852)\tPrec@1 85.938 (86.744)\tPrec@5 100.000 (99.420)\n",
      "Epoch: [118][40/97], lr: 0.01000\tTime 0.341 (0.389)\tData 0.000 (0.033)\tLoss 2.2922 (2.1456)\tPrec@1 84.375 (86.643)\tPrec@5 98.438 (99.352)\n",
      "Epoch: [118][50/97], lr: 0.01000\tTime 0.385 (0.380)\tData 0.000 (0.030)\tLoss 2.9871 (2.0950)\tPrec@1 84.375 (87.163)\tPrec@5 96.094 (99.311)\n",
      "Epoch: [118][60/97], lr: 0.01000\tTime 0.370 (0.379)\tData 0.000 (0.027)\tLoss 1.9687 (2.0357)\tPrec@1 87.500 (87.423)\tPrec@5 98.438 (99.347)\n",
      "Epoch: [118][70/97], lr: 0.01000\tTime 0.338 (0.375)\tData 0.000 (0.026)\tLoss 1.4842 (1.9818)\tPrec@1 89.844 (87.731)\tPrec@5 100.000 (99.351)\n",
      "Epoch: [118][80/97], lr: 0.01000\tTime 0.342 (0.372)\tData 0.000 (0.025)\tLoss 2.1844 (1.9999)\tPrec@1 87.500 (87.712)\tPrec@5 100.000 (99.286)\n",
      "Epoch: [118][90/97], lr: 0.01000\tTime 0.365 (0.372)\tData 0.000 (0.024)\tLoss 2.1091 (1.9974)\tPrec@1 83.594 (87.758)\tPrec@5 98.438 (99.245)\n",
      "Epoch: [118][96/97], lr: 0.01000\tTime 0.341 (0.372)\tData 0.000 (0.024)\tLoss 2.2469 (2.0167)\tPrec@1 87.288 (87.683)\tPrec@5 100.000 (99.234)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.431 (0.431)\tLoss 8.5107 (8.5107)\tPrec@1 57.000 (57.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.074 (0.106)\tLoss 6.6767 (8.7482)\tPrec@1 67.000 (57.091)\tPrec@5 98.000 (95.455)\n",
      "Test: [20/100]\tTime 0.074 (0.091)\tLoss 7.5283 (8.6002)\tPrec@1 63.000 (57.762)\tPrec@5 96.000 (94.762)\n",
      "Test: [30/100]\tTime 0.075 (0.086)\tLoss 8.6894 (8.6247)\tPrec@1 56.000 (57.484)\tPrec@5 97.000 (94.774)\n",
      "Test: [40/100]\tTime 0.074 (0.084)\tLoss 9.4041 (8.6740)\tPrec@1 51.000 (57.439)\tPrec@5 94.000 (94.732)\n",
      "Test: [50/100]\tTime 0.074 (0.083)\tLoss 8.9188 (8.6383)\tPrec@1 57.000 (57.667)\tPrec@5 98.000 (94.961)\n",
      "Test: [60/100]\tTime 0.074 (0.083)\tLoss 7.0853 (8.6813)\tPrec@1 64.000 (57.148)\tPrec@5 94.000 (94.803)\n",
      "Test: [70/100]\tTime 0.081 (0.083)\tLoss 8.6228 (8.6723)\tPrec@1 58.000 (57.113)\tPrec@5 97.000 (94.803)\n",
      "Test: [80/100]\tTime 0.074 (0.082)\tLoss 8.5258 (8.6295)\tPrec@1 56.000 (57.235)\tPrec@5 95.000 (94.926)\n",
      "Test: [90/100]\tTime 0.075 (0.081)\tLoss 8.3800 (8.6526)\tPrec@1 60.000 (57.165)\tPrec@5 97.000 (94.890)\n",
      "val Results: Prec@1 57.090 Prec@5 94.950 Loss 8.68072\n",
      "val Class Accuracy: [0.953,0.984,0.840,0.464,0.415,0.554,0.686,0.125,0.581,0.107]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [119][0/97], lr: 0.01000\tTime 1.349 (1.349)\tData 0.858 (0.858)\tLoss 2.4077 (2.4077)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [119][10/97], lr: 0.01000\tTime 0.324 (0.456)\tData 0.000 (0.089)\tLoss 2.0762 (2.0186)\tPrec@1 87.500 (88.068)\tPrec@5 98.438 (99.290)\n",
      "Epoch: [119][20/97], lr: 0.01000\tTime 0.321 (0.395)\tData 0.000 (0.055)\tLoss 1.8804 (1.9759)\tPrec@1 87.500 (88.170)\tPrec@5 100.000 (99.330)\n",
      "Epoch: [119][30/97], lr: 0.01000\tTime 0.340 (0.380)\tData 0.000 (0.042)\tLoss 1.8255 (2.0518)\tPrec@1 91.406 (87.802)\tPrec@5 100.000 (99.168)\n",
      "Epoch: [119][40/97], lr: 0.01000\tTime 0.326 (0.375)\tData 0.000 (0.036)\tLoss 2.1852 (2.0452)\tPrec@1 84.375 (87.729)\tPrec@5 98.438 (99.238)\n",
      "Epoch: [119][50/97], lr: 0.01000\tTime 0.326 (0.367)\tData 0.000 (0.032)\tLoss 1.3805 (2.0408)\tPrec@1 90.625 (87.791)\tPrec@5 100.000 (99.357)\n",
      "Epoch: [119][60/97], lr: 0.01000\tTime 0.327 (0.364)\tData 0.000 (0.030)\tLoss 1.7396 (2.0097)\tPrec@1 88.281 (87.833)\tPrec@5 100.000 (99.360)\n",
      "Epoch: [119][70/97], lr: 0.01000\tTime 0.351 (0.364)\tData 0.000 (0.028)\tLoss 1.6890 (2.0130)\tPrec@1 90.625 (87.764)\tPrec@5 100.000 (99.384)\n",
      "Epoch: [119][80/97], lr: 0.01000\tTime 0.332 (0.363)\tData 0.000 (0.027)\tLoss 1.9300 (1.9927)\tPrec@1 87.500 (87.886)\tPrec@5 99.219 (99.392)\n",
      "Epoch: [119][90/97], lr: 0.01000\tTime 0.325 (0.360)\tData 0.000 (0.025)\tLoss 1.9789 (1.9991)\tPrec@1 85.156 (87.792)\tPrec@5 99.219 (99.425)\n",
      "Epoch: [119][96/97], lr: 0.01000\tTime 0.317 (0.358)\tData 0.000 (0.026)\tLoss 1.3047 (2.0251)\tPrec@1 92.373 (87.635)\tPrec@5 99.153 (99.395)\n",
      "Gated Network Weight Gate= Flip:0.62, Sc:0.38\n",
      "Test: [0/100]\tTime 0.333 (0.333)\tLoss 9.4744 (9.4744)\tPrec@1 53.000 (53.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.074 (0.097)\tLoss 6.3112 (8.3367)\tPrec@1 69.000 (58.636)\tPrec@5 92.000 (93.182)\n",
      "Test: [20/100]\tTime 0.073 (0.086)\tLoss 7.8366 (8.3250)\tPrec@1 58.000 (58.333)\tPrec@5 93.000 (93.667)\n",
      "Test: [30/100]\tTime 0.074 (0.082)\tLoss 7.9377 (8.2750)\tPrec@1 61.000 (59.194)\tPrec@5 95.000 (93.645)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 7.6623 (8.2540)\tPrec@1 61.000 (59.659)\tPrec@5 94.000 (93.805)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 7.9833 (8.2223)\tPrec@1 62.000 (59.922)\tPrec@5 94.000 (93.725)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 7.4312 (8.2094)\tPrec@1 64.000 (60.016)\tPrec@5 93.000 (93.721)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 7.8506 (8.1968)\tPrec@1 62.000 (60.211)\tPrec@5 96.000 (93.620)\n",
      "Test: [80/100]\tTime 0.073 (0.077)\tLoss 7.8537 (8.1544)\tPrec@1 64.000 (60.383)\tPrec@5 94.000 (93.815)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 8.5196 (8.2265)\tPrec@1 60.000 (60.033)\tPrec@5 97.000 (93.604)\n",
      "val Results: Prec@1 60.080 Prec@5 93.610 Loss 8.22999\n",
      "val Class Accuracy: [0.901,0.986,0.763,0.850,0.776,0.456,0.381,0.452,0.339,0.104]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [120][0/97], lr: 0.01000\tTime 0.477 (0.477)\tData 0.270 (0.270)\tLoss 2.8050 (2.8050)\tPrec@1 84.375 (84.375)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [120][10/97], lr: 0.01000\tTime 0.323 (0.341)\tData 0.000 (0.039)\tLoss 2.6679 (1.9865)\tPrec@1 82.812 (88.281)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [120][20/97], lr: 0.01000\tTime 0.355 (0.337)\tData 0.000 (0.029)\tLoss 1.4110 (1.9110)\tPrec@1 92.188 (88.393)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [120][30/97], lr: 0.01000\tTime 0.322 (0.333)\tData 0.000 (0.025)\tLoss 1.4416 (1.8863)\tPrec@1 92.188 (88.785)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [120][40/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.023)\tLoss 1.8336 (1.9239)\tPrec@1 90.625 (88.453)\tPrec@5 99.219 (99.276)\n",
      "Epoch: [120][50/97], lr: 0.01000\tTime 0.320 (0.330)\tData 0.000 (0.022)\tLoss 3.0364 (1.9708)\tPrec@1 78.125 (88.159)\tPrec@5 99.219 (99.295)\n",
      "Epoch: [120][60/97], lr: 0.01000\tTime 0.324 (0.329)\tData 0.000 (0.021)\tLoss 2.7929 (2.0134)\tPrec@1 82.812 (87.974)\tPrec@5 100.000 (99.334)\n",
      "Epoch: [120][70/97], lr: 0.01000\tTime 0.383 (0.331)\tData 0.000 (0.020)\tLoss 2.3183 (2.0239)\tPrec@1 84.375 (87.852)\tPrec@5 99.219 (99.329)\n",
      "Epoch: [120][80/97], lr: 0.01000\tTime 0.320 (0.334)\tData 0.000 (0.020)\tLoss 1.5888 (2.0261)\tPrec@1 89.844 (87.770)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [120][90/97], lr: 0.01000\tTime 0.317 (0.334)\tData 0.000 (0.019)\tLoss 1.3619 (2.0324)\tPrec@1 91.406 (87.680)\tPrec@5 100.000 (99.305)\n",
      "Epoch: [120][96/97], lr: 0.01000\tTime 0.317 (0.333)\tData 0.000 (0.020)\tLoss 1.5597 (2.0205)\tPrec@1 90.678 (87.740)\tPrec@5 99.153 (99.291)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.298 (0.298)\tLoss 10.9291 (10.9291)\tPrec@1 48.000 (48.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.094)\tLoss 8.8241 (9.7039)\tPrec@1 56.000 (51.818)\tPrec@5 89.000 (93.455)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 7.5325 (9.5768)\tPrec@1 63.000 (52.667)\tPrec@5 94.000 (92.476)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 9.3134 (9.5175)\tPrec@1 54.000 (52.839)\tPrec@5 93.000 (92.742)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 10.0061 (9.5049)\tPrec@1 49.000 (53.073)\tPrec@5 90.000 (92.707)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 9.2041 (9.4213)\tPrec@1 57.000 (53.627)\tPrec@5 93.000 (92.824)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 7.5693 (9.4108)\tPrec@1 61.000 (53.508)\tPrec@5 93.000 (92.754)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 9.2255 (9.3918)\tPrec@1 53.000 (53.535)\tPrec@5 96.000 (92.944)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 9.3528 (9.3315)\tPrec@1 54.000 (53.864)\tPrec@5 96.000 (93.198)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 9.5091 (9.4114)\tPrec@1 54.000 (53.451)\tPrec@5 94.000 (92.978)\n",
      "val Results: Prec@1 53.540 Prec@5 92.950 Loss 9.40977\n",
      "val Class Accuracy: [0.977,0.967,0.727,0.625,0.495,0.602,0.228,0.394,0.290,0.049]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [121][0/97], lr: 0.01000\tTime 0.530 (0.530)\tData 0.305 (0.305)\tLoss 2.2348 (2.2348)\tPrec@1 88.281 (88.281)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [121][10/97], lr: 0.01000\tTime 0.321 (0.358)\tData 0.000 (0.040)\tLoss 2.4843 (1.8229)\tPrec@1 85.156 (89.062)\tPrec@5 98.438 (99.361)\n",
      "Epoch: [121][20/97], lr: 0.01000\tTime 0.321 (0.344)\tData 0.000 (0.029)\tLoss 2.4161 (1.8486)\tPrec@1 86.719 (88.653)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [121][30/97], lr: 0.01000\tTime 0.322 (0.339)\tData 0.000 (0.025)\tLoss 2.4978 (1.9107)\tPrec@1 85.938 (88.332)\tPrec@5 99.219 (99.446)\n",
      "Epoch: [121][40/97], lr: 0.01000\tTime 0.321 (0.337)\tData 0.000 (0.023)\tLoss 1.6601 (1.9753)\tPrec@1 89.844 (88.091)\tPrec@5 99.219 (99.428)\n",
      "Epoch: [121][50/97], lr: 0.01000\tTime 0.323 (0.335)\tData 0.000 (0.022)\tLoss 1.8936 (1.9682)\tPrec@1 88.281 (88.021)\tPrec@5 100.000 (99.433)\n",
      "Epoch: [121][60/97], lr: 0.01000\tTime 0.321 (0.334)\tData 0.000 (0.021)\tLoss 2.2875 (1.9713)\tPrec@1 88.281 (88.025)\tPrec@5 99.219 (99.436)\n",
      "Epoch: [121][70/97], lr: 0.01000\tTime 0.320 (0.333)\tData 0.000 (0.021)\tLoss 1.7092 (1.9841)\tPrec@1 89.062 (87.907)\tPrec@5 99.219 (99.461)\n",
      "Epoch: [121][80/97], lr: 0.01000\tTime 0.320 (0.334)\tData 0.000 (0.020)\tLoss 1.9326 (1.9988)\tPrec@1 89.062 (87.780)\tPrec@5 100.000 (99.412)\n",
      "Epoch: [121][90/97], lr: 0.01000\tTime 0.318 (0.333)\tData 0.000 (0.020)\tLoss 1.8227 (1.9938)\tPrec@1 89.844 (87.809)\tPrec@5 100.000 (99.425)\n",
      "Epoch: [121][96/97], lr: 0.01000\tTime 0.316 (0.333)\tData 0.000 (0.020)\tLoss 2.0593 (1.9994)\tPrec@1 86.441 (87.788)\tPrec@5 100.000 (99.436)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.283 (0.283)\tLoss 8.0642 (8.0642)\tPrec@1 61.000 (61.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.074 (0.092)\tLoss 7.0646 (7.6827)\tPrec@1 65.000 (63.273)\tPrec@5 96.000 (96.545)\n",
      "Test: [20/100]\tTime 0.074 (0.083)\tLoss 5.5629 (7.5788)\tPrec@1 73.000 (63.381)\tPrec@5 97.000 (96.524)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 6.8156 (7.5763)\tPrec@1 68.000 (63.194)\tPrec@5 98.000 (96.548)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 6.8164 (7.5731)\tPrec@1 66.000 (63.488)\tPrec@5 99.000 (96.463)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 8.1173 (7.5682)\tPrec@1 59.000 (63.431)\tPrec@5 98.000 (96.686)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 5.9790 (7.5488)\tPrec@1 71.000 (63.410)\tPrec@5 95.000 (96.574)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 7.0756 (7.5377)\tPrec@1 68.000 (63.437)\tPrec@5 98.000 (96.507)\n",
      "Test: [80/100]\tTime 0.075 (0.077)\tLoss 7.5485 (7.4702)\tPrec@1 62.000 (63.765)\tPrec@5 98.000 (96.605)\n",
      "Test: [90/100]\tTime 0.076 (0.076)\tLoss 7.0401 (7.5116)\tPrec@1 68.000 (63.615)\tPrec@5 99.000 (96.615)\n",
      "val Results: Prec@1 63.640 Prec@5 96.670 Loss 7.51844\n",
      "val Class Accuracy: [0.923,0.988,0.726,0.514,0.511,0.830,0.361,0.720,0.628,0.163]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [122][0/97], lr: 0.01000\tTime 0.666 (0.666)\tData 0.370 (0.370)\tLoss 2.3928 (2.3928)\tPrec@1 84.375 (84.375)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [122][10/97], lr: 0.01000\tTime 0.321 (0.377)\tData 0.000 (0.046)\tLoss 2.3080 (2.1793)\tPrec@1 85.938 (86.719)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [122][20/97], lr: 0.01000\tTime 0.325 (0.356)\tData 0.000 (0.032)\tLoss 2.3003 (2.1232)\tPrec@1 85.938 (86.979)\tPrec@5 100.000 (99.330)\n",
      "Epoch: [122][30/97], lr: 0.01000\tTime 0.325 (0.348)\tData 0.000 (0.027)\tLoss 2.0726 (2.0624)\tPrec@1 89.062 (87.324)\tPrec@5 98.438 (99.420)\n",
      "Epoch: [122][40/97], lr: 0.01000\tTime 0.320 (0.344)\tData 0.000 (0.025)\tLoss 2.2049 (2.0254)\tPrec@1 85.156 (87.462)\tPrec@5 99.219 (99.466)\n",
      "Epoch: [122][50/97], lr: 0.01000\tTime 0.324 (0.342)\tData 0.000 (0.023)\tLoss 1.7464 (2.0217)\tPrec@1 91.406 (87.699)\tPrec@5 100.000 (99.418)\n",
      "Epoch: [122][60/97], lr: 0.01000\tTime 0.322 (0.340)\tData 0.000 (0.022)\tLoss 2.1348 (2.0269)\tPrec@1 88.281 (87.641)\tPrec@5 100.000 (99.449)\n",
      "Epoch: [122][70/97], lr: 0.01000\tTime 0.321 (0.339)\tData 0.000 (0.022)\tLoss 1.6205 (2.0054)\tPrec@1 90.625 (87.775)\tPrec@5 99.219 (99.406)\n",
      "Epoch: [122][80/97], lr: 0.01000\tTime 0.324 (0.338)\tData 0.000 (0.021)\tLoss 2.6336 (2.0089)\tPrec@1 82.812 (87.703)\tPrec@5 100.000 (99.402)\n",
      "Epoch: [122][90/97], lr: 0.01000\tTime 0.326 (0.337)\tData 0.000 (0.021)\tLoss 1.9522 (2.0485)\tPrec@1 89.844 (87.509)\tPrec@5 97.656 (99.365)\n",
      "Epoch: [122][96/97], lr: 0.01000\tTime 0.323 (0.336)\tData 0.000 (0.021)\tLoss 2.5226 (2.0401)\tPrec@1 83.898 (87.587)\tPrec@5 98.305 (99.363)\n",
      "Gated Network Weight Gate= Flip:0.53, Sc:0.47\n",
      "Test: [0/100]\tTime 0.312 (0.312)\tLoss 9.4906 (9.4906)\tPrec@1 57.000 (57.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.073 (0.095)\tLoss 7.3538 (9.1084)\tPrec@1 65.000 (55.727)\tPrec@5 91.000 (93.091)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 8.0536 (8.9931)\tPrec@1 58.000 (55.810)\tPrec@5 95.000 (93.571)\n",
      "Test: [30/100]\tTime 0.075 (0.081)\tLoss 8.6345 (8.9965)\tPrec@1 55.000 (55.742)\tPrec@5 93.000 (93.000)\n",
      "Test: [40/100]\tTime 0.075 (0.080)\tLoss 9.3688 (9.0560)\tPrec@1 55.000 (55.537)\tPrec@5 94.000 (92.951)\n",
      "Test: [50/100]\tTime 0.076 (0.079)\tLoss 9.0709 (9.0147)\tPrec@1 58.000 (56.059)\tPrec@5 93.000 (92.922)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 6.7166 (9.0076)\tPrec@1 67.000 (56.098)\tPrec@5 97.000 (92.967)\n",
      "Test: [70/100]\tTime 0.075 (0.077)\tLoss 8.9555 (9.0148)\tPrec@1 54.000 (56.113)\tPrec@5 96.000 (92.915)\n",
      "Test: [80/100]\tTime 0.075 (0.077)\tLoss 8.3600 (8.9433)\tPrec@1 58.000 (56.519)\tPrec@5 92.000 (93.111)\n",
      "Test: [90/100]\tTime 0.076 (0.077)\tLoss 8.8081 (9.0077)\tPrec@1 54.000 (56.176)\tPrec@5 97.000 (93.066)\n",
      "val Results: Prec@1 56.300 Prec@5 93.020 Loss 9.00307\n",
      "val Class Accuracy: [0.938,0.982,0.835,0.804,0.392,0.496,0.233,0.464,0.436,0.050]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [123][0/97], lr: 0.01000\tTime 0.607 (0.607)\tData 0.312 (0.312)\tLoss 1.8682 (1.8682)\tPrec@1 87.500 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [123][10/97], lr: 0.01000\tTime 0.321 (0.370)\tData 0.000 (0.041)\tLoss 1.7745 (2.0753)\tPrec@1 88.281 (86.790)\tPrec@5 99.219 (99.574)\n",
      "Epoch: [123][20/97], lr: 0.01000\tTime 0.323 (0.352)\tData 0.000 (0.030)\tLoss 1.6872 (2.0525)\tPrec@1 87.500 (87.314)\tPrec@5 98.438 (99.405)\n",
      "Epoch: [123][30/97], lr: 0.01000\tTime 0.326 (0.345)\tData 0.000 (0.025)\tLoss 1.5751 (2.0746)\tPrec@1 89.844 (87.122)\tPrec@5 99.219 (99.269)\n",
      "Epoch: [123][40/97], lr: 0.01000\tTime 0.326 (0.342)\tData 0.000 (0.023)\tLoss 1.5074 (2.0381)\tPrec@1 89.062 (87.405)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [123][50/97], lr: 0.01000\tTime 0.325 (0.340)\tData 0.000 (0.022)\tLoss 2.1291 (2.0674)\tPrec@1 84.375 (87.194)\tPrec@5 99.219 (99.265)\n",
      "Epoch: [123][60/97], lr: 0.01000\tTime 0.327 (0.338)\tData 0.000 (0.021)\tLoss 1.9950 (2.0824)\tPrec@1 88.281 (87.257)\tPrec@5 99.219 (99.270)\n",
      "Epoch: [123][70/97], lr: 0.01000\tTime 0.323 (0.337)\tData 0.000 (0.021)\tLoss 2.3025 (2.0455)\tPrec@1 85.156 (87.533)\tPrec@5 100.000 (99.318)\n",
      "Epoch: [123][80/97], lr: 0.01000\tTime 0.325 (0.336)\tData 0.000 (0.020)\tLoss 1.6861 (2.0076)\tPrec@1 89.062 (87.751)\tPrec@5 98.438 (99.286)\n",
      "Epoch: [123][90/97], lr: 0.01000\tTime 0.319 (0.336)\tData 0.000 (0.020)\tLoss 2.8896 (2.0160)\tPrec@1 80.469 (87.697)\tPrec@5 100.000 (99.313)\n",
      "Epoch: [123][96/97], lr: 0.01000\tTime 0.316 (0.335)\tData 0.000 (0.020)\tLoss 1.8125 (2.0131)\tPrec@1 88.983 (87.724)\tPrec@5 98.305 (99.266)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.323 (0.323)\tLoss 8.2192 (8.2192)\tPrec@1 65.000 (65.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.073 (0.096)\tLoss 5.9708 (7.8625)\tPrec@1 70.000 (63.091)\tPrec@5 98.000 (94.182)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 8.0109 (7.9760)\tPrec@1 60.000 (62.333)\tPrec@5 95.000 (93.952)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 7.7649 (8.0212)\tPrec@1 62.000 (62.161)\tPrec@5 91.000 (93.548)\n",
      "Test: [40/100]\tTime 0.074 (0.080)\tLoss 8.7698 (8.0720)\tPrec@1 59.000 (62.268)\tPrec@5 93.000 (93.317)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 8.0850 (8.0415)\tPrec@1 63.000 (62.235)\tPrec@5 96.000 (93.490)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 7.1289 (8.0476)\tPrec@1 67.000 (62.066)\tPrec@5 95.000 (93.393)\n",
      "Test: [70/100]\tTime 0.074 (0.078)\tLoss 7.7896 (8.0167)\tPrec@1 62.000 (62.239)\tPrec@5 95.000 (93.394)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 7.9483 (7.9899)\tPrec@1 62.000 (62.346)\tPrec@5 94.000 (93.543)\n",
      "Test: [90/100]\tTime 0.075 (0.077)\tLoss 8.4939 (8.0260)\tPrec@1 60.000 (62.209)\tPrec@5 97.000 (93.484)\n",
      "val Results: Prec@1 62.320 Prec@5 93.480 Loss 8.03662\n",
      "val Class Accuracy: [0.941,0.966,0.812,0.692,0.627,0.463,0.758,0.451,0.515,0.007]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [124][0/97], lr: 0.01000\tTime 0.608 (0.608)\tData 0.367 (0.367)\tLoss 1.9156 (1.9156)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [124][10/97], lr: 0.01000\tTime 0.327 (0.372)\tData 0.000 (0.047)\tLoss 2.5718 (2.0335)\tPrec@1 83.594 (87.429)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [124][20/97], lr: 0.01000\tTime 0.326 (0.353)\tData 0.000 (0.033)\tLoss 2.2595 (1.9271)\tPrec@1 85.938 (88.021)\tPrec@5 100.000 (99.591)\n",
      "Epoch: [124][30/97], lr: 0.01000\tTime 0.325 (0.347)\tData 0.000 (0.028)\tLoss 1.9086 (1.9353)\tPrec@1 87.500 (87.878)\tPrec@5 100.000 (99.521)\n",
      "Epoch: [124][40/97], lr: 0.01000\tTime 0.326 (0.349)\tData 0.001 (0.025)\tLoss 1.3373 (1.9159)\tPrec@1 89.062 (88.034)\tPrec@5 99.219 (99.486)\n",
      "Epoch: [124][50/97], lr: 0.01000\tTime 0.331 (0.350)\tData 0.000 (0.023)\tLoss 1.8780 (1.9535)\tPrec@1 86.719 (87.868)\tPrec@5 100.000 (99.464)\n",
      "Epoch: [124][60/97], lr: 0.01000\tTime 0.328 (0.347)\tData 0.000 (0.022)\tLoss 2.2496 (1.9308)\tPrec@1 86.719 (87.974)\tPrec@5 99.219 (99.424)\n",
      "Epoch: [124][70/97], lr: 0.01000\tTime 0.322 (0.344)\tData 0.000 (0.022)\tLoss 2.6195 (1.9732)\tPrec@1 85.938 (87.786)\tPrec@5 98.438 (99.395)\n",
      "Epoch: [124][80/97], lr: 0.01000\tTime 0.328 (0.341)\tData 0.000 (0.021)\tLoss 1.9905 (1.9876)\tPrec@1 89.062 (87.703)\tPrec@5 100.000 (99.354)\n",
      "Epoch: [124][90/97], lr: 0.01000\tTime 0.324 (0.340)\tData 0.000 (0.021)\tLoss 2.5535 (1.9876)\tPrec@1 84.375 (87.749)\tPrec@5 100.000 (99.382)\n",
      "Epoch: [124][96/97], lr: 0.01000\tTime 0.316 (0.339)\tData 0.000 (0.021)\tLoss 2.0110 (1.9835)\tPrec@1 87.288 (87.804)\tPrec@5 100.000 (99.395)\n",
      "Gated Network Weight Gate= Flip:0.52, Sc:0.48\n",
      "Test: [0/100]\tTime 0.257 (0.257)\tLoss 8.0593 (8.0593)\tPrec@1 62.000 (62.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.3223 (7.5541)\tPrec@1 72.000 (63.455)\tPrec@5 96.000 (98.182)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.6370 (7.4943)\tPrec@1 68.000 (63.333)\tPrec@5 97.000 (97.095)\n",
      "Test: [30/100]\tTime 0.074 (0.079)\tLoss 7.8167 (7.5752)\tPrec@1 61.000 (62.806)\tPrec@5 96.000 (96.968)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 6.2248 (7.5391)\tPrec@1 72.000 (63.293)\tPrec@5 96.000 (96.854)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 8.3147 (7.5547)\tPrec@1 58.000 (63.275)\tPrec@5 97.000 (96.863)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.7728 (7.5776)\tPrec@1 68.000 (63.148)\tPrec@5 99.000 (96.885)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 7.3504 (7.5708)\tPrec@1 67.000 (63.268)\tPrec@5 98.000 (96.930)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.9983 (7.5230)\tPrec@1 70.000 (63.531)\tPrec@5 99.000 (96.951)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.1748 (7.5492)\tPrec@1 63.000 (63.363)\tPrec@5 100.000 (96.967)\n",
      "val Results: Prec@1 63.390 Prec@5 96.990 Loss 7.55548\n",
      "val Class Accuracy: [0.922,0.995,0.710,0.832,0.637,0.337,0.449,0.688,0.612,0.157]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [125][0/97], lr: 0.01000\tTime 0.609 (0.609)\tData 0.374 (0.374)\tLoss 2.3984 (2.3984)\tPrec@1 85.156 (85.156)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [125][10/97], lr: 0.01000\tTime 0.321 (0.360)\tData 0.000 (0.048)\tLoss 2.3451 (2.0432)\tPrec@1 86.719 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [125][20/97], lr: 0.01000\tTime 0.335 (0.343)\tData 0.000 (0.033)\tLoss 1.6473 (1.9771)\tPrec@1 89.844 (87.723)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [125][30/97], lr: 0.01000\tTime 0.317 (0.336)\tData 0.000 (0.028)\tLoss 2.4531 (1.9804)\tPrec@1 84.375 (87.702)\tPrec@5 100.000 (99.370)\n",
      "Epoch: [125][40/97], lr: 0.01000\tTime 0.323 (0.333)\tData 0.000 (0.025)\tLoss 2.1033 (1.9848)\tPrec@1 88.281 (87.595)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [125][50/97], lr: 0.01000\tTime 0.316 (0.331)\tData 0.000 (0.024)\tLoss 1.7522 (2.0048)\tPrec@1 90.625 (87.531)\tPrec@5 100.000 (99.433)\n",
      "Epoch: [125][60/97], lr: 0.01000\tTime 0.322 (0.330)\tData 0.000 (0.022)\tLoss 1.5969 (2.0086)\tPrec@1 89.844 (87.487)\tPrec@5 100.000 (99.436)\n",
      "Epoch: [125][70/97], lr: 0.01000\tTime 0.324 (0.329)\tData 0.000 (0.022)\tLoss 1.9324 (2.0248)\tPrec@1 89.062 (87.324)\tPrec@5 99.219 (99.384)\n",
      "Epoch: [125][80/97], lr: 0.01000\tTime 0.324 (0.328)\tData 0.000 (0.021)\tLoss 1.8149 (2.0049)\tPrec@1 88.281 (87.519)\tPrec@5 100.000 (99.392)\n",
      "Epoch: [125][90/97], lr: 0.01000\tTime 0.317 (0.328)\tData 0.000 (0.021)\tLoss 2.0233 (2.0011)\tPrec@1 87.500 (87.543)\tPrec@5 100.000 (99.390)\n",
      "Epoch: [125][96/97], lr: 0.01000\tTime 0.311 (0.327)\tData 0.000 (0.021)\tLoss 2.6615 (2.0100)\tPrec@1 83.051 (87.450)\tPrec@5 96.610 (99.371)\n",
      "Gated Network Weight Gate= Flip:0.49, Sc:0.51\n",
      "Test: [0/100]\tTime 0.286 (0.286)\tLoss 7.9341 (7.9341)\tPrec@1 64.000 (64.000)\tPrec@5 98.000 (98.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 5.9272 (7.1577)\tPrec@1 69.000 (65.545)\tPrec@5 99.000 (97.182)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.1895 (7.1340)\tPrec@1 71.000 (65.619)\tPrec@5 96.000 (96.905)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 7.0748 (7.1757)\tPrec@1 66.000 (65.613)\tPrec@5 96.000 (96.935)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 6.6316 (7.2326)\tPrec@1 70.000 (65.293)\tPrec@5 92.000 (96.561)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 6.7490 (7.1838)\tPrec@1 66.000 (65.569)\tPrec@5 96.000 (96.608)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 6.6908 (7.1494)\tPrec@1 63.000 (65.787)\tPrec@5 94.000 (96.607)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 6.4895 (7.1315)\tPrec@1 69.000 (66.014)\tPrec@5 99.000 (96.690)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.8674 (7.1145)\tPrec@1 67.000 (66.160)\tPrec@5 97.000 (96.704)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 7.4043 (7.1928)\tPrec@1 63.000 (65.824)\tPrec@5 99.000 (96.692)\n",
      "val Results: Prec@1 65.810 Prec@5 96.700 Loss 7.19879\n",
      "val Class Accuracy: [0.940,0.994,0.642,0.718,0.674,0.669,0.683,0.629,0.350,0.282]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [126][0/97], lr: 0.01000\tTime 0.469 (0.469)\tData 0.256 (0.256)\tLoss 1.4055 (1.4055)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [126][10/97], lr: 0.01000\tTime 0.329 (0.341)\tData 0.000 (0.038)\tLoss 1.9276 (1.8526)\tPrec@1 89.844 (88.849)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [126][20/97], lr: 0.01000\tTime 0.322 (0.334)\tData 0.000 (0.028)\tLoss 2.0475 (1.9084)\tPrec@1 85.938 (88.356)\tPrec@5 100.000 (99.368)\n",
      "Epoch: [126][30/97], lr: 0.01000\tTime 0.321 (0.331)\tData 0.000 (0.024)\tLoss 1.6725 (1.8906)\tPrec@1 88.281 (88.458)\tPrec@5 100.000 (99.446)\n",
      "Epoch: [126][40/97], lr: 0.01000\tTime 0.341 (0.330)\tData 0.000 (0.023)\tLoss 1.4016 (1.8747)\tPrec@1 91.406 (88.643)\tPrec@5 99.219 (99.447)\n",
      "Epoch: [126][50/97], lr: 0.01000\tTime 0.324 (0.329)\tData 0.000 (0.021)\tLoss 1.8945 (1.9334)\tPrec@1 89.844 (88.358)\tPrec@5 100.000 (99.449)\n",
      "Epoch: [126][60/97], lr: 0.01000\tTime 0.323 (0.328)\tData 0.000 (0.021)\tLoss 2.0425 (1.9517)\tPrec@1 89.062 (88.243)\tPrec@5 99.219 (99.411)\n",
      "Epoch: [126][70/97], lr: 0.01000\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 1.5900 (1.9764)\tPrec@1 91.406 (88.105)\tPrec@5 99.219 (99.417)\n",
      "Epoch: [126][80/97], lr: 0.01000\tTime 0.322 (0.327)\tData 0.000 (0.020)\tLoss 1.7041 (1.9899)\tPrec@1 86.719 (88.030)\tPrec@5 100.000 (99.383)\n",
      "Epoch: [126][90/97], lr: 0.01000\tTime 0.319 (0.327)\tData 0.000 (0.020)\tLoss 1.9806 (1.9976)\tPrec@1 87.500 (87.989)\tPrec@5 98.438 (99.365)\n",
      "Epoch: [126][96/97], lr: 0.01000\tTime 0.310 (0.327)\tData 0.000 (0.020)\tLoss 2.4539 (1.9941)\tPrec@1 84.746 (88.070)\tPrec@5 98.305 (99.355)\n",
      "Gated Network Weight Gate= Flip:0.52, Sc:0.48\n",
      "Test: [0/100]\tTime 0.256 (0.256)\tLoss 7.6222 (7.6222)\tPrec@1 64.000 (64.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.0640 (7.5850)\tPrec@1 72.000 (63.636)\tPrec@5 99.000 (97.091)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.5846 (7.6355)\tPrec@1 60.000 (63.238)\tPrec@5 97.000 (96.905)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 7.0543 (7.6138)\tPrec@1 63.000 (63.613)\tPrec@5 97.000 (96.710)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 7.7682 (7.6025)\tPrec@1 60.000 (63.854)\tPrec@5 94.000 (96.390)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.7960 (7.5747)\tPrec@1 63.000 (63.941)\tPrec@5 98.000 (96.588)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.9891 (7.5822)\tPrec@1 66.000 (63.656)\tPrec@5 97.000 (96.525)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 6.8368 (7.5886)\tPrec@1 68.000 (63.648)\tPrec@5 98.000 (96.592)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 7.1951 (7.5404)\tPrec@1 65.000 (63.802)\tPrec@5 99.000 (96.642)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 7.5577 (7.6070)\tPrec@1 64.000 (63.549)\tPrec@5 99.000 (96.736)\n",
      "val Results: Prec@1 63.440 Prec@5 96.800 Loss 7.64546\n",
      "val Class Accuracy: [0.912,0.997,0.755,0.653,0.768,0.339,0.697,0.643,0.494,0.086]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [127][0/97], lr: 0.01000\tTime 0.498 (0.498)\tData 0.254 (0.254)\tLoss 1.9290 (1.9290)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [127][10/97], lr: 0.01000\tTime 0.320 (0.341)\tData 0.000 (0.036)\tLoss 1.6755 (2.0425)\tPrec@1 89.844 (87.429)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [127][20/97], lr: 0.01000\tTime 0.321 (0.333)\tData 0.000 (0.027)\tLoss 2.0694 (1.9673)\tPrec@1 86.719 (87.946)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [127][30/97], lr: 0.01000\tTime 0.324 (0.330)\tData 0.000 (0.024)\tLoss 1.9686 (1.9516)\tPrec@1 88.281 (87.878)\tPrec@5 98.438 (99.294)\n",
      "Epoch: [127][40/97], lr: 0.01000\tTime 0.320 (0.328)\tData 0.000 (0.022)\tLoss 1.9707 (1.9511)\tPrec@1 87.500 (87.881)\tPrec@5 98.438 (99.371)\n",
      "Epoch: [127][50/97], lr: 0.01000\tTime 0.318 (0.328)\tData 0.000 (0.021)\tLoss 1.9994 (1.9518)\tPrec@1 88.281 (87.960)\tPrec@5 100.000 (99.403)\n",
      "Epoch: [127][60/97], lr: 0.01000\tTime 0.319 (0.327)\tData 0.000 (0.020)\tLoss 1.7303 (1.9590)\tPrec@1 89.844 (88.076)\tPrec@5 99.219 (99.334)\n",
      "Epoch: [127][70/97], lr: 0.01000\tTime 0.317 (0.326)\tData 0.000 (0.020)\tLoss 1.8559 (1.9551)\tPrec@1 89.062 (88.127)\tPrec@5 99.219 (99.340)\n",
      "Epoch: [127][80/97], lr: 0.01000\tTime 0.322 (0.326)\tData 0.000 (0.020)\tLoss 2.5479 (1.9562)\tPrec@1 84.375 (88.137)\tPrec@5 99.219 (99.344)\n",
      "Epoch: [127][90/97], lr: 0.01000\tTime 0.317 (0.325)\tData 0.000 (0.019)\tLoss 2.2530 (1.9913)\tPrec@1 85.938 (87.852)\tPrec@5 96.875 (99.287)\n",
      "Epoch: [127][96/97], lr: 0.01000\tTime 0.316 (0.325)\tData 0.000 (0.020)\tLoss 2.1046 (1.9919)\tPrec@1 85.593 (87.893)\tPrec@5 100.000 (99.299)\n",
      "Gated Network Weight Gate= Flip:0.62, Sc:0.38\n",
      "Test: [0/100]\tTime 0.255 (0.255)\tLoss 10.3517 (10.3517)\tPrec@1 50.000 (50.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.074 (0.090)\tLoss 7.4066 (8.6475)\tPrec@1 62.000 (59.273)\tPrec@5 94.000 (93.273)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.7030 (8.5949)\tPrec@1 64.000 (59.476)\tPrec@5 98.000 (93.286)\n",
      "Test: [30/100]\tTime 0.074 (0.079)\tLoss 7.5638 (8.6046)\tPrec@1 65.000 (59.387)\tPrec@5 93.000 (93.677)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 8.5153 (8.5498)\tPrec@1 61.000 (59.561)\tPrec@5 91.000 (93.756)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 7.1746 (8.4460)\tPrec@1 68.000 (59.882)\tPrec@5 97.000 (94.039)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 7.2810 (8.3971)\tPrec@1 63.000 (59.885)\tPrec@5 93.000 (94.115)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 8.2023 (8.4242)\tPrec@1 64.000 (59.915)\tPrec@5 97.000 (94.014)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 7.8159 (8.4018)\tPrec@1 64.000 (60.000)\tPrec@5 95.000 (94.086)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 8.9071 (8.5153)\tPrec@1 58.000 (59.418)\tPrec@5 95.000 (93.824)\n",
      "val Results: Prec@1 59.160 Prec@5 93.840 Loss 8.57658\n",
      "val Class Accuracy: [0.972,0.986,0.724,0.464,0.836,0.590,0.647,0.465,0.145,0.087]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [128][0/97], lr: 0.01000\tTime 0.453 (0.453)\tData 0.232 (0.232)\tLoss 1.8980 (1.8980)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [128][10/97], lr: 0.01000\tTime 0.322 (0.340)\tData 0.000 (0.035)\tLoss 2.5855 (2.1082)\tPrec@1 85.938 (87.713)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [128][20/97], lr: 0.01000\tTime 0.321 (0.333)\tData 0.000 (0.027)\tLoss 1.4554 (2.0677)\tPrec@1 90.625 (87.574)\tPrec@5 99.219 (99.442)\n",
      "Epoch: [128][30/97], lr: 0.01000\tTime 0.322 (0.332)\tData 0.000 (0.024)\tLoss 2.3681 (2.1130)\tPrec@1 85.156 (87.072)\tPrec@5 97.656 (99.269)\n",
      "Epoch: [128][40/97], lr: 0.01000\tTime 0.329 (0.330)\tData 0.000 (0.022)\tLoss 2.2204 (2.1313)\tPrec@1 85.938 (86.890)\tPrec@5 100.000 (99.257)\n",
      "Epoch: [128][50/97], lr: 0.01000\tTime 0.321 (0.329)\tData 0.000 (0.021)\tLoss 2.0490 (2.1061)\tPrec@1 85.938 (86.964)\tPrec@5 99.219 (99.265)\n",
      "Epoch: [128][60/97], lr: 0.01000\tTime 0.323 (0.328)\tData 0.000 (0.020)\tLoss 1.3452 (2.0859)\tPrec@1 92.188 (87.141)\tPrec@5 98.438 (99.283)\n",
      "Epoch: [128][70/97], lr: 0.01000\tTime 0.323 (0.327)\tData 0.000 (0.020)\tLoss 1.4947 (2.0754)\tPrec@1 90.625 (87.082)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [128][80/97], lr: 0.01000\tTime 0.325 (0.327)\tData 0.000 (0.019)\tLoss 1.5781 (2.0483)\tPrec@1 90.625 (87.278)\tPrec@5 99.219 (99.325)\n",
      "Epoch: [128][90/97], lr: 0.01000\tTime 0.321 (0.327)\tData 0.000 (0.019)\tLoss 2.3868 (2.0522)\tPrec@1 85.938 (87.251)\tPrec@5 99.219 (99.322)\n",
      "Epoch: [128][96/97], lr: 0.01000\tTime 0.310 (0.327)\tData 0.000 (0.020)\tLoss 2.0319 (2.0690)\tPrec@1 86.441 (87.167)\tPrec@5 99.153 (99.307)\n",
      "Gated Network Weight Gate= Flip:0.66, Sc:0.34\n",
      "Test: [0/100]\tTime 0.252 (0.252)\tLoss 9.3656 (9.3656)\tPrec@1 55.000 (55.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 6.8583 (8.4441)\tPrec@1 65.000 (59.091)\tPrec@5 97.000 (95.636)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 7.3737 (8.4097)\tPrec@1 63.000 (58.762)\tPrec@5 99.000 (95.476)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.0016 (8.4036)\tPrec@1 60.000 (58.903)\tPrec@5 97.000 (95.452)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 7.8772 (8.3741)\tPrec@1 63.000 (59.366)\tPrec@5 94.000 (95.244)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 8.1901 (8.2940)\tPrec@1 61.000 (59.843)\tPrec@5 96.000 (95.412)\n",
      "Test: [60/100]\tTime 0.074 (0.076)\tLoss 7.4253 (8.3219)\tPrec@1 60.000 (59.492)\tPrec@5 97.000 (95.475)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 7.9014 (8.2915)\tPrec@1 63.000 (59.676)\tPrec@5 95.000 (95.366)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 8.1174 (8.2508)\tPrec@1 63.000 (59.926)\tPrec@5 95.000 (95.494)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.6423 (8.3322)\tPrec@1 60.000 (59.604)\tPrec@5 96.000 (95.418)\n",
      "val Results: Prec@1 59.730 Prec@5 95.340 Loss 8.34962\n",
      "val Class Accuracy: [0.950,0.988,0.794,0.805,0.637,0.399,0.424,0.555,0.406,0.015]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [129][0/97], lr: 0.01000\tTime 0.475 (0.475)\tData 0.252 (0.252)\tLoss 1.7039 (1.7039)\tPrec@1 89.062 (89.062)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [129][10/97], lr: 0.01000\tTime 0.325 (0.345)\tData 0.000 (0.037)\tLoss 1.7797 (1.9510)\tPrec@1 89.062 (88.423)\tPrec@5 99.219 (99.077)\n",
      "Epoch: [129][20/97], lr: 0.01000\tTime 0.335 (0.334)\tData 0.000 (0.027)\tLoss 1.6102 (1.9926)\tPrec@1 88.281 (88.095)\tPrec@5 100.000 (99.256)\n",
      "Epoch: [129][30/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.024)\tLoss 1.5719 (1.9429)\tPrec@1 89.844 (88.458)\tPrec@5 99.219 (99.269)\n",
      "Epoch: [129][40/97], lr: 0.01000\tTime 0.322 (0.330)\tData 0.000 (0.022)\tLoss 1.8566 (1.9768)\tPrec@1 90.625 (88.338)\tPrec@5 100.000 (99.352)\n",
      "Epoch: [129][50/97], lr: 0.01000\tTime 0.320 (0.328)\tData 0.000 (0.021)\tLoss 1.0952 (1.9618)\tPrec@1 92.188 (88.358)\tPrec@5 100.000 (99.326)\n",
      "Epoch: [129][60/97], lr: 0.01000\tTime 0.325 (0.328)\tData 0.000 (0.020)\tLoss 2.0131 (1.9478)\tPrec@1 86.719 (88.448)\tPrec@5 100.000 (99.347)\n",
      "Epoch: [129][70/97], lr: 0.01000\tTime 0.323 (0.327)\tData 0.000 (0.020)\tLoss 2.0488 (1.9693)\tPrec@1 90.625 (88.160)\tPrec@5 99.219 (99.340)\n",
      "Epoch: [129][80/97], lr: 0.01000\tTime 0.321 (0.327)\tData 0.000 (0.020)\tLoss 1.8437 (1.9604)\tPrec@1 89.844 (88.194)\tPrec@5 98.438 (99.325)\n",
      "Epoch: [129][90/97], lr: 0.01000\tTime 0.320 (0.326)\tData 0.000 (0.019)\tLoss 1.6048 (1.9694)\tPrec@1 89.062 (88.092)\tPrec@5 100.000 (99.313)\n",
      "Epoch: [129][96/97], lr: 0.01000\tTime 0.309 (0.326)\tData 0.000 (0.020)\tLoss 1.2027 (1.9817)\tPrec@1 94.068 (87.990)\tPrec@5 100.000 (99.323)\n",
      "Gated Network Weight Gate= Flip:0.61, Sc:0.39\n",
      "Test: [0/100]\tTime 0.288 (0.288)\tLoss 11.7247 (11.7247)\tPrec@1 41.000 (41.000)\tPrec@5 91.000 (91.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 9.0972 (9.9929)\tPrec@1 53.000 (49.455)\tPrec@5 96.000 (93.727)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 8.9951 (9.9417)\tPrec@1 52.000 (49.857)\tPrec@5 94.000 (93.667)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 9.7755 (9.8986)\tPrec@1 51.000 (50.387)\tPrec@5 96.000 (93.677)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 10.7097 (9.8698)\tPrec@1 45.000 (50.634)\tPrec@5 93.000 (93.707)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 9.7187 (9.7808)\tPrec@1 53.000 (51.176)\tPrec@5 93.000 (93.882)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.9576 (9.8214)\tPrec@1 52.000 (50.885)\tPrec@5 96.000 (93.705)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 9.8178 (9.8181)\tPrec@1 50.000 (51.042)\tPrec@5 95.000 (93.845)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 8.9034 (9.7789)\tPrec@1 54.000 (51.160)\tPrec@5 95.000 (93.988)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 9.7832 (9.8863)\tPrec@1 51.000 (50.615)\tPrec@5 97.000 (93.791)\n",
      "val Results: Prec@1 50.540 Prec@5 93.800 Loss 9.91609\n",
      "val Class Accuracy: [0.948,0.991,0.859,0.650,0.563,0.291,0.067,0.318,0.262,0.105]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [130][0/97], lr: 0.01000\tTime 0.452 (0.452)\tData 0.253 (0.253)\tLoss 1.6640 (1.6640)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [130][10/97], lr: 0.01000\tTime 0.326 (0.342)\tData 0.000 (0.038)\tLoss 2.4540 (1.9416)\tPrec@1 87.500 (88.991)\tPrec@5 99.219 (99.361)\n",
      "Epoch: [130][20/97], lr: 0.01000\tTime 0.323 (0.335)\tData 0.000 (0.028)\tLoss 2.0697 (1.9776)\tPrec@1 85.938 (88.281)\tPrec@5 99.219 (99.479)\n",
      "Epoch: [130][30/97], lr: 0.01000\tTime 0.324 (0.333)\tData 0.000 (0.024)\tLoss 1.9776 (1.9609)\tPrec@1 88.281 (88.231)\tPrec@5 98.438 (99.521)\n",
      "Epoch: [130][40/97], lr: 0.01000\tTime 0.351 (0.331)\tData 0.000 (0.023)\tLoss 1.6106 (1.9756)\tPrec@1 90.625 (88.167)\tPrec@5 99.219 (99.524)\n",
      "Epoch: [130][50/97], lr: 0.01000\tTime 0.321 (0.330)\tData 0.000 (0.021)\tLoss 1.8355 (1.9955)\tPrec@1 87.500 (87.914)\tPrec@5 98.438 (99.449)\n",
      "Epoch: [130][60/97], lr: 0.01000\tTime 0.323 (0.329)\tData 0.000 (0.021)\tLoss 1.9327 (1.9775)\tPrec@1 88.281 (88.115)\tPrec@5 99.219 (99.436)\n",
      "Epoch: [130][70/97], lr: 0.01000\tTime 0.322 (0.328)\tData 0.000 (0.020)\tLoss 1.8380 (1.9404)\tPrec@1 89.062 (88.292)\tPrec@5 100.000 (99.406)\n",
      "Epoch: [130][80/97], lr: 0.01000\tTime 0.323 (0.328)\tData 0.000 (0.020)\tLoss 1.7714 (1.9482)\tPrec@1 89.844 (88.223)\tPrec@5 99.219 (99.383)\n",
      "Epoch: [130][90/97], lr: 0.01000\tTime 0.324 (0.327)\tData 0.000 (0.020)\tLoss 1.7229 (1.9516)\tPrec@1 86.719 (88.187)\tPrec@5 100.000 (99.339)\n",
      "Epoch: [130][96/97], lr: 0.01000\tTime 0.312 (0.327)\tData 0.000 (0.020)\tLoss 1.6424 (1.9514)\tPrec@1 89.831 (88.199)\tPrec@5 98.305 (99.323)\n",
      "Gated Network Weight Gate= Flip:0.54, Sc:0.46\n",
      "Test: [0/100]\tTime 0.317 (0.317)\tLoss 11.4951 (11.4951)\tPrec@1 43.000 (43.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.095)\tLoss 8.8250 (10.1846)\tPrec@1 58.000 (51.727)\tPrec@5 94.000 (92.818)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 7.1900 (9.9284)\tPrec@1 65.000 (52.762)\tPrec@5 97.000 (93.048)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 10.6159 (10.0155)\tPrec@1 48.000 (52.419)\tPrec@5 93.000 (92.806)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 11.1611 (10.0381)\tPrec@1 50.000 (52.463)\tPrec@5 91.000 (92.659)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 9.3750 (9.9253)\tPrec@1 55.000 (52.941)\tPrec@5 92.000 (92.765)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 7.9598 (9.9251)\tPrec@1 60.000 (52.836)\tPrec@5 91.000 (92.689)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 9.7774 (9.8834)\tPrec@1 53.000 (53.070)\tPrec@5 95.000 (92.789)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 9.6934 (9.7996)\tPrec@1 55.000 (53.593)\tPrec@5 92.000 (92.988)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 9.4971 (9.8688)\tPrec@1 55.000 (53.330)\tPrec@5 97.000 (93.011)\n",
      "val Results: Prec@1 53.320 Prec@5 93.000 Loss 9.88671\n",
      "val Class Accuracy: [0.906,0.991,0.708,0.589,0.460,0.858,0.258,0.278,0.210,0.074]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [131][0/97], lr: 0.01000\tTime 0.485 (0.485)\tData 0.264 (0.264)\tLoss 2.0913 (2.0913)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [131][10/97], lr: 0.01000\tTime 0.327 (0.343)\tData 0.000 (0.038)\tLoss 2.2386 (1.9957)\tPrec@1 87.500 (88.565)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [131][20/97], lr: 0.01000\tTime 0.323 (0.333)\tData 0.000 (0.028)\tLoss 1.0219 (1.9060)\tPrec@1 94.531 (88.765)\tPrec@5 98.438 (99.405)\n",
      "Epoch: [131][30/97], lr: 0.01000\tTime 0.320 (0.331)\tData 0.000 (0.024)\tLoss 1.7491 (1.9699)\tPrec@1 90.625 (88.206)\tPrec@5 100.000 (99.420)\n",
      "Epoch: [131][40/97], lr: 0.01000\tTime 0.322 (0.329)\tData 0.000 (0.023)\tLoss 1.6297 (1.9492)\tPrec@1 90.625 (88.167)\tPrec@5 100.000 (99.314)\n",
      "Epoch: [131][50/97], lr: 0.01000\tTime 0.320 (0.328)\tData 0.000 (0.021)\tLoss 2.2973 (1.9649)\tPrec@1 87.500 (88.097)\tPrec@5 98.438 (99.311)\n",
      "Epoch: [131][60/97], lr: 0.01000\tTime 0.317 (0.327)\tData 0.000 (0.021)\tLoss 2.0401 (1.9963)\tPrec@1 86.719 (87.795)\tPrec@5 99.219 (99.308)\n",
      "Epoch: [131][70/97], lr: 0.01000\tTime 0.321 (0.327)\tData 0.000 (0.020)\tLoss 2.1462 (1.9977)\tPrec@1 87.500 (87.808)\tPrec@5 99.219 (99.307)\n",
      "Epoch: [131][80/97], lr: 0.01000\tTime 0.322 (0.326)\tData 0.000 (0.020)\tLoss 1.9796 (1.9839)\tPrec@1 89.062 (87.953)\tPrec@5 100.000 (99.306)\n",
      "Epoch: [131][90/97], lr: 0.01000\tTime 0.317 (0.326)\tData 0.000 (0.020)\tLoss 1.8956 (1.9943)\tPrec@1 88.281 (87.835)\tPrec@5 99.219 (99.253)\n",
      "Epoch: [131][96/97], lr: 0.01000\tTime 0.311 (0.326)\tData 0.000 (0.020)\tLoss 2.4922 (2.0126)\tPrec@1 81.356 (87.635)\tPrec@5 99.153 (99.258)\n",
      "Gated Network Weight Gate= Flip:0.63, Sc:0.37\n",
      "Test: [0/100]\tTime 0.285 (0.285)\tLoss 8.5840 (8.5840)\tPrec@1 57.000 (57.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 7.5936 (8.1952)\tPrec@1 57.000 (60.273)\tPrec@5 95.000 (94.273)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 8.1552 (8.2277)\tPrec@1 59.000 (60.429)\tPrec@5 94.000 (94.000)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 8.1757 (8.3148)\tPrec@1 59.000 (60.161)\tPrec@5 94.000 (93.645)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 9.3262 (8.3319)\tPrec@1 58.000 (60.512)\tPrec@5 92.000 (93.610)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 7.6067 (8.2794)\tPrec@1 68.000 (60.745)\tPrec@5 92.000 (93.608)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 7.0568 (8.2745)\tPrec@1 67.000 (60.656)\tPrec@5 92.000 (93.623)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.1805 (8.3005)\tPrec@1 57.000 (60.338)\tPrec@5 98.000 (93.662)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 8.0961 (8.2664)\tPrec@1 60.000 (60.469)\tPrec@5 96.000 (93.877)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 8.3804 (8.3298)\tPrec@1 62.000 (60.220)\tPrec@5 95.000 (93.769)\n",
      "val Results: Prec@1 60.010 Prec@5 93.650 Loss 8.37137\n",
      "val Class Accuracy: [0.946,0.971,0.862,0.676,0.741,0.267,0.610,0.480,0.347,0.101]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [132][0/97], lr: 0.01000\tTime 0.500 (0.500)\tData 0.246 (0.246)\tLoss 2.0001 (2.0001)\tPrec@1 89.062 (89.062)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [132][10/97], lr: 0.01000\tTime 0.321 (0.353)\tData 0.000 (0.036)\tLoss 2.1448 (1.9975)\tPrec@1 86.719 (88.068)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [132][20/97], lr: 0.01000\tTime 0.318 (0.338)\tData 0.000 (0.027)\tLoss 1.3412 (1.8928)\tPrec@1 92.188 (88.504)\tPrec@5 98.438 (99.330)\n",
      "Epoch: [132][30/97], lr: 0.01000\tTime 0.320 (0.333)\tData 0.000 (0.024)\tLoss 1.8359 (1.9211)\tPrec@1 89.844 (88.533)\tPrec@5 99.219 (99.420)\n",
      "Epoch: [132][40/97], lr: 0.01000\tTime 0.339 (0.331)\tData 0.000 (0.022)\tLoss 2.4604 (1.8941)\tPrec@1 84.375 (88.586)\tPrec@5 97.656 (99.333)\n",
      "Epoch: [132][50/97], lr: 0.01000\tTime 0.320 (0.330)\tData 0.000 (0.021)\tLoss 2.0993 (1.9083)\tPrec@1 88.281 (88.496)\tPrec@5 100.000 (99.341)\n",
      "Epoch: [132][60/97], lr: 0.01000\tTime 0.320 (0.329)\tData 0.000 (0.020)\tLoss 1.4807 (1.8991)\tPrec@1 90.625 (88.589)\tPrec@5 100.000 (99.347)\n",
      "Epoch: [132][70/97], lr: 0.01000\tTime 0.322 (0.328)\tData 0.000 (0.020)\tLoss 1.5254 (1.8809)\tPrec@1 91.406 (88.666)\tPrec@5 99.219 (99.384)\n",
      "Epoch: [132][80/97], lr: 0.01000\tTime 0.321 (0.327)\tData 0.000 (0.020)\tLoss 1.5340 (1.9038)\tPrec@1 90.625 (88.513)\tPrec@5 100.000 (99.402)\n",
      "Epoch: [132][90/97], lr: 0.01000\tTime 0.319 (0.327)\tData 0.000 (0.019)\tLoss 1.4927 (1.9239)\tPrec@1 90.625 (88.367)\tPrec@5 100.000 (99.365)\n",
      "Epoch: [132][96/97], lr: 0.01000\tTime 0.309 (0.326)\tData 0.000 (0.020)\tLoss 1.9355 (1.9268)\tPrec@1 86.441 (88.336)\tPrec@5 100.000 (99.379)\n",
      "Gated Network Weight Gate= Flip:0.62, Sc:0.38\n",
      "Test: [0/100]\tTime 0.255 (0.255)\tLoss 7.1630 (7.1630)\tPrec@1 66.000 (66.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 5.2670 (7.5667)\tPrec@1 75.000 (62.091)\tPrec@5 96.000 (96.182)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.9011 (7.7124)\tPrec@1 68.000 (62.048)\tPrec@5 96.000 (95.810)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.4943 (7.7507)\tPrec@1 52.000 (61.355)\tPrec@5 96.000 (95.677)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 8.6539 (7.8517)\tPrec@1 59.000 (61.122)\tPrec@5 95.000 (95.634)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 7.8516 (7.7573)\tPrec@1 64.000 (61.824)\tPrec@5 96.000 (95.784)\n",
      "Test: [60/100]\tTime 0.074 (0.076)\tLoss 6.6424 (7.7654)\tPrec@1 67.000 (61.754)\tPrec@5 97.000 (95.672)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 7.9808 (7.7266)\tPrec@1 60.000 (61.944)\tPrec@5 94.000 (95.676)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 7.0929 (7.6991)\tPrec@1 67.000 (62.049)\tPrec@5 98.000 (95.827)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 7.0525 (7.7132)\tPrec@1 71.000 (62.022)\tPrec@5 98.000 (95.791)\n",
      "val Results: Prec@1 61.990 Prec@5 95.780 Loss 7.72629\n",
      "val Class Accuracy: [0.921,0.965,0.845,0.414,0.345,0.569,0.859,0.297,0.698,0.286]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [133][0/97], lr: 0.01000\tTime 0.463 (0.463)\tData 0.229 (0.229)\tLoss 1.9178 (1.9178)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [133][10/97], lr: 0.01000\tTime 0.319 (0.339)\tData 0.000 (0.035)\tLoss 2.1082 (2.0369)\tPrec@1 88.281 (88.139)\tPrec@5 98.438 (99.361)\n",
      "Epoch: [133][20/97], lr: 0.01000\tTime 0.319 (0.333)\tData 0.000 (0.026)\tLoss 2.4762 (1.9421)\tPrec@1 85.938 (88.616)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [133][30/97], lr: 0.01000\tTime 0.325 (0.330)\tData 0.000 (0.023)\tLoss 1.9969 (1.9424)\tPrec@1 88.281 (88.533)\tPrec@5 99.219 (99.446)\n",
      "Epoch: [133][40/97], lr: 0.01000\tTime 0.320 (0.328)\tData 0.000 (0.022)\tLoss 2.9892 (1.9641)\tPrec@1 78.906 (88.167)\tPrec@5 96.875 (99.466)\n",
      "Epoch: [133][50/97], lr: 0.01000\tTime 0.322 (0.327)\tData 0.000 (0.021)\tLoss 2.8127 (2.0139)\tPrec@1 81.250 (87.745)\tPrec@5 98.438 (99.433)\n",
      "Epoch: [133][60/97], lr: 0.01000\tTime 0.319 (0.326)\tData 0.000 (0.020)\tLoss 2.0221 (2.0111)\tPrec@1 85.938 (87.756)\tPrec@5 100.000 (99.372)\n",
      "Epoch: [133][70/97], lr: 0.01000\tTime 0.318 (0.326)\tData 0.000 (0.020)\tLoss 1.7995 (2.0207)\tPrec@1 90.625 (87.720)\tPrec@5 98.438 (99.329)\n",
      "Epoch: [133][80/97], lr: 0.01000\tTime 0.321 (0.326)\tData 0.000 (0.019)\tLoss 1.0513 (1.9699)\tPrec@1 94.531 (88.069)\tPrec@5 100.000 (99.344)\n",
      "Epoch: [133][90/97], lr: 0.01000\tTime 0.317 (0.325)\tData 0.000 (0.019)\tLoss 1.6778 (1.9635)\tPrec@1 90.625 (88.135)\tPrec@5 98.438 (99.365)\n",
      "Epoch: [133][96/97], lr: 0.01000\tTime 0.312 (0.325)\tData 0.000 (0.020)\tLoss 1.6706 (1.9695)\tPrec@1 88.136 (88.030)\tPrec@5 99.153 (99.363)\n",
      "Gated Network Weight Gate= Flip:0.67, Sc:0.33\n",
      "Test: [0/100]\tTime 0.255 (0.255)\tLoss 9.9672 (9.9672)\tPrec@1 50.000 (50.000)\tPrec@5 90.000 (90.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 8.3951 (9.1289)\tPrec@1 56.000 (57.182)\tPrec@5 88.000 (91.273)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 8.4187 (9.1176)\tPrec@1 58.000 (56.524)\tPrec@5 93.000 (92.048)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 9.1688 (9.1371)\tPrec@1 54.000 (56.419)\tPrec@5 92.000 (92.323)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 9.6522 (9.0685)\tPrec@1 55.000 (56.976)\tPrec@5 91.000 (92.366)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 8.4666 (9.0143)\tPrec@1 62.000 (57.137)\tPrec@5 93.000 (92.529)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.5119 (9.0215)\tPrec@1 59.000 (56.820)\tPrec@5 92.000 (92.492)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.4697 (9.0106)\tPrec@1 61.000 (56.859)\tPrec@5 92.000 (92.549)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 8.2608 (8.9468)\tPrec@1 61.000 (57.173)\tPrec@5 90.000 (92.667)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 8.3513 (9.0479)\tPrec@1 61.000 (56.736)\tPrec@5 96.000 (92.407)\n",
      "val Results: Prec@1 56.600 Prec@5 92.420 Loss 9.08535\n",
      "val Class Accuracy: [0.955,0.977,0.869,0.721,0.714,0.371,0.086,0.545,0.224,0.198]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [134][0/97], lr: 0.01000\tTime 0.472 (0.472)\tData 0.263 (0.263)\tLoss 2.2837 (2.2837)\tPrec@1 86.719 (86.719)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [134][10/97], lr: 0.01000\tTime 0.321 (0.343)\tData 0.000 (0.037)\tLoss 2.2830 (1.8727)\tPrec@1 85.156 (88.281)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [134][20/97], lr: 0.01000\tTime 0.349 (0.336)\tData 0.000 (0.027)\tLoss 1.7398 (1.8388)\tPrec@1 91.406 (88.728)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [134][30/97], lr: 0.01000\tTime 0.322 (0.332)\tData 0.000 (0.024)\tLoss 1.9264 (1.8740)\tPrec@1 87.500 (88.357)\tPrec@5 100.000 (99.471)\n",
      "Epoch: [134][40/97], lr: 0.01000\tTime 0.323 (0.331)\tData 0.000 (0.022)\tLoss 1.9317 (1.8821)\tPrec@1 87.500 (88.338)\tPrec@5 98.438 (99.447)\n",
      "Epoch: [134][50/97], lr: 0.01000\tTime 0.322 (0.329)\tData 0.000 (0.021)\tLoss 2.2881 (1.9081)\tPrec@1 87.500 (88.205)\tPrec@5 100.000 (99.464)\n",
      "Epoch: [134][60/97], lr: 0.01000\tTime 0.323 (0.328)\tData 0.000 (0.021)\tLoss 1.4439 (1.8734)\tPrec@1 89.844 (88.345)\tPrec@5 100.000 (99.475)\n",
      "Epoch: [134][70/97], lr: 0.01000\tTime 0.325 (0.328)\tData 0.000 (0.020)\tLoss 2.0462 (1.9015)\tPrec@1 87.500 (88.171)\tPrec@5 100.000 (99.450)\n",
      "Epoch: [134][80/97], lr: 0.01000\tTime 0.321 (0.327)\tData 0.000 (0.020)\tLoss 1.2313 (1.9010)\tPrec@1 92.188 (88.175)\tPrec@5 100.000 (99.460)\n",
      "Epoch: [134][90/97], lr: 0.01000\tTime 0.322 (0.327)\tData 0.000 (0.019)\tLoss 1.9371 (1.8830)\tPrec@1 88.281 (88.316)\tPrec@5 100.000 (99.502)\n",
      "Epoch: [134][96/97], lr: 0.01000\tTime 0.311 (0.327)\tData 0.000 (0.020)\tLoss 1.1742 (1.8792)\tPrec@1 93.220 (88.296)\tPrec@5 98.305 (99.484)\n",
      "Gated Network Weight Gate= Flip:0.61, Sc:0.39\n",
      "Test: [0/100]\tTime 0.257 (0.257)\tLoss 10.0610 (10.0610)\tPrec@1 56.000 (56.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 7.4832 (9.2894)\tPrec@1 64.000 (56.909)\tPrec@5 96.000 (97.000)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 8.3744 (9.1862)\tPrec@1 53.000 (57.190)\tPrec@5 97.000 (96.762)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 8.6437 (9.2534)\tPrec@1 58.000 (56.710)\tPrec@5 96.000 (96.742)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 9.0173 (9.2711)\tPrec@1 57.000 (56.341)\tPrec@5 95.000 (96.488)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 9.0745 (9.1431)\tPrec@1 56.000 (57.078)\tPrec@5 95.000 (96.490)\n",
      "Test: [60/100]\tTime 0.074 (0.076)\tLoss 8.6048 (9.1492)\tPrec@1 59.000 (56.902)\tPrec@5 97.000 (96.459)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.0900 (9.1283)\tPrec@1 63.000 (56.958)\tPrec@5 100.000 (96.479)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 8.5125 (9.0746)\tPrec@1 60.000 (57.395)\tPrec@5 97.000 (96.481)\n",
      "Test: [90/100]\tTime 0.074 (0.075)\tLoss 9.4303 (9.1246)\tPrec@1 54.000 (57.176)\tPrec@5 98.000 (96.418)\n",
      "val Results: Prec@1 57.190 Prec@5 96.380 Loss 9.15014\n",
      "val Class Accuracy: [0.883,0.997,0.561,0.888,0.627,0.295,0.533,0.600,0.322,0.013]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [135][0/97], lr: 0.01000\tTime 0.441 (0.441)\tData 0.248 (0.248)\tLoss 1.6400 (1.6400)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [135][10/97], lr: 0.01000\tTime 0.325 (0.336)\tData 0.000 (0.037)\tLoss 2.2898 (1.9971)\tPrec@1 83.594 (87.713)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [135][20/97], lr: 0.01000\tTime 0.321 (0.329)\tData 0.000 (0.027)\tLoss 1.4495 (1.9174)\tPrec@1 91.406 (88.356)\tPrec@5 100.000 (99.591)\n",
      "Epoch: [135][30/97], lr: 0.01000\tTime 0.318 (0.327)\tData 0.000 (0.024)\tLoss 2.5572 (1.9570)\tPrec@1 87.500 (88.281)\tPrec@5 97.656 (99.420)\n",
      "Epoch: [135][40/97], lr: 0.01000\tTime 0.318 (0.326)\tData 0.000 (0.022)\tLoss 1.8191 (1.9782)\tPrec@1 89.062 (88.319)\tPrec@5 100.000 (99.390)\n",
      "Epoch: [135][50/97], lr: 0.01000\tTime 0.324 (0.326)\tData 0.000 (0.021)\tLoss 2.6185 (2.0095)\tPrec@1 85.156 (88.067)\tPrec@5 99.219 (99.449)\n",
      "Epoch: [135][60/97], lr: 0.01000\tTime 0.320 (0.325)\tData 0.000 (0.021)\tLoss 1.9768 (1.9872)\tPrec@1 84.375 (88.166)\tPrec@5 98.438 (99.385)\n",
      "Epoch: [135][70/97], lr: 0.01000\tTime 0.317 (0.325)\tData 0.000 (0.020)\tLoss 1.8813 (1.9600)\tPrec@1 89.062 (88.248)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [135][80/97], lr: 0.01000\tTime 0.320 (0.325)\tData 0.000 (0.020)\tLoss 1.9160 (1.9542)\tPrec@1 88.281 (88.204)\tPrec@5 100.000 (99.441)\n",
      "Epoch: [135][90/97], lr: 0.01000\tTime 0.320 (0.325)\tData 0.000 (0.019)\tLoss 3.1779 (2.0006)\tPrec@1 79.688 (87.861)\tPrec@5 98.438 (99.408)\n",
      "Epoch: [135][96/97], lr: 0.01000\tTime 0.309 (0.324)\tData 0.000 (0.020)\tLoss 2.4155 (1.9955)\tPrec@1 84.746 (87.869)\tPrec@5 100.000 (99.395)\n",
      "Gated Network Weight Gate= Flip:0.59, Sc:0.41\n",
      "Test: [0/100]\tTime 0.269 (0.269)\tLoss 7.6071 (7.6071)\tPrec@1 63.000 (63.000)\tPrec@5 96.000 (96.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 6.4720 (7.7517)\tPrec@1 65.000 (61.182)\tPrec@5 92.000 (95.545)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 7.3846 (7.8010)\tPrec@1 62.000 (61.190)\tPrec@5 99.000 (96.143)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 7.3540 (7.8056)\tPrec@1 63.000 (61.419)\tPrec@5 94.000 (95.903)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 7.9952 (7.7985)\tPrec@1 59.000 (61.732)\tPrec@5 96.000 (95.707)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 8.5058 (7.7553)\tPrec@1 58.000 (61.961)\tPrec@5 97.000 (95.706)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 7.1882 (7.8546)\tPrec@1 65.000 (61.246)\tPrec@5 93.000 (95.590)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 7.7823 (7.8671)\tPrec@1 59.000 (61.113)\tPrec@5 97.000 (95.606)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 7.7193 (7.8519)\tPrec@1 64.000 (61.123)\tPrec@5 96.000 (95.728)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 8.9294 (7.9198)\tPrec@1 57.000 (60.846)\tPrec@5 97.000 (95.637)\n",
      "val Results: Prec@1 60.880 Prec@5 95.660 Loss 7.91729\n",
      "val Class Accuracy: [0.905,0.982,0.674,0.821,0.723,0.382,0.224,0.537,0.567,0.273]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [136][0/97], lr: 0.01000\tTime 0.421 (0.421)\tData 0.225 (0.225)\tLoss 1.9475 (1.9475)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [136][10/97], lr: 0.01000\tTime 0.321 (0.341)\tData 0.000 (0.035)\tLoss 2.0604 (1.7844)\tPrec@1 86.719 (89.205)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [136][20/97], lr: 0.01000\tTime 0.339 (0.334)\tData 0.000 (0.027)\tLoss 1.8555 (1.9166)\tPrec@1 89.844 (88.170)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [136][30/97], lr: 0.01000\tTime 0.320 (0.330)\tData 0.000 (0.023)\tLoss 1.8718 (1.8746)\tPrec@1 88.281 (88.584)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [136][40/97], lr: 0.01000\tTime 0.320 (0.329)\tData 0.000 (0.022)\tLoss 2.6105 (1.9368)\tPrec@1 85.156 (88.148)\tPrec@5 99.219 (99.409)\n",
      "Epoch: [136][50/97], lr: 0.01000\tTime 0.321 (0.328)\tData 0.000 (0.021)\tLoss 1.8391 (1.9734)\tPrec@1 89.062 (87.960)\tPrec@5 99.219 (99.418)\n",
      "Epoch: [136][60/97], lr: 0.01000\tTime 0.321 (0.327)\tData 0.000 (0.020)\tLoss 1.7966 (2.0060)\tPrec@1 89.844 (87.782)\tPrec@5 99.219 (99.270)\n",
      "Epoch: [136][70/97], lr: 0.01000\tTime 0.320 (0.326)\tData 0.000 (0.020)\tLoss 1.7996 (1.9869)\tPrec@1 90.625 (87.940)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [136][80/97], lr: 0.01000\tTime 0.347 (0.331)\tData 0.001 (0.020)\tLoss 1.4024 (1.9583)\tPrec@1 89.844 (88.098)\tPrec@5 99.219 (99.286)\n",
      "Epoch: [136][90/97], lr: 0.01000\tTime 0.323 (0.333)\tData 0.000 (0.019)\tLoss 1.6808 (1.9649)\tPrec@1 91.406 (88.015)\tPrec@5 100.000 (99.322)\n",
      "Epoch: [136][96/97], lr: 0.01000\tTime 0.315 (0.333)\tData 0.000 (0.020)\tLoss 1.5280 (1.9600)\tPrec@1 90.678 (88.078)\tPrec@5 100.000 (99.339)\n",
      "Gated Network Weight Gate= Flip:0.54, Sc:0.46\n",
      "Test: [0/100]\tTime 0.283 (0.283)\tLoss 8.7988 (8.7988)\tPrec@1 58.000 (58.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 6.6947 (8.6785)\tPrec@1 65.000 (59.000)\tPrec@5 96.000 (94.727)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 9.2112 (8.8539)\tPrec@1 51.000 (57.238)\tPrec@5 97.000 (94.857)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 8.3505 (8.8723)\tPrec@1 64.000 (57.452)\tPrec@5 95.000 (94.581)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 9.3485 (8.8767)\tPrec@1 55.000 (57.732)\tPrec@5 95.000 (94.390)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 9.3270 (8.8135)\tPrec@1 55.000 (57.961)\tPrec@5 94.000 (94.235)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.2700 (8.8642)\tPrec@1 58.000 (57.541)\tPrec@5 94.000 (94.311)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.5294 (8.8692)\tPrec@1 56.000 (57.507)\tPrec@5 96.000 (94.423)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 9.3938 (8.8328)\tPrec@1 57.000 (57.802)\tPrec@5 92.000 (94.444)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 8.5414 (8.8422)\tPrec@1 60.000 (57.868)\tPrec@5 97.000 (94.396)\n",
      "val Results: Prec@1 57.850 Prec@5 94.450 Loss 8.85727\n",
      "val Class Accuracy: [0.862,0.953,0.763,0.837,0.394,0.187,0.807,0.422,0.377,0.183]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [137][0/97], lr: 0.01000\tTime 0.484 (0.484)\tData 0.241 (0.241)\tLoss 2.1084 (2.1084)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [137][10/97], lr: 0.01000\tTime 0.387 (0.398)\tData 0.000 (0.036)\tLoss 2.4026 (2.0130)\tPrec@1 87.500 (87.997)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [137][20/97], lr: 0.01000\tTime 0.376 (0.385)\tData 0.000 (0.026)\tLoss 2.0155 (1.9842)\tPrec@1 87.500 (88.170)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [137][30/97], lr: 0.01000\tTime 0.321 (0.374)\tData 0.000 (0.022)\tLoss 1.6094 (2.0094)\tPrec@1 90.625 (88.130)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [137][40/97], lr: 0.01000\tTime 0.339 (0.365)\tData 0.000 (0.021)\tLoss 1.8332 (1.9824)\tPrec@1 88.281 (88.415)\tPrec@5 100.000 (99.333)\n",
      "Epoch: [137][50/97], lr: 0.01000\tTime 0.321 (0.360)\tData 0.000 (0.020)\tLoss 2.8127 (1.9766)\tPrec@1 83.594 (88.419)\tPrec@5 100.000 (99.249)\n",
      "Epoch: [137][60/97], lr: 0.01000\tTime 0.344 (0.357)\tData 0.000 (0.019)\tLoss 1.7151 (1.9541)\tPrec@1 88.281 (88.448)\tPrec@5 99.219 (99.283)\n",
      "Epoch: [137][70/97], lr: 0.01000\tTime 0.346 (0.357)\tData 0.000 (0.019)\tLoss 1.9518 (1.9403)\tPrec@1 90.625 (88.622)\tPrec@5 99.219 (99.252)\n",
      "Epoch: [137][80/97], lr: 0.01000\tTime 0.338 (0.355)\tData 0.000 (0.019)\tLoss 2.2660 (1.9368)\tPrec@1 85.156 (88.609)\tPrec@5 100.000 (99.306)\n",
      "Epoch: [137][90/97], lr: 0.01000\tTime 0.343 (0.354)\tData 0.000 (0.018)\tLoss 2.8071 (1.9679)\tPrec@1 85.938 (88.453)\tPrec@5 99.219 (99.287)\n",
      "Epoch: [137][96/97], lr: 0.01000\tTime 0.327 (0.354)\tData 0.000 (0.019)\tLoss 2.1037 (1.9874)\tPrec@1 87.288 (88.336)\tPrec@5 99.153 (99.275)\n",
      "Gated Network Weight Gate= Flip:0.62, Sc:0.38\n",
      "Test: [0/100]\tTime 0.290 (0.290)\tLoss 9.3194 (9.3194)\tPrec@1 59.000 (59.000)\tPrec@5 90.000 (90.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 7.2219 (9.0012)\tPrec@1 66.000 (57.000)\tPrec@5 93.000 (94.000)\n",
      "Test: [20/100]\tTime 0.074 (0.084)\tLoss 6.8857 (8.7796)\tPrec@1 64.000 (57.762)\tPrec@5 97.000 (94.048)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 9.0843 (8.8396)\tPrec@1 56.000 (57.032)\tPrec@5 93.000 (93.484)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 9.1421 (8.9191)\tPrec@1 57.000 (56.854)\tPrec@5 89.000 (93.317)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 9.0292 (8.8767)\tPrec@1 54.000 (57.196)\tPrec@5 97.000 (93.373)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 7.2137 (8.8846)\tPrec@1 62.000 (56.852)\tPrec@5 94.000 (93.443)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 9.0834 (8.8514)\tPrec@1 57.000 (57.099)\tPrec@5 92.000 (93.423)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 8.8180 (8.8253)\tPrec@1 59.000 (57.099)\tPrec@5 95.000 (93.543)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 9.0477 (8.8820)\tPrec@1 57.000 (56.846)\tPrec@5 97.000 (93.484)\n",
      "val Results: Prec@1 56.800 Prec@5 93.400 Loss 8.90811\n",
      "val Class Accuracy: [0.899,0.976,0.814,0.732,0.519,0.687,0.443,0.071,0.478,0.061]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [138][0/97], lr: 0.01000\tTime 0.527 (0.527)\tData 0.301 (0.301)\tLoss 2.5912 (2.5912)\tPrec@1 85.156 (85.156)\tPrec@5 96.094 (96.094)\n",
      "Epoch: [138][10/97], lr: 0.01000\tTime 0.368 (0.361)\tData 0.000 (0.042)\tLoss 1.3687 (2.0255)\tPrec@1 91.406 (87.358)\tPrec@5 99.219 (99.077)\n",
      "Epoch: [138][20/97], lr: 0.01000\tTime 0.410 (0.384)\tData 0.000 (0.029)\tLoss 1.8223 (1.9876)\tPrec@1 89.844 (87.760)\tPrec@5 99.219 (99.405)\n",
      "Epoch: [138][30/97], lr: 0.01000\tTime 0.326 (0.380)\tData 0.000 (0.024)\tLoss 0.9953 (1.9044)\tPrec@1 94.531 (88.558)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [138][40/97], lr: 0.01000\tTime 0.346 (0.369)\tData 0.000 (0.022)\tLoss 2.4655 (1.9352)\tPrec@1 86.719 (88.434)\tPrec@5 97.656 (99.219)\n",
      "Epoch: [138][50/97], lr: 0.01000\tTime 0.419 (0.376)\tData 0.000 (0.021)\tLoss 2.2412 (1.9897)\tPrec@1 85.156 (88.067)\tPrec@5 98.438 (99.188)\n",
      "Epoch: [138][60/97], lr: 0.01000\tTime 0.445 (0.381)\tData 0.000 (0.020)\tLoss 1.9730 (1.9995)\tPrec@1 88.281 (88.025)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [138][70/97], lr: 0.01000\tTime 0.376 (0.382)\tData 0.001 (0.019)\tLoss 1.7392 (1.9985)\tPrec@1 89.844 (87.962)\tPrec@5 100.000 (99.274)\n",
      "Epoch: [138][80/97], lr: 0.01000\tTime 0.427 (0.386)\tData 0.001 (0.019)\tLoss 1.6606 (2.0006)\tPrec@1 90.625 (87.876)\tPrec@5 99.219 (99.228)\n",
      "Epoch: [138][90/97], lr: 0.01000\tTime 0.352 (0.391)\tData 0.000 (0.018)\tLoss 1.4038 (1.9692)\tPrec@1 90.625 (88.067)\tPrec@5 100.000 (99.245)\n",
      "Epoch: [138][96/97], lr: 0.01000\tTime 0.341 (0.388)\tData 0.000 (0.019)\tLoss 2.3742 (1.9722)\tPrec@1 84.746 (88.022)\tPrec@5 100.000 (99.266)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.385 (0.385)\tLoss 12.7399 (12.7399)\tPrec@1 36.000 (36.000)\tPrec@5 90.000 (90.000)\n",
      "Test: [10/100]\tTime 0.073 (0.102)\tLoss 8.8949 (10.9538)\tPrec@1 59.000 (47.455)\tPrec@5 95.000 (92.000)\n",
      "Test: [20/100]\tTime 0.074 (0.089)\tLoss 9.7292 (10.9201)\tPrec@1 51.000 (46.905)\tPrec@5 95.000 (92.143)\n",
      "Test: [30/100]\tTime 0.073 (0.084)\tLoss 10.9607 (10.9465)\tPrec@1 45.000 (46.613)\tPrec@5 90.000 (91.968)\n",
      "Test: [40/100]\tTime 0.073 (0.081)\tLoss 11.5029 (10.9375)\tPrec@1 45.000 (47.000)\tPrec@5 90.000 (91.659)\n",
      "Test: [50/100]\tTime 0.073 (0.080)\tLoss 10.1340 (10.7928)\tPrec@1 54.000 (47.980)\tPrec@5 89.000 (91.569)\n",
      "Test: [60/100]\tTime 0.073 (0.079)\tLoss 9.4378 (10.7705)\tPrec@1 53.000 (48.016)\tPrec@5 91.000 (91.656)\n",
      "Test: [70/100]\tTime 0.074 (0.078)\tLoss 9.5943 (10.7557)\tPrec@1 55.000 (48.225)\tPrec@5 92.000 (91.549)\n",
      "Test: [80/100]\tTime 0.073 (0.077)\tLoss 9.3188 (10.6863)\tPrec@1 61.000 (48.580)\tPrec@5 90.000 (91.691)\n",
      "Test: [90/100]\tTime 0.073 (0.077)\tLoss 10.7897 (10.7585)\tPrec@1 50.000 (48.253)\tPrec@5 94.000 (91.582)\n",
      "val Results: Prec@1 48.390 Prec@5 91.510 Loss 10.76842\n",
      "val Class Accuracy: [0.907,0.976,0.742,0.895,0.419,0.261,0.229,0.342,0.053,0.015]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [139][0/97], lr: 0.01000\tTime 0.467 (0.467)\tData 0.244 (0.244)\tLoss 1.8734 (1.8734)\tPrec@1 89.844 (89.844)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [139][10/97], lr: 0.01000\tTime 0.357 (0.345)\tData 0.000 (0.036)\tLoss 1.2875 (1.8457)\tPrec@1 91.406 (88.494)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [139][20/97], lr: 0.01000\tTime 0.318 (0.334)\tData 0.000 (0.027)\tLoss 1.9155 (1.8513)\tPrec@1 88.281 (88.318)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [139][30/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.024)\tLoss 2.1179 (1.8652)\tPrec@1 85.156 (88.407)\tPrec@5 99.219 (99.269)\n",
      "Epoch: [139][40/97], lr: 0.01000\tTime 0.319 (0.329)\tData 0.000 (0.022)\tLoss 2.2606 (1.8729)\tPrec@1 86.719 (88.415)\tPrec@5 100.000 (99.371)\n",
      "Epoch: [139][50/97], lr: 0.01000\tTime 0.323 (0.328)\tData 0.000 (0.021)\tLoss 1.6246 (1.8612)\tPrec@1 89.844 (88.419)\tPrec@5 100.000 (99.387)\n",
      "Epoch: [139][60/97], lr: 0.01000\tTime 0.342 (0.328)\tData 0.000 (0.021)\tLoss 2.0438 (1.8783)\tPrec@1 88.281 (88.332)\tPrec@5 99.219 (99.347)\n",
      "Epoch: [139][70/97], lr: 0.01000\tTime 0.320 (0.328)\tData 0.000 (0.020)\tLoss 1.6409 (1.9055)\tPrec@1 89.062 (88.292)\tPrec@5 98.438 (99.263)\n",
      "Epoch: [139][80/97], lr: 0.01000\tTime 0.318 (0.327)\tData 0.000 (0.019)\tLoss 1.2092 (1.9062)\tPrec@1 89.844 (88.339)\tPrec@5 99.219 (99.286)\n",
      "Epoch: [139][90/97], lr: 0.01000\tTime 0.322 (0.327)\tData 0.000 (0.019)\tLoss 1.3043 (1.9130)\tPrec@1 92.188 (88.393)\tPrec@5 99.219 (99.296)\n",
      "Epoch: [139][96/97], lr: 0.01000\tTime 0.312 (0.327)\tData 0.000 (0.020)\tLoss 1.5123 (1.9107)\tPrec@1 92.373 (88.377)\tPrec@5 99.153 (99.283)\n",
      "Gated Network Weight Gate= Flip:0.45, Sc:0.55\n",
      "Test: [0/100]\tTime 0.276 (0.276)\tLoss 11.5456 (11.5456)\tPrec@1 43.000 (43.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.074 (0.092)\tLoss 7.5044 (9.5481)\tPrec@1 65.000 (55.273)\tPrec@5 92.000 (93.364)\n",
      "Test: [20/100]\tTime 0.074 (0.083)\tLoss 7.5982 (9.5684)\tPrec@1 59.000 (54.857)\tPrec@5 96.000 (93.762)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 8.6734 (9.6015)\tPrec@1 59.000 (54.452)\tPrec@5 96.000 (93.935)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 9.3736 (9.6000)\tPrec@1 54.000 (54.878)\tPrec@5 93.000 (94.000)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 8.4599 (9.4794)\tPrec@1 61.000 (55.275)\tPrec@5 93.000 (94.000)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 7.8058 (9.4318)\tPrec@1 64.000 (55.541)\tPrec@5 95.000 (94.016)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 8.4578 (9.3755)\tPrec@1 60.000 (55.817)\tPrec@5 97.000 (94.042)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 8.7749 (9.3166)\tPrec@1 59.000 (56.074)\tPrec@5 95.000 (94.198)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 9.7241 (9.3958)\tPrec@1 53.000 (55.736)\tPrec@5 97.000 (94.055)\n",
      "val Results: Prec@1 55.630 Prec@5 93.990 Loss 9.41836\n",
      "val Class Accuracy: [0.971,0.989,0.584,0.823,0.621,0.606,0.271,0.561,0.118,0.019]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [140][0/97], lr: 0.01000\tTime 0.562 (0.562)\tData 0.338 (0.338)\tLoss 2.0903 (2.0903)\tPrec@1 84.375 (84.375)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [140][10/97], lr: 0.01000\tTime 0.322 (0.351)\tData 0.000 (0.044)\tLoss 2.8062 (2.0495)\tPrec@1 82.031 (87.358)\tPrec@5 97.656 (99.148)\n",
      "Epoch: [140][20/97], lr: 0.01000\tTime 0.323 (0.340)\tData 0.000 (0.031)\tLoss 1.8984 (1.9726)\tPrec@1 89.062 (87.760)\tPrec@5 100.000 (99.330)\n",
      "Epoch: [140][30/97], lr: 0.01000\tTime 0.328 (0.336)\tData 0.000 (0.027)\tLoss 1.8946 (1.9809)\tPrec@1 89.062 (87.828)\tPrec@5 99.219 (99.294)\n",
      "Epoch: [140][40/97], lr: 0.01000\tTime 0.320 (0.334)\tData 0.000 (0.024)\tLoss 1.7404 (1.9110)\tPrec@1 89.062 (88.186)\tPrec@5 98.438 (99.238)\n",
      "Epoch: [140][50/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.023)\tLoss 1.9560 (1.9009)\tPrec@1 85.156 (88.266)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [140][60/97], lr: 0.01000\tTime 0.324 (0.332)\tData 0.000 (0.022)\tLoss 3.2772 (1.9502)\tPrec@1 79.688 (87.999)\tPrec@5 99.219 (99.206)\n",
      "Epoch: [140][70/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.021)\tLoss 2.0389 (1.9523)\tPrec@1 88.281 (88.017)\tPrec@5 99.219 (99.230)\n",
      "Epoch: [140][80/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.021)\tLoss 1.9694 (1.9431)\tPrec@1 88.281 (88.175)\tPrec@5 99.219 (99.257)\n",
      "Epoch: [140][90/97], lr: 0.01000\tTime 0.337 (0.335)\tData 0.000 (0.020)\tLoss 2.1653 (1.9290)\tPrec@1 86.719 (88.307)\tPrec@5 97.656 (99.236)\n",
      "Epoch: [140][96/97], lr: 0.01000\tTime 0.368 (0.337)\tData 0.000 (0.021)\tLoss 2.4433 (1.9494)\tPrec@1 85.593 (88.199)\tPrec@5 99.153 (99.226)\n",
      "Gated Network Weight Gate= Flip:0.45, Sc:0.55\n",
      "Test: [0/100]\tTime 0.499 (0.499)\tLoss 8.3295 (8.3295)\tPrec@1 61.000 (61.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.074 (0.113)\tLoss 7.8409 (9.1794)\tPrec@1 61.000 (57.091)\tPrec@5 98.000 (93.727)\n",
      "Test: [20/100]\tTime 0.074 (0.094)\tLoss 9.5527 (9.1078)\tPrec@1 54.000 (57.619)\tPrec@5 97.000 (93.571)\n",
      "Test: [30/100]\tTime 0.074 (0.089)\tLoss 9.9033 (9.2475)\tPrec@1 54.000 (56.839)\tPrec@5 90.000 (93.032)\n",
      "Test: [40/100]\tTime 0.073 (0.086)\tLoss 9.5949 (9.3215)\tPrec@1 54.000 (56.268)\tPrec@5 91.000 (92.902)\n",
      "Test: [50/100]\tTime 0.073 (0.084)\tLoss 9.5649 (9.2686)\tPrec@1 55.000 (56.373)\tPrec@5 93.000 (92.784)\n",
      "Test: [60/100]\tTime 0.074 (0.083)\tLoss 8.3886 (9.2664)\tPrec@1 61.000 (56.230)\tPrec@5 93.000 (92.836)\n",
      "Test: [70/100]\tTime 0.074 (0.081)\tLoss 8.2465 (9.2099)\tPrec@1 61.000 (56.465)\tPrec@5 95.000 (92.901)\n",
      "Test: [80/100]\tTime 0.074 (0.081)\tLoss 8.9982 (9.1741)\tPrec@1 55.000 (56.494)\tPrec@5 93.000 (93.099)\n",
      "Test: [90/100]\tTime 0.074 (0.080)\tLoss 9.3322 (9.1672)\tPrec@1 57.000 (56.670)\tPrec@5 97.000 (93.099)\n",
      "val Results: Prec@1 56.610 Prec@5 93.060 Loss 9.19723\n",
      "val Class Accuracy: [0.851,0.977,0.598,0.679,0.615,0.155,0.882,0.308,0.550,0.046]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [141][0/97], lr: 0.01000\tTime 0.877 (0.877)\tData 0.491 (0.491)\tLoss 2.4702 (2.4702)\tPrec@1 84.375 (84.375)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [141][10/97], lr: 0.01000\tTime 0.335 (0.425)\tData 0.000 (0.055)\tLoss 2.1762 (2.0286)\tPrec@1 87.500 (87.429)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [141][20/97], lr: 0.01000\tTime 0.321 (0.384)\tData 0.000 (0.037)\tLoss 2.4481 (2.0271)\tPrec@1 85.156 (87.686)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [141][30/97], lr: 0.01000\tTime 0.322 (0.368)\tData 0.000 (0.031)\tLoss 1.7538 (2.0316)\tPrec@1 87.500 (87.601)\tPrec@5 100.000 (99.370)\n",
      "Epoch: [141][40/97], lr: 0.01000\tTime 0.324 (0.359)\tData 0.000 (0.027)\tLoss 1.7935 (1.9447)\tPrec@1 89.844 (88.186)\tPrec@5 100.000 (99.390)\n",
      "Epoch: [141][50/97], lr: 0.01000\tTime 0.324 (0.354)\tData 0.000 (0.025)\tLoss 1.3421 (1.9397)\tPrec@1 92.188 (88.113)\tPrec@5 100.000 (99.403)\n",
      "Epoch: [141][60/97], lr: 0.01000\tTime 0.323 (0.351)\tData 0.000 (0.024)\tLoss 1.9686 (1.9448)\tPrec@1 87.500 (88.038)\tPrec@5 100.000 (99.411)\n",
      "Epoch: [141][70/97], lr: 0.01000\tTime 0.420 (0.362)\tData 0.001 (0.023)\tLoss 2.1681 (1.9340)\tPrec@1 86.719 (88.149)\tPrec@5 98.438 (99.406)\n",
      "Epoch: [141][80/97], lr: 0.01000\tTime 0.402 (0.371)\tData 0.001 (0.022)\tLoss 2.0148 (1.9400)\tPrec@1 85.938 (88.108)\tPrec@5 99.219 (99.373)\n",
      "Epoch: [141][90/97], lr: 0.01000\tTime 0.335 (0.373)\tData 0.000 (0.021)\tLoss 2.2227 (1.9650)\tPrec@1 86.719 (87.964)\tPrec@5 100.000 (99.365)\n",
      "Epoch: [141][96/97], lr: 0.01000\tTime 0.396 (0.374)\tData 0.000 (0.021)\tLoss 1.9581 (1.9690)\tPrec@1 87.288 (87.957)\tPrec@5 100.000 (99.363)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.514 (0.514)\tLoss 9.9934 (9.9934)\tPrec@1 51.000 (51.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.074 (0.114)\tLoss 6.9874 (8.5786)\tPrec@1 64.000 (58.818)\tPrec@5 95.000 (96.455)\n",
      "Test: [20/100]\tTime 0.074 (0.096)\tLoss 7.4419 (8.4885)\tPrec@1 61.000 (58.619)\tPrec@5 97.000 (96.381)\n",
      "Test: [30/100]\tTime 0.075 (0.089)\tLoss 8.0430 (8.4567)\tPrec@1 64.000 (58.774)\tPrec@5 97.000 (96.484)\n",
      "Test: [40/100]\tTime 0.097 (0.088)\tLoss 9.0560 (8.4300)\tPrec@1 58.000 (59.439)\tPrec@5 92.000 (96.122)\n",
      "Test: [50/100]\tTime 0.074 (0.086)\tLoss 8.1173 (8.3893)\tPrec@1 60.000 (59.373)\tPrec@5 93.000 (96.078)\n",
      "Test: [60/100]\tTime 0.076 (0.084)\tLoss 7.4118 (8.3746)\tPrec@1 59.000 (59.262)\tPrec@5 97.000 (96.213)\n",
      "Test: [70/100]\tTime 0.074 (0.083)\tLoss 8.4347 (8.3666)\tPrec@1 59.000 (59.254)\tPrec@5 96.000 (96.268)\n",
      "Test: [80/100]\tTime 0.074 (0.082)\tLoss 8.0826 (8.3412)\tPrec@1 62.000 (59.469)\tPrec@5 96.000 (96.407)\n",
      "Test: [90/100]\tTime 0.074 (0.081)\tLoss 7.9811 (8.4173)\tPrec@1 58.000 (59.088)\tPrec@5 98.000 (96.363)\n",
      "val Results: Prec@1 59.060 Prec@5 96.370 Loss 8.44122\n",
      "val Class Accuracy: [0.956,0.988,0.829,0.610,0.665,0.666,0.330,0.436,0.329,0.097]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [142][0/97], lr: 0.01000\tTime 0.899 (0.899)\tData 0.508 (0.508)\tLoss 1.9951 (1.9951)\tPrec@1 86.719 (86.719)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [142][10/97], lr: 0.01000\tTime 0.334 (0.427)\tData 0.000 (0.057)\tLoss 2.1871 (2.0339)\tPrec@1 87.500 (88.210)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [142][20/97], lr: 0.01000\tTime 0.381 (0.399)\tData 0.000 (0.037)\tLoss 1.8200 (2.0036)\tPrec@1 88.281 (88.058)\tPrec@5 99.219 (99.405)\n",
      "Epoch: [142][30/97], lr: 0.01000\tTime 0.340 (0.391)\tData 0.000 (0.030)\tLoss 1.9307 (2.0153)\tPrec@1 86.719 (87.928)\tPrec@5 100.000 (99.370)\n",
      "Epoch: [142][40/97], lr: 0.01000\tTime 0.324 (0.382)\tData 0.000 (0.027)\tLoss 2.7842 (2.0265)\tPrec@1 83.594 (87.786)\tPrec@5 99.219 (99.390)\n",
      "Epoch: [142][50/97], lr: 0.01000\tTime 0.327 (0.371)\tData 0.000 (0.025)\tLoss 0.9567 (1.9838)\tPrec@1 93.750 (87.898)\tPrec@5 100.000 (99.418)\n",
      "Epoch: [142][60/97], lr: 0.01000\tTime 0.339 (0.364)\tData 0.000 (0.024)\tLoss 2.0335 (1.9426)\tPrec@1 83.594 (88.153)\tPrec@5 100.000 (99.424)\n",
      "Epoch: [142][70/97], lr: 0.01000\tTime 0.333 (0.365)\tData 0.000 (0.023)\tLoss 1.9891 (1.9380)\tPrec@1 88.281 (88.204)\tPrec@5 100.000 (99.450)\n",
      "Epoch: [142][80/97], lr: 0.01000\tTime 0.335 (0.362)\tData 0.000 (0.022)\tLoss 1.7435 (1.9241)\tPrec@1 89.844 (88.349)\tPrec@5 98.438 (99.431)\n",
      "Epoch: [142][90/97], lr: 0.01000\tTime 0.345 (0.362)\tData 0.000 (0.021)\tLoss 2.4709 (1.9273)\tPrec@1 86.719 (88.384)\tPrec@5 99.219 (99.433)\n",
      "Epoch: [142][96/97], lr: 0.01000\tTime 0.314 (0.359)\tData 0.000 (0.022)\tLoss 2.3760 (1.9147)\tPrec@1 84.746 (88.393)\tPrec@5 100.000 (99.436)\n",
      "Gated Network Weight Gate= Flip:0.50, Sc:0.50\n",
      "Test: [0/100]\tTime 0.384 (0.384)\tLoss 8.4665 (8.4665)\tPrec@1 56.000 (56.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.073 (0.102)\tLoss 7.4624 (8.6955)\tPrec@1 63.000 (55.909)\tPrec@5 96.000 (93.000)\n",
      "Test: [20/100]\tTime 0.073 (0.088)\tLoss 7.8918 (8.7904)\tPrec@1 61.000 (56.190)\tPrec@5 91.000 (92.095)\n",
      "Test: [30/100]\tTime 0.073 (0.083)\tLoss 9.3197 (8.8835)\tPrec@1 54.000 (56.097)\tPrec@5 94.000 (92.419)\n",
      "Test: [40/100]\tTime 0.074 (0.081)\tLoss 8.7588 (8.8115)\tPrec@1 55.000 (56.707)\tPrec@5 94.000 (92.415)\n",
      "Test: [50/100]\tTime 0.074 (0.080)\tLoss 8.6129 (8.7431)\tPrec@1 61.000 (57.020)\tPrec@5 95.000 (92.608)\n",
      "Test: [60/100]\tTime 0.074 (0.079)\tLoss 8.2632 (8.7985)\tPrec@1 59.000 (56.459)\tPrec@5 93.000 (92.393)\n",
      "Test: [70/100]\tTime 0.074 (0.078)\tLoss 8.6259 (8.7909)\tPrec@1 55.000 (56.423)\tPrec@5 92.000 (92.366)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 8.5496 (8.7240)\tPrec@1 58.000 (56.815)\tPrec@5 93.000 (92.346)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 8.2534 (8.7642)\tPrec@1 60.000 (56.747)\tPrec@5 95.000 (92.374)\n",
      "val Results: Prec@1 56.650 Prec@5 92.450 Loss 8.78009\n",
      "val Class Accuracy: [0.936,0.988,0.570,0.342,0.398,0.328,0.669,0.460,0.530,0.444]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [143][0/97], lr: 0.01000\tTime 0.484 (0.484)\tData 0.259 (0.259)\tLoss 2.3601 (2.3601)\tPrec@1 84.375 (84.375)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [143][10/97], lr: 0.01000\tTime 0.324 (0.344)\tData 0.000 (0.038)\tLoss 2.2508 (1.9617)\tPrec@1 87.500 (87.926)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [143][20/97], lr: 0.01000\tTime 0.330 (0.337)\tData 0.000 (0.028)\tLoss 1.6372 (1.9506)\tPrec@1 88.281 (87.760)\tPrec@5 100.000 (99.368)\n",
      "Epoch: [143][30/97], lr: 0.01000\tTime 0.353 (0.336)\tData 0.000 (0.024)\tLoss 2.0851 (1.9237)\tPrec@1 87.500 (88.306)\tPrec@5 98.438 (99.370)\n",
      "Epoch: [143][40/97], lr: 0.01000\tTime 0.332 (0.341)\tData 0.000 (0.022)\tLoss 1.5822 (1.9311)\tPrec@1 91.406 (88.281)\tPrec@5 100.000 (99.352)\n",
      "Epoch: [143][50/97], lr: 0.01000\tTime 0.337 (0.341)\tData 0.000 (0.021)\tLoss 1.5730 (1.9027)\tPrec@1 90.625 (88.511)\tPrec@5 99.219 (99.234)\n",
      "Epoch: [143][60/97], lr: 0.01000\tTime 0.334 (0.341)\tData 0.000 (0.020)\tLoss 2.2659 (1.9275)\tPrec@1 85.156 (88.409)\tPrec@5 100.000 (99.180)\n",
      "Epoch: [143][70/97], lr: 0.01000\tTime 0.337 (0.343)\tData 0.000 (0.020)\tLoss 2.3153 (1.9199)\tPrec@1 83.594 (88.446)\tPrec@5 99.219 (99.186)\n",
      "Epoch: [143][80/97], lr: 0.01000\tTime 0.359 (0.344)\tData 0.000 (0.020)\tLoss 2.1527 (1.9301)\tPrec@1 89.062 (88.397)\tPrec@5 100.000 (99.209)\n",
      "Epoch: [143][90/97], lr: 0.01000\tTime 0.367 (0.344)\tData 0.000 (0.019)\tLoss 1.7551 (1.9481)\tPrec@1 89.844 (88.238)\tPrec@5 99.219 (99.193)\n",
      "Epoch: [143][96/97], lr: 0.01000\tTime 0.326 (0.345)\tData 0.000 (0.020)\tLoss 2.2936 (1.9498)\tPrec@1 83.051 (88.207)\tPrec@5 99.153 (99.202)\n",
      "Gated Network Weight Gate= Flip:0.54, Sc:0.46\n",
      "Test: [0/100]\tTime 0.375 (0.375)\tLoss 10.3980 (10.3980)\tPrec@1 51.000 (51.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.073 (0.101)\tLoss 7.9148 (9.0941)\tPrec@1 61.000 (56.818)\tPrec@5 93.000 (91.182)\n",
      "Test: [20/100]\tTime 0.074 (0.088)\tLoss 8.2260 (8.9772)\tPrec@1 61.000 (57.714)\tPrec@5 96.000 (91.905)\n",
      "Test: [30/100]\tTime 0.073 (0.083)\tLoss 8.9441 (9.0980)\tPrec@1 54.000 (56.839)\tPrec@5 90.000 (91.581)\n",
      "Test: [40/100]\tTime 0.073 (0.081)\tLoss 9.4193 (9.1638)\tPrec@1 58.000 (56.756)\tPrec@5 88.000 (91.244)\n",
      "Test: [50/100]\tTime 0.074 (0.080)\tLoss 8.2458 (9.0878)\tPrec@1 59.000 (57.078)\tPrec@5 94.000 (91.333)\n",
      "Test: [60/100]\tTime 0.074 (0.079)\tLoss 7.6310 (9.0323)\tPrec@1 64.000 (57.393)\tPrec@5 90.000 (91.475)\n",
      "Test: [70/100]\tTime 0.075 (0.078)\tLoss 9.4885 (9.0519)\tPrec@1 55.000 (57.254)\tPrec@5 91.000 (91.366)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 9.2921 (9.0412)\tPrec@1 57.000 (57.272)\tPrec@5 87.000 (91.321)\n",
      "Test: [90/100]\tTime 0.075 (0.077)\tLoss 8.7002 (9.0843)\tPrec@1 59.000 (57.066)\tPrec@5 94.000 (91.220)\n",
      "val Results: Prec@1 56.920 Prec@5 91.120 Loss 9.13258\n",
      "val Class Accuracy: [0.863,0.953,0.526,0.624,0.902,0.626,0.670,0.242,0.256,0.030]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [144][0/97], lr: 0.01000\tTime 1.191 (1.191)\tData 0.665 (0.665)\tLoss 2.5263 (2.5263)\tPrec@1 84.375 (84.375)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [144][10/97], lr: 0.01000\tTime 0.421 (0.543)\tData 0.000 (0.070)\tLoss 2.4857 (1.9954)\tPrec@1 85.938 (87.784)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [144][20/97], lr: 0.01000\tTime 0.398 (0.491)\tData 0.000 (0.043)\tLoss 1.3562 (1.9468)\tPrec@1 91.406 (88.356)\tPrec@5 100.000 (99.554)\n",
      "Epoch: [144][30/97], lr: 0.01000\tTime 0.431 (0.471)\tData 0.001 (0.034)\tLoss 1.8572 (1.8957)\tPrec@1 87.500 (88.684)\tPrec@5 100.000 (99.572)\n",
      "Epoch: [144][40/97], lr: 0.01000\tTime 0.422 (0.459)\tData 0.001 (0.031)\tLoss 1.6354 (1.9142)\tPrec@1 90.625 (88.434)\tPrec@5 99.219 (99.581)\n",
      "Epoch: [144][50/97], lr: 0.01000\tTime 0.331 (0.444)\tData 0.000 (0.027)\tLoss 1.5884 (1.9440)\tPrec@1 91.406 (88.174)\tPrec@5 98.438 (99.494)\n",
      "Epoch: [144][60/97], lr: 0.01000\tTime 0.370 (0.429)\tData 0.000 (0.026)\tLoss 1.8765 (1.9860)\tPrec@1 89.844 (87.974)\tPrec@5 99.219 (99.488)\n",
      "Epoch: [144][70/97], lr: 0.01000\tTime 0.322 (0.415)\tData 0.000 (0.024)\tLoss 1.9446 (1.9711)\tPrec@1 88.281 (88.006)\tPrec@5 99.219 (99.450)\n",
      "Epoch: [144][80/97], lr: 0.01000\tTime 0.321 (0.403)\tData 0.000 (0.023)\tLoss 1.8436 (1.9822)\tPrec@1 87.500 (87.905)\tPrec@5 100.000 (99.460)\n",
      "Epoch: [144][90/97], lr: 0.01000\tTime 0.374 (0.396)\tData 0.000 (0.023)\tLoss 1.6299 (1.9664)\tPrec@1 89.844 (87.989)\tPrec@5 100.000 (99.416)\n",
      "Epoch: [144][96/97], lr: 0.01000\tTime 0.354 (0.394)\tData 0.000 (0.023)\tLoss 2.4457 (1.9572)\tPrec@1 83.898 (88.038)\tPrec@5 97.458 (99.404)\n",
      "Gated Network Weight Gate= Flip:0.53, Sc:0.47\n",
      "Test: [0/100]\tTime 0.429 (0.429)\tLoss 9.4994 (9.4994)\tPrec@1 53.000 (53.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.076 (0.106)\tLoss 7.2689 (8.6251)\tPrec@1 63.000 (58.545)\tPrec@5 99.000 (96.273)\n",
      "Test: [20/100]\tTime 0.074 (0.091)\tLoss 7.7763 (8.7219)\tPrec@1 59.000 (57.667)\tPrec@5 96.000 (95.810)\n",
      "Test: [30/100]\tTime 0.074 (0.086)\tLoss 7.8065 (8.7238)\tPrec@1 65.000 (58.226)\tPrec@5 96.000 (95.742)\n",
      "Test: [40/100]\tTime 0.074 (0.083)\tLoss 8.8466 (8.6731)\tPrec@1 57.000 (58.195)\tPrec@5 95.000 (95.561)\n",
      "Test: [50/100]\tTime 0.075 (0.082)\tLoss 8.4637 (8.6039)\tPrec@1 60.000 (58.392)\tPrec@5 97.000 (95.824)\n",
      "Test: [60/100]\tTime 0.074 (0.081)\tLoss 7.0828 (8.5337)\tPrec@1 67.000 (58.492)\tPrec@5 96.000 (95.820)\n",
      "Test: [70/100]\tTime 0.074 (0.080)\tLoss 8.2460 (8.5569)\tPrec@1 61.000 (58.437)\tPrec@5 96.000 (95.887)\n",
      "Test: [80/100]\tTime 0.076 (0.080)\tLoss 8.1075 (8.4926)\tPrec@1 58.000 (58.790)\tPrec@5 98.000 (96.012)\n",
      "Test: [90/100]\tTime 0.075 (0.079)\tLoss 8.7374 (8.5740)\tPrec@1 56.000 (58.462)\tPrec@5 100.000 (95.934)\n",
      "val Results: Prec@1 58.380 Prec@5 95.840 Loss 8.60555\n",
      "val Class Accuracy: [0.971,0.957,0.825,0.229,0.661,0.575,0.687,0.559,0.181,0.193]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [145][0/97], lr: 0.01000\tTime 1.037 (1.037)\tData 0.632 (0.632)\tLoss 1.5029 (1.5029)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [145][10/97], lr: 0.01000\tTime 0.395 (0.493)\tData 0.000 (0.067)\tLoss 2.0387 (1.8305)\tPrec@1 89.062 (88.991)\tPrec@5 100.000 (99.787)\n",
      "Epoch: [145][20/97], lr: 0.01000\tTime 0.325 (0.429)\tData 0.001 (0.042)\tLoss 1.6187 (1.8015)\tPrec@1 89.062 (88.876)\tPrec@5 100.000 (99.777)\n",
      "Epoch: [145][30/97], lr: 0.01000\tTime 0.328 (0.409)\tData 0.000 (0.034)\tLoss 2.0650 (1.7717)\tPrec@1 86.719 (89.088)\tPrec@5 97.656 (99.597)\n",
      "Epoch: [145][40/97], lr: 0.01000\tTime 0.362 (0.400)\tData 0.000 (0.030)\tLoss 1.7684 (1.7881)\tPrec@1 90.625 (89.196)\tPrec@5 99.219 (99.505)\n",
      "Epoch: [145][50/97], lr: 0.01000\tTime 0.346 (0.396)\tData 0.000 (0.027)\tLoss 2.6693 (1.8620)\tPrec@1 85.938 (88.894)\tPrec@5 99.219 (99.418)\n",
      "Epoch: [145][60/97], lr: 0.01000\tTime 0.331 (0.388)\tData 0.000 (0.025)\tLoss 3.1579 (1.9202)\tPrec@1 80.469 (88.448)\tPrec@5 96.875 (99.308)\n",
      "Epoch: [145][70/97], lr: 0.01000\tTime 0.342 (0.382)\tData 0.000 (0.024)\tLoss 1.3706 (1.9534)\tPrec@1 92.188 (88.138)\tPrec@5 100.000 (99.274)\n",
      "Epoch: [145][80/97], lr: 0.01000\tTime 0.340 (0.380)\tData 0.000 (0.023)\tLoss 1.9332 (1.9446)\tPrec@1 89.844 (88.214)\tPrec@5 100.000 (99.315)\n",
      "Epoch: [145][90/97], lr: 0.01000\tTime 0.344 (0.377)\tData 0.000 (0.022)\tLoss 1.3244 (1.9489)\tPrec@1 90.625 (88.161)\tPrec@5 100.000 (99.287)\n",
      "Epoch: [145][96/97], lr: 0.01000\tTime 0.350 (0.376)\tData 0.000 (0.023)\tLoss 2.8290 (1.9606)\tPrec@1 83.898 (88.143)\tPrec@5 99.153 (99.258)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.377 (0.377)\tLoss 10.5721 (10.5721)\tPrec@1 48.000 (48.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.074 (0.101)\tLoss 6.6296 (8.9946)\tPrec@1 67.000 (57.182)\tPrec@5 98.000 (94.727)\n",
      "Test: [20/100]\tTime 0.074 (0.088)\tLoss 7.7074 (8.8855)\tPrec@1 61.000 (57.143)\tPrec@5 97.000 (94.667)\n",
      "Test: [30/100]\tTime 0.073 (0.084)\tLoss 8.0265 (8.9218)\tPrec@1 58.000 (56.839)\tPrec@5 93.000 (94.226)\n",
      "Test: [40/100]\tTime 0.083 (0.082)\tLoss 9.8003 (8.8890)\tPrec@1 50.000 (57.317)\tPrec@5 90.000 (93.756)\n",
      "Test: [50/100]\tTime 0.075 (0.080)\tLoss 8.2712 (8.7955)\tPrec@1 64.000 (57.863)\tPrec@5 96.000 (93.745)\n",
      "Test: [60/100]\tTime 0.074 (0.079)\tLoss 7.6338 (8.7687)\tPrec@1 61.000 (57.869)\tPrec@5 94.000 (93.721)\n",
      "Test: [70/100]\tTime 0.074 (0.079)\tLoss 8.9852 (8.7843)\tPrec@1 54.000 (57.761)\tPrec@5 95.000 (93.775)\n",
      "Test: [80/100]\tTime 0.075 (0.079)\tLoss 8.8170 (8.7280)\tPrec@1 58.000 (58.062)\tPrec@5 91.000 (93.790)\n",
      "Test: [90/100]\tTime 0.075 (0.079)\tLoss 8.4531 (8.7816)\tPrec@1 62.000 (57.791)\tPrec@5 95.000 (93.670)\n",
      "val Results: Prec@1 57.800 Prec@5 93.710 Loss 8.79600\n",
      "val Class Accuracy: [0.972,0.986,0.630,0.707,0.505,0.665,0.751,0.307,0.098,0.159]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [146][0/97], lr: 0.01000\tTime 1.213 (1.213)\tData 0.698 (0.698)\tLoss 1.8433 (1.8433)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [146][10/97], lr: 0.01000\tTime 0.329 (0.483)\tData 0.000 (0.075)\tLoss 1.9373 (1.9438)\tPrec@1 89.062 (87.997)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [146][20/97], lr: 0.01000\tTime 0.327 (0.419)\tData 0.000 (0.047)\tLoss 1.5966 (1.9294)\tPrec@1 91.406 (87.984)\tPrec@5 99.219 (99.442)\n",
      "Epoch: [146][30/97], lr: 0.01000\tTime 0.335 (0.394)\tData 0.000 (0.037)\tLoss 1.3905 (1.8948)\tPrec@1 92.188 (88.533)\tPrec@5 99.219 (99.471)\n",
      "Epoch: [146][40/97], lr: 0.01000\tTime 0.341 (0.388)\tData 0.000 (0.032)\tLoss 1.4313 (1.9315)\tPrec@1 91.406 (88.110)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [146][50/97], lr: 0.01000\tTime 0.372 (0.382)\tData 0.000 (0.029)\tLoss 1.6794 (1.9481)\tPrec@1 91.406 (88.128)\tPrec@5 99.219 (99.403)\n",
      "Epoch: [146][60/97], lr: 0.01000\tTime 0.363 (0.380)\tData 0.000 (0.027)\tLoss 1.0245 (1.9379)\tPrec@1 95.312 (88.153)\tPrec@5 100.000 (99.449)\n",
      "Epoch: [146][70/97], lr: 0.01000\tTime 0.387 (0.380)\tData 0.000 (0.025)\tLoss 1.2731 (1.9623)\tPrec@1 93.750 (88.083)\tPrec@5 99.219 (99.373)\n",
      "Epoch: [146][80/97], lr: 0.01000\tTime 0.337 (0.383)\tData 0.000 (0.024)\tLoss 1.1164 (1.9481)\tPrec@1 92.969 (88.233)\tPrec@5 100.000 (99.373)\n",
      "Epoch: [146][90/97], lr: 0.01000\tTime 0.344 (0.382)\tData 0.000 (0.023)\tLoss 2.3478 (1.9480)\tPrec@1 87.500 (88.264)\tPrec@5 100.000 (99.373)\n",
      "Epoch: [146][96/97], lr: 0.01000\tTime 0.348 (0.382)\tData 0.000 (0.023)\tLoss 1.7576 (1.9377)\tPrec@1 90.678 (88.312)\tPrec@5 99.153 (99.331)\n",
      "Gated Network Weight Gate= Flip:0.50, Sc:0.50\n",
      "Test: [0/100]\tTime 0.419 (0.419)\tLoss 9.4547 (9.4547)\tPrec@1 54.000 (54.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.105)\tLoss 6.8695 (8.5536)\tPrec@1 68.000 (59.364)\tPrec@5 95.000 (94.455)\n",
      "Test: [20/100]\tTime 0.073 (0.090)\tLoss 7.1913 (8.5801)\tPrec@1 64.000 (59.333)\tPrec@5 96.000 (94.857)\n",
      "Test: [30/100]\tTime 0.074 (0.085)\tLoss 7.8095 (8.5559)\tPrec@1 62.000 (59.226)\tPrec@5 94.000 (95.032)\n",
      "Test: [40/100]\tTime 0.074 (0.083)\tLoss 9.4009 (8.6204)\tPrec@1 51.000 (59.098)\tPrec@5 92.000 (94.707)\n",
      "Test: [50/100]\tTime 0.074 (0.082)\tLoss 9.0894 (8.5980)\tPrec@1 56.000 (59.196)\tPrec@5 95.000 (94.647)\n",
      "Test: [60/100]\tTime 0.074 (0.082)\tLoss 7.3986 (8.5950)\tPrec@1 64.000 (59.082)\tPrec@5 96.000 (94.672)\n",
      "Test: [70/100]\tTime 0.074 (0.081)\tLoss 8.7072 (8.5768)\tPrec@1 57.000 (59.099)\tPrec@5 98.000 (94.732)\n",
      "Test: [80/100]\tTime 0.074 (0.080)\tLoss 8.1627 (8.5580)\tPrec@1 62.000 (59.235)\tPrec@5 96.000 (94.926)\n",
      "Test: [90/100]\tTime 0.074 (0.080)\tLoss 8.0079 (8.5798)\tPrec@1 65.000 (59.253)\tPrec@5 98.000 (94.945)\n",
      "val Results: Prec@1 59.190 Prec@5 94.880 Loss 8.61646\n",
      "val Class Accuracy: [0.825,0.983,0.900,0.606,0.492,0.650,0.473,0.333,0.595,0.062]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [147][0/97], lr: 0.01000\tTime 0.481 (0.481)\tData 0.270 (0.270)\tLoss 1.4324 (1.4324)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [147][10/97], lr: 0.01000\tTime 0.324 (0.344)\tData 0.000 (0.038)\tLoss 1.4901 (1.9942)\tPrec@1 89.844 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [147][20/97], lr: 0.01000\tTime 0.322 (0.333)\tData 0.000 (0.028)\tLoss 2.1873 (2.0467)\tPrec@1 86.719 (87.612)\tPrec@5 97.656 (98.996)\n",
      "Epoch: [147][30/97], lr: 0.01000\tTime 0.318 (0.330)\tData 0.000 (0.025)\tLoss 1.2755 (2.0281)\tPrec@1 91.406 (87.676)\tPrec@5 100.000 (99.194)\n",
      "Epoch: [147][40/97], lr: 0.01000\tTime 0.324 (0.329)\tData 0.000 (0.023)\tLoss 1.4257 (2.0221)\tPrec@1 93.750 (87.748)\tPrec@5 99.219 (99.200)\n",
      "Epoch: [147][50/97], lr: 0.01000\tTime 0.319 (0.328)\tData 0.000 (0.022)\tLoss 1.0732 (1.9614)\tPrec@1 92.969 (88.067)\tPrec@5 99.219 (99.249)\n",
      "Epoch: [147][60/97], lr: 0.01000\tTime 0.322 (0.327)\tData 0.000 (0.021)\tLoss 2.6086 (1.9603)\tPrec@1 84.375 (88.051)\tPrec@5 100.000 (99.283)\n",
      "Epoch: [147][70/97], lr: 0.01000\tTime 0.325 (0.327)\tData 0.000 (0.020)\tLoss 1.9773 (1.9223)\tPrec@1 86.719 (88.303)\tPrec@5 100.000 (99.373)\n",
      "Epoch: [147][80/97], lr: 0.01000\tTime 0.323 (0.327)\tData 0.000 (0.020)\tLoss 1.9031 (1.9030)\tPrec@1 87.500 (88.329)\tPrec@5 99.219 (99.402)\n",
      "Epoch: [147][90/97], lr: 0.01000\tTime 0.317 (0.326)\tData 0.000 (0.020)\tLoss 1.5363 (1.9350)\tPrec@1 89.062 (88.101)\tPrec@5 100.000 (99.399)\n",
      "Epoch: [147][96/97], lr: 0.01000\tTime 0.310 (0.326)\tData 0.000 (0.020)\tLoss 2.2071 (1.9369)\tPrec@1 88.983 (88.046)\tPrec@5 100.000 (99.404)\n",
      "Gated Network Weight Gate= Flip:0.52, Sc:0.48\n",
      "Test: [0/100]\tTime 0.287 (0.287)\tLoss 10.4427 (10.4427)\tPrec@1 52.000 (52.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 7.9967 (9.3383)\tPrec@1 62.000 (54.091)\tPrec@5 93.000 (94.545)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 8.3146 (9.4015)\tPrec@1 57.000 (54.000)\tPrec@5 95.000 (93.857)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 7.8926 (9.4300)\tPrec@1 60.000 (53.871)\tPrec@5 94.000 (93.935)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 9.1414 (9.3814)\tPrec@1 56.000 (54.146)\tPrec@5 94.000 (94.024)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 8.8814 (9.3106)\tPrec@1 58.000 (54.294)\tPrec@5 94.000 (94.333)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.6258 (9.3064)\tPrec@1 55.000 (54.164)\tPrec@5 95.000 (94.262)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 8.6279 (9.2946)\tPrec@1 58.000 (54.127)\tPrec@5 95.000 (94.521)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 8.8357 (9.2463)\tPrec@1 58.000 (54.395)\tPrec@5 95.000 (94.580)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 9.8439 (9.3081)\tPrec@1 51.000 (54.143)\tPrec@5 97.000 (94.582)\n",
      "val Results: Prec@1 54.100 Prec@5 94.620 Loss 9.31859\n",
      "val Class Accuracy: [0.991,0.954,0.573,0.644,0.443,0.418,0.419,0.481,0.328,0.159]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [148][0/97], lr: 0.01000\tTime 0.496 (0.496)\tData 0.279 (0.279)\tLoss 2.3109 (2.3109)\tPrec@1 84.375 (84.375)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [148][10/97], lr: 0.01000\tTime 0.326 (0.349)\tData 0.000 (0.039)\tLoss 2.2061 (1.9652)\tPrec@1 85.156 (87.784)\tPrec@5 97.656 (99.290)\n",
      "Epoch: [148][20/97], lr: 0.01000\tTime 0.324 (0.337)\tData 0.000 (0.029)\tLoss 1.9902 (1.9274)\tPrec@1 87.500 (87.909)\tPrec@5 99.219 (99.405)\n",
      "Epoch: [148][30/97], lr: 0.01000\tTime 0.322 (0.333)\tData 0.000 (0.025)\tLoss 1.8475 (1.9702)\tPrec@1 86.719 (87.702)\tPrec@5 99.219 (99.420)\n",
      "Epoch: [148][40/97], lr: 0.01000\tTime 0.320 (0.331)\tData 0.000 (0.023)\tLoss 2.1377 (1.9933)\tPrec@1 85.938 (87.576)\tPrec@5 98.438 (99.466)\n",
      "Epoch: [148][50/97], lr: 0.01000\tTime 0.322 (0.329)\tData 0.000 (0.022)\tLoss 2.6247 (1.9929)\tPrec@1 82.812 (87.546)\tPrec@5 97.656 (99.372)\n",
      "Epoch: [148][60/97], lr: 0.01000\tTime 0.325 (0.329)\tData 0.000 (0.021)\tLoss 1.4496 (2.0021)\tPrec@1 90.625 (87.308)\tPrec@5 100.000 (99.308)\n",
      "Epoch: [148][70/97], lr: 0.01000\tTime 0.324 (0.328)\tData 0.000 (0.020)\tLoss 2.1268 (1.9999)\tPrec@1 86.719 (87.357)\tPrec@5 99.219 (99.307)\n",
      "Epoch: [148][80/97], lr: 0.01000\tTime 0.323 (0.328)\tData 0.000 (0.020)\tLoss 2.5814 (1.9931)\tPrec@1 85.938 (87.510)\tPrec@5 100.000 (99.354)\n",
      "Epoch: [148][90/97], lr: 0.01000\tTime 0.322 (0.327)\tData 0.000 (0.020)\tLoss 1.8388 (1.9900)\tPrec@1 89.062 (87.491)\tPrec@5 100.000 (99.382)\n",
      "Epoch: [148][96/97], lr: 0.01000\tTime 0.308 (0.327)\tData 0.000 (0.020)\tLoss 1.7138 (1.9868)\tPrec@1 88.983 (87.554)\tPrec@5 100.000 (99.387)\n",
      "Gated Network Weight Gate= Flip:0.50, Sc:0.50\n",
      "Test: [0/100]\tTime 0.282 (0.282)\tLoss 11.1844 (11.1844)\tPrec@1 46.000 (46.000)\tPrec@5 89.000 (89.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 9.2282 (9.7696)\tPrec@1 56.000 (52.182)\tPrec@5 88.000 (90.364)\n",
      "Test: [20/100]\tTime 0.074 (0.083)\tLoss 8.5719 (9.6423)\tPrec@1 57.000 (52.714)\tPrec@5 94.000 (91.000)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 8.7020 (9.5753)\tPrec@1 57.000 (53.290)\tPrec@5 90.000 (90.710)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 9.9412 (9.5748)\tPrec@1 52.000 (53.195)\tPrec@5 91.000 (90.854)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 9.0675 (9.4949)\tPrec@1 54.000 (53.373)\tPrec@5 91.000 (91.078)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.4881 (9.5165)\tPrec@1 59.000 (53.279)\tPrec@5 92.000 (90.869)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 8.9261 (9.5004)\tPrec@1 56.000 (53.408)\tPrec@5 93.000 (90.944)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 8.7689 (9.4508)\tPrec@1 57.000 (53.605)\tPrec@5 92.000 (91.037)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 10.2063 (9.5667)\tPrec@1 51.000 (53.033)\tPrec@5 95.000 (90.879)\n",
      "val Results: Prec@1 52.890 Prec@5 90.850 Loss 9.61313\n",
      "val Class Accuracy: [0.977,0.990,0.640,0.691,0.699,0.367,0.082,0.535,0.126,0.182]\n",
      "Best Prec@1: 66.250\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [149][0/97], lr: 0.01000\tTime 0.514 (0.514)\tData 0.304 (0.304)\tLoss 2.3701 (2.3701)\tPrec@1 85.156 (85.156)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [149][10/97], lr: 0.01000\tTime 0.324 (0.352)\tData 0.000 (0.042)\tLoss 1.8491 (1.9706)\tPrec@1 89.844 (88.068)\tPrec@5 100.000 (99.077)\n",
      "Epoch: [149][20/97], lr: 0.01000\tTime 0.362 (0.344)\tData 0.000 (0.030)\tLoss 2.0555 (1.9087)\tPrec@1 87.500 (88.616)\tPrec@5 96.875 (99.107)\n",
      "Epoch: [149][30/97], lr: 0.01000\tTime 0.326 (0.343)\tData 0.000 (0.026)\tLoss 1.8939 (1.9135)\tPrec@1 89.062 (88.306)\tPrec@5 100.000 (99.269)\n",
      "Epoch: [149][40/97], lr: 0.01000\tTime 0.321 (0.340)\tData 0.000 (0.023)\tLoss 1.9740 (1.8705)\tPrec@1 87.500 (88.529)\tPrec@5 98.438 (99.333)\n",
      "Epoch: [149][50/97], lr: 0.01000\tTime 0.323 (0.337)\tData 0.000 (0.022)\tLoss 1.9524 (1.8521)\tPrec@1 88.281 (88.664)\tPrec@5 100.000 (99.418)\n",
      "Epoch: [149][60/97], lr: 0.01000\tTime 0.325 (0.335)\tData 0.000 (0.021)\tLoss 2.4164 (1.9041)\tPrec@1 82.812 (88.268)\tPrec@5 100.000 (99.411)\n",
      "Epoch: [149][70/97], lr: 0.01000\tTime 0.328 (0.334)\tData 0.000 (0.021)\tLoss 1.3462 (1.9088)\tPrec@1 91.406 (88.138)\tPrec@5 100.000 (99.406)\n",
      "Epoch: [149][80/97], lr: 0.01000\tTime 0.325 (0.333)\tData 0.000 (0.020)\tLoss 1.7868 (1.9220)\tPrec@1 89.844 (88.127)\tPrec@5 98.438 (99.402)\n",
      "Epoch: [149][90/97], lr: 0.01000\tTime 0.318 (0.332)\tData 0.000 (0.020)\tLoss 1.8898 (1.9096)\tPrec@1 89.844 (88.324)\tPrec@5 98.438 (99.399)\n",
      "Epoch: [149][96/97], lr: 0.01000\tTime 0.318 (0.331)\tData 0.000 (0.020)\tLoss 1.7337 (1.9220)\tPrec@1 90.678 (88.296)\tPrec@5 100.000 (99.379)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.263 (0.263)\tLoss 7.3974 (7.3974)\tPrec@1 67.000 (67.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.074 (0.091)\tLoss 5.2655 (6.6685)\tPrec@1 75.000 (69.091)\tPrec@5 99.000 (96.545)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.0641 (6.4644)\tPrec@1 71.000 (69.667)\tPrec@5 98.000 (97.000)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.1281 (6.4158)\tPrec@1 73.000 (69.903)\tPrec@5 94.000 (97.065)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 6.4220 (6.3989)\tPrec@1 71.000 (70.000)\tPrec@5 94.000 (96.683)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 7.4702 (6.3916)\tPrec@1 62.000 (70.137)\tPrec@5 95.000 (96.706)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 6.2839 (6.4141)\tPrec@1 68.000 (69.967)\tPrec@5 96.000 (96.590)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 6.6842 (6.4498)\tPrec@1 67.000 (69.831)\tPrec@5 95.000 (96.577)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.0972 (6.4019)\tPrec@1 69.000 (70.049)\tPrec@5 96.000 (96.667)\n",
      "Test: [90/100]\tTime 0.074 (0.075)\tLoss 6.1374 (6.5000)\tPrec@1 73.000 (69.692)\tPrec@5 98.000 (96.593)\n",
      "val Results: Prec@1 69.540 Prec@5 96.640 Loss 6.52889\n",
      "val Class Accuracy: [0.869,0.973,0.818,0.656,0.832,0.712,0.505,0.578,0.499,0.512]\n",
      "Best Prec@1: 69.540\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [150][0/97], lr: 0.01000\tTime 0.481 (0.481)\tData 0.256 (0.256)\tLoss 2.5369 (2.5369)\tPrec@1 82.812 (82.812)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [150][10/97], lr: 0.01000\tTime 0.325 (0.342)\tData 0.000 (0.037)\tLoss 2.1477 (1.9881)\tPrec@1 87.500 (87.713)\tPrec@5 98.438 (99.503)\n",
      "Epoch: [150][20/97], lr: 0.01000\tTime 0.323 (0.334)\tData 0.000 (0.028)\tLoss 1.3853 (1.8690)\tPrec@1 92.969 (88.616)\tPrec@5 99.219 (99.405)\n",
      "Epoch: [150][30/97], lr: 0.01000\tTime 0.322 (0.331)\tData 0.000 (0.024)\tLoss 1.3931 (1.8029)\tPrec@1 90.625 (88.836)\tPrec@5 100.000 (99.446)\n",
      "Epoch: [150][40/97], lr: 0.01000\tTime 0.330 (0.330)\tData 0.000 (0.022)\tLoss 2.4944 (1.8500)\tPrec@1 86.719 (88.624)\tPrec@5 98.438 (99.390)\n",
      "Epoch: [150][50/97], lr: 0.01000\tTime 0.324 (0.329)\tData 0.000 (0.021)\tLoss 1.6492 (1.9114)\tPrec@1 92.969 (88.205)\tPrec@5 98.438 (99.357)\n",
      "Epoch: [150][60/97], lr: 0.01000\tTime 0.322 (0.329)\tData 0.000 (0.021)\tLoss 2.1013 (1.8990)\tPrec@1 85.938 (88.243)\tPrec@5 99.219 (99.372)\n",
      "Epoch: [150][70/97], lr: 0.01000\tTime 0.320 (0.328)\tData 0.000 (0.020)\tLoss 2.2011 (1.9256)\tPrec@1 87.500 (88.116)\tPrec@5 100.000 (99.362)\n",
      "Epoch: [150][80/97], lr: 0.01000\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 1.4869 (1.9122)\tPrec@1 90.625 (88.252)\tPrec@5 99.219 (99.363)\n",
      "Epoch: [150][90/97], lr: 0.01000\tTime 0.319 (0.327)\tData 0.000 (0.019)\tLoss 2.5519 (1.9273)\tPrec@1 86.719 (88.187)\tPrec@5 98.438 (99.373)\n",
      "Epoch: [150][96/97], lr: 0.01000\tTime 0.314 (0.327)\tData 0.000 (0.020)\tLoss 1.8458 (1.9190)\tPrec@1 90.678 (88.312)\tPrec@5 98.305 (99.355)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.287 (0.287)\tLoss 8.4568 (8.4568)\tPrec@1 60.000 (60.000)\tPrec@5 97.000 (97.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 6.8132 (7.8378)\tPrec@1 67.000 (62.818)\tPrec@5 93.000 (95.182)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.0970 (7.8593)\tPrec@1 71.000 (62.190)\tPrec@5 99.000 (95.095)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.6928 (7.8511)\tPrec@1 72.000 (62.194)\tPrec@5 95.000 (95.000)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 8.0975 (7.8723)\tPrec@1 62.000 (62.171)\tPrec@5 94.000 (94.805)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 7.4719 (7.8409)\tPrec@1 64.000 (62.392)\tPrec@5 96.000 (94.902)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 5.9786 (7.7781)\tPrec@1 71.000 (62.689)\tPrec@5 98.000 (94.902)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 7.6318 (7.8047)\tPrec@1 63.000 (62.676)\tPrec@5 99.000 (94.732)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 8.3908 (7.7332)\tPrec@1 60.000 (63.086)\tPrec@5 89.000 (94.864)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 7.8384 (7.7881)\tPrec@1 64.000 (62.846)\tPrec@5 97.000 (94.824)\n",
      "val Results: Prec@1 62.730 Prec@5 94.870 Loss 7.80308\n",
      "val Class Accuracy: [0.974,0.968,0.789,0.538,0.468,0.784,0.568,0.621,0.295,0.268]\n",
      "Best Prec@1: 69.540\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [151][0/97], lr: 0.01000\tTime 0.414 (0.414)\tData 0.217 (0.217)\tLoss 1.4306 (1.4306)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [151][10/97], lr: 0.01000\tTime 0.321 (0.339)\tData 0.000 (0.034)\tLoss 1.8355 (1.7772)\tPrec@1 86.719 (88.991)\tPrec@5 99.219 (99.503)\n",
      "Epoch: [151][20/97], lr: 0.01000\tTime 0.325 (0.331)\tData 0.000 (0.026)\tLoss 1.6934 (1.8024)\tPrec@1 92.188 (88.988)\tPrec@5 98.438 (99.330)\n",
      "Epoch: [151][30/97], lr: 0.01000\tTime 0.322 (0.328)\tData 0.000 (0.023)\tLoss 2.2345 (1.8721)\tPrec@1 90.625 (88.634)\tPrec@5 98.438 (99.370)\n",
      "Epoch: [151][40/97], lr: 0.01000\tTime 0.323 (0.328)\tData 0.000 (0.022)\tLoss 2.1912 (1.8520)\tPrec@1 85.938 (88.834)\tPrec@5 100.000 (99.409)\n",
      "Epoch: [151][50/97], lr: 0.01000\tTime 0.324 (0.327)\tData 0.000 (0.021)\tLoss 1.6204 (1.8524)\tPrec@1 89.844 (88.771)\tPrec@5 100.000 (99.403)\n",
      "Epoch: [151][60/97], lr: 0.01000\tTime 0.322 (0.327)\tData 0.000 (0.020)\tLoss 1.7495 (1.8713)\tPrec@1 88.281 (88.601)\tPrec@5 100.000 (99.372)\n",
      "Epoch: [151][70/97], lr: 0.01000\tTime 0.321 (0.326)\tData 0.000 (0.020)\tLoss 2.0281 (1.8944)\tPrec@1 89.844 (88.567)\tPrec@5 99.219 (99.329)\n",
      "Epoch: [151][80/97], lr: 0.01000\tTime 0.326 (0.326)\tData 0.000 (0.019)\tLoss 1.8365 (1.9034)\tPrec@1 88.281 (88.368)\tPrec@5 98.438 (99.373)\n",
      "Epoch: [151][90/97], lr: 0.01000\tTime 0.318 (0.326)\tData 0.000 (0.019)\tLoss 1.9377 (1.9060)\tPrec@1 88.281 (88.376)\tPrec@5 98.438 (99.390)\n",
      "Epoch: [151][96/97], lr: 0.01000\tTime 0.314 (0.325)\tData 0.000 (0.020)\tLoss 1.3687 (1.9166)\tPrec@1 93.220 (88.336)\tPrec@5 100.000 (99.404)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.289 (0.289)\tLoss 8.4433 (8.4433)\tPrec@1 60.000 (60.000)\tPrec@5 92.000 (92.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 6.0635 (7.3473)\tPrec@1 70.000 (65.727)\tPrec@5 98.000 (95.273)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 5.9743 (7.3393)\tPrec@1 69.000 (64.810)\tPrec@5 97.000 (95.476)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 6.8233 (7.4352)\tPrec@1 66.000 (64.323)\tPrec@5 95.000 (95.548)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 7.4767 (7.4637)\tPrec@1 65.000 (64.268)\tPrec@5 93.000 (95.293)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 6.6766 (7.3586)\tPrec@1 68.000 (64.941)\tPrec@5 94.000 (95.490)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 6.5110 (7.3465)\tPrec@1 68.000 (65.098)\tPrec@5 95.000 (95.361)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 8.3047 (7.3822)\tPrec@1 55.000 (64.873)\tPrec@5 97.000 (95.437)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 7.4956 (7.3569)\tPrec@1 67.000 (64.926)\tPrec@5 96.000 (95.494)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 7.1627 (7.4214)\tPrec@1 64.000 (64.560)\tPrec@5 97.000 (95.484)\n",
      "val Results: Prec@1 64.480 Prec@5 95.490 Loss 7.44240\n",
      "val Class Accuracy: [0.994,0.909,0.604,0.644,0.754,0.707,0.642,0.308,0.476,0.410]\n",
      "Best Prec@1: 69.540\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [152][0/97], lr: 0.01000\tTime 0.482 (0.482)\tData 0.278 (0.278)\tLoss 1.2506 (1.2506)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [152][10/97], lr: 0.01000\tTime 0.323 (0.344)\tData 0.000 (0.040)\tLoss 2.0152 (1.7326)\tPrec@1 89.062 (89.134)\tPrec@5 100.000 (99.716)\n",
      "Epoch: [152][20/97], lr: 0.01000\tTime 0.320 (0.335)\tData 0.000 (0.029)\tLoss 1.7011 (1.7377)\tPrec@1 88.281 (89.211)\tPrec@5 99.219 (99.591)\n",
      "Epoch: [152][30/97], lr: 0.01000\tTime 0.322 (0.332)\tData 0.000 (0.025)\tLoss 1.8722 (1.8336)\tPrec@1 89.062 (88.609)\tPrec@5 99.219 (99.521)\n",
      "Epoch: [152][40/97], lr: 0.01000\tTime 0.322 (0.330)\tData 0.000 (0.023)\tLoss 1.8924 (1.8808)\tPrec@1 88.281 (88.224)\tPrec@5 99.219 (99.409)\n",
      "Epoch: [152][50/97], lr: 0.01000\tTime 0.321 (0.328)\tData 0.000 (0.022)\tLoss 1.5535 (1.8990)\tPrec@1 92.188 (88.266)\tPrec@5 98.438 (99.449)\n",
      "Epoch: [152][60/97], lr: 0.01000\tTime 0.324 (0.328)\tData 0.000 (0.021)\tLoss 2.2551 (1.8958)\tPrec@1 86.719 (88.294)\tPrec@5 100.000 (99.475)\n",
      "Epoch: [152][70/97], lr: 0.01000\tTime 0.324 (0.327)\tData 0.000 (0.021)\tLoss 2.5266 (1.9267)\tPrec@1 85.938 (88.149)\tPrec@5 99.219 (99.406)\n",
      "Epoch: [152][80/97], lr: 0.01000\tTime 0.327 (0.327)\tData 0.000 (0.020)\tLoss 1.8693 (1.9526)\tPrec@1 88.281 (88.040)\tPrec@5 99.219 (99.354)\n",
      "Epoch: [152][90/97], lr: 0.01000\tTime 0.323 (0.327)\tData 0.000 (0.020)\tLoss 2.1004 (1.9584)\tPrec@1 85.938 (87.998)\tPrec@5 99.219 (99.322)\n",
      "Epoch: [152][96/97], lr: 0.01000\tTime 0.311 (0.327)\tData 0.000 (0.020)\tLoss 1.5439 (1.9499)\tPrec@1 89.831 (88.030)\tPrec@5 99.153 (99.331)\n",
      "Gated Network Weight Gate= Flip:0.53, Sc:0.47\n",
      "Test: [0/100]\tTime 0.260 (0.260)\tLoss 6.2514 (6.2514)\tPrec@1 73.000 (73.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 4.8467 (6.3067)\tPrec@1 75.000 (69.818)\tPrec@5 100.000 (98.091)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.0773 (6.3709)\tPrec@1 71.000 (69.095)\tPrec@5 97.000 (97.667)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.4516 (6.4482)\tPrec@1 70.000 (68.742)\tPrec@5 96.000 (97.677)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 5.8606 (6.4640)\tPrec@1 70.000 (69.073)\tPrec@5 99.000 (97.659)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.0398 (6.4167)\tPrec@1 72.000 (69.353)\tPrec@5 96.000 (97.549)\n",
      "Test: [60/100]\tTime 0.074 (0.076)\tLoss 6.2964 (6.4490)\tPrec@1 69.000 (69.164)\tPrec@5 99.000 (97.574)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 6.6597 (6.4357)\tPrec@1 68.000 (69.211)\tPrec@5 100.000 (97.648)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.2795 (6.3976)\tPrec@1 70.000 (69.432)\tPrec@5 97.000 (97.790)\n",
      "Test: [90/100]\tTime 0.074 (0.075)\tLoss 6.4135 (6.4304)\tPrec@1 68.000 (69.352)\tPrec@5 100.000 (97.769)\n",
      "val Results: Prec@1 69.210 Prec@5 97.780 Loss 6.46857\n",
      "val Class Accuracy: [0.908,0.988,0.802,0.605,0.725,0.578,0.864,0.515,0.606,0.330]\n",
      "Best Prec@1: 69.540\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [153][0/97], lr: 0.01000\tTime 0.467 (0.467)\tData 0.257 (0.257)\tLoss 1.4688 (1.4688)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [153][10/97], lr: 0.01000\tTime 0.323 (0.341)\tData 0.000 (0.037)\tLoss 2.7962 (1.8830)\tPrec@1 85.938 (88.707)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [153][20/97], lr: 0.01000\tTime 0.322 (0.333)\tData 0.000 (0.028)\tLoss 1.4643 (1.7247)\tPrec@1 91.406 (89.546)\tPrec@5 100.000 (99.554)\n",
      "Epoch: [153][30/97], lr: 0.01000\tTime 0.321 (0.331)\tData 0.000 (0.024)\tLoss 1.9332 (1.7714)\tPrec@1 88.281 (89.163)\tPrec@5 99.219 (99.572)\n",
      "Epoch: [153][40/97], lr: 0.01000\tTime 0.324 (0.330)\tData 0.000 (0.022)\tLoss 2.6956 (1.8004)\tPrec@1 84.375 (89.234)\tPrec@5 99.219 (99.486)\n",
      "Epoch: [153][50/97], lr: 0.01000\tTime 0.321 (0.329)\tData 0.000 (0.021)\tLoss 1.2571 (1.8224)\tPrec@1 92.969 (89.124)\tPrec@5 98.438 (99.479)\n",
      "Epoch: [153][60/97], lr: 0.01000\tTime 0.320 (0.328)\tData 0.000 (0.021)\tLoss 2.5475 (1.8962)\tPrec@1 85.156 (88.640)\tPrec@5 98.438 (99.411)\n",
      "Epoch: [153][70/97], lr: 0.01000\tTime 0.326 (0.328)\tData 0.000 (0.020)\tLoss 2.5247 (1.9010)\tPrec@1 83.594 (88.457)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [153][80/97], lr: 0.01000\tTime 0.320 (0.327)\tData 0.000 (0.020)\tLoss 1.6213 (1.8877)\tPrec@1 90.625 (88.474)\tPrec@5 98.438 (99.344)\n",
      "Epoch: [153][90/97], lr: 0.01000\tTime 0.324 (0.328)\tData 0.000 (0.019)\tLoss 2.0357 (1.8845)\tPrec@1 86.719 (88.496)\tPrec@5 100.000 (99.348)\n",
      "Epoch: [153][96/97], lr: 0.01000\tTime 0.313 (0.328)\tData 0.000 (0.020)\tLoss 1.8409 (1.8966)\tPrec@1 88.983 (88.441)\tPrec@5 98.305 (99.363)\n",
      "Gated Network Weight Gate= Flip:0.50, Sc:0.50\n",
      "Test: [0/100]\tTime 0.289 (0.289)\tLoss 8.3359 (8.3359)\tPrec@1 65.000 (65.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 6.5000 (8.2528)\tPrec@1 69.000 (58.818)\tPrec@5 99.000 (95.455)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 7.0552 (8.3601)\tPrec@1 64.000 (58.238)\tPrec@5 91.000 (94.762)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 8.2808 (8.4261)\tPrec@1 61.000 (58.645)\tPrec@5 92.000 (94.452)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 8.4897 (8.4058)\tPrec@1 54.000 (58.634)\tPrec@5 92.000 (93.902)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 8.1269 (8.3685)\tPrec@1 62.000 (58.804)\tPrec@5 94.000 (94.039)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.0492 (8.3581)\tPrec@1 59.000 (58.836)\tPrec@5 94.000 (93.967)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 7.9822 (8.3221)\tPrec@1 63.000 (59.254)\tPrec@5 96.000 (94.099)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 7.9615 (8.2810)\tPrec@1 62.000 (59.444)\tPrec@5 94.000 (94.049)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 9.0154 (8.3509)\tPrec@1 54.000 (59.110)\tPrec@5 97.000 (94.011)\n",
      "val Results: Prec@1 59.100 Prec@5 94.040 Loss 8.36653\n",
      "val Class Accuracy: [0.989,0.959,0.626,0.549,0.313,0.551,0.706,0.444,0.312,0.461]\n",
      "Best Prec@1: 69.540\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [154][0/97], lr: 0.01000\tTime 0.450 (0.450)\tData 0.247 (0.247)\tLoss 2.2233 (2.2233)\tPrec@1 85.938 (85.938)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [154][10/97], lr: 0.01000\tTime 0.326 (0.345)\tData 0.000 (0.037)\tLoss 1.5155 (1.9644)\tPrec@1 91.406 (87.926)\tPrec@5 99.219 (98.935)\n",
      "Epoch: [154][20/97], lr: 0.01000\tTime 0.328 (0.340)\tData 0.000 (0.027)\tLoss 1.7354 (1.9265)\tPrec@1 89.062 (88.281)\tPrec@5 99.219 (99.107)\n",
      "Epoch: [154][30/97], lr: 0.01000\tTime 0.332 (0.336)\tData 0.000 (0.024)\tLoss 2.4758 (1.9341)\tPrec@1 85.156 (88.231)\tPrec@5 99.219 (99.294)\n",
      "Epoch: [154][40/97], lr: 0.01000\tTime 0.341 (0.333)\tData 0.000 (0.022)\tLoss 1.9599 (1.9308)\tPrec@1 87.500 (88.224)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [154][50/97], lr: 0.01000\tTime 0.334 (0.334)\tData 0.000 (0.021)\tLoss 2.1624 (1.9497)\tPrec@1 88.281 (88.051)\tPrec@5 98.438 (99.127)\n",
      "Epoch: [154][60/97], lr: 0.01000\tTime 0.326 (0.334)\tData 0.000 (0.020)\tLoss 2.1187 (1.9770)\tPrec@1 85.938 (87.820)\tPrec@5 99.219 (99.180)\n",
      "Epoch: [154][70/97], lr: 0.01000\tTime 0.334 (0.333)\tData 0.000 (0.020)\tLoss 2.3890 (1.9450)\tPrec@1 85.938 (87.984)\tPrec@5 100.000 (99.263)\n",
      "Epoch: [154][80/97], lr: 0.01000\tTime 0.321 (0.332)\tData 0.000 (0.019)\tLoss 2.2190 (1.9448)\tPrec@1 87.500 (88.079)\tPrec@5 98.438 (99.286)\n",
      "Epoch: [154][90/97], lr: 0.01000\tTime 0.320 (0.332)\tData 0.000 (0.019)\tLoss 1.2758 (1.9263)\tPrec@1 91.406 (88.221)\tPrec@5 100.000 (99.287)\n",
      "Epoch: [154][96/97], lr: 0.01000\tTime 0.310 (0.331)\tData 0.000 (0.020)\tLoss 2.1236 (1.9387)\tPrec@1 86.441 (88.143)\tPrec@5 100.000 (99.299)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.271 (0.271)\tLoss 6.7928 (6.7928)\tPrec@1 74.000 (74.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 5.5594 (6.8507)\tPrec@1 73.000 (68.364)\tPrec@5 97.000 (96.364)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.1216 (6.7156)\tPrec@1 72.000 (68.905)\tPrec@5 97.000 (96.429)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.4202 (6.8203)\tPrec@1 70.000 (68.161)\tPrec@5 98.000 (96.355)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 6.3594 (6.8388)\tPrec@1 72.000 (67.902)\tPrec@5 98.000 (96.195)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.6775 (6.7839)\tPrec@1 71.000 (67.941)\tPrec@5 97.000 (96.275)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 5.7460 (6.7977)\tPrec@1 73.000 (67.721)\tPrec@5 98.000 (96.344)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 6.3804 (6.7982)\tPrec@1 71.000 (67.803)\tPrec@5 99.000 (96.155)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.8154 (6.7503)\tPrec@1 69.000 (68.000)\tPrec@5 94.000 (96.259)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 6.9426 (6.8245)\tPrec@1 67.000 (67.637)\tPrec@5 99.000 (96.121)\n",
      "val Results: Prec@1 67.610 Prec@5 96.130 Loss 6.84685\n",
      "val Class Accuracy: [0.856,0.990,0.809,0.787,0.815,0.471,0.653,0.517,0.560,0.303]\n",
      "Best Prec@1: 69.540\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [155][0/97], lr: 0.01000\tTime 0.541 (0.541)\tData 0.308 (0.308)\tLoss 1.1792 (1.1792)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [155][10/97], lr: 0.01000\tTime 0.324 (0.352)\tData 0.000 (0.042)\tLoss 1.8330 (1.7463)\tPrec@1 87.500 (88.778)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [155][20/97], lr: 0.01000\tTime 0.330 (0.339)\tData 0.000 (0.030)\tLoss 1.5722 (1.9529)\tPrec@1 91.406 (87.760)\tPrec@5 100.000 (99.665)\n",
      "Epoch: [155][30/97], lr: 0.01000\tTime 0.321 (0.335)\tData 0.000 (0.026)\tLoss 1.9656 (1.8779)\tPrec@1 86.719 (88.281)\tPrec@5 99.219 (99.622)\n",
      "Epoch: [155][40/97], lr: 0.01000\tTime 0.322 (0.332)\tData 0.000 (0.024)\tLoss 2.4651 (1.9125)\tPrec@1 83.594 (88.148)\tPrec@5 99.219 (99.524)\n",
      "Epoch: [155][50/97], lr: 0.01000\tTime 0.324 (0.330)\tData 0.000 (0.022)\tLoss 2.3300 (1.9321)\tPrec@1 85.938 (88.051)\tPrec@5 99.219 (99.464)\n",
      "Epoch: [155][60/97], lr: 0.01000\tTime 0.325 (0.330)\tData 0.000 (0.021)\tLoss 1.4660 (1.9390)\tPrec@1 90.625 (88.153)\tPrec@5 99.219 (99.372)\n",
      "Epoch: [155][70/97], lr: 0.01000\tTime 0.325 (0.329)\tData 0.000 (0.021)\tLoss 2.1320 (1.9514)\tPrec@1 85.156 (88.039)\tPrec@5 99.219 (99.318)\n",
      "Epoch: [155][80/97], lr: 0.01000\tTime 0.327 (0.329)\tData 0.000 (0.020)\tLoss 2.1361 (1.9543)\tPrec@1 86.719 (87.982)\tPrec@5 98.438 (99.296)\n",
      "Epoch: [155][90/97], lr: 0.01000\tTime 0.318 (0.328)\tData 0.000 (0.020)\tLoss 2.1096 (1.9576)\tPrec@1 86.719 (87.938)\tPrec@5 99.219 (99.322)\n",
      "Epoch: [155][96/97], lr: 0.01000\tTime 0.307 (0.328)\tData 0.000 (0.020)\tLoss 2.5816 (1.9604)\tPrec@1 83.898 (87.925)\tPrec@5 99.153 (99.283)\n",
      "Gated Network Weight Gate= Flip:0.53, Sc:0.47\n",
      "Test: [0/100]\tTime 0.320 (0.320)\tLoss 8.5012 (8.5012)\tPrec@1 60.000 (60.000)\tPrec@5 94.000 (94.000)\n",
      "Test: [10/100]\tTime 0.074 (0.096)\tLoss 8.0009 (8.4147)\tPrec@1 59.000 (60.182)\tPrec@5 96.000 (95.273)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 8.1876 (8.4529)\tPrec@1 58.000 (59.619)\tPrec@5 96.000 (95.333)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 8.8353 (8.4800)\tPrec@1 56.000 (59.613)\tPrec@5 94.000 (94.645)\n",
      "Test: [40/100]\tTime 0.074 (0.080)\tLoss 8.7297 (8.4104)\tPrec@1 60.000 (60.317)\tPrec@5 95.000 (94.561)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 8.7632 (8.3223)\tPrec@1 62.000 (60.725)\tPrec@5 94.000 (94.706)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 8.6214 (8.3591)\tPrec@1 57.000 (60.410)\tPrec@5 94.000 (94.754)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 7.6307 (8.3554)\tPrec@1 63.000 (60.268)\tPrec@5 96.000 (94.803)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 7.9671 (8.2808)\tPrec@1 64.000 (60.679)\tPrec@5 93.000 (94.901)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 9.0211 (8.3519)\tPrec@1 58.000 (60.429)\tPrec@5 96.000 (94.846)\n",
      "val Results: Prec@1 60.630 Prec@5 94.820 Loss 8.33753\n",
      "val Class Accuracy: [0.861,0.925,0.763,0.910,0.528,0.238,0.381,0.618,0.367,0.472]\n",
      "Best Prec@1: 69.540\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [156][0/97], lr: 0.01000\tTime 0.502 (0.502)\tData 0.259 (0.259)\tLoss 1.0980 (1.0980)\tPrec@1 94.531 (94.531)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [156][10/97], lr: 0.01000\tTime 0.332 (0.346)\tData 0.000 (0.038)\tLoss 1.8233 (1.7820)\tPrec@1 86.719 (89.347)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [156][20/97], lr: 0.01000\tTime 0.324 (0.337)\tData 0.000 (0.028)\tLoss 1.7574 (1.7574)\tPrec@1 89.844 (89.509)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [156][30/97], lr: 0.01000\tTime 0.324 (0.334)\tData 0.000 (0.024)\tLoss 1.7389 (1.7465)\tPrec@1 89.062 (89.491)\tPrec@5 100.000 (99.420)\n",
      "Epoch: [156][40/97], lr: 0.01000\tTime 0.323 (0.332)\tData 0.000 (0.023)\tLoss 1.2204 (1.8142)\tPrec@1 92.188 (89.024)\tPrec@5 98.438 (99.390)\n",
      "Epoch: [156][50/97], lr: 0.01000\tTime 0.322 (0.331)\tData 0.000 (0.021)\tLoss 1.6039 (1.8463)\tPrec@1 92.188 (88.848)\tPrec@5 98.438 (99.311)\n",
      "Epoch: [156][60/97], lr: 0.01000\tTime 0.322 (0.330)\tData 0.000 (0.021)\tLoss 1.8019 (1.8684)\tPrec@1 91.406 (88.870)\tPrec@5 100.000 (99.334)\n",
      "Epoch: [156][70/97], lr: 0.01000\tTime 0.327 (0.329)\tData 0.000 (0.020)\tLoss 1.6309 (1.8669)\tPrec@1 89.844 (88.809)\tPrec@5 99.219 (99.340)\n",
      "Epoch: [156][80/97], lr: 0.01000\tTime 0.333 (0.330)\tData 0.000 (0.020)\tLoss 2.7240 (1.8861)\tPrec@1 82.812 (88.648)\tPrec@5 98.438 (99.344)\n",
      "Epoch: [156][90/97], lr: 0.01000\tTime 0.321 (0.330)\tData 0.000 (0.019)\tLoss 1.9447 (1.8909)\tPrec@1 89.062 (88.547)\tPrec@5 98.438 (99.322)\n",
      "Epoch: [156][96/97], lr: 0.01000\tTime 0.310 (0.329)\tData 0.000 (0.020)\tLoss 1.8877 (1.8989)\tPrec@1 88.983 (88.481)\tPrec@5 100.000 (99.323)\n",
      "Gated Network Weight Gate= Flip:0.59, Sc:0.41\n",
      "Test: [0/100]\tTime 0.326 (0.326)\tLoss 7.4115 (7.4115)\tPrec@1 68.000 (68.000)\tPrec@5 93.000 (93.000)\n",
      "Test: [10/100]\tTime 0.073 (0.096)\tLoss 6.7710 (7.1008)\tPrec@1 66.000 (66.818)\tPrec@5 95.000 (95.545)\n",
      "Test: [20/100]\tTime 0.074 (0.085)\tLoss 6.3506 (7.0193)\tPrec@1 70.000 (66.905)\tPrec@5 98.000 (95.286)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.2182 (7.0106)\tPrec@1 74.000 (66.613)\tPrec@5 97.000 (95.613)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 6.2381 (6.9469)\tPrec@1 70.000 (67.000)\tPrec@5 95.000 (95.634)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 6.4589 (6.8685)\tPrec@1 69.000 (67.569)\tPrec@5 94.000 (95.667)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 6.7292 (6.8626)\tPrec@1 68.000 (67.443)\tPrec@5 96.000 (95.852)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 6.9949 (6.8459)\tPrec@1 65.000 (67.423)\tPrec@5 98.000 (95.859)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6272 (6.7899)\tPrec@1 71.000 (67.654)\tPrec@5 96.000 (96.012)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 7.1110 (6.8740)\tPrec@1 67.000 (67.088)\tPrec@5 97.000 (95.791)\n",
      "val Results: Prec@1 66.930 Prec@5 95.830 Loss 6.90077\n",
      "val Class Accuracy: [0.965,0.973,0.698,0.705,0.818,0.549,0.497,0.538,0.566,0.384]\n",
      "Best Prec@1: 69.540\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [157][0/97], lr: 0.01000\tTime 0.570 (0.570)\tData 0.341 (0.341)\tLoss 1.6273 (1.6273)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [157][10/97], lr: 0.01000\tTime 0.324 (0.353)\tData 0.000 (0.045)\tLoss 1.6883 (1.8025)\tPrec@1 90.625 (89.276)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [157][20/97], lr: 0.01000\tTime 0.334 (0.339)\tData 0.000 (0.032)\tLoss 1.8338 (1.8264)\tPrec@1 86.719 (88.876)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [157][30/97], lr: 0.01000\tTime 0.322 (0.335)\tData 0.000 (0.027)\tLoss 1.8370 (1.8156)\tPrec@1 89.844 (88.962)\tPrec@5 98.438 (99.420)\n",
      "Epoch: [157][40/97], lr: 0.01000\tTime 0.323 (0.333)\tData 0.000 (0.024)\tLoss 2.1359 (1.8367)\tPrec@1 85.938 (88.929)\tPrec@5 98.438 (99.409)\n",
      "Epoch: [157][50/97], lr: 0.01000\tTime 0.319 (0.331)\tData 0.000 (0.023)\tLoss 1.9689 (1.9026)\tPrec@1 89.844 (88.557)\tPrec@5 98.438 (99.449)\n",
      "Epoch: [157][60/97], lr: 0.01000\tTime 0.327 (0.330)\tData 0.000 (0.022)\tLoss 1.6607 (1.8944)\tPrec@1 91.406 (88.640)\tPrec@5 100.000 (99.424)\n",
      "Epoch: [157][70/97], lr: 0.01000\tTime 0.324 (0.329)\tData 0.000 (0.021)\tLoss 2.1175 (1.9449)\tPrec@1 87.500 (88.281)\tPrec@5 98.438 (99.395)\n",
      "Epoch: [157][80/97], lr: 0.01000\tTime 0.319 (0.329)\tData 0.000 (0.021)\tLoss 1.7299 (1.9354)\tPrec@1 89.062 (88.329)\tPrec@5 99.219 (99.402)\n",
      "Epoch: [157][90/97], lr: 0.01000\tTime 0.318 (0.328)\tData 0.000 (0.020)\tLoss 1.9303 (1.9410)\tPrec@1 90.625 (88.341)\tPrec@5 99.219 (99.373)\n",
      "Epoch: [157][96/97], lr: 0.01000\tTime 0.307 (0.328)\tData 0.000 (0.021)\tLoss 2.1238 (1.9482)\tPrec@1 87.288 (88.312)\tPrec@5 100.000 (99.371)\n",
      "Gated Network Weight Gate= Flip:0.50, Sc:0.50\n",
      "Test: [0/100]\tTime 0.267 (0.267)\tLoss 10.7713 (10.7713)\tPrec@1 49.000 (49.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 8.2082 (9.8101)\tPrec@1 57.000 (53.091)\tPrec@5 95.000 (93.636)\n",
      "Test: [20/100]\tTime 0.074 (0.083)\tLoss 7.7707 (9.8744)\tPrec@1 60.000 (51.952)\tPrec@5 95.000 (93.381)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 10.0190 (9.9414)\tPrec@1 48.000 (51.613)\tPrec@5 93.000 (93.129)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 11.0586 (9.9736)\tPrec@1 46.000 (51.878)\tPrec@5 90.000 (92.780)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 9.9282 (9.9102)\tPrec@1 52.000 (52.196)\tPrec@5 91.000 (92.804)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.2349 (9.8712)\tPrec@1 58.000 (52.246)\tPrec@5 92.000 (92.885)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 9.2724 (9.8424)\tPrec@1 55.000 (52.380)\tPrec@5 92.000 (92.887)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 10.0797 (9.7992)\tPrec@1 52.000 (52.543)\tPrec@5 93.000 (93.099)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 10.2823 (9.8857)\tPrec@1 48.000 (52.022)\tPrec@5 96.000 (93.077)\n",
      "val Results: Prec@1 52.060 Prec@5 93.090 Loss 9.88960\n",
      "val Class Accuracy: [0.968,0.980,0.541,0.836,0.340,0.462,0.445,0.359,0.262,0.013]\n",
      "Best Prec@1: 69.540\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [158][0/97], lr: 0.01000\tTime 0.528 (0.528)\tData 0.292 (0.292)\tLoss 1.6004 (1.6004)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [158][10/97], lr: 0.01000\tTime 0.325 (0.348)\tData 0.000 (0.041)\tLoss 1.6058 (1.9272)\tPrec@1 91.406 (88.494)\tPrec@5 99.219 (99.503)\n",
      "Epoch: [158][20/97], lr: 0.01000\tTime 0.322 (0.337)\tData 0.000 (0.030)\tLoss 2.5112 (1.9211)\tPrec@1 85.156 (88.393)\tPrec@5 99.219 (99.479)\n",
      "Epoch: [158][30/97], lr: 0.01000\tTime 0.319 (0.333)\tData 0.000 (0.026)\tLoss 1.5889 (1.8933)\tPrec@1 92.188 (88.785)\tPrec@5 99.219 (99.546)\n",
      "Epoch: [158][40/97], lr: 0.01000\tTime 0.324 (0.331)\tData 0.000 (0.023)\tLoss 2.3637 (1.9275)\tPrec@1 82.812 (88.377)\tPrec@5 99.219 (99.447)\n",
      "Epoch: [158][50/97], lr: 0.01000\tTime 0.326 (0.330)\tData 0.000 (0.022)\tLoss 1.7885 (1.9436)\tPrec@1 90.625 (88.388)\tPrec@5 99.219 (99.357)\n",
      "Epoch: [158][60/97], lr: 0.01000\tTime 0.329 (0.331)\tData 0.000 (0.021)\tLoss 1.6086 (1.9601)\tPrec@1 89.844 (88.281)\tPrec@5 100.000 (99.308)\n",
      "Epoch: [158][70/97], lr: 0.01000\tTime 0.321 (0.330)\tData 0.000 (0.021)\tLoss 2.5397 (1.9369)\tPrec@1 83.594 (88.446)\tPrec@5 99.219 (99.318)\n",
      "Epoch: [158][80/97], lr: 0.01000\tTime 0.323 (0.330)\tData 0.000 (0.020)\tLoss 2.6185 (1.9521)\tPrec@1 85.156 (88.339)\tPrec@5 100.000 (99.354)\n",
      "Epoch: [158][90/97], lr: 0.01000\tTime 0.337 (0.330)\tData 0.000 (0.020)\tLoss 1.6476 (1.9501)\tPrec@1 86.719 (88.316)\tPrec@5 99.219 (99.348)\n",
      "Epoch: [158][96/97], lr: 0.01000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 2.3653 (1.9680)\tPrec@1 84.746 (88.183)\tPrec@5 99.153 (99.339)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.380 (0.380)\tLoss 8.6946 (8.6946)\tPrec@1 57.000 (57.000)\tPrec@5 95.000 (95.000)\n",
      "Test: [10/100]\tTime 0.073 (0.102)\tLoss 7.2631 (7.7906)\tPrec@1 62.000 (61.727)\tPrec@5 95.000 (95.545)\n",
      "Test: [20/100]\tTime 0.078 (0.089)\tLoss 7.9614 (7.7622)\tPrec@1 55.000 (61.381)\tPrec@5 94.000 (95.667)\n",
      "Test: [30/100]\tTime 0.074 (0.085)\tLoss 7.8627 (7.8189)\tPrec@1 59.000 (61.323)\tPrec@5 97.000 (95.903)\n",
      "Test: [40/100]\tTime 0.077 (0.083)\tLoss 7.6032 (7.7910)\tPrec@1 64.000 (61.683)\tPrec@5 94.000 (95.829)\n",
      "Test: [50/100]\tTime 0.074 (0.081)\tLoss 8.0032 (7.7630)\tPrec@1 57.000 (61.745)\tPrec@5 97.000 (96.039)\n",
      "Test: [60/100]\tTime 0.074 (0.080)\tLoss 6.1645 (7.7737)\tPrec@1 66.000 (61.639)\tPrec@5 99.000 (96.230)\n",
      "Test: [70/100]\tTime 0.074 (0.079)\tLoss 7.1692 (7.7805)\tPrec@1 66.000 (61.465)\tPrec@5 97.000 (96.070)\n",
      "Test: [80/100]\tTime 0.076 (0.079)\tLoss 7.8736 (7.7567)\tPrec@1 60.000 (61.667)\tPrec@5 96.000 (96.185)\n",
      "Test: [90/100]\tTime 0.074 (0.078)\tLoss 6.9931 (7.8024)\tPrec@1 69.000 (61.615)\tPrec@5 98.000 (96.110)\n",
      "val Results: Prec@1 61.750 Prec@5 96.150 Loss 7.80102\n",
      "val Class Accuracy: [0.834,0.954,0.843,0.839,0.650,0.414,0.387,0.334,0.531,0.389]\n",
      "Best Prec@1: 69.540\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [159][0/97], lr: 0.01000\tTime 0.660 (0.660)\tData 0.395 (0.395)\tLoss 1.3551 (1.3551)\tPrec@1 92.969 (92.969)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [159][10/97], lr: 0.01000\tTime 0.360 (0.415)\tData 0.000 (0.049)\tLoss 1.8282 (1.8007)\tPrec@1 88.281 (89.276)\tPrec@5 99.219 (99.574)\n",
      "Epoch: [159][20/97], lr: 0.01000\tTime 0.339 (0.396)\tData 0.000 (0.033)\tLoss 2.0670 (1.7207)\tPrec@1 87.500 (89.472)\tPrec@5 99.219 (99.554)\n",
      "Epoch: [159][30/97], lr: 0.01000\tTime 0.323 (0.383)\tData 0.000 (0.028)\tLoss 2.6775 (1.8188)\tPrec@1 84.375 (88.810)\tPrec@5 98.438 (99.370)\n",
      "Epoch: [159][40/97], lr: 0.01000\tTime 0.365 (0.379)\tData 0.000 (0.025)\tLoss 1.6659 (1.8825)\tPrec@1 88.281 (88.472)\tPrec@5 100.000 (99.276)\n",
      "Epoch: [159][50/97], lr: 0.01000\tTime 0.321 (0.373)\tData 0.000 (0.023)\tLoss 1.8341 (1.8829)\tPrec@1 86.719 (88.373)\tPrec@5 100.000 (99.326)\n",
      "Epoch: [159][60/97], lr: 0.01000\tTime 0.324 (0.366)\tData 0.000 (0.022)\tLoss 2.2174 (1.8986)\tPrec@1 85.938 (88.332)\tPrec@5 99.219 (99.321)\n",
      "Epoch: [159][70/97], lr: 0.01000\tTime 0.328 (0.361)\tData 0.000 (0.022)\tLoss 2.2467 (1.8967)\tPrec@1 86.719 (88.369)\tPrec@5 100.000 (99.329)\n",
      "Epoch: [159][80/97], lr: 0.01000\tTime 0.325 (0.358)\tData 0.000 (0.021)\tLoss 2.1562 (1.9232)\tPrec@1 85.156 (88.146)\tPrec@5 99.219 (99.354)\n",
      "Epoch: [159][90/97], lr: 0.01000\tTime 0.320 (0.355)\tData 0.000 (0.021)\tLoss 2.0822 (1.9216)\tPrec@1 85.938 (88.058)\tPrec@5 98.438 (99.348)\n",
      "Epoch: [159][96/97], lr: 0.01000\tTime 0.318 (0.353)\tData 0.000 (0.021)\tLoss 2.0003 (1.9306)\tPrec@1 87.288 (88.014)\tPrec@5 99.153 (99.323)\n",
      "Gated Network Weight Gate= Flip:0.46, Sc:0.54\n",
      "Test: [0/100]\tTime 0.293 (0.293)\tLoss 9.9367 (9.9367)\tPrec@1 56.000 (56.000)\tPrec@5 87.000 (87.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 6.4431 (9.8014)\tPrec@1 69.000 (54.182)\tPrec@5 95.000 (88.545)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 9.9830 (9.7408)\tPrec@1 51.000 (53.857)\tPrec@5 91.000 (89.000)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 9.2480 (9.8220)\tPrec@1 54.000 (53.452)\tPrec@5 87.000 (88.613)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 10.7243 (9.9188)\tPrec@1 48.000 (53.220)\tPrec@5 85.000 (88.341)\n",
      "Test: [50/100]\tTime 0.077 (0.078)\tLoss 10.3945 (9.8695)\tPrec@1 48.000 (53.412)\tPrec@5 86.000 (88.137)\n",
      "Test: [60/100]\tTime 0.075 (0.078)\tLoss 8.8583 (9.8713)\tPrec@1 55.000 (53.164)\tPrec@5 90.000 (88.082)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 10.1247 (9.8818)\tPrec@1 54.000 (53.268)\tPrec@5 83.000 (87.803)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 9.7529 (9.8567)\tPrec@1 55.000 (53.395)\tPrec@5 86.000 (88.049)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 10.1719 (9.9020)\tPrec@1 51.000 (53.110)\tPrec@5 87.000 (87.978)\n",
      "val Results: Prec@1 53.150 Prec@5 87.910 Loss 9.91378\n",
      "val Class Accuracy: [0.799,0.824,0.665,0.565,0.619,0.267,0.951,0.080,0.273,0.272]\n",
      "Best Prec@1: 69.540\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [160][0/97], lr: 0.00010\tTime 0.692 (0.692)\tData 0.402 (0.402)\tLoss 5.6663 (5.6663)\tPrec@1 86.719 (86.719)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [160][10/97], lr: 0.00010\tTime 0.325 (0.375)\tData 0.000 (0.048)\tLoss 3.1119 (5.2798)\tPrec@1 88.281 (89.205)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [160][20/97], lr: 0.00010\tTime 0.319 (0.354)\tData 0.000 (0.033)\tLoss 7.8339 (5.4902)\tPrec@1 89.844 (89.025)\tPrec@5 99.219 (99.442)\n",
      "Epoch: [160][30/97], lr: 0.00010\tTime 0.331 (0.348)\tData 0.000 (0.028)\tLoss 4.8803 (5.0747)\tPrec@1 92.188 (89.239)\tPrec@5 100.000 (99.420)\n",
      "Epoch: [160][40/97], lr: 0.00010\tTime 0.326 (0.344)\tData 0.000 (0.025)\tLoss 5.4756 (4.9138)\tPrec@1 84.375 (88.815)\tPrec@5 99.219 (99.466)\n",
      "Epoch: [160][50/97], lr: 0.00010\tTime 0.322 (0.342)\tData 0.000 (0.024)\tLoss 7.7406 (4.9195)\tPrec@1 82.812 (88.434)\tPrec@5 99.219 (99.433)\n",
      "Epoch: [160][60/97], lr: 0.00010\tTime 0.325 (0.340)\tData 0.000 (0.023)\tLoss 4.5242 (4.7736)\tPrec@1 86.719 (88.320)\tPrec@5 98.438 (99.436)\n",
      "Epoch: [160][70/97], lr: 0.00010\tTime 0.324 (0.339)\tData 0.000 (0.022)\tLoss 2.4844 (4.7825)\tPrec@1 89.062 (88.083)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [160][80/97], lr: 0.00010\tTime 0.326 (0.339)\tData 0.000 (0.021)\tLoss 2.8602 (4.7335)\tPrec@1 86.719 (87.876)\tPrec@5 99.219 (99.402)\n",
      "Epoch: [160][90/97], lr: 0.00010\tTime 0.319 (0.338)\tData 0.000 (0.021)\tLoss 3.2166 (4.6190)\tPrec@1 86.719 (87.946)\tPrec@5 99.219 (99.425)\n",
      "Epoch: [160][96/97], lr: 0.00010\tTime 0.316 (0.338)\tData 0.000 (0.021)\tLoss 2.2736 (4.5976)\tPrec@1 89.831 (87.885)\tPrec@5 99.153 (99.404)\n",
      "Gated Network Weight Gate= Flip:0.48, Sc:0.52\n",
      "Test: [0/100]\tTime 0.300 (0.300)\tLoss 5.4009 (5.4009)\tPrec@1 77.000 (77.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.094)\tLoss 5.2483 (6.6052)\tPrec@1 83.000 (77.091)\tPrec@5 97.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 7.7379 (6.5959)\tPrec@1 74.000 (77.048)\tPrec@5 97.000 (98.238)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 7.0152 (6.7373)\tPrec@1 80.000 (76.677)\tPrec@5 98.000 (98.452)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 5.5734 (6.7776)\tPrec@1 76.000 (76.317)\tPrec@5 96.000 (98.244)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 7.0660 (6.7713)\tPrec@1 77.000 (76.608)\tPrec@5 98.000 (98.275)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 7.9879 (6.7525)\tPrec@1 77.000 (76.393)\tPrec@5 99.000 (98.311)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 6.2740 (6.7610)\tPrec@1 75.000 (76.479)\tPrec@5 98.000 (98.324)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 7.0390 (6.6194)\tPrec@1 75.000 (76.963)\tPrec@5 99.000 (98.309)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 5.2220 (6.6458)\tPrec@1 83.000 (76.758)\tPrec@5 99.000 (98.253)\n",
      "val Results: Prec@1 76.810 Prec@5 98.260 Loss 6.65490\n",
      "val Class Accuracy: [0.914,0.916,0.772,0.722,0.794,0.636,0.790,0.657,0.674,0.806]\n",
      "Best Prec@1: 76.810\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [161][0/97], lr: 0.00010\tTime 0.724 (0.724)\tData 0.412 (0.412)\tLoss 9.0145 (9.0145)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [161][10/97], lr: 0.00010\tTime 0.324 (0.379)\tData 0.000 (0.050)\tLoss 3.4620 (3.9832)\tPrec@1 82.812 (87.074)\tPrec@5 98.438 (99.432)\n",
      "Epoch: [161][20/97], lr: 0.00010\tTime 0.323 (0.356)\tData 0.000 (0.034)\tLoss 2.5704 (3.7624)\tPrec@1 86.719 (87.165)\tPrec@5 100.000 (99.368)\n",
      "Epoch: [161][30/97], lr: 0.00010\tTime 0.321 (0.348)\tData 0.000 (0.029)\tLoss 4.1508 (3.8170)\tPrec@1 84.375 (87.324)\tPrec@5 98.438 (99.269)\n",
      "Epoch: [161][40/97], lr: 0.00010\tTime 0.321 (0.343)\tData 0.000 (0.026)\tLoss 4.3797 (3.6490)\tPrec@1 85.156 (87.691)\tPrec@5 97.656 (99.238)\n",
      "Epoch: [161][50/97], lr: 0.00010\tTime 0.322 (0.340)\tData 0.000 (0.024)\tLoss 3.3484 (3.5423)\tPrec@1 86.719 (87.929)\tPrec@5 100.000 (99.265)\n",
      "Epoch: [161][60/97], lr: 0.00010\tTime 0.326 (0.339)\tData 0.000 (0.023)\tLoss 5.1619 (3.5240)\tPrec@1 92.188 (88.038)\tPrec@5 99.219 (99.257)\n",
      "Epoch: [161][70/97], lr: 0.00010\tTime 0.321 (0.338)\tData 0.000 (0.022)\tLoss 1.6333 (3.4273)\tPrec@1 89.062 (87.885)\tPrec@5 100.000 (99.274)\n",
      "Epoch: [161][80/97], lr: 0.00010\tTime 0.326 (0.337)\tData 0.000 (0.021)\tLoss 3.7359 (3.6147)\tPrec@1 85.156 (87.751)\tPrec@5 100.000 (99.267)\n",
      "Epoch: [161][90/97], lr: 0.00010\tTime 0.319 (0.336)\tData 0.000 (0.021)\tLoss 3.5052 (3.6368)\tPrec@1 89.844 (87.637)\tPrec@5 98.438 (99.236)\n",
      "Epoch: [161][96/97], lr: 0.00010\tTime 0.322 (0.336)\tData 0.000 (0.021)\tLoss 3.2957 (3.6126)\tPrec@1 83.051 (87.603)\tPrec@5 100.000 (99.275)\n",
      "Gated Network Weight Gate= Flip:0.52, Sc:0.48\n",
      "Test: [0/100]\tTime 0.294 (0.294)\tLoss 4.6859 (4.6859)\tPrec@1 77.000 (77.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 4.7635 (5.9040)\tPrec@1 81.000 (78.000)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.074 (0.084)\tLoss 6.9543 (5.9347)\tPrec@1 75.000 (77.952)\tPrec@5 98.000 (98.286)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 6.4346 (6.0695)\tPrec@1 81.000 (78.032)\tPrec@5 98.000 (98.452)\n",
      "Test: [40/100]\tTime 0.075 (0.079)\tLoss 5.2533 (6.1375)\tPrec@1 78.000 (77.756)\tPrec@5 96.000 (98.317)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 6.4293 (6.1309)\tPrec@1 83.000 (78.216)\tPrec@5 99.000 (98.392)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 7.7474 (6.1289)\tPrec@1 82.000 (78.066)\tPrec@5 99.000 (98.508)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.7846 (6.1170)\tPrec@1 74.000 (78.070)\tPrec@5 99.000 (98.549)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.2684 (6.0197)\tPrec@1 77.000 (78.358)\tPrec@5 99.000 (98.556)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 4.3466 (6.0457)\tPrec@1 84.000 (78.154)\tPrec@5 99.000 (98.484)\n",
      "val Results: Prec@1 78.260 Prec@5 98.480 Loss 6.04322\n",
      "val Class Accuracy: [0.889,0.928,0.772,0.677,0.817,0.706,0.840,0.674,0.708,0.815]\n",
      "Best Prec@1: 78.260\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [162][0/97], lr: 0.00010\tTime 0.603 (0.603)\tData 0.325 (0.325)\tLoss 1.8898 (1.8898)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [162][10/97], lr: 0.00010\tTime 0.329 (0.372)\tData 0.000 (0.041)\tLoss 2.5727 (3.1624)\tPrec@1 91.406 (88.423)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [162][20/97], lr: 0.00010\tTime 0.327 (0.352)\tData 0.000 (0.030)\tLoss 2.1062 (3.2715)\tPrec@1 85.938 (87.612)\tPrec@5 99.219 (99.256)\n",
      "Epoch: [162][30/97], lr: 0.00010\tTime 0.323 (0.346)\tData 0.000 (0.026)\tLoss 1.7219 (3.2271)\tPrec@1 91.406 (87.878)\tPrec@5 100.000 (99.143)\n",
      "Epoch: [162][40/97], lr: 0.00010\tTime 0.324 (0.343)\tData 0.000 (0.024)\tLoss 4.5414 (3.3907)\tPrec@1 86.719 (87.081)\tPrec@5 99.219 (99.200)\n",
      "Epoch: [162][50/97], lr: 0.00010\tTime 0.327 (0.341)\tData 0.000 (0.022)\tLoss 2.9981 (3.2991)\tPrec@1 84.375 (87.178)\tPrec@5 99.219 (99.173)\n",
      "Epoch: [162][60/97], lr: 0.00010\tTime 0.325 (0.340)\tData 0.000 (0.021)\tLoss 2.1889 (3.3156)\tPrec@1 87.500 (87.231)\tPrec@5 99.219 (99.180)\n",
      "Epoch: [162][70/97], lr: 0.00010\tTime 0.322 (0.338)\tData 0.000 (0.021)\tLoss 2.4182 (3.2646)\tPrec@1 85.156 (87.082)\tPrec@5 96.875 (99.109)\n",
      "Epoch: [162][80/97], lr: 0.00010\tTime 0.338 (0.340)\tData 0.000 (0.020)\tLoss 4.7451 (3.3435)\tPrec@1 87.500 (87.076)\tPrec@5 99.219 (99.142)\n",
      "Epoch: [162][90/97], lr: 0.00010\tTime 0.333 (0.341)\tData 0.000 (0.020)\tLoss 4.2253 (3.3372)\tPrec@1 89.844 (87.088)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [162][96/97], lr: 0.00010\tTime 0.376 (0.342)\tData 0.000 (0.020)\tLoss 3.3118 (3.3762)\tPrec@1 84.746 (86.958)\tPrec@5 99.153 (99.218)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.526 (0.526)\tLoss 4.5123 (4.5123)\tPrec@1 79.000 (79.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.115)\tLoss 4.6900 (5.8116)\tPrec@1 82.000 (78.182)\tPrec@5 98.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.096)\tLoss 6.8475 (5.8533)\tPrec@1 75.000 (78.000)\tPrec@5 98.000 (98.429)\n",
      "Test: [30/100]\tTime 0.074 (0.089)\tLoss 6.2117 (5.9889)\tPrec@1 81.000 (78.032)\tPrec@5 97.000 (98.484)\n",
      "Test: [40/100]\tTime 0.074 (0.086)\tLoss 5.2222 (6.0818)\tPrec@1 77.000 (77.805)\tPrec@5 96.000 (98.341)\n",
      "Test: [50/100]\tTime 0.079 (0.084)\tLoss 6.4175 (6.0792)\tPrec@1 81.000 (78.294)\tPrec@5 99.000 (98.373)\n",
      "Test: [60/100]\tTime 0.074 (0.083)\tLoss 7.7204 (6.0712)\tPrec@1 82.000 (78.279)\tPrec@5 99.000 (98.508)\n",
      "Test: [70/100]\tTime 0.094 (0.084)\tLoss 5.6493 (6.0649)\tPrec@1 75.000 (78.310)\tPrec@5 99.000 (98.563)\n",
      "Test: [80/100]\tTime 0.090 (0.084)\tLoss 6.1086 (5.9715)\tPrec@1 78.000 (78.630)\tPrec@5 99.000 (98.568)\n",
      "Test: [90/100]\tTime 0.093 (0.086)\tLoss 4.1101 (5.9928)\tPrec@1 85.000 (78.495)\tPrec@5 99.000 (98.505)\n",
      "val Results: Prec@1 78.450 Prec@5 98.510 Loss 6.00294\n",
      "val Class Accuracy: [0.898,0.933,0.790,0.677,0.801,0.713,0.830,0.679,0.719,0.805]\n",
      "Best Prec@1: 78.450\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [163][0/97], lr: 0.00010\tTime 1.442 (1.442)\tData 0.872 (0.872)\tLoss 3.9077 (3.9077)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [163][10/97], lr: 0.00010\tTime 0.323 (0.484)\tData 0.000 (0.087)\tLoss 3.2103 (3.6610)\tPrec@1 89.844 (86.861)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [163][20/97], lr: 0.00010\tTime 0.327 (0.412)\tData 0.000 (0.053)\tLoss 3.3234 (3.0757)\tPrec@1 92.188 (87.128)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [163][30/97], lr: 0.00010\tTime 0.342 (0.391)\tData 0.000 (0.042)\tLoss 3.4055 (3.3153)\tPrec@1 90.625 (87.248)\tPrec@5 99.219 (99.496)\n",
      "Epoch: [163][40/97], lr: 0.00010\tTime 0.321 (0.375)\tData 0.000 (0.035)\tLoss 3.9071 (3.1517)\tPrec@1 86.719 (87.195)\tPrec@5 99.219 (99.447)\n",
      "Epoch: [163][50/97], lr: 0.00010\tTime 0.320 (0.365)\tData 0.000 (0.032)\tLoss 2.7521 (3.2383)\tPrec@1 83.594 (86.994)\tPrec@5 99.219 (99.433)\n",
      "Epoch: [163][60/97], lr: 0.00010\tTime 0.320 (0.358)\tData 0.000 (0.029)\tLoss 4.4530 (3.3452)\tPrec@1 85.938 (87.001)\tPrec@5 99.219 (99.424)\n",
      "Epoch: [163][70/97], lr: 0.00010\tTime 0.324 (0.354)\tData 0.000 (0.028)\tLoss 4.6186 (3.2970)\tPrec@1 84.375 (87.104)\tPrec@5 99.219 (99.362)\n",
      "Epoch: [163][80/97], lr: 0.00010\tTime 0.321 (0.350)\tData 0.000 (0.026)\tLoss 3.8651 (3.2928)\tPrec@1 88.281 (87.076)\tPrec@5 100.000 (99.334)\n",
      "Epoch: [163][90/97], lr: 0.00010\tTime 0.317 (0.347)\tData 0.000 (0.025)\tLoss 3.9967 (3.2422)\tPrec@1 82.812 (87.114)\tPrec@5 99.219 (99.356)\n",
      "Epoch: [163][96/97], lr: 0.00010\tTime 0.309 (0.345)\tData 0.000 (0.025)\tLoss 2.7548 (3.2161)\tPrec@1 90.678 (87.159)\tPrec@5 100.000 (99.347)\n",
      "Gated Network Weight Gate= Flip:0.54, Sc:0.46\n",
      "Test: [0/100]\tTime 0.256 (0.256)\tLoss 4.6449 (4.6449)\tPrec@1 81.000 (81.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.090)\tLoss 4.5182 (5.7857)\tPrec@1 81.000 (79.455)\tPrec@5 98.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.4699 (5.8079)\tPrec@1 76.000 (78.857)\tPrec@5 98.000 (98.381)\n",
      "Test: [30/100]\tTime 0.074 (0.079)\tLoss 5.8953 (5.9466)\tPrec@1 81.000 (78.710)\tPrec@5 97.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 5.1368 (6.0452)\tPrec@1 77.000 (78.634)\tPrec@5 97.000 (98.268)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 6.2918 (6.0044)\tPrec@1 83.000 (79.235)\tPrec@5 99.000 (98.353)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.8093 (5.9963)\tPrec@1 79.000 (79.049)\tPrec@5 99.000 (98.459)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 5.6709 (5.9925)\tPrec@1 78.000 (79.127)\tPrec@5 99.000 (98.549)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 5.9811 (5.9321)\tPrec@1 80.000 (79.481)\tPrec@5 99.000 (98.568)\n",
      "Test: [90/100]\tTime 0.074 (0.075)\tLoss 3.7521 (5.9258)\tPrec@1 86.000 (79.319)\tPrec@5 99.000 (98.495)\n",
      "val Results: Prec@1 79.190 Prec@5 98.500 Loss 5.95542\n",
      "val Class Accuracy: [0.885,0.953,0.776,0.668,0.812,0.727,0.843,0.672,0.808,0.775]\n",
      "Best Prec@1: 79.190\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [164][0/97], lr: 0.00010\tTime 0.530 (0.530)\tData 0.266 (0.266)\tLoss 3.4484 (3.4484)\tPrec@1 87.500 (87.500)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [164][10/97], lr: 0.00010\tTime 0.327 (0.350)\tData 0.000 (0.038)\tLoss 2.7793 (3.2653)\tPrec@1 89.062 (87.287)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [164][20/97], lr: 0.00010\tTime 0.325 (0.339)\tData 0.000 (0.028)\tLoss 4.4510 (3.6637)\tPrec@1 85.938 (86.868)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [164][30/97], lr: 0.00010\tTime 0.322 (0.335)\tData 0.000 (0.024)\tLoss 1.4560 (3.2327)\tPrec@1 92.188 (87.399)\tPrec@5 100.000 (99.168)\n",
      "Epoch: [164][40/97], lr: 0.00010\tTime 0.322 (0.332)\tData 0.000 (0.023)\tLoss 4.2454 (3.3255)\tPrec@1 85.156 (87.138)\tPrec@5 100.000 (99.238)\n",
      "Epoch: [164][50/97], lr: 0.00010\tTime 0.322 (0.331)\tData 0.000 (0.021)\tLoss 3.7521 (3.2358)\tPrec@1 83.594 (87.178)\tPrec@5 97.656 (99.234)\n",
      "Epoch: [164][60/97], lr: 0.00010\tTime 0.324 (0.330)\tData 0.000 (0.021)\tLoss 4.2599 (3.2383)\tPrec@1 89.844 (87.065)\tPrec@5 99.219 (99.232)\n",
      "Epoch: [164][70/97], lr: 0.00010\tTime 0.322 (0.330)\tData 0.000 (0.020)\tLoss 2.5292 (3.1874)\tPrec@1 85.938 (87.203)\tPrec@5 97.656 (99.197)\n",
      "Epoch: [164][80/97], lr: 0.00010\tTime 0.324 (0.330)\tData 0.000 (0.020)\tLoss 1.6126 (3.2298)\tPrec@1 87.500 (87.056)\tPrec@5 100.000 (99.199)\n",
      "Epoch: [164][90/97], lr: 0.00010\tTime 0.325 (0.329)\tData 0.000 (0.020)\tLoss 1.7253 (3.2435)\tPrec@1 90.625 (86.968)\tPrec@5 100.000 (99.236)\n",
      "Epoch: [164][96/97], lr: 0.00010\tTime 0.312 (0.329)\tData 0.000 (0.020)\tLoss 2.7147 (3.2399)\tPrec@1 84.746 (86.958)\tPrec@5 99.153 (99.258)\n",
      "Gated Network Weight Gate= Flip:0.54, Sc:0.46\n",
      "Test: [0/100]\tTime 0.343 (0.343)\tLoss 4.3056 (4.3056)\tPrec@1 80.000 (80.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.098)\tLoss 4.3045 (5.5101)\tPrec@1 82.000 (79.455)\tPrec@5 98.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.086)\tLoss 6.4171 (5.5314)\tPrec@1 76.000 (78.905)\tPrec@5 98.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.082)\tLoss 6.1036 (5.6610)\tPrec@1 78.000 (78.742)\tPrec@5 97.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 4.6852 (5.7557)\tPrec@1 80.000 (78.561)\tPrec@5 98.000 (98.366)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.9587 (5.7459)\tPrec@1 83.000 (79.000)\tPrec@5 99.000 (98.431)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 7.9049 (5.7233)\tPrec@1 78.000 (78.951)\tPrec@5 99.000 (98.541)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.0838 (5.7107)\tPrec@1 79.000 (79.085)\tPrec@5 99.000 (98.606)\n",
      "Test: [80/100]\tTime 0.073 (0.077)\tLoss 5.5522 (5.6434)\tPrec@1 83.000 (79.420)\tPrec@5 99.000 (98.654)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.6377 (5.6556)\tPrec@1 87.000 (79.275)\tPrec@5 99.000 (98.560)\n",
      "val Results: Prec@1 79.190 Prec@5 98.540 Loss 5.67682\n",
      "val Class Accuracy: [0.891,0.941,0.800,0.629,0.810,0.755,0.810,0.716,0.774,0.793]\n",
      "Best Prec@1: 79.190\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [165][0/97], lr: 0.00010\tTime 0.476 (0.476)\tData 0.274 (0.274)\tLoss 4.4217 (4.4217)\tPrec@1 83.594 (83.594)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [165][10/97], lr: 0.00010\tTime 0.321 (0.343)\tData 0.000 (0.039)\tLoss 1.3340 (3.7982)\tPrec@1 91.406 (86.435)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [165][20/97], lr: 0.00010\tTime 0.324 (0.339)\tData 0.000 (0.029)\tLoss 4.3352 (3.7351)\tPrec@1 87.500 (86.607)\tPrec@5 99.219 (99.256)\n",
      "Epoch: [165][30/97], lr: 0.00010\tTime 0.323 (0.335)\tData 0.000 (0.025)\tLoss 3.8995 (3.6343)\tPrec@1 89.062 (86.694)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [165][40/97], lr: 0.00010\tTime 0.322 (0.332)\tData 0.000 (0.023)\tLoss 3.1421 (3.4012)\tPrec@1 89.062 (86.909)\tPrec@5 98.438 (99.371)\n",
      "Epoch: [165][50/97], lr: 0.00010\tTime 0.321 (0.331)\tData 0.000 (0.022)\tLoss 3.0874 (3.3083)\tPrec@1 86.719 (86.811)\tPrec@5 98.438 (99.265)\n",
      "Epoch: [165][60/97], lr: 0.00010\tTime 0.325 (0.330)\tData 0.000 (0.021)\tLoss 7.4516 (3.2785)\tPrec@1 84.375 (87.001)\tPrec@5 98.438 (99.270)\n",
      "Epoch: [165][70/97], lr: 0.00010\tTime 0.319 (0.329)\tData 0.000 (0.020)\tLoss 7.2482 (3.3114)\tPrec@1 79.688 (86.818)\tPrec@5 97.656 (99.252)\n",
      "Epoch: [165][80/97], lr: 0.00010\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 1.3780 (3.2369)\tPrec@1 90.625 (86.834)\tPrec@5 100.000 (99.190)\n",
      "Epoch: [165][90/97], lr: 0.00010\tTime 0.318 (0.328)\tData 0.000 (0.020)\tLoss 5.0759 (3.2171)\tPrec@1 81.250 (87.002)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [165][96/97], lr: 0.00010\tTime 0.312 (0.328)\tData 0.000 (0.020)\tLoss 4.9211 (3.2349)\tPrec@1 85.593 (87.039)\tPrec@5 99.153 (99.218)\n",
      "Gated Network Weight Gate= Flip:0.54, Sc:0.46\n",
      "Test: [0/100]\tTime 0.300 (0.300)\tLoss 4.4052 (4.4052)\tPrec@1 81.000 (81.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.094)\tLoss 4.5444 (5.6182)\tPrec@1 81.000 (79.909)\tPrec@5 99.000 (99.000)\n",
      "Test: [20/100]\tTime 0.074 (0.084)\tLoss 6.2982 (5.6585)\tPrec@1 76.000 (79.000)\tPrec@5 98.000 (98.619)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 6.8266 (5.8049)\tPrec@1 78.000 (78.677)\tPrec@5 97.000 (98.548)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.6742 (5.8613)\tPrec@1 78.000 (78.561)\tPrec@5 98.000 (98.463)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.7692 (5.8589)\tPrec@1 83.000 (79.157)\tPrec@5 99.000 (98.510)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.0765 (5.8252)\tPrec@1 80.000 (79.016)\tPrec@5 99.000 (98.574)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.3320 (5.8064)\tPrec@1 78.000 (79.155)\tPrec@5 99.000 (98.648)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.1589 (5.7509)\tPrec@1 82.000 (79.469)\tPrec@5 99.000 (98.691)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 4.0072 (5.7838)\tPrec@1 87.000 (79.286)\tPrec@5 99.000 (98.571)\n",
      "val Results: Prec@1 79.200 Prec@5 98.580 Loss 5.79578\n",
      "val Class Accuracy: [0.896,0.952,0.775,0.665,0.813,0.739,0.826,0.706,0.734,0.814]\n",
      "Best Prec@1: 79.200\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [166][0/97], lr: 0.00010\tTime 0.464 (0.464)\tData 0.261 (0.261)\tLoss 3.0721 (3.0721)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [166][10/97], lr: 0.00010\tTime 0.323 (0.345)\tData 0.000 (0.038)\tLoss 2.1466 (3.4431)\tPrec@1 90.625 (86.790)\tPrec@5 99.219 (99.361)\n",
      "Epoch: [166][20/97], lr: 0.00010\tTime 0.331 (0.336)\tData 0.000 (0.028)\tLoss 4.2069 (3.1092)\tPrec@1 85.938 (87.314)\tPrec@5 100.000 (99.368)\n",
      "Epoch: [166][30/97], lr: 0.00010\tTime 0.321 (0.334)\tData 0.000 (0.025)\tLoss 4.6723 (2.9673)\tPrec@1 82.812 (87.424)\tPrec@5 100.000 (99.521)\n",
      "Epoch: [166][40/97], lr: 0.00010\tTime 0.322 (0.332)\tData 0.000 (0.023)\tLoss 4.9612 (2.9457)\tPrec@1 87.500 (87.652)\tPrec@5 100.000 (99.524)\n",
      "Epoch: [166][50/97], lr: 0.00010\tTime 0.322 (0.330)\tData 0.000 (0.022)\tLoss 1.8021 (2.8521)\tPrec@1 87.500 (87.760)\tPrec@5 99.219 (99.479)\n",
      "Epoch: [166][60/97], lr: 0.00010\tTime 0.321 (0.330)\tData 0.000 (0.021)\tLoss 2.0722 (2.8926)\tPrec@1 88.281 (87.833)\tPrec@5 100.000 (99.501)\n",
      "Epoch: [166][70/97], lr: 0.00010\tTime 0.324 (0.329)\tData 0.000 (0.020)\tLoss 2.5536 (2.9861)\tPrec@1 89.062 (87.830)\tPrec@5 99.219 (99.461)\n",
      "Epoch: [166][80/97], lr: 0.00010\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 2.0065 (2.9967)\tPrec@1 87.500 (87.780)\tPrec@5 99.219 (99.479)\n",
      "Epoch: [166][90/97], lr: 0.00010\tTime 0.316 (0.328)\tData 0.000 (0.020)\tLoss 3.0654 (2.9922)\tPrec@1 84.375 (87.672)\tPrec@5 99.219 (99.442)\n",
      "Epoch: [166][96/97], lr: 0.00010\tTime 0.312 (0.328)\tData 0.000 (0.020)\tLoss 2.9727 (2.9980)\tPrec@1 84.746 (87.691)\tPrec@5 100.000 (99.444)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.261 (0.261)\tLoss 3.7085 (3.7085)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 3.7720 (5.1073)\tPrec@1 82.000 (80.636)\tPrec@5 100.000 (99.182)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.7320 (5.1887)\tPrec@1 77.000 (79.476)\tPrec@5 99.000 (98.810)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 5.9609 (5.3263)\tPrec@1 80.000 (79.161)\tPrec@5 97.000 (98.710)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 3.9230 (5.4116)\tPrec@1 81.000 (79.146)\tPrec@5 99.000 (98.634)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.2826 (5.3664)\tPrec@1 83.000 (79.647)\tPrec@5 99.000 (98.627)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.6164 (5.3368)\tPrec@1 79.000 (79.705)\tPrec@5 100.000 (98.738)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 4.9231 (5.2903)\tPrec@1 78.000 (79.831)\tPrec@5 100.000 (98.789)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 5.4440 (5.2249)\tPrec@1 81.000 (80.074)\tPrec@5 99.000 (98.802)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 3.1649 (5.2278)\tPrec@1 87.000 (79.912)\tPrec@5 100.000 (98.758)\n",
      "val Results: Prec@1 79.930 Prec@5 98.750 Loss 5.23557\n",
      "val Class Accuracy: [0.868,0.948,0.804,0.650,0.796,0.710,0.853,0.740,0.806,0.818]\n",
      "Best Prec@1: 79.930\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [167][0/97], lr: 0.00010\tTime 0.509 (0.509)\tData 0.298 (0.298)\tLoss 2.0311 (2.0311)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [167][10/97], lr: 0.00010\tTime 0.327 (0.348)\tData 0.000 (0.041)\tLoss 2.4039 (2.5600)\tPrec@1 85.938 (87.855)\tPrec@5 98.438 (99.077)\n",
      "Epoch: [167][20/97], lr: 0.00010\tTime 0.319 (0.337)\tData 0.000 (0.030)\tLoss 5.4695 (3.0489)\tPrec@1 89.844 (88.095)\tPrec@5 99.219 (99.330)\n",
      "Epoch: [167][30/97], lr: 0.00010\tTime 0.323 (0.334)\tData 0.000 (0.026)\tLoss 2.0934 (2.9732)\tPrec@1 90.625 (88.054)\tPrec@5 100.000 (99.294)\n",
      "Epoch: [167][40/97], lr: 0.00010\tTime 0.328 (0.332)\tData 0.000 (0.024)\tLoss 2.2021 (3.0064)\tPrec@1 91.406 (88.110)\tPrec@5 100.000 (99.257)\n",
      "Epoch: [167][50/97], lr: 0.00010\tTime 0.323 (0.330)\tData 0.000 (0.022)\tLoss 2.7086 (2.9559)\tPrec@1 86.719 (87.699)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [167][60/97], lr: 0.00010\tTime 0.325 (0.330)\tData 0.000 (0.021)\tLoss 2.3007 (3.0037)\tPrec@1 89.844 (87.666)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [167][70/97], lr: 0.00010\tTime 0.321 (0.329)\tData 0.000 (0.021)\tLoss 3.8344 (2.9466)\tPrec@1 88.281 (87.709)\tPrec@5 99.219 (99.197)\n",
      "Epoch: [167][80/97], lr: 0.00010\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 3.5845 (2.9988)\tPrec@1 85.156 (87.413)\tPrec@5 99.219 (99.228)\n",
      "Epoch: [167][90/97], lr: 0.00010\tTime 0.320 (0.328)\tData 0.000 (0.020)\tLoss 4.0193 (2.9642)\tPrec@1 90.625 (87.414)\tPrec@5 99.219 (99.202)\n",
      "Epoch: [167][96/97], lr: 0.00010\tTime 0.312 (0.328)\tData 0.000 (0.020)\tLoss 1.6836 (2.9787)\tPrec@1 90.678 (87.498)\tPrec@5 100.000 (99.218)\n",
      "Gated Network Weight Gate= Flip:0.54, Sc:0.46\n",
      "Test: [0/100]\tTime 0.264 (0.264)\tLoss 3.9519 (3.9519)\tPrec@1 81.000 (81.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 4.3285 (5.2824)\tPrec@1 81.000 (80.727)\tPrec@5 100.000 (99.091)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.0140 (5.3178)\tPrec@1 77.000 (79.714)\tPrec@5 99.000 (98.762)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.5571 (5.4873)\tPrec@1 79.000 (79.548)\tPrec@5 97.000 (98.645)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.1397 (5.5607)\tPrec@1 80.000 (79.220)\tPrec@5 98.000 (98.512)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.3236 (5.5630)\tPrec@1 83.000 (79.647)\tPrec@5 99.000 (98.549)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.7494 (5.5229)\tPrec@1 80.000 (79.574)\tPrec@5 100.000 (98.639)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 4.2883 (5.4806)\tPrec@1 78.000 (79.535)\tPrec@5 99.000 (98.676)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 5.7068 (5.3954)\tPrec@1 82.000 (79.877)\tPrec@5 99.000 (98.716)\n",
      "Test: [90/100]\tTime 0.074 (0.075)\tLoss 3.8101 (5.4154)\tPrec@1 87.000 (79.648)\tPrec@5 99.000 (98.648)\n",
      "val Results: Prec@1 79.580 Prec@5 98.650 Loss 5.42438\n",
      "val Class Accuracy: [0.891,0.942,0.803,0.647,0.803,0.745,0.828,0.718,0.745,0.836]\n",
      "Best Prec@1: 79.930\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [168][0/97], lr: 0.00010\tTime 0.534 (0.534)\tData 0.304 (0.304)\tLoss 1.7661 (1.7661)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [168][10/97], lr: 0.00010\tTime 0.317 (0.349)\tData 0.000 (0.042)\tLoss 2.7522 (2.8606)\tPrec@1 89.062 (88.210)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [168][20/97], lr: 0.00010\tTime 0.316 (0.339)\tData 0.000 (0.030)\tLoss 2.4261 (2.5117)\tPrec@1 87.500 (88.504)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [168][30/97], lr: 0.00010\tTime 0.323 (0.335)\tData 0.000 (0.026)\tLoss 2.4966 (2.6776)\tPrec@1 85.156 (88.054)\tPrec@5 100.000 (99.496)\n",
      "Epoch: [168][40/97], lr: 0.00010\tTime 0.319 (0.332)\tData 0.000 (0.024)\tLoss 1.6929 (2.8601)\tPrec@1 89.844 (88.205)\tPrec@5 100.000 (99.409)\n",
      "Epoch: [168][50/97], lr: 0.00010\tTime 0.320 (0.331)\tData 0.000 (0.022)\tLoss 2.4658 (2.7953)\tPrec@1 87.500 (88.251)\tPrec@5 99.219 (99.418)\n",
      "Epoch: [168][60/97], lr: 0.00010\tTime 0.323 (0.330)\tData 0.000 (0.021)\tLoss 3.9232 (2.8283)\tPrec@1 87.500 (88.307)\tPrec@5 100.000 (99.424)\n",
      "Epoch: [168][70/97], lr: 0.00010\tTime 0.324 (0.329)\tData 0.000 (0.021)\tLoss 4.0216 (2.8812)\tPrec@1 86.719 (88.347)\tPrec@5 99.219 (99.417)\n",
      "Epoch: [168][80/97], lr: 0.00010\tTime 0.324 (0.329)\tData 0.000 (0.020)\tLoss 3.1689 (2.8919)\tPrec@1 83.594 (88.156)\tPrec@5 99.219 (99.392)\n",
      "Epoch: [168][90/97], lr: 0.00010\tTime 0.323 (0.328)\tData 0.000 (0.020)\tLoss 1.6312 (2.9303)\tPrec@1 91.406 (88.307)\tPrec@5 97.656 (99.348)\n",
      "Epoch: [168][96/97], lr: 0.00010\tTime 0.312 (0.328)\tData 0.000 (0.020)\tLoss 4.1928 (2.9487)\tPrec@1 84.746 (88.183)\tPrec@5 99.153 (99.347)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.245 (0.245)\tLoss 3.9139 (3.9139)\tPrec@1 81.000 (81.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 4.1970 (5.0033)\tPrec@1 82.000 (80.909)\tPrec@5 100.000 (98.818)\n",
      "Test: [20/100]\tTime 0.074 (0.082)\tLoss 5.9513 (5.1019)\tPrec@1 76.000 (80.000)\tPrec@5 98.000 (98.667)\n",
      "Test: [30/100]\tTime 0.074 (0.079)\tLoss 6.4621 (5.2414)\tPrec@1 79.000 (80.000)\tPrec@5 97.000 (98.516)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 4.1527 (5.3234)\tPrec@1 80.000 (79.585)\tPrec@5 98.000 (98.439)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 5.0616 (5.3289)\tPrec@1 82.000 (80.020)\tPrec@5 99.000 (98.471)\n",
      "Test: [60/100]\tTime 0.074 (0.076)\tLoss 7.3635 (5.2919)\tPrec@1 80.000 (80.016)\tPrec@5 100.000 (98.557)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 3.9610 (5.2469)\tPrec@1 78.000 (80.099)\tPrec@5 100.000 (98.648)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 5.3227 (5.1560)\tPrec@1 81.000 (80.358)\tPrec@5 99.000 (98.704)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.7295 (5.1906)\tPrec@1 84.000 (80.011)\tPrec@5 98.000 (98.637)\n",
      "val Results: Prec@1 79.900 Prec@5 98.630 Loss 5.19964\n",
      "val Class Accuracy: [0.899,0.925,0.779,0.676,0.815,0.743,0.837,0.719,0.749,0.848]\n",
      "Best Prec@1: 79.930\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [169][0/97], lr: 0.00010\tTime 0.489 (0.489)\tData 0.281 (0.281)\tLoss 2.3527 (2.3527)\tPrec@1 92.969 (92.969)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [169][10/97], lr: 0.00010\tTime 0.324 (0.346)\tData 0.000 (0.039)\tLoss 2.4559 (2.4567)\tPrec@1 87.500 (88.849)\tPrec@5 100.000 (99.787)\n",
      "Epoch: [169][20/97], lr: 0.00010\tTime 0.359 (0.338)\tData 0.000 (0.029)\tLoss 2.3590 (2.4550)\tPrec@1 89.062 (88.170)\tPrec@5 99.219 (99.628)\n",
      "Epoch: [169][30/97], lr: 0.00010\tTime 0.322 (0.334)\tData 0.000 (0.025)\tLoss 1.9368 (2.5421)\tPrec@1 89.062 (88.130)\tPrec@5 99.219 (99.471)\n",
      "Epoch: [169][40/97], lr: 0.00010\tTime 0.323 (0.332)\tData 0.000 (0.023)\tLoss 3.8532 (2.6471)\tPrec@1 85.938 (87.786)\tPrec@5 98.438 (99.466)\n",
      "Epoch: [169][50/97], lr: 0.00010\tTime 0.320 (0.330)\tData 0.000 (0.022)\tLoss 2.3368 (2.6364)\tPrec@1 89.844 (87.990)\tPrec@5 99.219 (99.464)\n",
      "Epoch: [169][60/97], lr: 0.00010\tTime 0.321 (0.330)\tData 0.000 (0.021)\tLoss 2.8398 (2.7363)\tPrec@1 90.625 (88.115)\tPrec@5 100.000 (99.411)\n",
      "Epoch: [169][70/97], lr: 0.00010\tTime 0.323 (0.329)\tData 0.000 (0.020)\tLoss 2.3756 (2.7894)\tPrec@1 85.938 (88.226)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [169][80/97], lr: 0.00010\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 2.3368 (2.7874)\tPrec@1 87.500 (88.252)\tPrec@5 99.219 (99.344)\n",
      "Epoch: [169][90/97], lr: 0.00010\tTime 0.318 (0.328)\tData 0.000 (0.020)\tLoss 3.3713 (2.7869)\tPrec@1 84.375 (88.367)\tPrec@5 99.219 (99.382)\n",
      "Epoch: [169][96/97], lr: 0.00010\tTime 0.310 (0.327)\tData 0.000 (0.020)\tLoss 1.2748 (2.8562)\tPrec@1 91.525 (88.360)\tPrec@5 100.000 (99.404)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.302 (0.302)\tLoss 3.9149 (3.9149)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.094)\tLoss 4.1489 (5.0114)\tPrec@1 82.000 (81.273)\tPrec@5 100.000 (98.818)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.0768 (5.1151)\tPrec@1 77.000 (79.952)\tPrec@5 98.000 (98.619)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.1040 (5.2342)\tPrec@1 79.000 (79.968)\tPrec@5 98.000 (98.548)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.1382 (5.3045)\tPrec@1 80.000 (79.878)\tPrec@5 98.000 (98.512)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 4.9167 (5.2994)\tPrec@1 86.000 (80.314)\tPrec@5 99.000 (98.529)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 7.2895 (5.2630)\tPrec@1 80.000 (80.180)\tPrec@5 100.000 (98.607)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 4.1753 (5.2237)\tPrec@1 79.000 (80.282)\tPrec@5 100.000 (98.676)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 5.2598 (5.1392)\tPrec@1 83.000 (80.481)\tPrec@5 99.000 (98.728)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.6883 (5.1640)\tPrec@1 88.000 (80.198)\tPrec@5 99.000 (98.637)\n",
      "val Results: Prec@1 80.050 Prec@5 98.640 Loss 5.18047\n",
      "val Class Accuracy: [0.899,0.930,0.797,0.655,0.824,0.733,0.849,0.710,0.765,0.843]\n",
      "Best Prec@1: 80.050\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [170][0/97], lr: 0.00010\tTime 0.485 (0.485)\tData 0.262 (0.262)\tLoss 1.7077 (1.7077)\tPrec@1 92.969 (92.969)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [170][10/97], lr: 0.00010\tTime 0.327 (0.346)\tData 0.000 (0.039)\tLoss 3.5781 (2.6892)\tPrec@1 88.281 (88.281)\tPrec@5 98.438 (99.361)\n",
      "Epoch: [170][20/97], lr: 0.00010\tTime 0.322 (0.335)\tData 0.000 (0.028)\tLoss 2.6355 (2.9846)\tPrec@1 85.938 (87.946)\tPrec@5 99.219 (99.442)\n",
      "Epoch: [170][30/97], lr: 0.00010\tTime 0.323 (0.332)\tData 0.000 (0.025)\tLoss 2.1230 (2.9461)\tPrec@1 93.750 (88.105)\tPrec@5 100.000 (99.496)\n",
      "Epoch: [170][40/97], lr: 0.00010\tTime 0.329 (0.330)\tData 0.000 (0.023)\tLoss 2.3486 (2.8932)\tPrec@1 89.062 (87.767)\tPrec@5 100.000 (99.466)\n",
      "Epoch: [170][50/97], lr: 0.00010\tTime 0.323 (0.329)\tData 0.000 (0.022)\tLoss 1.7202 (2.9309)\tPrec@1 88.281 (87.960)\tPrec@5 98.438 (99.479)\n",
      "Epoch: [170][60/97], lr: 0.00010\tTime 0.323 (0.329)\tData 0.000 (0.021)\tLoss 3.3948 (3.0012)\tPrec@1 87.500 (87.859)\tPrec@5 98.438 (99.462)\n",
      "Epoch: [170][70/97], lr: 0.00010\tTime 0.327 (0.328)\tData 0.000 (0.020)\tLoss 1.3311 (2.9498)\tPrec@1 92.188 (88.061)\tPrec@5 98.438 (99.439)\n",
      "Epoch: [170][80/97], lr: 0.00010\tTime 0.337 (0.329)\tData 0.000 (0.020)\tLoss 2.5701 (2.9811)\tPrec@1 89.062 (87.828)\tPrec@5 100.000 (99.421)\n",
      "Epoch: [170][90/97], lr: 0.00010\tTime 0.339 (0.329)\tData 0.000 (0.020)\tLoss 3.5087 (2.9043)\tPrec@1 89.062 (88.067)\tPrec@5 99.219 (99.433)\n",
      "Epoch: [170][96/97], lr: 0.00010\tTime 0.335 (0.332)\tData 0.000 (0.020)\tLoss 2.4636 (2.8968)\tPrec@1 86.441 (88.022)\tPrec@5 98.305 (99.404)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.542 (0.542)\tLoss 4.4138 (4.4138)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.123)\tLoss 4.9285 (5.8161)\tPrec@1 82.000 (80.091)\tPrec@5 99.000 (98.727)\n",
      "Test: [20/100]\tTime 0.075 (0.105)\tLoss 6.3287 (5.8753)\tPrec@1 75.000 (79.571)\tPrec@5 98.000 (98.476)\n",
      "Test: [30/100]\tTime 0.074 (0.100)\tLoss 6.6776 (6.0698)\tPrec@1 80.000 (79.387)\tPrec@5 96.000 (98.387)\n",
      "Test: [40/100]\tTime 0.097 (0.099)\tLoss 4.5891 (6.1412)\tPrec@1 80.000 (79.171)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.075 (0.097)\tLoss 5.6518 (6.1382)\tPrec@1 85.000 (79.647)\tPrec@5 99.000 (98.314)\n",
      "Test: [60/100]\tTime 0.074 (0.095)\tLoss 7.8793 (6.0675)\tPrec@1 79.000 (79.656)\tPrec@5 100.000 (98.443)\n",
      "Test: [70/100]\tTime 0.082 (0.095)\tLoss 4.9430 (6.0375)\tPrec@1 78.000 (79.803)\tPrec@5 100.000 (98.521)\n",
      "Test: [80/100]\tTime 0.076 (0.093)\tLoss 6.5082 (5.9664)\tPrec@1 84.000 (80.160)\tPrec@5 99.000 (98.580)\n",
      "Test: [90/100]\tTime 0.074 (0.091)\tLoss 4.2513 (5.9985)\tPrec@1 87.000 (79.912)\tPrec@5 99.000 (98.538)\n",
      "val Results: Prec@1 79.770 Prec@5 98.560 Loss 6.03901\n",
      "val Class Accuracy: [0.914,0.956,0.799,0.672,0.802,0.746,0.833,0.734,0.753,0.768]\n",
      "Best Prec@1: 80.050\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [171][0/97], lr: 0.00010\tTime 0.790 (0.790)\tData 0.419 (0.419)\tLoss 1.7338 (1.7338)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [171][10/97], lr: 0.00010\tTime 0.322 (0.408)\tData 0.000 (0.049)\tLoss 1.4723 (3.1504)\tPrec@1 91.406 (88.778)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [171][20/97], lr: 0.00010\tTime 0.321 (0.374)\tData 0.000 (0.034)\tLoss 3.1015 (2.7214)\tPrec@1 88.281 (88.690)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [171][30/97], lr: 0.00010\tTime 0.321 (0.361)\tData 0.000 (0.028)\tLoss 3.8642 (2.8688)\tPrec@1 94.531 (88.634)\tPrec@5 100.000 (99.546)\n",
      "Epoch: [171][40/97], lr: 0.00010\tTime 0.333 (0.354)\tData 0.000 (0.026)\tLoss 2.4348 (2.8389)\tPrec@1 91.406 (88.491)\tPrec@5 99.219 (99.505)\n",
      "Epoch: [171][50/97], lr: 0.00010\tTime 0.321 (0.350)\tData 0.000 (0.024)\tLoss 2.0497 (2.8460)\tPrec@1 89.062 (88.159)\tPrec@5 98.438 (99.464)\n",
      "Epoch: [171][60/97], lr: 0.00010\tTime 0.323 (0.347)\tData 0.000 (0.023)\tLoss 1.5984 (2.9297)\tPrec@1 89.062 (88.217)\tPrec@5 99.219 (99.449)\n",
      "Epoch: [171][70/97], lr: 0.00010\tTime 0.322 (0.345)\tData 0.000 (0.022)\tLoss 4.0590 (2.9496)\tPrec@1 84.375 (88.006)\tPrec@5 100.000 (99.439)\n",
      "Epoch: [171][80/97], lr: 0.00010\tTime 0.323 (0.344)\tData 0.000 (0.021)\tLoss 3.8202 (2.9555)\tPrec@1 87.500 (88.002)\tPrec@5 100.000 (99.431)\n",
      "Epoch: [171][90/97], lr: 0.00010\tTime 0.322 (0.343)\tData 0.000 (0.021)\tLoss 1.9663 (2.8279)\tPrec@1 88.281 (88.230)\tPrec@5 100.000 (99.451)\n",
      "Epoch: [171][96/97], lr: 0.00010\tTime 0.322 (0.342)\tData 0.000 (0.021)\tLoss 2.2719 (2.7985)\tPrec@1 86.441 (88.199)\tPrec@5 100.000 (99.460)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.314 (0.314)\tLoss 3.9188 (3.9188)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.095)\tLoss 4.4977 (5.3958)\tPrec@1 81.000 (80.455)\tPrec@5 100.000 (98.727)\n",
      "Test: [20/100]\tTime 0.074 (0.085)\tLoss 6.0402 (5.4662)\tPrec@1 74.000 (79.524)\tPrec@5 98.000 (98.571)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 6.4398 (5.6365)\tPrec@1 80.000 (79.581)\tPrec@5 97.000 (98.452)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.3031 (5.7095)\tPrec@1 82.000 (79.537)\tPrec@5 98.000 (98.268)\n",
      "Test: [50/100]\tTime 0.075 (0.078)\tLoss 5.0985 (5.6751)\tPrec@1 81.000 (80.000)\tPrec@5 99.000 (98.333)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 7.5303 (5.6138)\tPrec@1 80.000 (80.033)\tPrec@5 100.000 (98.426)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 4.6270 (5.5860)\tPrec@1 80.000 (80.085)\tPrec@5 100.000 (98.507)\n",
      "Test: [80/100]\tTime 0.075 (0.077)\tLoss 5.7776 (5.5251)\tPrec@1 82.000 (80.321)\tPrec@5 99.000 (98.580)\n",
      "Test: [90/100]\tTime 0.075 (0.077)\tLoss 3.5319 (5.5532)\tPrec@1 87.000 (80.132)\tPrec@5 99.000 (98.527)\n",
      "val Results: Prec@1 80.090 Prec@5 98.550 Loss 5.59067\n",
      "val Class Accuracy: [0.899,0.955,0.801,0.678,0.813,0.734,0.818,0.747,0.778,0.786]\n",
      "Best Prec@1: 80.090\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [172][0/97], lr: 0.00010\tTime 0.652 (0.652)\tData 0.373 (0.373)\tLoss 1.0814 (1.0814)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [172][10/97], lr: 0.00010\tTime 0.326 (0.384)\tData 0.000 (0.045)\tLoss 3.9131 (2.1842)\tPrec@1 85.156 (89.347)\tPrec@5 99.219 (99.645)\n",
      "Epoch: [172][20/97], lr: 0.00010\tTime 0.330 (0.361)\tData 0.000 (0.032)\tLoss 3.1583 (2.7116)\tPrec@1 85.938 (88.356)\tPrec@5 100.000 (99.591)\n",
      "Epoch: [172][30/97], lr: 0.00010\tTime 0.330 (0.353)\tData 0.000 (0.027)\tLoss 3.3709 (2.8487)\tPrec@1 88.281 (88.231)\tPrec@5 100.000 (99.546)\n",
      "Epoch: [172][40/97], lr: 0.00010\tTime 0.321 (0.350)\tData 0.000 (0.024)\tLoss 5.2492 (2.8949)\tPrec@1 83.594 (88.186)\tPrec@5 100.000 (99.562)\n",
      "Epoch: [172][50/97], lr: 0.00010\tTime 0.333 (0.347)\tData 0.000 (0.023)\tLoss 2.2566 (2.8844)\tPrec@1 85.156 (87.898)\tPrec@5 100.000 (99.464)\n",
      "Epoch: [172][60/97], lr: 0.00010\tTime 0.322 (0.345)\tData 0.000 (0.022)\tLoss 3.1970 (2.8710)\tPrec@1 89.844 (87.846)\tPrec@5 99.219 (99.424)\n",
      "Epoch: [172][70/97], lr: 0.00010\tTime 0.331 (0.344)\tData 0.000 (0.021)\tLoss 1.4528 (2.8228)\tPrec@1 88.281 (88.028)\tPrec@5 100.000 (99.417)\n",
      "Epoch: [172][80/97], lr: 0.00010\tTime 0.324 (0.343)\tData 0.000 (0.021)\tLoss 2.6293 (2.8143)\tPrec@1 89.062 (88.127)\tPrec@5 99.219 (99.402)\n",
      "Epoch: [172][90/97], lr: 0.00010\tTime 0.322 (0.342)\tData 0.000 (0.020)\tLoss 2.1225 (2.7918)\tPrec@1 92.188 (88.152)\tPrec@5 99.219 (99.399)\n",
      "Epoch: [172][96/97], lr: 0.00010\tTime 0.336 (0.342)\tData 0.000 (0.021)\tLoss 4.6386 (2.8369)\tPrec@1 84.746 (87.998)\tPrec@5 98.305 (99.395)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.375 (0.375)\tLoss 3.8381 (3.8381)\tPrec@1 83.000 (83.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.101)\tLoss 4.5772 (5.4470)\tPrec@1 83.000 (80.818)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.074 (0.088)\tLoss 5.9958 (5.5035)\tPrec@1 76.000 (79.905)\tPrec@5 98.000 (98.238)\n",
      "Test: [30/100]\tTime 0.073 (0.083)\tLoss 6.5021 (5.6816)\tPrec@1 79.000 (79.871)\tPrec@5 97.000 (98.258)\n",
      "Test: [40/100]\tTime 0.074 (0.081)\tLoss 4.2294 (5.7390)\tPrec@1 84.000 (79.805)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.074 (0.080)\tLoss 5.4079 (5.7036)\tPrec@1 85.000 (80.471)\tPrec@5 99.000 (98.275)\n",
      "Test: [60/100]\tTime 0.074 (0.079)\tLoss 7.9677 (5.6548)\tPrec@1 80.000 (80.377)\tPrec@5 100.000 (98.410)\n",
      "Test: [70/100]\tTime 0.075 (0.078)\tLoss 4.8883 (5.6215)\tPrec@1 82.000 (80.507)\tPrec@5 100.000 (98.493)\n",
      "Test: [80/100]\tTime 0.075 (0.078)\tLoss 6.0990 (5.5577)\tPrec@1 81.000 (80.753)\tPrec@5 99.000 (98.580)\n",
      "Test: [90/100]\tTime 0.074 (0.078)\tLoss 3.5509 (5.5727)\tPrec@1 88.000 (80.549)\tPrec@5 99.000 (98.505)\n",
      "val Results: Prec@1 80.420 Prec@5 98.500 Loss 5.61210\n",
      "val Class Accuracy: [0.899,0.953,0.798,0.684,0.837,0.733,0.829,0.723,0.804,0.782]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [173][0/97], lr: 0.00010\tTime 0.620 (0.620)\tData 0.338 (0.338)\tLoss 4.8005 (4.8005)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [173][10/97], lr: 0.00010\tTime 0.333 (0.381)\tData 0.000 (0.042)\tLoss 1.8710 (2.4381)\tPrec@1 88.281 (88.920)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [173][20/97], lr: 0.00010\tTime 0.326 (0.358)\tData 0.000 (0.030)\tLoss 2.0157 (2.8142)\tPrec@1 91.406 (88.467)\tPrec@5 100.000 (99.554)\n",
      "Epoch: [173][30/97], lr: 0.00010\tTime 0.321 (0.350)\tData 0.000 (0.026)\tLoss 3.6829 (2.7612)\tPrec@1 83.594 (88.054)\tPrec@5 99.219 (99.521)\n",
      "Epoch: [173][40/97], lr: 0.00010\tTime 0.326 (0.347)\tData 0.000 (0.024)\tLoss 1.6070 (2.6131)\tPrec@1 85.156 (88.167)\tPrec@5 96.875 (99.409)\n",
      "Epoch: [173][50/97], lr: 0.00010\tTime 0.321 (0.344)\tData 0.000 (0.022)\tLoss 2.2961 (2.6612)\tPrec@1 89.062 (88.097)\tPrec@5 98.438 (99.311)\n",
      "Epoch: [173][60/97], lr: 0.00010\tTime 0.325 (0.343)\tData 0.000 (0.021)\tLoss 2.1151 (2.7264)\tPrec@1 88.281 (88.051)\tPrec@5 100.000 (99.334)\n",
      "Epoch: [173][70/97], lr: 0.00010\tTime 0.324 (0.342)\tData 0.000 (0.021)\tLoss 3.5166 (2.7564)\tPrec@1 89.062 (88.094)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [173][80/97], lr: 0.00010\tTime 0.320 (0.340)\tData 0.000 (0.020)\tLoss 1.3958 (2.6785)\tPrec@1 86.719 (88.320)\tPrec@5 100.000 (99.441)\n",
      "Epoch: [173][90/97], lr: 0.00010\tTime 0.318 (0.340)\tData 0.000 (0.020)\tLoss 2.1222 (2.7067)\tPrec@1 91.406 (88.273)\tPrec@5 100.000 (99.451)\n",
      "Epoch: [173][96/97], lr: 0.00010\tTime 0.322 (0.340)\tData 0.000 (0.020)\tLoss 2.7360 (2.7070)\tPrec@1 86.441 (88.288)\tPrec@5 99.153 (99.460)\n",
      "Gated Network Weight Gate= Flip:0.54, Sc:0.46\n",
      "Test: [0/100]\tTime 0.363 (0.363)\tLoss 3.9226 (3.9226)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.100)\tLoss 4.8730 (5.6357)\tPrec@1 82.000 (81.000)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.074 (0.087)\tLoss 6.1095 (5.7344)\tPrec@1 78.000 (79.952)\tPrec@5 98.000 (98.429)\n",
      "Test: [30/100]\tTime 0.074 (0.083)\tLoss 6.9754 (5.8777)\tPrec@1 80.000 (79.806)\tPrec@5 97.000 (98.323)\n",
      "Test: [40/100]\tTime 0.075 (0.081)\tLoss 4.4539 (5.9282)\tPrec@1 83.000 (79.707)\tPrec@5 98.000 (98.268)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 5.3088 (5.9070)\tPrec@1 85.000 (80.137)\tPrec@5 99.000 (98.314)\n",
      "Test: [60/100]\tTime 0.074 (0.079)\tLoss 8.9184 (5.8700)\tPrec@1 80.000 (80.164)\tPrec@5 100.000 (98.410)\n",
      "Test: [70/100]\tTime 0.075 (0.078)\tLoss 5.4039 (5.8478)\tPrec@1 77.000 (80.239)\tPrec@5 99.000 (98.479)\n",
      "Test: [80/100]\tTime 0.075 (0.078)\tLoss 6.5527 (5.7992)\tPrec@1 82.000 (80.469)\tPrec@5 98.000 (98.531)\n",
      "Test: [90/100]\tTime 0.075 (0.077)\tLoss 4.0399 (5.8247)\tPrec@1 87.000 (80.176)\tPrec@5 99.000 (98.473)\n",
      "val Results: Prec@1 80.120 Prec@5 98.500 Loss 5.86414\n",
      "val Class Accuracy: [0.904,0.964,0.799,0.667,0.824,0.748,0.830,0.734,0.754,0.788]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [174][0/97], lr: 0.00010\tTime 0.637 (0.637)\tData 0.348 (0.348)\tLoss 7.0813 (7.0813)\tPrec@1 85.156 (85.156)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [174][10/97], lr: 0.00010\tTime 0.320 (0.377)\tData 0.000 (0.045)\tLoss 3.7395 (2.9146)\tPrec@1 86.719 (87.784)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [174][20/97], lr: 0.00010\tTime 0.323 (0.354)\tData 0.000 (0.031)\tLoss 3.4577 (2.6370)\tPrec@1 88.281 (88.132)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [174][30/97], lr: 0.00010\tTime 0.322 (0.347)\tData 0.000 (0.027)\tLoss 2.2500 (2.5525)\tPrec@1 89.844 (87.928)\tPrec@5 98.438 (99.320)\n",
      "Epoch: [174][40/97], lr: 0.00010\tTime 0.325 (0.345)\tData 0.000 (0.024)\tLoss 1.4543 (2.5781)\tPrec@1 89.062 (87.900)\tPrec@5 99.219 (99.352)\n",
      "Epoch: [174][50/97], lr: 0.00010\tTime 0.330 (0.342)\tData 0.000 (0.023)\tLoss 2.8960 (2.6418)\tPrec@1 89.062 (88.097)\tPrec@5 100.000 (99.372)\n",
      "Epoch: [174][60/97], lr: 0.00010\tTime 0.321 (0.341)\tData 0.000 (0.022)\tLoss 3.6817 (2.6500)\tPrec@1 86.719 (88.179)\tPrec@5 100.000 (99.385)\n",
      "Epoch: [174][70/97], lr: 0.00010\tTime 0.322 (0.340)\tData 0.000 (0.021)\tLoss 3.5461 (2.6223)\tPrec@1 86.719 (88.226)\tPrec@5 99.219 (99.373)\n",
      "Epoch: [174][80/97], lr: 0.00010\tTime 0.326 (0.339)\tData 0.000 (0.021)\tLoss 4.6321 (2.6864)\tPrec@1 88.281 (88.272)\tPrec@5 98.438 (99.392)\n",
      "Epoch: [174][90/97], lr: 0.00010\tTime 0.320 (0.339)\tData 0.000 (0.020)\tLoss 1.1449 (2.6684)\tPrec@1 91.406 (88.333)\tPrec@5 99.219 (99.373)\n",
      "Epoch: [174][96/97], lr: 0.00010\tTime 0.323 (0.338)\tData 0.000 (0.021)\tLoss 2.5710 (2.6542)\tPrec@1 86.441 (88.457)\tPrec@5 100.000 (99.395)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.311 (0.311)\tLoss 3.8860 (3.8860)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.095)\tLoss 4.3369 (5.2455)\tPrec@1 82.000 (80.727)\tPrec@5 99.000 (98.818)\n",
      "Test: [20/100]\tTime 0.074 (0.085)\tLoss 6.1402 (5.3661)\tPrec@1 77.000 (79.714)\tPrec@5 98.000 (98.667)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 6.9194 (5.4962)\tPrec@1 79.000 (79.806)\tPrec@5 97.000 (98.516)\n",
      "Test: [40/100]\tTime 0.074 (0.080)\tLoss 4.2400 (5.5524)\tPrec@1 83.000 (79.805)\tPrec@5 98.000 (98.415)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 4.9852 (5.5423)\tPrec@1 84.000 (80.275)\tPrec@5 99.000 (98.431)\n",
      "Test: [60/100]\tTime 0.076 (0.078)\tLoss 8.2405 (5.5267)\tPrec@1 79.000 (80.213)\tPrec@5 100.000 (98.475)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 4.8681 (5.4927)\tPrec@1 79.000 (80.324)\tPrec@5 100.000 (98.535)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.1239 (5.4265)\tPrec@1 81.000 (80.543)\tPrec@5 98.000 (98.556)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 3.6902 (5.4634)\tPrec@1 87.000 (80.330)\tPrec@5 99.000 (98.516)\n",
      "val Results: Prec@1 80.260 Prec@5 98.540 Loss 5.49134\n",
      "val Class Accuracy: [0.907,0.952,0.802,0.695,0.811,0.727,0.833,0.725,0.751,0.823]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [175][0/97], lr: 0.00010\tTime 0.643 (0.643)\tData 0.376 (0.376)\tLoss 1.4377 (1.4377)\tPrec@1 92.188 (92.188)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [175][10/97], lr: 0.00010\tTime 0.327 (0.384)\tData 0.000 (0.047)\tLoss 4.1827 (2.3331)\tPrec@1 85.156 (89.205)\tPrec@5 98.438 (99.361)\n",
      "Epoch: [175][20/97], lr: 0.00010\tTime 0.323 (0.360)\tData 0.000 (0.033)\tLoss 2.4329 (2.3601)\tPrec@1 88.281 (88.579)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [175][30/97], lr: 0.00010\tTime 0.321 (0.352)\tData 0.000 (0.028)\tLoss 4.0907 (2.6223)\tPrec@1 90.625 (88.584)\tPrec@5 100.000 (99.446)\n",
      "Epoch: [175][40/97], lr: 0.00010\tTime 0.327 (0.348)\tData 0.000 (0.025)\tLoss 3.6295 (2.7264)\tPrec@1 88.281 (88.453)\tPrec@5 99.219 (99.352)\n",
      "Epoch: [175][50/97], lr: 0.00010\tTime 0.322 (0.345)\tData 0.000 (0.024)\tLoss 2.1121 (2.7081)\tPrec@1 89.844 (88.373)\tPrec@5 100.000 (99.418)\n",
      "Epoch: [175][60/97], lr: 0.00010\tTime 0.322 (0.343)\tData 0.000 (0.022)\tLoss 2.3832 (2.6021)\tPrec@1 89.062 (88.665)\tPrec@5 99.219 (99.385)\n",
      "Epoch: [175][70/97], lr: 0.00010\tTime 0.329 (0.342)\tData 0.000 (0.022)\tLoss 2.2652 (2.6402)\tPrec@1 92.188 (88.754)\tPrec@5 100.000 (99.406)\n",
      "Epoch: [175][80/97], lr: 0.00010\tTime 0.322 (0.341)\tData 0.000 (0.021)\tLoss 2.1240 (2.6333)\tPrec@1 92.188 (88.937)\tPrec@5 99.219 (99.450)\n",
      "Epoch: [175][90/97], lr: 0.00010\tTime 0.319 (0.340)\tData 0.000 (0.021)\tLoss 1.8989 (2.6021)\tPrec@1 89.844 (89.037)\tPrec@5 99.219 (99.468)\n",
      "Epoch: [175][96/97], lr: 0.00010\tTime 0.317 (0.339)\tData 0.000 (0.021)\tLoss 2.0334 (2.5695)\tPrec@1 89.831 (89.094)\tPrec@5 99.153 (99.460)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.287 (0.287)\tLoss 4.5979 (4.5979)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.093)\tLoss 5.0388 (6.1398)\tPrec@1 81.000 (80.364)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.074 (0.084)\tLoss 6.2201 (6.1882)\tPrec@1 79.000 (79.667)\tPrec@5 98.000 (98.381)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 6.8667 (6.3403)\tPrec@1 80.000 (79.581)\tPrec@5 96.000 (98.290)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.6330 (6.3784)\tPrec@1 84.000 (79.585)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 6.0050 (6.3864)\tPrec@1 84.000 (80.137)\tPrec@5 99.000 (98.196)\n",
      "Test: [60/100]\tTime 0.075 (0.077)\tLoss 9.1261 (6.3331)\tPrec@1 79.000 (80.016)\tPrec@5 99.000 (98.279)\n",
      "Test: [70/100]\tTime 0.075 (0.077)\tLoss 6.2307 (6.3384)\tPrec@1 79.000 (80.014)\tPrec@5 100.000 (98.366)\n",
      "Test: [80/100]\tTime 0.075 (0.077)\tLoss 7.2477 (6.2936)\tPrec@1 80.000 (80.296)\tPrec@5 98.000 (98.420)\n",
      "Test: [90/100]\tTime 0.075 (0.077)\tLoss 4.2124 (6.3122)\tPrec@1 88.000 (80.011)\tPrec@5 99.000 (98.385)\n",
      "val Results: Prec@1 79.920 Prec@5 98.390 Loss 6.35061\n",
      "val Class Accuracy: [0.908,0.972,0.806,0.696,0.812,0.740,0.835,0.716,0.764,0.743]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [176][0/97], lr: 0.00010\tTime 0.738 (0.738)\tData 0.456 (0.456)\tLoss 4.7408 (4.7408)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [176][10/97], lr: 0.00010\tTime 0.324 (0.395)\tData 0.000 (0.054)\tLoss 1.2997 (2.8665)\tPrec@1 91.406 (89.489)\tPrec@5 98.438 (99.574)\n",
      "Epoch: [176][20/97], lr: 0.00010\tTime 0.319 (0.365)\tData 0.000 (0.036)\tLoss 2.4345 (2.7733)\tPrec@1 88.281 (89.397)\tPrec@5 98.438 (99.368)\n",
      "Epoch: [176][30/97], lr: 0.00010\tTime 0.320 (0.356)\tData 0.000 (0.030)\tLoss 2.5229 (2.7209)\tPrec@1 86.719 (89.289)\tPrec@5 98.438 (99.395)\n",
      "Epoch: [176][40/97], lr: 0.00010\tTime 0.327 (0.351)\tData 0.000 (0.027)\tLoss 2.8313 (2.7619)\tPrec@1 89.844 (89.024)\tPrec@5 99.219 (99.447)\n",
      "Epoch: [176][50/97], lr: 0.00010\tTime 0.322 (0.348)\tData 0.000 (0.025)\tLoss 5.6567 (2.7057)\tPrec@1 91.406 (89.415)\tPrec@5 98.438 (99.449)\n",
      "Epoch: [176][60/97], lr: 0.00010\tTime 0.325 (0.346)\tData 0.000 (0.024)\tLoss 2.7192 (2.7137)\tPrec@1 90.625 (89.178)\tPrec@5 98.438 (99.398)\n",
      "Epoch: [176][70/97], lr: 0.00010\tTime 0.322 (0.345)\tData 0.000 (0.023)\tLoss 2.6252 (2.6753)\tPrec@1 89.844 (89.184)\tPrec@5 98.438 (99.428)\n",
      "Epoch: [176][80/97], lr: 0.00010\tTime 0.325 (0.343)\tData 0.000 (0.022)\tLoss 2.8034 (2.6686)\tPrec@1 90.625 (89.236)\tPrec@5 100.000 (99.421)\n",
      "Epoch: [176][90/97], lr: 0.00010\tTime 0.320 (0.342)\tData 0.000 (0.021)\tLoss 3.3760 (2.6318)\tPrec@1 88.281 (89.217)\tPrec@5 99.219 (99.408)\n",
      "Epoch: [176][96/97], lr: 0.00010\tTime 0.331 (0.342)\tData 0.000 (0.022)\tLoss 5.1699 (2.6475)\tPrec@1 88.136 (89.279)\tPrec@5 100.000 (99.420)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.386 (0.386)\tLoss 4.1009 (4.1009)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.102)\tLoss 4.6997 (5.6042)\tPrec@1 79.000 (81.000)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.074 (0.089)\tLoss 6.4275 (5.7051)\tPrec@1 78.000 (80.000)\tPrec@5 98.000 (98.286)\n",
      "Test: [30/100]\tTime 0.074 (0.084)\tLoss 7.0974 (5.8493)\tPrec@1 80.000 (79.806)\tPrec@5 96.000 (98.226)\n",
      "Test: [40/100]\tTime 0.074 (0.081)\tLoss 4.4508 (5.8973)\tPrec@1 84.000 (79.780)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.074 (0.080)\tLoss 5.4835 (5.8576)\tPrec@1 84.000 (80.333)\tPrec@5 99.000 (98.196)\n",
      "Test: [60/100]\tTime 0.074 (0.079)\tLoss 8.3166 (5.8056)\tPrec@1 81.000 (80.262)\tPrec@5 99.000 (98.262)\n",
      "Test: [70/100]\tTime 0.074 (0.078)\tLoss 5.2251 (5.8025)\tPrec@1 80.000 (80.254)\tPrec@5 100.000 (98.352)\n",
      "Test: [80/100]\tTime 0.075 (0.078)\tLoss 6.7383 (5.7499)\tPrec@1 81.000 (80.531)\tPrec@5 98.000 (98.370)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 3.6875 (5.7794)\tPrec@1 87.000 (80.264)\tPrec@5 99.000 (98.319)\n",
      "val Results: Prec@1 80.190 Prec@5 98.320 Loss 5.81370\n",
      "val Class Accuracy: [0.908,0.958,0.794,0.703,0.803,0.744,0.835,0.737,0.750,0.787]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [177][0/97], lr: 0.00010\tTime 0.558 (0.558)\tData 0.303 (0.303)\tLoss 2.2108 (2.2108)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [177][10/97], lr: 0.00010\tTime 0.321 (0.353)\tData 0.000 (0.042)\tLoss 1.0856 (2.6095)\tPrec@1 93.750 (89.134)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [177][20/97], lr: 0.00010\tTime 0.324 (0.341)\tData 0.000 (0.030)\tLoss 2.6680 (2.4711)\tPrec@1 90.625 (89.174)\tPrec@5 100.000 (99.591)\n",
      "Epoch: [177][30/97], lr: 0.00010\tTime 0.318 (0.335)\tData 0.000 (0.026)\tLoss 3.1574 (2.5861)\tPrec@1 89.062 (89.037)\tPrec@5 100.000 (99.597)\n",
      "Epoch: [177][40/97], lr: 0.00010\tTime 0.319 (0.333)\tData 0.000 (0.024)\tLoss 1.7784 (2.5669)\tPrec@1 91.406 (89.139)\tPrec@5 99.219 (99.524)\n",
      "Epoch: [177][50/97], lr: 0.00010\tTime 0.316 (0.331)\tData 0.000 (0.022)\tLoss 3.9213 (2.5927)\tPrec@1 86.719 (89.062)\tPrec@5 100.000 (99.525)\n",
      "Epoch: [177][60/97], lr: 0.00010\tTime 0.322 (0.330)\tData 0.000 (0.021)\tLoss 3.1965 (2.5782)\tPrec@1 87.500 (88.832)\tPrec@5 99.219 (99.501)\n",
      "Epoch: [177][70/97], lr: 0.00010\tTime 0.320 (0.329)\tData 0.000 (0.021)\tLoss 1.7279 (2.5825)\tPrec@1 92.969 (88.864)\tPrec@5 100.000 (99.483)\n",
      "Epoch: [177][80/97], lr: 0.00010\tTime 0.323 (0.329)\tData 0.000 (0.020)\tLoss 2.9271 (2.5514)\tPrec@1 88.281 (88.860)\tPrec@5 100.000 (99.498)\n",
      "Epoch: [177][90/97], lr: 0.00010\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 2.2332 (2.5940)\tPrec@1 92.188 (88.934)\tPrec@5 100.000 (99.511)\n",
      "Epoch: [177][96/97], lr: 0.00010\tTime 0.312 (0.328)\tData 0.000 (0.021)\tLoss 3.0126 (2.6244)\tPrec@1 88.136 (89.013)\tPrec@5 98.305 (99.476)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.303 (0.303)\tLoss 4.0321 (4.0321)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.094)\tLoss 5.0313 (5.6367)\tPrec@1 81.000 (81.545)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.5619 (5.7897)\tPrec@1 78.000 (80.381)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 7.1906 (5.9158)\tPrec@1 79.000 (79.968)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.4029 (5.9683)\tPrec@1 82.000 (79.902)\tPrec@5 98.000 (98.171)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.3575 (5.9543)\tPrec@1 87.000 (80.392)\tPrec@5 99.000 (98.196)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.3847 (5.8912)\tPrec@1 80.000 (80.295)\tPrec@5 99.000 (98.262)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.3531 (5.8913)\tPrec@1 79.000 (80.282)\tPrec@5 100.000 (98.338)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.7913 (5.8486)\tPrec@1 83.000 (80.605)\tPrec@5 98.000 (98.370)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.7817 (5.8828)\tPrec@1 88.000 (80.319)\tPrec@5 99.000 (98.352)\n",
      "val Results: Prec@1 80.270 Prec@5 98.360 Loss 5.91517\n",
      "val Class Accuracy: [0.914,0.962,0.807,0.706,0.796,0.738,0.839,0.734,0.742,0.789]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [178][0/97], lr: 0.00010\tTime 0.495 (0.495)\tData 0.274 (0.274)\tLoss 2.5287 (2.5287)\tPrec@1 92.188 (92.188)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [178][10/97], lr: 0.00010\tTime 0.323 (0.350)\tData 0.000 (0.039)\tLoss 0.7747 (1.8867)\tPrec@1 95.312 (91.264)\tPrec@5 99.219 (99.006)\n",
      "Epoch: [178][20/97], lr: 0.00010\tTime 0.321 (0.337)\tData 0.000 (0.028)\tLoss 2.1132 (2.1342)\tPrec@1 89.062 (90.848)\tPrec@5 99.219 (99.368)\n",
      "Epoch: [178][30/97], lr: 0.00010\tTime 0.322 (0.333)\tData 0.000 (0.025)\tLoss 2.4475 (2.1225)\tPrec@1 89.062 (90.222)\tPrec@5 98.438 (99.370)\n",
      "Epoch: [178][40/97], lr: 0.00010\tTime 0.325 (0.332)\tData 0.000 (0.023)\tLoss 1.3576 (2.3212)\tPrec@1 96.875 (89.806)\tPrec@5 99.219 (99.390)\n",
      "Epoch: [178][50/97], lr: 0.00010\tTime 0.324 (0.330)\tData 0.000 (0.022)\tLoss 1.9835 (2.4256)\tPrec@1 92.188 (89.292)\tPrec@5 100.000 (99.357)\n",
      "Epoch: [178][60/97], lr: 0.00010\tTime 0.322 (0.329)\tData 0.000 (0.021)\tLoss 2.7270 (2.3604)\tPrec@1 87.500 (89.280)\tPrec@5 100.000 (99.385)\n",
      "Epoch: [178][70/97], lr: 0.00010\tTime 0.324 (0.329)\tData 0.000 (0.020)\tLoss 1.4740 (2.3976)\tPrec@1 87.500 (89.272)\tPrec@5 100.000 (99.439)\n",
      "Epoch: [178][80/97], lr: 0.00010\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 3.1062 (2.3753)\tPrec@1 84.375 (89.284)\tPrec@5 100.000 (99.470)\n",
      "Epoch: [178][90/97], lr: 0.00010\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 5.0302 (2.4173)\tPrec@1 85.156 (89.140)\tPrec@5 100.000 (99.485)\n",
      "Epoch: [178][96/97], lr: 0.00010\tTime 0.315 (0.327)\tData 0.000 (0.020)\tLoss 1.7053 (2.4282)\tPrec@1 89.831 (89.086)\tPrec@5 100.000 (99.492)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.303 (0.303)\tLoss 4.0037 (4.0037)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.094)\tLoss 4.8729 (5.7496)\tPrec@1 79.000 (80.182)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.4337 (5.8410)\tPrec@1 78.000 (79.762)\tPrec@5 97.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.8047 (6.0004)\tPrec@1 80.000 (79.484)\tPrec@5 96.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.5146 (6.0627)\tPrec@1 82.000 (79.415)\tPrec@5 98.000 (98.098)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.6350 (6.0529)\tPrec@1 86.000 (79.980)\tPrec@5 99.000 (98.157)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.3087 (5.9753)\tPrec@1 81.000 (80.016)\tPrec@5 100.000 (98.246)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.2622 (5.9755)\tPrec@1 81.000 (80.014)\tPrec@5 100.000 (98.352)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.8269 (5.9226)\tPrec@1 82.000 (80.358)\tPrec@5 98.000 (98.395)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4236 (5.9428)\tPrec@1 89.000 (80.099)\tPrec@5 99.000 (98.319)\n",
      "val Results: Prec@1 80.070 Prec@5 98.300 Loss 5.97861\n",
      "val Class Accuracy: [0.907,0.962,0.800,0.686,0.807,0.752,0.840,0.735,0.762,0.756]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [179][0/97], lr: 0.00010\tTime 0.450 (0.450)\tData 0.255 (0.255)\tLoss 3.0973 (3.0973)\tPrec@1 92.969 (92.969)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [179][10/97], lr: 0.00010\tTime 0.324 (0.342)\tData 0.000 (0.038)\tLoss 6.6894 (2.7559)\tPrec@1 88.281 (89.489)\tPrec@5 100.000 (99.148)\n",
      "Epoch: [179][20/97], lr: 0.00010\tTime 0.324 (0.334)\tData 0.000 (0.028)\tLoss 2.6392 (2.6702)\tPrec@1 85.938 (88.728)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [179][30/97], lr: 0.00010\tTime 0.322 (0.331)\tData 0.000 (0.024)\tLoss 1.6378 (2.5503)\tPrec@1 89.844 (88.533)\tPrec@5 100.000 (99.168)\n",
      "Epoch: [179][40/97], lr: 0.00010\tTime 0.322 (0.330)\tData 0.000 (0.023)\tLoss 2.5852 (2.6005)\tPrec@1 89.844 (88.777)\tPrec@5 98.438 (99.200)\n",
      "Epoch: [179][50/97], lr: 0.00010\tTime 0.322 (0.329)\tData 0.000 (0.022)\tLoss 2.3018 (2.6433)\tPrec@1 86.719 (88.680)\tPrec@5 100.000 (99.265)\n",
      "Epoch: [179][60/97], lr: 0.00010\tTime 0.323 (0.328)\tData 0.000 (0.021)\tLoss 6.6277 (2.7061)\tPrec@1 90.625 (88.717)\tPrec@5 100.000 (99.270)\n",
      "Epoch: [179][70/97], lr: 0.00010\tTime 0.322 (0.327)\tData 0.000 (0.020)\tLoss 6.4697 (2.7240)\tPrec@1 93.750 (88.941)\tPrec@5 99.219 (99.296)\n",
      "Epoch: [179][80/97], lr: 0.00010\tTime 0.321 (0.327)\tData 0.000 (0.020)\tLoss 4.8519 (2.6834)\tPrec@1 88.281 (89.053)\tPrec@5 98.438 (99.286)\n",
      "Epoch: [179][90/97], lr: 0.00010\tTime 0.321 (0.327)\tData 0.000 (0.020)\tLoss 2.7594 (2.6708)\tPrec@1 86.719 (88.925)\tPrec@5 99.219 (99.287)\n",
      "Epoch: [179][96/97], lr: 0.00010\tTime 0.311 (0.326)\tData 0.000 (0.020)\tLoss 1.3239 (2.7225)\tPrec@1 89.831 (88.860)\tPrec@5 99.153 (99.307)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.275 (0.275)\tLoss 3.9909 (3.9909)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 4.9815 (5.4534)\tPrec@1 76.000 (81.273)\tPrec@5 100.000 (98.727)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.1465 (5.5462)\tPrec@1 79.000 (80.190)\tPrec@5 97.000 (98.524)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 6.9098 (5.6808)\tPrec@1 81.000 (79.935)\tPrec@5 97.000 (98.452)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.3974 (5.7413)\tPrec@1 83.000 (80.049)\tPrec@5 98.000 (98.293)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.0819 (5.7157)\tPrec@1 86.000 (80.431)\tPrec@5 99.000 (98.294)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.0363 (5.6507)\tPrec@1 81.000 (80.410)\tPrec@5 100.000 (98.361)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 4.8287 (5.6340)\tPrec@1 80.000 (80.394)\tPrec@5 100.000 (98.451)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.3688 (5.5732)\tPrec@1 83.000 (80.654)\tPrec@5 98.000 (98.481)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.4136 (5.6210)\tPrec@1 88.000 (80.429)\tPrec@5 99.000 (98.440)\n",
      "val Results: Prec@1 80.300 Prec@5 98.440 Loss 5.66441\n",
      "val Class Accuracy: [0.913,0.956,0.801,0.668,0.803,0.763,0.836,0.734,0.746,0.810]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [180][0/97], lr: 0.00000\tTime 0.502 (0.502)\tData 0.276 (0.276)\tLoss 2.2127 (2.2127)\tPrec@1 92.969 (92.969)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [180][10/97], lr: 0.00000\tTime 0.323 (0.344)\tData 0.000 (0.040)\tLoss 1.1701 (2.5791)\tPrec@1 86.719 (88.920)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [180][20/97], lr: 0.00000\tTime 0.327 (0.335)\tData 0.000 (0.029)\tLoss 3.2220 (2.5883)\tPrec@1 88.281 (88.095)\tPrec@5 99.219 (99.293)\n",
      "Epoch: [180][30/97], lr: 0.00000\tTime 0.321 (0.332)\tData 0.000 (0.025)\tLoss 1.9273 (2.4161)\tPrec@1 91.406 (88.609)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [180][40/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.023)\tLoss 2.6959 (2.4172)\tPrec@1 89.844 (88.662)\tPrec@5 99.219 (99.314)\n",
      "Epoch: [180][50/97], lr: 0.00000\tTime 0.332 (0.329)\tData 0.000 (0.022)\tLoss 1.7877 (2.3751)\tPrec@1 89.844 (88.741)\tPrec@5 100.000 (99.387)\n",
      "Epoch: [180][60/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.021)\tLoss 3.8608 (2.4241)\tPrec@1 86.719 (88.806)\tPrec@5 99.219 (99.424)\n",
      "Epoch: [180][70/97], lr: 0.00000\tTime 0.324 (0.328)\tData 0.000 (0.021)\tLoss 1.8628 (2.4269)\tPrec@1 88.281 (88.875)\tPrec@5 99.219 (99.417)\n",
      "Epoch: [180][80/97], lr: 0.00000\tTime 0.322 (0.327)\tData 0.000 (0.020)\tLoss 1.2388 (2.4801)\tPrec@1 89.844 (88.956)\tPrec@5 99.219 (99.402)\n",
      "Epoch: [180][90/97], lr: 0.00000\tTime 0.319 (0.327)\tData 0.000 (0.020)\tLoss 3.9181 (2.5384)\tPrec@1 86.719 (88.968)\tPrec@5 100.000 (99.433)\n",
      "Epoch: [180][96/97], lr: 0.00000\tTime 0.310 (0.327)\tData 0.000 (0.020)\tLoss 2.3734 (2.5275)\tPrec@1 90.678 (89.013)\tPrec@5 100.000 (99.452)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.270 (0.270)\tLoss 4.1108 (4.1108)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 5.0248 (5.5125)\tPrec@1 78.000 (80.545)\tPrec@5 100.000 (98.727)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.2376 (5.6217)\tPrec@1 78.000 (79.571)\tPrec@5 97.000 (98.476)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.9550 (5.7530)\tPrec@1 80.000 (79.484)\tPrec@5 97.000 (98.419)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.3712 (5.8068)\tPrec@1 84.000 (79.585)\tPrec@5 98.000 (98.293)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.1427 (5.7863)\tPrec@1 84.000 (80.059)\tPrec@5 99.000 (98.294)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.9810 (5.7218)\tPrec@1 81.000 (80.131)\tPrec@5 99.000 (98.344)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 4.8203 (5.7067)\tPrec@1 80.000 (80.141)\tPrec@5 100.000 (98.423)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.5118 (5.6508)\tPrec@1 81.000 (80.420)\tPrec@5 98.000 (98.444)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 3.4585 (5.6967)\tPrec@1 89.000 (80.187)\tPrec@5 99.000 (98.396)\n",
      "val Results: Prec@1 80.100 Prec@5 98.390 Loss 5.73541\n",
      "val Class Accuracy: [0.902,0.956,0.814,0.683,0.792,0.744,0.832,0.738,0.738,0.811]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [181][0/97], lr: 0.00000\tTime 0.469 (0.469)\tData 0.239 (0.239)\tLoss 2.5749 (2.5749)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [181][10/97], lr: 0.00000\tTime 0.320 (0.341)\tData 0.000 (0.036)\tLoss 0.9948 (2.8919)\tPrec@1 92.188 (88.139)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [181][20/97], lr: 0.00000\tTime 0.322 (0.332)\tData 0.000 (0.027)\tLoss 2.0550 (2.5094)\tPrec@1 89.062 (88.430)\tPrec@5 99.219 (99.442)\n",
      "Epoch: [181][30/97], lr: 0.00000\tTime 0.325 (0.329)\tData 0.000 (0.024)\tLoss 1.7472 (2.2784)\tPrec@1 89.844 (88.609)\tPrec@5 100.000 (99.521)\n",
      "Epoch: [181][40/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.022)\tLoss 3.7185 (2.2260)\tPrec@1 85.938 (88.796)\tPrec@5 98.438 (99.486)\n",
      "Epoch: [181][50/97], lr: 0.00000\tTime 0.323 (0.327)\tData 0.000 (0.021)\tLoss 2.8063 (2.3257)\tPrec@1 88.281 (88.741)\tPrec@5 98.438 (99.387)\n",
      "Epoch: [181][60/97], lr: 0.00000\tTime 0.324 (0.328)\tData 0.000 (0.021)\tLoss 4.4320 (2.4064)\tPrec@1 87.500 (88.473)\tPrec@5 98.438 (99.398)\n",
      "Epoch: [181][70/97], lr: 0.00000\tTime 0.323 (0.328)\tData 0.000 (0.020)\tLoss 0.7331 (2.4456)\tPrec@1 92.188 (88.501)\tPrec@5 99.219 (99.384)\n",
      "Epoch: [181][80/97], lr: 0.00000\tTime 0.320 (0.327)\tData 0.000 (0.020)\tLoss 3.8992 (2.5000)\tPrec@1 88.281 (88.542)\tPrec@5 98.438 (99.373)\n",
      "Epoch: [181][90/97], lr: 0.00000\tTime 0.320 (0.327)\tData 0.000 (0.019)\tLoss 3.6585 (2.5550)\tPrec@1 86.719 (88.462)\tPrec@5 98.438 (99.365)\n",
      "Epoch: [181][96/97], lr: 0.00000\tTime 0.312 (0.326)\tData 0.000 (0.020)\tLoss 1.6579 (2.5184)\tPrec@1 89.831 (88.514)\tPrec@5 100.000 (99.395)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.303 (0.303)\tLoss 4.3234 (4.3234)\tPrec@1 81.000 (81.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.094)\tLoss 5.3253 (5.7521)\tPrec@1 78.000 (80.273)\tPrec@5 99.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.3719 (5.8409)\tPrec@1 77.000 (79.476)\tPrec@5 97.000 (98.429)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 7.1548 (5.9776)\tPrec@1 81.000 (79.419)\tPrec@5 97.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.6158 (6.0328)\tPrec@1 85.000 (79.366)\tPrec@5 97.000 (98.171)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.4712 (6.0240)\tPrec@1 85.000 (79.725)\tPrec@5 99.000 (98.196)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.2704 (5.9563)\tPrec@1 81.000 (79.770)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 5.1083 (5.9529)\tPrec@1 81.000 (79.761)\tPrec@5 99.000 (98.394)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.8658 (5.8974)\tPrec@1 82.000 (80.148)\tPrec@5 98.000 (98.432)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.6722 (5.9457)\tPrec@1 89.000 (80.000)\tPrec@5 99.000 (98.407)\n",
      "val Results: Prec@1 79.910 Prec@5 98.420 Loss 5.98671\n",
      "val Class Accuracy: [0.909,0.958,0.811,0.703,0.788,0.753,0.814,0.736,0.727,0.792]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [182][0/97], lr: 0.00000\tTime 0.510 (0.510)\tData 0.272 (0.272)\tLoss 2.5101 (2.5101)\tPrec@1 88.281 (88.281)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [182][10/97], lr: 0.00000\tTime 0.321 (0.347)\tData 0.000 (0.039)\tLoss 1.9596 (2.2931)\tPrec@1 89.062 (89.773)\tPrec@5 98.438 (99.290)\n",
      "Epoch: [182][20/97], lr: 0.00000\tTime 0.326 (0.339)\tData 0.000 (0.028)\tLoss 2.2551 (2.2548)\tPrec@1 86.719 (89.881)\tPrec@5 99.219 (99.442)\n",
      "Epoch: [182][30/97], lr: 0.00000\tTime 0.320 (0.335)\tData 0.000 (0.025)\tLoss 0.8954 (2.3121)\tPrec@1 90.625 (89.919)\tPrec@5 100.000 (99.546)\n",
      "Epoch: [182][40/97], lr: 0.00000\tTime 0.320 (0.332)\tData 0.000 (0.023)\tLoss 1.0459 (2.2483)\tPrec@1 87.500 (89.596)\tPrec@5 99.219 (99.466)\n",
      "Epoch: [182][50/97], lr: 0.00000\tTime 0.321 (0.332)\tData 0.000 (0.022)\tLoss 3.4616 (2.3610)\tPrec@1 85.938 (89.032)\tPrec@5 99.219 (99.464)\n",
      "Epoch: [182][60/97], lr: 0.00000\tTime 0.321 (0.332)\tData 0.000 (0.021)\tLoss 1.2410 (2.2896)\tPrec@1 92.188 (89.191)\tPrec@5 99.219 (99.449)\n",
      "Epoch: [182][70/97], lr: 0.00000\tTime 0.337 (0.333)\tData 0.000 (0.020)\tLoss 1.0681 (2.2649)\tPrec@1 92.188 (89.338)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [182][80/97], lr: 0.00000\tTime 0.326 (0.333)\tData 0.000 (0.020)\tLoss 4.4668 (2.2814)\tPrec@1 85.156 (89.284)\tPrec@5 99.219 (99.402)\n",
      "Epoch: [182][90/97], lr: 0.00000\tTime 0.319 (0.333)\tData 0.000 (0.020)\tLoss 5.3502 (2.3770)\tPrec@1 89.062 (89.157)\tPrec@5 100.000 (99.373)\n",
      "Epoch: [182][96/97], lr: 0.00000\tTime 0.320 (0.333)\tData 0.000 (0.020)\tLoss 2.0029 (2.3889)\tPrec@1 88.136 (89.126)\tPrec@5 100.000 (99.379)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.332 (0.332)\tLoss 4.0272 (4.0272)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.097)\tLoss 5.0290 (5.4788)\tPrec@1 78.000 (80.818)\tPrec@5 100.000 (98.727)\n",
      "Test: [20/100]\tTime 0.073 (0.086)\tLoss 6.2192 (5.5806)\tPrec@1 78.000 (79.857)\tPrec@5 97.000 (98.524)\n",
      "Test: [30/100]\tTime 0.074 (0.082)\tLoss 6.8976 (5.7083)\tPrec@1 81.000 (79.677)\tPrec@5 97.000 (98.419)\n",
      "Test: [40/100]\tTime 0.074 (0.080)\tLoss 4.3510 (5.7643)\tPrec@1 85.000 (79.780)\tPrec@5 98.000 (98.293)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 5.1559 (5.7434)\tPrec@1 86.000 (80.255)\tPrec@5 99.000 (98.294)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 8.0108 (5.6782)\tPrec@1 83.000 (80.279)\tPrec@5 100.000 (98.377)\n",
      "Test: [70/100]\tTime 0.074 (0.078)\tLoss 4.8374 (5.6632)\tPrec@1 79.000 (80.225)\tPrec@5 100.000 (98.465)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.4769 (5.6057)\tPrec@1 82.000 (80.469)\tPrec@5 98.000 (98.494)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 3.4297 (5.6528)\tPrec@1 89.000 (80.275)\tPrec@5 99.000 (98.451)\n",
      "val Results: Prec@1 80.160 Prec@5 98.450 Loss 5.69230\n",
      "val Class Accuracy: [0.908,0.956,0.805,0.683,0.801,0.750,0.820,0.744,0.740,0.809]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [183][0/97], lr: 0.00000\tTime 0.471 (0.471)\tData 0.245 (0.245)\tLoss 0.8432 (0.8432)\tPrec@1 92.969 (92.969)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [183][10/97], lr: 0.00000\tTime 0.321 (0.345)\tData 0.000 (0.037)\tLoss 3.4631 (2.3956)\tPrec@1 85.938 (90.199)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [183][20/97], lr: 0.00000\tTime 0.323 (0.336)\tData 0.000 (0.027)\tLoss 3.8691 (2.5262)\tPrec@1 85.156 (89.137)\tPrec@5 98.438 (99.182)\n",
      "Epoch: [183][30/97], lr: 0.00000\tTime 0.323 (0.332)\tData 0.000 (0.024)\tLoss 2.3192 (2.5648)\tPrec@1 88.281 (88.785)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [183][40/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.022)\tLoss 5.0209 (2.6336)\tPrec@1 91.406 (88.872)\tPrec@5 100.000 (99.295)\n",
      "Epoch: [183][50/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.021)\tLoss 2.3387 (2.6733)\tPrec@1 89.062 (89.032)\tPrec@5 99.219 (99.311)\n",
      "Epoch: [183][60/97], lr: 0.00000\tTime 0.326 (0.331)\tData 0.000 (0.021)\tLoss 1.1859 (2.5981)\tPrec@1 90.625 (89.062)\tPrec@5 99.219 (99.334)\n",
      "Epoch: [183][70/97], lr: 0.00000\tTime 0.372 (0.338)\tData 0.000 (0.020)\tLoss 2.9861 (2.6862)\tPrec@1 91.406 (89.007)\tPrec@5 100.000 (99.373)\n",
      "Epoch: [183][80/97], lr: 0.00000\tTime 0.361 (0.345)\tData 0.000 (0.020)\tLoss 2.7631 (2.7241)\tPrec@1 85.938 (88.725)\tPrec@5 100.000 (99.383)\n",
      "Epoch: [183][90/97], lr: 0.00000\tTime 0.336 (0.352)\tData 0.000 (0.019)\tLoss 3.2141 (2.6969)\tPrec@1 89.844 (88.814)\tPrec@5 99.219 (99.408)\n",
      "Epoch: [183][96/97], lr: 0.00000\tTime 0.318 (0.351)\tData 0.000 (0.020)\tLoss 2.1892 (2.6827)\tPrec@1 86.441 (88.868)\tPrec@5 100.000 (99.420)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.421 (0.421)\tLoss 3.9952 (3.9952)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.105)\tLoss 4.8304 (5.4475)\tPrec@1 78.000 (80.636)\tPrec@5 100.000 (98.636)\n",
      "Test: [20/100]\tTime 0.074 (0.090)\tLoss 6.1449 (5.5344)\tPrec@1 80.000 (79.810)\tPrec@5 97.000 (98.476)\n",
      "Test: [30/100]\tTime 0.075 (0.085)\tLoss 6.8477 (5.6732)\tPrec@1 81.000 (79.613)\tPrec@5 97.000 (98.387)\n",
      "Test: [40/100]\tTime 0.074 (0.082)\tLoss 4.3728 (5.7318)\tPrec@1 84.000 (79.732)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.076 (0.081)\tLoss 5.0854 (5.7078)\tPrec@1 86.000 (80.196)\tPrec@5 99.000 (98.275)\n",
      "Test: [60/100]\tTime 0.074 (0.080)\tLoss 7.9609 (5.6414)\tPrec@1 82.000 (80.344)\tPrec@5 99.000 (98.344)\n",
      "Test: [70/100]\tTime 0.073 (0.079)\tLoss 4.8451 (5.6266)\tPrec@1 80.000 (80.338)\tPrec@5 100.000 (98.437)\n",
      "Test: [80/100]\tTime 0.073 (0.078)\tLoss 6.3751 (5.5663)\tPrec@1 84.000 (80.605)\tPrec@5 98.000 (98.469)\n",
      "Test: [90/100]\tTime 0.074 (0.078)\tLoss 3.3081 (5.6096)\tPrec@1 88.000 (80.396)\tPrec@5 99.000 (98.440)\n",
      "val Results: Prec@1 80.300 Prec@5 98.450 Loss 5.65313\n",
      "val Class Accuracy: [0.912,0.956,0.805,0.678,0.807,0.739,0.838,0.732,0.758,0.805]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [184][0/97], lr: 0.00000\tTime 0.778 (0.778)\tData 0.434 (0.434)\tLoss 2.6251 (2.6251)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [184][10/97], lr: 0.00000\tTime 0.322 (0.390)\tData 0.000 (0.051)\tLoss 3.1059 (2.5325)\tPrec@1 89.062 (88.565)\tPrec@5 99.219 (99.645)\n",
      "Epoch: [184][20/97], lr: 0.00000\tTime 0.320 (0.361)\tData 0.000 (0.035)\tLoss 1.8718 (2.4851)\tPrec@1 85.156 (88.765)\tPrec@5 99.219 (99.516)\n",
      "Epoch: [184][30/97], lr: 0.00000\tTime 0.331 (0.354)\tData 0.000 (0.029)\tLoss 1.6355 (2.5747)\tPrec@1 89.844 (89.012)\tPrec@5 100.000 (99.521)\n",
      "Epoch: [184][40/97], lr: 0.00000\tTime 0.336 (0.352)\tData 0.000 (0.026)\tLoss 2.8386 (2.6597)\tPrec@1 87.500 (89.101)\tPrec@5 99.219 (99.524)\n",
      "Epoch: [184][50/97], lr: 0.00000\tTime 0.406 (0.361)\tData 0.001 (0.024)\tLoss 3.6636 (2.6075)\tPrec@1 84.375 (89.047)\tPrec@5 100.000 (99.525)\n",
      "Epoch: [184][60/97], lr: 0.00000\tTime 0.392 (0.374)\tData 0.001 (0.023)\tLoss 2.0292 (2.5970)\tPrec@1 89.062 (88.986)\tPrec@5 99.219 (99.462)\n",
      "Epoch: [184][70/97], lr: 0.00000\tTime 0.447 (0.383)\tData 0.000 (0.022)\tLoss 2.7873 (2.5549)\tPrec@1 89.844 (88.996)\tPrec@5 99.219 (99.461)\n",
      "Epoch: [184][80/97], lr: 0.00000\tTime 0.450 (0.395)\tData 0.001 (0.021)\tLoss 3.4898 (2.5605)\tPrec@1 87.500 (89.082)\tPrec@5 98.438 (99.450)\n",
      "Epoch: [184][90/97], lr: 0.00000\tTime 0.329 (0.396)\tData 0.000 (0.020)\tLoss 2.1609 (2.5095)\tPrec@1 89.062 (89.174)\tPrec@5 97.656 (99.425)\n",
      "Epoch: [184][96/97], lr: 0.00000\tTime 0.322 (0.392)\tData 0.000 (0.020)\tLoss 3.0329 (2.5310)\tPrec@1 89.831 (89.199)\tPrec@5 99.153 (99.428)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.333 (0.333)\tLoss 3.8819 (3.8819)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.097)\tLoss 4.8185 (5.3498)\tPrec@1 78.000 (81.000)\tPrec@5 100.000 (98.727)\n",
      "Test: [20/100]\tTime 0.074 (0.086)\tLoss 6.1692 (5.4505)\tPrec@1 78.000 (79.810)\tPrec@5 98.000 (98.619)\n",
      "Test: [30/100]\tTime 0.074 (0.082)\tLoss 6.7731 (5.5911)\tPrec@1 81.000 (79.742)\tPrec@5 97.000 (98.452)\n",
      "Test: [40/100]\tTime 0.075 (0.080)\tLoss 4.2234 (5.6563)\tPrec@1 85.000 (79.829)\tPrec@5 98.000 (98.293)\n",
      "Test: [50/100]\tTime 0.073 (0.079)\tLoss 5.0933 (5.6300)\tPrec@1 86.000 (80.333)\tPrec@5 99.000 (98.294)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 7.8368 (5.5675)\tPrec@1 83.000 (80.393)\tPrec@5 100.000 (98.393)\n",
      "Test: [70/100]\tTime 0.073 (0.078)\tLoss 4.6322 (5.5518)\tPrec@1 82.000 (80.437)\tPrec@5 99.000 (98.465)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.2969 (5.4904)\tPrec@1 82.000 (80.704)\tPrec@5 98.000 (98.506)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 3.2989 (5.5361)\tPrec@1 89.000 (80.495)\tPrec@5 99.000 (98.451)\n",
      "val Results: Prec@1 80.400 Prec@5 98.440 Loss 5.57771\n",
      "val Class Accuracy: [0.906,0.952,0.807,0.687,0.805,0.746,0.821,0.748,0.760,0.808]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [185][0/97], lr: 0.00000\tTime 1.068 (1.068)\tData 0.586 (0.586)\tLoss 3.1288 (3.1288)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [185][10/97], lr: 0.00000\tTime 0.325 (0.439)\tData 0.000 (0.065)\tLoss 3.5472 (2.6444)\tPrec@1 89.062 (89.702)\tPrec@5 97.656 (99.219)\n",
      "Epoch: [185][20/97], lr: 0.00000\tTime 0.327 (0.392)\tData 0.000 (0.042)\tLoss 2.1504 (2.8675)\tPrec@1 92.188 (89.286)\tPrec@5 100.000 (99.368)\n",
      "Epoch: [185][30/97], lr: 0.00000\tTime 0.333 (0.377)\tData 0.000 (0.034)\tLoss 3.0984 (2.7310)\tPrec@1 93.750 (89.415)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [185][40/97], lr: 0.00000\tTime 0.335 (0.370)\tData 0.000 (0.030)\tLoss 2.7720 (2.6545)\tPrec@1 89.062 (89.367)\tPrec@5 99.219 (99.428)\n",
      "Epoch: [185][50/97], lr: 0.00000\tTime 0.324 (0.365)\tData 0.000 (0.027)\tLoss 1.7513 (2.6415)\tPrec@1 92.969 (89.139)\tPrec@5 100.000 (99.494)\n",
      "Epoch: [185][60/97], lr: 0.00000\tTime 0.338 (0.360)\tData 0.000 (0.025)\tLoss 1.1382 (2.5593)\tPrec@1 91.406 (89.152)\tPrec@5 99.219 (99.488)\n",
      "Epoch: [185][70/97], lr: 0.00000\tTime 0.337 (0.358)\tData 0.000 (0.024)\tLoss 1.6413 (2.5100)\tPrec@1 88.281 (89.040)\tPrec@5 100.000 (99.472)\n",
      "Epoch: [185][80/97], lr: 0.00000\tTime 0.328 (0.357)\tData 0.000 (0.023)\tLoss 2.4840 (2.5402)\tPrec@1 91.406 (89.101)\tPrec@5 100.000 (99.431)\n",
      "Epoch: [185][90/97], lr: 0.00000\tTime 0.333 (0.359)\tData 0.000 (0.023)\tLoss 1.8174 (2.4704)\tPrec@1 90.625 (89.166)\tPrec@5 99.219 (99.459)\n",
      "Epoch: [185][96/97], lr: 0.00000\tTime 0.318 (0.357)\tData 0.000 (0.023)\tLoss 1.3334 (2.4667)\tPrec@1 91.525 (89.158)\tPrec@5 99.153 (99.436)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.276 (0.276)\tLoss 3.9421 (3.9421)\tPrec@1 83.000 (83.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 4.8422 (5.4106)\tPrec@1 78.000 (81.091)\tPrec@5 100.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.1460 (5.4996)\tPrec@1 80.000 (80.000)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.8316 (5.6352)\tPrec@1 81.000 (79.806)\tPrec@5 97.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.3048 (5.6999)\tPrec@1 84.000 (79.902)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 5.0609 (5.6788)\tPrec@1 86.000 (80.333)\tPrec@5 99.000 (98.196)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 7.9479 (5.6182)\tPrec@1 82.000 (80.426)\tPrec@5 100.000 (98.295)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 4.8147 (5.6028)\tPrec@1 80.000 (80.394)\tPrec@5 100.000 (98.380)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.2876 (5.5429)\tPrec@1 82.000 (80.642)\tPrec@5 98.000 (98.432)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.3225 (5.5839)\tPrec@1 88.000 (80.407)\tPrec@5 99.000 (98.396)\n",
      "val Results: Prec@1 80.310 Prec@5 98.380 Loss 5.62491\n",
      "val Class Accuracy: [0.902,0.956,0.805,0.674,0.814,0.748,0.834,0.734,0.753,0.811]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [186][0/97], lr: 0.00000\tTime 0.486 (0.486)\tData 0.248 (0.248)\tLoss 1.9423 (1.9423)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [186][10/97], lr: 0.00000\tTime 0.336 (0.348)\tData 0.000 (0.037)\tLoss 5.2219 (2.8232)\tPrec@1 82.812 (87.358)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [186][20/97], lr: 0.00000\tTime 0.322 (0.338)\tData 0.000 (0.027)\tLoss 1.6542 (2.6691)\tPrec@1 92.969 (88.467)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [186][30/97], lr: 0.00000\tTime 0.321 (0.336)\tData 0.000 (0.024)\tLoss 1.2497 (2.6098)\tPrec@1 92.969 (88.810)\tPrec@5 99.219 (99.420)\n",
      "Epoch: [186][40/97], lr: 0.00000\tTime 0.322 (0.334)\tData 0.000 (0.022)\tLoss 1.4821 (2.5709)\tPrec@1 88.281 (89.024)\tPrec@5 100.000 (99.466)\n",
      "Epoch: [186][50/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.021)\tLoss 2.8587 (2.6494)\tPrec@1 85.938 (89.001)\tPrec@5 99.219 (99.341)\n",
      "Epoch: [186][60/97], lr: 0.00000\tTime 0.323 (0.332)\tData 0.000 (0.021)\tLoss 2.2191 (2.5830)\tPrec@1 85.938 (88.986)\tPrec@5 100.000 (99.436)\n",
      "Epoch: [186][70/97], lr: 0.00000\tTime 0.322 (0.331)\tData 0.000 (0.020)\tLoss 1.1100 (2.5256)\tPrec@1 91.406 (88.996)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [186][80/97], lr: 0.00000\tTime 0.322 (0.331)\tData 0.000 (0.020)\tLoss 1.8099 (2.4484)\tPrec@1 89.844 (88.792)\tPrec@5 100.000 (99.402)\n",
      "Epoch: [186][90/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.019)\tLoss 1.7954 (2.4372)\tPrec@1 88.281 (88.848)\tPrec@5 100.000 (99.408)\n",
      "Epoch: [186][96/97], lr: 0.00000\tTime 0.313 (0.330)\tData 0.000 (0.020)\tLoss 1.7631 (2.4548)\tPrec@1 86.441 (88.804)\tPrec@5 100.000 (99.428)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.288 (0.288)\tLoss 4.1824 (4.1824)\tPrec@1 83.000 (83.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 5.0095 (5.5765)\tPrec@1 78.000 (80.455)\tPrec@5 100.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.3164 (5.6635)\tPrec@1 78.000 (79.619)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 6.9488 (5.7991)\tPrec@1 81.000 (79.452)\tPrec@5 97.000 (98.323)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.4274 (5.8563)\tPrec@1 85.000 (79.561)\tPrec@5 97.000 (98.171)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.3015 (5.8386)\tPrec@1 84.000 (80.000)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.0976 (5.7739)\tPrec@1 83.000 (80.148)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 4.9513 (5.7615)\tPrec@1 82.000 (80.169)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.5706 (5.7038)\tPrec@1 82.000 (80.494)\tPrec@5 98.000 (98.469)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4563 (5.7480)\tPrec@1 89.000 (80.286)\tPrec@5 99.000 (98.429)\n",
      "val Results: Prec@1 80.180 Prec@5 98.420 Loss 5.78991\n",
      "val Class Accuracy: [0.909,0.957,0.815,0.697,0.810,0.736,0.806,0.739,0.748,0.801]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [187][0/97], lr: 0.00000\tTime 0.486 (0.486)\tData 0.278 (0.278)\tLoss 2.0673 (2.0673)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [187][10/97], lr: 0.00000\tTime 0.335 (0.346)\tData 0.000 (0.039)\tLoss 2.6724 (2.5192)\tPrec@1 88.281 (88.210)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [187][20/97], lr: 0.00000\tTime 0.338 (0.337)\tData 0.000 (0.029)\tLoss 0.6627 (2.5729)\tPrec@1 92.969 (88.988)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [187][30/97], lr: 0.00000\tTime 0.320 (0.333)\tData 0.000 (0.025)\tLoss 2.6908 (2.4709)\tPrec@1 85.156 (89.088)\tPrec@5 99.219 (99.370)\n",
      "Epoch: [187][40/97], lr: 0.00000\tTime 0.327 (0.332)\tData 0.000 (0.023)\tLoss 3.0590 (2.4594)\tPrec@1 88.281 (88.853)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [187][50/97], lr: 0.00000\tTime 0.325 (0.330)\tData 0.000 (0.022)\tLoss 2.5038 (2.3700)\tPrec@1 86.719 (88.894)\tPrec@5 100.000 (99.418)\n",
      "Epoch: [187][60/97], lr: 0.00000\tTime 0.318 (0.330)\tData 0.000 (0.021)\tLoss 2.3235 (2.4072)\tPrec@1 89.062 (88.781)\tPrec@5 100.000 (99.436)\n",
      "Epoch: [187][70/97], lr: 0.00000\tTime 0.332 (0.330)\tData 0.000 (0.020)\tLoss 1.2386 (2.4087)\tPrec@1 92.188 (88.974)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [187][80/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.020)\tLoss 2.0039 (2.4471)\tPrec@1 85.156 (88.956)\tPrec@5 99.219 (99.421)\n",
      "Epoch: [187][90/97], lr: 0.00000\tTime 0.327 (0.329)\tData 0.000 (0.020)\tLoss 2.6258 (2.4473)\tPrec@1 87.500 (88.822)\tPrec@5 100.000 (99.399)\n",
      "Epoch: [187][96/97], lr: 0.00000\tTime 0.314 (0.328)\tData 0.000 (0.020)\tLoss 2.1846 (2.4622)\tPrec@1 85.593 (88.755)\tPrec@5 99.153 (99.404)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.302 (0.302)\tLoss 4.0834 (4.0834)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.094)\tLoss 4.9067 (5.5744)\tPrec@1 78.000 (80.636)\tPrec@5 100.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.1652 (5.6593)\tPrec@1 80.000 (79.667)\tPrec@5 97.000 (98.524)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.9136 (5.7926)\tPrec@1 80.000 (79.484)\tPrec@5 97.000 (98.419)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.4947 (5.8541)\tPrec@1 84.000 (79.463)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.2329 (5.8339)\tPrec@1 86.000 (80.039)\tPrec@5 99.000 (98.275)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.1501 (5.7696)\tPrec@1 82.000 (80.131)\tPrec@5 100.000 (98.377)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 5.0505 (5.7586)\tPrec@1 82.000 (80.141)\tPrec@5 100.000 (98.451)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.5871 (5.7055)\tPrec@1 80.000 (80.432)\tPrec@5 98.000 (98.481)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.3510 (5.7436)\tPrec@1 88.000 (80.220)\tPrec@5 99.000 (98.407)\n",
      "val Results: Prec@1 80.100 Prec@5 98.390 Loss 5.78704\n",
      "val Class Accuracy: [0.896,0.960,0.806,0.696,0.807,0.745,0.829,0.724,0.752,0.795]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [188][0/97], lr: 0.00000\tTime 0.455 (0.455)\tData 0.255 (0.255)\tLoss 1.5578 (1.5578)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [188][10/97], lr: 0.00000\tTime 0.326 (0.342)\tData 0.000 (0.038)\tLoss 1.4320 (1.9541)\tPrec@1 95.312 (91.406)\tPrec@5 100.000 (99.787)\n",
      "Epoch: [188][20/97], lr: 0.00000\tTime 0.323 (0.335)\tData 0.000 (0.028)\tLoss 1.6130 (2.1371)\tPrec@1 92.188 (90.923)\tPrec@5 100.000 (99.740)\n",
      "Epoch: [188][30/97], lr: 0.00000\tTime 0.325 (0.333)\tData 0.000 (0.024)\tLoss 3.6742 (2.3036)\tPrec@1 89.062 (90.625)\tPrec@5 99.219 (99.597)\n",
      "Epoch: [188][40/97], lr: 0.00000\tTime 0.329 (0.331)\tData 0.000 (0.022)\tLoss 2.3569 (2.3357)\tPrec@1 90.625 (90.015)\tPrec@5 100.000 (99.543)\n",
      "Epoch: [188][50/97], lr: 0.00000\tTime 0.323 (0.330)\tData 0.000 (0.021)\tLoss 2.4406 (2.3524)\tPrec@1 86.719 (89.828)\tPrec@5 98.438 (99.479)\n",
      "Epoch: [188][60/97], lr: 0.00000\tTime 0.325 (0.329)\tData 0.000 (0.021)\tLoss 2.2835 (2.3815)\tPrec@1 89.844 (89.613)\tPrec@5 99.219 (99.411)\n",
      "Epoch: [188][70/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.020)\tLoss 5.8899 (2.4259)\tPrec@1 85.156 (89.426)\tPrec@5 99.219 (99.417)\n",
      "Epoch: [188][80/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.020)\tLoss 3.9859 (2.4089)\tPrec@1 87.500 (89.333)\tPrec@5 100.000 (99.412)\n",
      "Epoch: [188][90/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.019)\tLoss 2.5374 (2.4662)\tPrec@1 90.625 (89.363)\tPrec@5 100.000 (99.451)\n",
      "Epoch: [188][96/97], lr: 0.00000\tTime 0.315 (0.328)\tData 0.000 (0.020)\tLoss 3.8171 (2.4721)\tPrec@1 90.678 (89.344)\tPrec@5 99.153 (99.460)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.309 (0.309)\tLoss 4.1498 (4.1498)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.095)\tLoss 5.0947 (5.5997)\tPrec@1 78.000 (80.818)\tPrec@5 100.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.3223 (5.6870)\tPrec@1 77.000 (79.905)\tPrec@5 97.000 (98.476)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 7.0252 (5.8237)\tPrec@1 81.000 (79.613)\tPrec@5 97.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.4903 (5.8842)\tPrec@1 85.000 (79.756)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.2860 (5.8699)\tPrec@1 86.000 (80.118)\tPrec@5 99.000 (98.235)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.1692 (5.8038)\tPrec@1 82.000 (80.131)\tPrec@5 100.000 (98.344)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 4.8832 (5.7969)\tPrec@1 81.000 (80.113)\tPrec@5 100.000 (98.437)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.5366 (5.7359)\tPrec@1 83.000 (80.444)\tPrec@5 98.000 (98.494)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.5653 (5.7846)\tPrec@1 88.000 (80.220)\tPrec@5 99.000 (98.429)\n",
      "val Results: Prec@1 80.140 Prec@5 98.430 Loss 5.82819\n",
      "val Class Accuracy: [0.908,0.957,0.810,0.674,0.809,0.754,0.820,0.741,0.739,0.802]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [189][0/97], lr: 0.00000\tTime 0.506 (0.506)\tData 0.273 (0.273)\tLoss 2.9627 (2.9627)\tPrec@1 85.156 (85.156)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [189][10/97], lr: 0.00000\tTime 0.322 (0.347)\tData 0.000 (0.039)\tLoss 1.0551 (2.5468)\tPrec@1 90.625 (88.423)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [189][20/97], lr: 0.00000\tTime 0.324 (0.336)\tData 0.000 (0.028)\tLoss 2.6749 (2.7022)\tPrec@1 92.188 (88.467)\tPrec@5 100.000 (99.144)\n",
      "Epoch: [189][30/97], lr: 0.00000\tTime 0.322 (0.333)\tData 0.000 (0.025)\tLoss 3.8616 (2.7447)\tPrec@1 86.719 (88.458)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [189][40/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.023)\tLoss 6.6030 (2.8655)\tPrec@1 86.719 (88.529)\tPrec@5 98.438 (99.066)\n",
      "Epoch: [189][50/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.022)\tLoss 0.8781 (2.7194)\tPrec@1 89.844 (88.496)\tPrec@5 100.000 (99.050)\n",
      "Epoch: [189][60/97], lr: 0.00000\tTime 0.324 (0.329)\tData 0.000 (0.021)\tLoss 2.6551 (2.6449)\tPrec@1 88.281 (88.794)\tPrec@5 99.219 (99.116)\n",
      "Epoch: [189][70/97], lr: 0.00000\tTime 0.325 (0.329)\tData 0.000 (0.020)\tLoss 1.6338 (2.6299)\tPrec@1 92.188 (88.864)\tPrec@5 100.000 (99.142)\n",
      "Epoch: [189][80/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.020)\tLoss 2.1025 (2.6406)\tPrec@1 90.625 (88.735)\tPrec@5 98.438 (99.142)\n",
      "Epoch: [189][90/97], lr: 0.00000\tTime 0.319 (0.329)\tData 0.000 (0.020)\tLoss 1.4935 (2.6091)\tPrec@1 93.750 (88.753)\tPrec@5 99.219 (99.193)\n",
      "Epoch: [189][96/97], lr: 0.00000\tTime 0.311 (0.328)\tData 0.000 (0.020)\tLoss 2.5406 (2.5968)\tPrec@1 88.983 (88.683)\tPrec@5 99.153 (99.202)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.282 (0.282)\tLoss 4.2309 (4.2309)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 5.0891 (5.7004)\tPrec@1 78.000 (80.727)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.2809 (5.7734)\tPrec@1 79.000 (79.857)\tPrec@5 97.000 (98.381)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 6.9939 (5.9111)\tPrec@1 80.000 (79.677)\tPrec@5 97.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.5948 (5.9687)\tPrec@1 85.000 (79.756)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 5.4074 (5.9584)\tPrec@1 86.000 (80.137)\tPrec@5 99.000 (98.255)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.2723 (5.8905)\tPrec@1 81.000 (80.180)\tPrec@5 100.000 (98.361)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 5.1712 (5.8850)\tPrec@1 81.000 (80.127)\tPrec@5 100.000 (98.437)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.6838 (5.8303)\tPrec@1 82.000 (80.407)\tPrec@5 98.000 (98.481)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.5009 (5.8703)\tPrec@1 89.000 (80.165)\tPrec@5 99.000 (98.418)\n",
      "val Results: Prec@1 80.050 Prec@5 98.410 Loss 5.91475\n",
      "val Class Accuracy: [0.908,0.962,0.814,0.680,0.807,0.749,0.818,0.734,0.747,0.786]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [190][0/97], lr: 0.00000\tTime 0.475 (0.475)\tData 0.281 (0.281)\tLoss 3.7102 (3.7102)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [190][10/97], lr: 0.00000\tTime 0.335 (0.345)\tData 0.000 (0.040)\tLoss 2.6506 (2.9451)\tPrec@1 91.406 (89.560)\tPrec@5 100.000 (99.148)\n",
      "Epoch: [190][20/97], lr: 0.00000\tTime 0.319 (0.336)\tData 0.000 (0.029)\tLoss 1.4278 (2.9492)\tPrec@1 89.844 (89.174)\tPrec@5 99.219 (99.368)\n",
      "Epoch: [190][30/97], lr: 0.00000\tTime 0.321 (0.332)\tData 0.000 (0.025)\tLoss 2.3918 (2.7878)\tPrec@1 85.156 (88.861)\tPrec@5 100.000 (99.496)\n",
      "Epoch: [190][40/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.023)\tLoss 1.6380 (2.7191)\tPrec@1 91.406 (88.624)\tPrec@5 100.000 (99.543)\n",
      "Epoch: [190][50/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.022)\tLoss 3.1727 (2.6372)\tPrec@1 88.281 (88.603)\tPrec@5 100.000 (99.540)\n",
      "Epoch: [190][60/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.021)\tLoss 2.1271 (2.6335)\tPrec@1 87.500 (88.473)\tPrec@5 98.438 (99.526)\n",
      "Epoch: [190][70/97], lr: 0.00000\tTime 0.319 (0.328)\tData 0.000 (0.021)\tLoss 1.1734 (2.6175)\tPrec@1 91.406 (88.468)\tPrec@5 100.000 (99.483)\n",
      "Epoch: [190][80/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.020)\tLoss 2.3594 (2.5915)\tPrec@1 88.281 (88.436)\tPrec@5 99.219 (99.508)\n",
      "Epoch: [190][90/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 5.2529 (2.5831)\tPrec@1 85.156 (88.547)\tPrec@5 100.000 (99.511)\n",
      "Epoch: [190][96/97], lr: 0.00000\tTime 0.312 (0.327)\tData 0.000 (0.020)\tLoss 1.3544 (2.5501)\tPrec@1 91.525 (88.635)\tPrec@5 99.153 (99.484)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.260 (0.260)\tLoss 4.1514 (4.1514)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 4.9357 (5.5621)\tPrec@1 78.000 (80.909)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.1863 (5.6212)\tPrec@1 80.000 (80.000)\tPrec@5 98.000 (98.476)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.9288 (5.7755)\tPrec@1 81.000 (79.806)\tPrec@5 97.000 (98.419)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.4303 (5.8435)\tPrec@1 84.000 (79.854)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.2606 (5.8254)\tPrec@1 86.000 (80.314)\tPrec@5 99.000 (98.275)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.1181 (5.7619)\tPrec@1 82.000 (80.426)\tPrec@5 100.000 (98.377)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 4.9761 (5.7561)\tPrec@1 81.000 (80.338)\tPrec@5 100.000 (98.451)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.4576 (5.6934)\tPrec@1 82.000 (80.617)\tPrec@5 98.000 (98.481)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 3.3993 (5.7314)\tPrec@1 88.000 (80.429)\tPrec@5 99.000 (98.429)\n",
      "val Results: Prec@1 80.310 Prec@5 98.440 Loss 5.77453\n",
      "val Class Accuracy: [0.914,0.955,0.811,0.672,0.807,0.751,0.842,0.730,0.756,0.793]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [191][0/97], lr: 0.00000\tTime 0.469 (0.469)\tData 0.258 (0.258)\tLoss 2.8786 (2.8786)\tPrec@1 91.406 (91.406)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [191][10/97], lr: 0.00000\tTime 0.321 (0.343)\tData 0.000 (0.038)\tLoss 2.8048 (1.8555)\tPrec@1 89.062 (89.347)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [191][20/97], lr: 0.00000\tTime 0.334 (0.334)\tData 0.000 (0.028)\tLoss 2.1953 (2.2674)\tPrec@1 86.719 (89.062)\tPrec@5 100.000 (99.591)\n",
      "Epoch: [191][30/97], lr: 0.00000\tTime 0.320 (0.331)\tData 0.000 (0.024)\tLoss 2.2406 (2.1813)\tPrec@1 90.625 (89.617)\tPrec@5 99.219 (99.521)\n",
      "Epoch: [191][40/97], lr: 0.00000\tTime 0.323 (0.330)\tData 0.000 (0.023)\tLoss 3.8178 (2.3715)\tPrec@1 85.156 (89.520)\tPrec@5 99.219 (99.466)\n",
      "Epoch: [191][50/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.021)\tLoss 2.0543 (2.2690)\tPrec@1 91.406 (89.675)\tPrec@5 100.000 (99.464)\n",
      "Epoch: [191][60/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.021)\tLoss 4.5401 (2.3914)\tPrec@1 82.031 (89.447)\tPrec@5 100.000 (99.449)\n",
      "Epoch: [191][70/97], lr: 0.00000\tTime 0.327 (0.328)\tData 0.000 (0.020)\tLoss 3.7717 (2.3815)\tPrec@1 84.375 (89.470)\tPrec@5 100.000 (99.450)\n",
      "Epoch: [191][80/97], lr: 0.00000\tTime 0.321 (0.327)\tData 0.000 (0.020)\tLoss 1.9051 (2.4159)\tPrec@1 92.969 (89.670)\tPrec@5 100.000 (99.470)\n",
      "Epoch: [191][90/97], lr: 0.00000\tTime 0.320 (0.327)\tData 0.000 (0.020)\tLoss 3.4931 (2.4331)\tPrec@1 85.156 (89.475)\tPrec@5 99.219 (99.451)\n",
      "Epoch: [191][96/97], lr: 0.00000\tTime 0.313 (0.327)\tData 0.000 (0.020)\tLoss 1.9241 (2.4824)\tPrec@1 87.288 (89.424)\tPrec@5 100.000 (99.460)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.329 (0.329)\tLoss 4.2786 (4.2786)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.097)\tLoss 5.1431 (5.7242)\tPrec@1 78.000 (80.091)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.086)\tLoss 6.3371 (5.8078)\tPrec@1 78.000 (79.476)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.074 (0.082)\tLoss 7.0448 (5.9528)\tPrec@1 80.000 (79.387)\tPrec@5 96.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 4.6165 (6.0151)\tPrec@1 84.000 (79.463)\tPrec@5 98.000 (98.122)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.4244 (6.0060)\tPrec@1 86.000 (79.902)\tPrec@5 99.000 (98.157)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 8.2601 (5.9387)\tPrec@1 80.000 (79.902)\tPrec@5 100.000 (98.279)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.1167 (5.9383)\tPrec@1 81.000 (79.901)\tPrec@5 100.000 (98.366)\n",
      "Test: [80/100]\tTime 0.073 (0.077)\tLoss 6.8071 (5.8858)\tPrec@1 81.000 (80.247)\tPrec@5 98.000 (98.407)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.5509 (5.9262)\tPrec@1 88.000 (80.066)\tPrec@5 99.000 (98.341)\n",
      "val Results: Prec@1 80.020 Prec@5 98.340 Loss 5.96862\n",
      "val Class Accuracy: [0.901,0.961,0.815,0.696,0.800,0.752,0.819,0.732,0.744,0.782]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [192][0/97], lr: 0.00000\tTime 0.524 (0.524)\tData 0.293 (0.293)\tLoss 2.2259 (2.2259)\tPrec@1 92.188 (92.188)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [192][10/97], lr: 0.00000\tTime 0.323 (0.350)\tData 0.000 (0.041)\tLoss 3.9022 (2.4475)\tPrec@1 86.719 (89.844)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [192][20/97], lr: 0.00000\tTime 0.326 (0.339)\tData 0.000 (0.030)\tLoss 1.5626 (2.5288)\tPrec@1 91.406 (89.658)\tPrec@5 98.438 (99.554)\n",
      "Epoch: [192][30/97], lr: 0.00000\tTime 0.325 (0.336)\tData 0.000 (0.026)\tLoss 1.7812 (2.3971)\tPrec@1 92.188 (89.617)\tPrec@5 99.219 (99.471)\n",
      "Epoch: [192][40/97], lr: 0.00000\tTime 0.327 (0.334)\tData 0.000 (0.023)\tLoss 3.0833 (2.6281)\tPrec@1 83.594 (88.796)\tPrec@5 100.000 (99.447)\n",
      "Epoch: [192][50/97], lr: 0.00000\tTime 0.322 (0.333)\tData 0.000 (0.022)\tLoss 2.0670 (2.6311)\tPrec@1 89.062 (88.909)\tPrec@5 100.000 (99.418)\n",
      "Epoch: [192][60/97], lr: 0.00000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 2.7112 (2.5525)\tPrec@1 87.500 (88.947)\tPrec@5 99.219 (99.424)\n",
      "Epoch: [192][70/97], lr: 0.00000\tTime 0.323 (0.331)\tData 0.000 (0.021)\tLoss 1.5153 (2.5418)\tPrec@1 86.719 (88.798)\tPrec@5 98.438 (99.395)\n",
      "Epoch: [192][80/97], lr: 0.00000\tTime 0.329 (0.331)\tData 0.000 (0.020)\tLoss 4.4919 (2.5052)\tPrec@1 91.406 (88.792)\tPrec@5 99.219 (99.421)\n",
      "Epoch: [192][90/97], lr: 0.00000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 2.1408 (2.5997)\tPrec@1 90.625 (88.676)\tPrec@5 99.219 (99.442)\n",
      "Epoch: [192][96/97], lr: 0.00000\tTime 0.313 (0.330)\tData 0.000 (0.020)\tLoss 1.3810 (2.5926)\tPrec@1 92.373 (88.659)\tPrec@5 99.153 (99.460)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.274 (0.274)\tLoss 4.0675 (4.0675)\tPrec@1 83.000 (83.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 5.0015 (5.5011)\tPrec@1 78.000 (81.182)\tPrec@5 100.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.2274 (5.5834)\tPrec@1 78.000 (80.000)\tPrec@5 98.000 (98.429)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 6.9375 (5.7287)\tPrec@1 81.000 (79.839)\tPrec@5 97.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.3714 (5.7899)\tPrec@1 84.000 (80.000)\tPrec@5 98.000 (98.220)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.1831 (5.7709)\tPrec@1 85.000 (80.373)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.0443 (5.7033)\tPrec@1 81.000 (80.344)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 4.7393 (5.6953)\tPrec@1 79.000 (80.296)\tPrec@5 100.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.4048 (5.6325)\tPrec@1 83.000 (80.593)\tPrec@5 98.000 (98.432)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4689 (5.6780)\tPrec@1 88.000 (80.363)\tPrec@5 99.000 (98.385)\n",
      "val Results: Prec@1 80.300 Prec@5 98.410 Loss 5.72118\n",
      "val Class Accuracy: [0.914,0.955,0.812,0.669,0.792,0.756,0.842,0.742,0.744,0.804]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [193][0/97], lr: 0.00000\tTime 0.469 (0.469)\tData 0.256 (0.256)\tLoss 2.8574 (2.8574)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [193][10/97], lr: 0.00000\tTime 0.322 (0.343)\tData 0.000 (0.037)\tLoss 1.1785 (2.2467)\tPrec@1 93.750 (90.341)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [193][20/97], lr: 0.00000\tTime 0.334 (0.334)\tData 0.000 (0.027)\tLoss 3.6076 (2.7438)\tPrec@1 92.188 (89.435)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [193][30/97], lr: 0.00000\tTime 0.319 (0.331)\tData 0.000 (0.024)\tLoss 2.6566 (2.6836)\tPrec@1 86.719 (89.214)\tPrec@5 99.219 (99.446)\n",
      "Epoch: [193][40/97], lr: 0.00000\tTime 0.331 (0.331)\tData 0.000 (0.022)\tLoss 1.2359 (2.6180)\tPrec@1 90.625 (89.310)\tPrec@5 99.219 (99.447)\n",
      "Epoch: [193][50/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.021)\tLoss 5.3140 (2.7143)\tPrec@1 85.156 (89.185)\tPrec@5 99.219 (99.387)\n",
      "Epoch: [193][60/97], lr: 0.00000\tTime 0.332 (0.330)\tData 0.000 (0.020)\tLoss 2.6035 (2.6512)\tPrec@1 83.594 (88.960)\tPrec@5 100.000 (99.462)\n",
      "Epoch: [193][70/97], lr: 0.00000\tTime 0.335 (0.332)\tData 0.000 (0.020)\tLoss 3.1924 (2.6414)\tPrec@1 88.281 (88.974)\tPrec@5 98.438 (99.428)\n",
      "Epoch: [193][80/97], lr: 0.00000\tTime 0.320 (0.331)\tData 0.000 (0.019)\tLoss 1.6385 (2.5588)\tPrec@1 93.750 (88.985)\tPrec@5 98.438 (99.402)\n",
      "Epoch: [193][90/97], lr: 0.00000\tTime 0.316 (0.331)\tData 0.000 (0.019)\tLoss 1.4267 (2.4945)\tPrec@1 88.281 (88.977)\tPrec@5 100.000 (99.425)\n",
      "Epoch: [193][96/97], lr: 0.00000\tTime 0.312 (0.330)\tData 0.000 (0.020)\tLoss 2.3343 (2.4877)\tPrec@1 88.983 (89.078)\tPrec@5 100.000 (99.452)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.300 (0.300)\tLoss 3.9730 (3.9730)\tPrec@1 83.000 (83.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.074 (0.094)\tLoss 4.9039 (5.4439)\tPrec@1 78.000 (80.636)\tPrec@5 100.000 (98.545)\n",
      "Test: [20/100]\tTime 0.074 (0.084)\tLoss 6.1729 (5.5469)\tPrec@1 78.000 (79.667)\tPrec@5 97.000 (98.429)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 6.7989 (5.6837)\tPrec@1 81.000 (79.548)\tPrec@5 97.000 (98.355)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.3218 (5.7405)\tPrec@1 85.000 (79.659)\tPrec@5 98.000 (98.268)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.1497 (5.7191)\tPrec@1 84.000 (80.196)\tPrec@5 99.000 (98.275)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 7.9159 (5.6497)\tPrec@1 82.000 (80.279)\tPrec@5 100.000 (98.361)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 4.8317 (5.6359)\tPrec@1 79.000 (80.225)\tPrec@5 100.000 (98.451)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.4289 (5.5815)\tPrec@1 81.000 (80.506)\tPrec@5 98.000 (98.481)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.3120 (5.6251)\tPrec@1 89.000 (80.308)\tPrec@5 99.000 (98.440)\n",
      "val Results: Prec@1 80.230 Prec@5 98.430 Loss 5.66647\n",
      "val Class Accuracy: [0.903,0.957,0.810,0.686,0.793,0.737,0.823,0.755,0.756,0.803]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [194][0/97], lr: 0.00000\tTime 0.492 (0.492)\tData 0.257 (0.257)\tLoss 2.6206 (2.6206)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [194][10/97], lr: 0.00000\tTime 0.324 (0.345)\tData 0.000 (0.037)\tLoss 3.6587 (2.9440)\tPrec@1 90.625 (88.636)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [194][20/97], lr: 0.00000\tTime 0.329 (0.335)\tData 0.000 (0.027)\tLoss 3.5253 (2.6712)\tPrec@1 89.062 (88.988)\tPrec@5 100.000 (99.665)\n",
      "Epoch: [194][30/97], lr: 0.00000\tTime 0.319 (0.332)\tData 0.000 (0.024)\tLoss 1.3714 (2.6726)\tPrec@1 90.625 (88.659)\tPrec@5 100.000 (99.647)\n",
      "Epoch: [194][40/97], lr: 0.00000\tTime 0.331 (0.330)\tData 0.000 (0.022)\tLoss 3.7258 (2.7539)\tPrec@1 84.375 (88.815)\tPrec@5 100.000 (99.676)\n",
      "Epoch: [194][50/97], lr: 0.00000\tTime 0.319 (0.329)\tData 0.000 (0.021)\tLoss 3.2417 (2.7103)\tPrec@1 86.719 (88.649)\tPrec@5 99.219 (99.632)\n",
      "Epoch: [194][60/97], lr: 0.00000\tTime 0.320 (0.329)\tData 0.000 (0.021)\tLoss 2.0035 (2.6382)\tPrec@1 89.062 (88.896)\tPrec@5 99.219 (99.641)\n",
      "Epoch: [194][70/97], lr: 0.00000\tTime 0.319 (0.328)\tData 0.000 (0.020)\tLoss 2.8694 (2.5759)\tPrec@1 87.500 (88.886)\tPrec@5 100.000 (99.670)\n",
      "Epoch: [194][80/97], lr: 0.00000\tTime 0.323 (0.328)\tData 0.000 (0.020)\tLoss 4.8319 (2.5860)\tPrec@1 89.844 (88.860)\tPrec@5 100.000 (99.624)\n",
      "Epoch: [194][90/97], lr: 0.00000\tTime 0.317 (0.328)\tData 0.000 (0.019)\tLoss 1.6208 (2.5844)\tPrec@1 90.625 (88.882)\tPrec@5 99.219 (99.571)\n",
      "Epoch: [194][96/97], lr: 0.00000\tTime 0.314 (0.327)\tData 0.000 (0.020)\tLoss 2.0838 (2.5634)\tPrec@1 93.220 (88.957)\tPrec@5 100.000 (99.581)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.364 (0.364)\tLoss 4.0884 (4.0884)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.100)\tLoss 4.9417 (5.4757)\tPrec@1 78.000 (80.636)\tPrec@5 99.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.087)\tLoss 6.2664 (5.5574)\tPrec@1 78.000 (79.762)\tPrec@5 97.000 (98.476)\n",
      "Test: [30/100]\tTime 0.073 (0.083)\tLoss 6.8790 (5.7126)\tPrec@1 81.000 (79.484)\tPrec@5 97.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 4.3427 (5.7761)\tPrec@1 85.000 (79.585)\tPrec@5 98.000 (98.268)\n",
      "Test: [50/100]\tTime 0.073 (0.079)\tLoss 5.2429 (5.7563)\tPrec@1 85.000 (80.059)\tPrec@5 99.000 (98.275)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 7.9413 (5.6903)\tPrec@1 83.000 (80.197)\tPrec@5 100.000 (98.377)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 4.6757 (5.6796)\tPrec@1 82.000 (80.225)\tPrec@5 100.000 (98.451)\n",
      "Test: [80/100]\tTime 0.073 (0.077)\tLoss 6.4013 (5.6177)\tPrec@1 83.000 (80.568)\tPrec@5 98.000 (98.481)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.4532 (5.6619)\tPrec@1 90.000 (80.363)\tPrec@5 99.000 (98.440)\n",
      "val Results: Prec@1 80.280 Prec@5 98.430 Loss 5.70399\n",
      "val Class Accuracy: [0.908,0.956,0.808,0.692,0.809,0.741,0.813,0.742,0.759,0.800]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [195][0/97], lr: 0.00000\tTime 0.477 (0.477)\tData 0.265 (0.265)\tLoss 3.9837 (3.9837)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [195][10/97], lr: 0.00000\tTime 0.322 (0.342)\tData 0.000 (0.038)\tLoss 1.2144 (2.3008)\tPrec@1 92.969 (89.134)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [195][20/97], lr: 0.00000\tTime 0.333 (0.333)\tData 0.000 (0.028)\tLoss 2.1220 (2.3575)\tPrec@1 85.938 (88.951)\tPrec@5 98.438 (99.330)\n",
      "Epoch: [195][30/97], lr: 0.00000\tTime 0.321 (0.331)\tData 0.000 (0.024)\tLoss 5.0731 (2.5068)\tPrec@1 86.719 (88.710)\tPrec@5 99.219 (99.320)\n",
      "Epoch: [195][40/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.022)\tLoss 5.1827 (2.5229)\tPrec@1 85.938 (88.967)\tPrec@5 99.219 (99.390)\n",
      "Epoch: [195][50/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.021)\tLoss 3.0269 (2.5080)\tPrec@1 86.719 (88.710)\tPrec@5 100.000 (99.418)\n",
      "Epoch: [195][60/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.021)\tLoss 1.7662 (2.6325)\tPrec@1 91.406 (88.960)\tPrec@5 100.000 (99.462)\n",
      "Epoch: [195][70/97], lr: 0.00000\tTime 0.321 (0.327)\tData 0.000 (0.020)\tLoss 2.3246 (2.5956)\tPrec@1 92.188 (89.074)\tPrec@5 100.000 (99.494)\n",
      "Epoch: [195][80/97], lr: 0.00000\tTime 0.317 (0.327)\tData 0.000 (0.020)\tLoss 8.1917 (2.6519)\tPrec@1 82.031 (88.956)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [195][90/97], lr: 0.00000\tTime 0.316 (0.326)\tData 0.000 (0.019)\tLoss 1.9753 (2.5980)\tPrec@1 89.844 (89.028)\tPrec@5 99.219 (99.493)\n",
      "Epoch: [195][96/97], lr: 0.00000\tTime 0.323 (0.326)\tData 0.000 (0.020)\tLoss 2.9309 (2.6158)\tPrec@1 89.831 (88.884)\tPrec@5 100.000 (99.500)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.275 (0.275)\tLoss 4.1767 (4.1767)\tPrec@1 81.000 (81.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 5.0799 (5.6166)\tPrec@1 78.000 (80.455)\tPrec@5 100.000 (98.545)\n",
      "Test: [20/100]\tTime 0.074 (0.083)\tLoss 6.2828 (5.7125)\tPrec@1 78.000 (79.429)\tPrec@5 97.000 (98.429)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 6.9696 (5.8463)\tPrec@1 82.000 (79.419)\tPrec@5 97.000 (98.355)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.5214 (5.8990)\tPrec@1 85.000 (79.610)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.2853 (5.8831)\tPrec@1 86.000 (80.059)\tPrec@5 99.000 (98.255)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.1248 (5.8140)\tPrec@1 81.000 (80.131)\tPrec@5 100.000 (98.344)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 4.9612 (5.8083)\tPrec@1 82.000 (80.155)\tPrec@5 100.000 (98.423)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6448 (5.7534)\tPrec@1 82.000 (80.432)\tPrec@5 98.000 (98.457)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.5374 (5.7987)\tPrec@1 88.000 (80.220)\tPrec@5 99.000 (98.418)\n",
      "val Results: Prec@1 80.100 Prec@5 98.410 Loss 5.84089\n",
      "val Class Accuracy: [0.906,0.960,0.804,0.689,0.799,0.749,0.824,0.741,0.741,0.797]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [196][0/97], lr: 0.00000\tTime 0.489 (0.489)\tData 0.270 (0.270)\tLoss 3.8724 (3.8724)\tPrec@1 92.969 (92.969)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [196][10/97], lr: 0.00000\tTime 0.325 (0.346)\tData 0.000 (0.038)\tLoss 3.5566 (2.4936)\tPrec@1 90.625 (87.429)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [196][20/97], lr: 0.00000\tTime 0.323 (0.338)\tData 0.000 (0.028)\tLoss 3.0819 (2.4556)\tPrec@1 89.844 (88.058)\tPrec@5 98.438 (99.405)\n",
      "Epoch: [196][30/97], lr: 0.00000\tTime 0.324 (0.335)\tData 0.000 (0.025)\tLoss 2.0534 (2.5594)\tPrec@1 90.625 (88.256)\tPrec@5 98.438 (99.395)\n",
      "Epoch: [196][40/97], lr: 0.00000\tTime 0.322 (0.332)\tData 0.000 (0.023)\tLoss 0.9372 (2.5193)\tPrec@1 93.750 (88.548)\tPrec@5 100.000 (99.409)\n",
      "Epoch: [196][50/97], lr: 0.00000\tTime 0.327 (0.331)\tData 0.000 (0.022)\tLoss 1.8084 (2.5407)\tPrec@1 91.406 (88.756)\tPrec@5 100.000 (99.403)\n",
      "Epoch: [196][60/97], lr: 0.00000\tTime 0.324 (0.330)\tData 0.000 (0.021)\tLoss 1.4931 (2.5443)\tPrec@1 91.406 (89.011)\tPrec@5 100.000 (99.398)\n",
      "Epoch: [196][70/97], lr: 0.00000\tTime 0.325 (0.329)\tData 0.000 (0.020)\tLoss 3.5203 (2.4730)\tPrec@1 92.188 (89.283)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [196][80/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.020)\tLoss 4.5744 (2.4558)\tPrec@1 85.938 (89.178)\tPrec@5 98.438 (99.383)\n",
      "Epoch: [196][90/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.020)\tLoss 5.7172 (2.5202)\tPrec@1 86.719 (89.002)\tPrec@5 99.219 (99.390)\n",
      "Epoch: [196][96/97], lr: 0.00000\tTime 0.311 (0.328)\tData 0.000 (0.020)\tLoss 2.1861 (2.5248)\tPrec@1 88.983 (89.038)\tPrec@5 99.153 (99.379)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.267 (0.267)\tLoss 3.8749 (3.8749)\tPrec@1 83.000 (83.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 4.7860 (5.3952)\tPrec@1 78.000 (81.182)\tPrec@5 100.000 (98.455)\n",
      "Test: [20/100]\tTime 0.074 (0.082)\tLoss 6.1347 (5.4975)\tPrec@1 79.000 (80.048)\tPrec@5 98.000 (98.429)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.7983 (5.6318)\tPrec@1 81.000 (79.871)\tPrec@5 97.000 (98.355)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 4.2953 (5.6918)\tPrec@1 84.000 (80.000)\tPrec@5 98.000 (98.220)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.0872 (5.6672)\tPrec@1 86.000 (80.510)\tPrec@5 99.000 (98.235)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 7.9106 (5.5987)\tPrec@1 82.000 (80.525)\tPrec@5 99.000 (98.311)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 4.7580 (5.5838)\tPrec@1 79.000 (80.465)\tPrec@5 100.000 (98.394)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.3392 (5.5243)\tPrec@1 83.000 (80.728)\tPrec@5 98.000 (98.432)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.2682 (5.5666)\tPrec@1 88.000 (80.505)\tPrec@5 99.000 (98.418)\n",
      "val Results: Prec@1 80.390 Prec@5 98.430 Loss 5.60928\n",
      "val Class Accuracy: [0.909,0.957,0.810,0.666,0.795,0.747,0.842,0.746,0.760,0.807]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [197][0/97], lr: 0.00000\tTime 0.431 (0.431)\tData 0.243 (0.243)\tLoss 2.2850 (2.2850)\tPrec@1 86.719 (86.719)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [197][10/97], lr: 0.00000\tTime 0.322 (0.342)\tData 0.000 (0.037)\tLoss 1.2088 (2.5623)\tPrec@1 87.500 (87.855)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [197][20/97], lr: 0.00000\tTime 0.334 (0.334)\tData 0.000 (0.027)\tLoss 2.0184 (2.5456)\tPrec@1 88.281 (88.430)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [197][30/97], lr: 0.00000\tTime 0.324 (0.331)\tData 0.000 (0.024)\tLoss 1.9514 (2.4699)\tPrec@1 89.844 (88.735)\tPrec@5 99.219 (99.345)\n",
      "Epoch: [197][40/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.022)\tLoss 2.4157 (2.3874)\tPrec@1 91.406 (88.891)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [197][50/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.021)\tLoss 2.1342 (2.3541)\tPrec@1 91.406 (89.032)\tPrec@5 99.219 (99.433)\n",
      "Epoch: [197][60/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.020)\tLoss 2.7500 (2.4683)\tPrec@1 86.719 (88.768)\tPrec@5 99.219 (99.424)\n",
      "Epoch: [197][70/97], lr: 0.00000\tTime 0.329 (0.328)\tData 0.000 (0.020)\tLoss 4.2396 (2.5301)\tPrec@1 82.812 (88.776)\tPrec@5 99.219 (99.428)\n",
      "Epoch: [197][80/97], lr: 0.00000\tTime 0.320 (0.327)\tData 0.000 (0.020)\tLoss 1.8557 (2.4521)\tPrec@1 85.938 (88.947)\tPrec@5 100.000 (99.460)\n",
      "Epoch: [197][90/97], lr: 0.00000\tTime 0.318 (0.327)\tData 0.000 (0.019)\tLoss 4.1495 (2.4923)\tPrec@1 86.719 (89.045)\tPrec@5 96.875 (99.459)\n",
      "Epoch: [197][96/97], lr: 0.00000\tTime 0.309 (0.327)\tData 0.000 (0.020)\tLoss 1.3534 (2.4906)\tPrec@1 88.983 (89.054)\tPrec@5 100.000 (99.460)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.290 (0.290)\tLoss 4.2421 (4.2421)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 5.0344 (5.6627)\tPrec@1 78.000 (80.636)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.2316 (5.7311)\tPrec@1 80.000 (79.952)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.9393 (5.8819)\tPrec@1 81.000 (79.710)\tPrec@5 97.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.5652 (5.9394)\tPrec@1 84.000 (79.854)\tPrec@5 98.000 (98.171)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.3135 (5.9261)\tPrec@1 86.000 (80.255)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.1778 (5.8540)\tPrec@1 81.000 (80.377)\tPrec@5 99.000 (98.311)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 5.0364 (5.8487)\tPrec@1 82.000 (80.324)\tPrec@5 100.000 (98.394)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6336 (5.7946)\tPrec@1 82.000 (80.568)\tPrec@5 98.000 (98.432)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4814 (5.8347)\tPrec@1 89.000 (80.363)\tPrec@5 99.000 (98.363)\n",
      "val Results: Prec@1 80.240 Prec@5 98.360 Loss 5.87863\n",
      "val Class Accuracy: [0.908,0.962,0.813,0.679,0.810,0.744,0.835,0.735,0.750,0.788]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [198][0/97], lr: 0.00000\tTime 0.521 (0.521)\tData 0.287 (0.287)\tLoss 1.1389 (1.1389)\tPrec@1 92.969 (92.969)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [198][10/97], lr: 0.00000\tTime 0.325 (0.349)\tData 0.000 (0.040)\tLoss 3.9599 (2.6741)\tPrec@1 83.594 (89.631)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [198][20/97], lr: 0.00000\tTime 0.324 (0.337)\tData 0.000 (0.029)\tLoss 3.5112 (2.8733)\tPrec@1 89.844 (89.695)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [198][30/97], lr: 0.00000\tTime 0.319 (0.334)\tData 0.000 (0.025)\tLoss 2.4549 (2.6626)\tPrec@1 89.062 (89.693)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [198][40/97], lr: 0.00000\tTime 0.334 (0.332)\tData 0.000 (0.023)\tLoss 1.9947 (2.5255)\tPrec@1 88.281 (89.615)\tPrec@5 99.219 (99.428)\n",
      "Epoch: [198][50/97], lr: 0.00000\tTime 0.320 (0.330)\tData 0.000 (0.022)\tLoss 2.1490 (2.5205)\tPrec@1 91.406 (89.599)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [198][60/97], lr: 0.00000\tTime 0.325 (0.330)\tData 0.000 (0.021)\tLoss 1.4847 (2.4755)\tPrec@1 89.844 (89.575)\tPrec@5 99.219 (99.424)\n",
      "Epoch: [198][70/97], lr: 0.00000\tTime 0.320 (0.329)\tData 0.000 (0.020)\tLoss 2.9427 (2.5547)\tPrec@1 90.625 (89.547)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [198][80/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.020)\tLoss 4.1855 (2.6800)\tPrec@1 93.750 (89.564)\tPrec@5 99.219 (99.354)\n",
      "Epoch: [198][90/97], lr: 0.00000\tTime 0.323 (0.328)\tData 0.000 (0.020)\tLoss 2.9839 (2.6634)\tPrec@1 84.375 (89.492)\tPrec@5 100.000 (99.365)\n",
      "Epoch: [198][96/97], lr: 0.00000\tTime 0.314 (0.328)\tData 0.000 (0.020)\tLoss 2.8689 (2.6858)\tPrec@1 88.136 (89.368)\tPrec@5 100.000 (99.379)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.294 (0.294)\tLoss 3.9479 (3.9479)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 4.8670 (5.4351)\tPrec@1 78.000 (80.818)\tPrec@5 100.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.1868 (5.5233)\tPrec@1 78.000 (79.905)\tPrec@5 97.000 (98.476)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.8140 (5.6740)\tPrec@1 80.000 (79.710)\tPrec@5 96.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.3559 (5.7435)\tPrec@1 85.000 (79.756)\tPrec@5 98.000 (98.171)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.1905 (5.7228)\tPrec@1 86.000 (80.216)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 7.9779 (5.6587)\tPrec@1 82.000 (80.279)\tPrec@5 100.000 (98.344)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 4.7425 (5.6498)\tPrec@1 82.000 (80.324)\tPrec@5 100.000 (98.423)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.3742 (5.5900)\tPrec@1 82.000 (80.630)\tPrec@5 98.000 (98.469)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.3589 (5.6312)\tPrec@1 88.000 (80.418)\tPrec@5 99.000 (98.429)\n",
      "val Results: Prec@1 80.350 Prec@5 98.420 Loss 5.67424\n",
      "val Class Accuracy: [0.902,0.954,0.810,0.687,0.808,0.750,0.822,0.738,0.759,0.805]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [199][0/97], lr: 0.00000\tTime 0.459 (0.459)\tData 0.263 (0.263)\tLoss 3.2259 (3.2259)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [199][10/97], lr: 0.00000\tTime 0.321 (0.344)\tData 0.000 (0.038)\tLoss 2.0822 (2.3351)\tPrec@1 86.719 (89.702)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [199][20/97], lr: 0.00000\tTime 0.333 (0.334)\tData 0.000 (0.028)\tLoss 2.1880 (2.4631)\tPrec@1 92.188 (89.472)\tPrec@5 99.219 (99.368)\n",
      "Epoch: [199][30/97], lr: 0.00000\tTime 0.322 (0.331)\tData 0.000 (0.024)\tLoss 2.5008 (2.4798)\tPrec@1 89.062 (89.062)\tPrec@5 98.438 (99.345)\n",
      "Epoch: [199][40/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.022)\tLoss 1.1213 (2.4924)\tPrec@1 85.938 (89.024)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [199][50/97], lr: 0.00000\tTime 0.319 (0.329)\tData 0.000 (0.021)\tLoss 2.1103 (2.4690)\tPrec@1 90.625 (88.940)\tPrec@5 98.438 (99.326)\n",
      "Epoch: [199][60/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.021)\tLoss 4.4224 (2.4470)\tPrec@1 85.938 (89.062)\tPrec@5 99.219 (99.385)\n",
      "Epoch: [199][70/97], lr: 0.00000\tTime 0.321 (0.327)\tData 0.000 (0.020)\tLoss 1.3905 (2.4923)\tPrec@1 91.406 (88.721)\tPrec@5 100.000 (99.406)\n",
      "Epoch: [199][80/97], lr: 0.00000\tTime 0.321 (0.327)\tData 0.000 (0.020)\tLoss 2.8461 (2.3967)\tPrec@1 89.062 (89.091)\tPrec@5 100.000 (99.450)\n",
      "Epoch: [199][90/97], lr: 0.00000\tTime 0.321 (0.327)\tData 0.000 (0.019)\tLoss 3.3671 (2.4062)\tPrec@1 89.062 (89.191)\tPrec@5 100.000 (99.451)\n",
      "Epoch: [199][96/97], lr: 0.00000\tTime 0.309 (0.326)\tData 0.000 (0.020)\tLoss 1.9128 (2.4347)\tPrec@1 88.983 (89.231)\tPrec@5 100.000 (99.476)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.313 (0.313)\tLoss 4.2151 (4.2151)\tPrec@1 82.000 (82.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.095)\tLoss 5.0054 (5.7172)\tPrec@1 78.000 (80.091)\tPrec@5 100.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 6.3181 (5.8113)\tPrec@1 78.000 (79.381)\tPrec@5 97.000 (98.524)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.9739 (5.9428)\tPrec@1 80.000 (79.452)\tPrec@5 97.000 (98.484)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.5640 (5.9949)\tPrec@1 85.000 (79.488)\tPrec@5 98.000 (98.366)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.4152 (5.9869)\tPrec@1 85.000 (80.000)\tPrec@5 99.000 (98.353)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.2099 (5.9167)\tPrec@1 83.000 (80.131)\tPrec@5 100.000 (98.426)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.2753 (5.9119)\tPrec@1 81.000 (80.099)\tPrec@5 100.000 (98.507)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.8247 (5.8635)\tPrec@1 82.000 (80.407)\tPrec@5 98.000 (98.531)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.5143 (5.9056)\tPrec@1 89.000 (80.220)\tPrec@5 99.000 (98.473)\n",
      "val Results: Prec@1 80.120 Prec@5 98.460 Loss 5.94818\n",
      "val Class Accuracy: [0.905,0.963,0.814,0.702,0.802,0.729,0.819,0.753,0.740,0.785]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [200][0/97], lr: 0.00000\tTime 0.442 (0.442)\tData 0.251 (0.251)\tLoss 3.5085 (3.5085)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [200][10/97], lr: 0.00000\tTime 0.324 (0.341)\tData 0.000 (0.037)\tLoss 2.4705 (2.3420)\tPrec@1 88.281 (88.991)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [200][20/97], lr: 0.00000\tTime 0.316 (0.334)\tData 0.000 (0.028)\tLoss 1.5662 (2.2482)\tPrec@1 91.406 (88.802)\tPrec@5 100.000 (99.554)\n",
      "Epoch: [200][30/97], lr: 0.00000\tTime 0.320 (0.332)\tData 0.000 (0.024)\tLoss 1.7519 (2.1388)\tPrec@1 92.969 (89.415)\tPrec@5 99.219 (99.546)\n",
      "Epoch: [200][40/97], lr: 0.00000\tTime 0.320 (0.330)\tData 0.000 (0.022)\tLoss 1.1027 (2.2613)\tPrec@1 90.625 (89.425)\tPrec@5 99.219 (99.562)\n",
      "Epoch: [200][50/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.021)\tLoss 2.4558 (2.2495)\tPrec@1 87.500 (89.277)\tPrec@5 99.219 (99.556)\n",
      "Epoch: [200][60/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.021)\tLoss 1.3060 (2.2795)\tPrec@1 87.500 (89.165)\tPrec@5 100.000 (99.565)\n",
      "Epoch: [200][70/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.020)\tLoss 1.9504 (2.2769)\tPrec@1 94.531 (89.162)\tPrec@5 99.219 (99.560)\n",
      "Epoch: [200][80/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.020)\tLoss 4.0127 (2.3468)\tPrec@1 86.719 (89.140)\tPrec@5 98.438 (99.547)\n",
      "Epoch: [200][90/97], lr: 0.00000\tTime 0.319 (0.327)\tData 0.000 (0.019)\tLoss 2.8779 (2.3480)\tPrec@1 88.281 (89.217)\tPrec@5 100.000 (99.588)\n",
      "Epoch: [200][96/97], lr: 0.00000\tTime 0.313 (0.327)\tData 0.000 (0.020)\tLoss 5.5952 (2.4166)\tPrec@1 86.441 (89.118)\tPrec@5 100.000 (99.589)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.289 (0.289)\tLoss 4.0953 (4.0953)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 4.9900 (5.5771)\tPrec@1 78.000 (80.909)\tPrec@5 100.000 (98.545)\n",
      "Test: [20/100]\tTime 0.074 (0.084)\tLoss 6.1524 (5.6501)\tPrec@1 79.000 (80.048)\tPrec@5 97.000 (98.476)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.8534 (5.7986)\tPrec@1 81.000 (79.806)\tPrec@5 97.000 (98.387)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.4647 (5.8588)\tPrec@1 84.000 (79.902)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.2994 (5.8454)\tPrec@1 87.000 (80.353)\tPrec@5 99.000 (98.275)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.1274 (5.7756)\tPrec@1 82.000 (80.426)\tPrec@5 100.000 (98.377)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 4.9988 (5.7677)\tPrec@1 81.000 (80.408)\tPrec@5 100.000 (98.451)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.4989 (5.7095)\tPrec@1 83.000 (80.667)\tPrec@5 98.000 (98.481)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.3955 (5.7504)\tPrec@1 88.000 (80.429)\tPrec@5 99.000 (98.429)\n",
      "val Results: Prec@1 80.310 Prec@5 98.430 Loss 5.79534\n",
      "val Class Accuracy: [0.912,0.960,0.803,0.671,0.811,0.756,0.832,0.740,0.757,0.789]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [201][0/97], lr: 0.00000\tTime 0.533 (0.533)\tData 0.332 (0.332)\tLoss 4.6782 (4.6782)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [201][10/97], lr: 0.00000\tTime 0.320 (0.348)\tData 0.000 (0.044)\tLoss 1.7391 (2.3967)\tPrec@1 88.281 (89.205)\tPrec@5 98.438 (99.148)\n",
      "Epoch: [201][20/97], lr: 0.00000\tTime 0.325 (0.337)\tData 0.000 (0.031)\tLoss 3.4319 (2.4893)\tPrec@1 88.281 (89.100)\tPrec@5 99.219 (99.405)\n",
      "Epoch: [201][30/97], lr: 0.00000\tTime 0.317 (0.333)\tData 0.000 (0.027)\tLoss 1.7068 (2.4139)\tPrec@1 89.062 (89.441)\tPrec@5 99.219 (99.521)\n",
      "Epoch: [201][40/97], lr: 0.00000\tTime 0.321 (0.332)\tData 0.000 (0.024)\tLoss 2.9769 (2.4706)\tPrec@1 91.406 (89.520)\tPrec@5 100.000 (99.466)\n",
      "Epoch: [201][50/97], lr: 0.00000\tTime 0.324 (0.330)\tData 0.000 (0.023)\tLoss 2.0465 (2.5227)\tPrec@1 88.281 (89.507)\tPrec@5 100.000 (99.433)\n",
      "Epoch: [201][60/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.022)\tLoss 2.1714 (2.5370)\tPrec@1 92.188 (89.319)\tPrec@5 99.219 (99.436)\n",
      "Epoch: [201][70/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.021)\tLoss 1.4363 (2.5544)\tPrec@1 92.188 (89.151)\tPrec@5 97.656 (99.362)\n",
      "Epoch: [201][80/97], lr: 0.00000\tTime 0.318 (0.328)\tData 0.000 (0.021)\tLoss 1.5830 (2.5685)\tPrec@1 89.062 (89.198)\tPrec@5 99.219 (99.383)\n",
      "Epoch: [201][90/97], lr: 0.00000\tTime 0.319 (0.328)\tData 0.000 (0.020)\tLoss 1.9425 (2.5160)\tPrec@1 90.625 (89.217)\tPrec@5 100.000 (99.399)\n",
      "Epoch: [201][96/97], lr: 0.00000\tTime 0.314 (0.328)\tData 0.000 (0.021)\tLoss 4.6552 (2.5151)\tPrec@1 88.136 (89.191)\tPrec@5 99.153 (99.412)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.266 (0.266)\tLoss 4.1122 (4.1122)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 4.9681 (5.5568)\tPrec@1 78.000 (80.455)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.1868 (5.6409)\tPrec@1 79.000 (79.714)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.9260 (5.7856)\tPrec@1 79.000 (79.581)\tPrec@5 97.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.4252 (5.8477)\tPrec@1 83.000 (79.512)\tPrec@5 98.000 (98.171)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 5.2838 (5.8318)\tPrec@1 88.000 (80.118)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.0310 (5.7665)\tPrec@1 82.000 (80.197)\tPrec@5 100.000 (98.344)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 4.9654 (5.7598)\tPrec@1 80.000 (80.239)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6523 (5.7041)\tPrec@1 81.000 (80.556)\tPrec@5 98.000 (98.444)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 3.4456 (5.7448)\tPrec@1 88.000 (80.352)\tPrec@5 99.000 (98.407)\n",
      "val Results: Prec@1 80.220 Prec@5 98.400 Loss 5.78716\n",
      "val Class Accuracy: [0.910,0.958,0.794,0.707,0.796,0.741,0.832,0.739,0.751,0.794]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [202][0/97], lr: 0.00000\tTime 0.478 (0.478)\tData 0.256 (0.256)\tLoss 3.5557 (3.5557)\tPrec@1 83.594 (83.594)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [202][10/97], lr: 0.00000\tTime 0.324 (0.344)\tData 0.000 (0.038)\tLoss 2.5679 (2.9988)\tPrec@1 93.750 (89.276)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [202][20/97], lr: 0.00000\tTime 0.321 (0.335)\tData 0.000 (0.028)\tLoss 2.8474 (2.8061)\tPrec@1 87.500 (88.802)\tPrec@5 99.219 (99.554)\n",
      "Epoch: [202][30/97], lr: 0.00000\tTime 0.318 (0.333)\tData 0.000 (0.024)\tLoss 1.4290 (2.7216)\tPrec@1 92.969 (89.163)\tPrec@5 100.000 (99.597)\n",
      "Epoch: [202][40/97], lr: 0.00000\tTime 0.320 (0.331)\tData 0.000 (0.023)\tLoss 3.3292 (2.7384)\tPrec@1 86.719 (88.910)\tPrec@5 99.219 (99.543)\n",
      "Epoch: [202][50/97], lr: 0.00000\tTime 0.320 (0.329)\tData 0.000 (0.022)\tLoss 3.6070 (2.7734)\tPrec@1 87.500 (89.017)\tPrec@5 99.219 (99.494)\n",
      "Epoch: [202][60/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.021)\tLoss 2.0459 (2.6704)\tPrec@1 92.188 (89.293)\tPrec@5 99.219 (99.475)\n",
      "Epoch: [202][70/97], lr: 0.00000\tTime 0.319 (0.328)\tData 0.000 (0.020)\tLoss 3.7363 (2.6051)\tPrec@1 88.281 (89.338)\tPrec@5 100.000 (99.494)\n",
      "Epoch: [202][80/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.020)\tLoss 2.4480 (2.6441)\tPrec@1 86.719 (89.207)\tPrec@5 98.438 (99.508)\n",
      "Epoch: [202][90/97], lr: 0.00000\tTime 0.324 (0.328)\tData 0.000 (0.020)\tLoss 3.6844 (2.6197)\tPrec@1 84.375 (89.020)\tPrec@5 99.219 (99.493)\n",
      "Epoch: [202][96/97], lr: 0.00000\tTime 0.313 (0.327)\tData 0.000 (0.020)\tLoss 2.5111 (2.5726)\tPrec@1 91.525 (89.062)\tPrec@5 100.000 (99.508)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.280 (0.280)\tLoss 4.1271 (4.1271)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 5.0026 (5.6810)\tPrec@1 78.000 (80.727)\tPrec@5 100.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.2462 (5.7663)\tPrec@1 80.000 (79.762)\tPrec@5 97.000 (98.476)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.9158 (5.8959)\tPrec@1 80.000 (79.710)\tPrec@5 97.000 (98.419)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 4.6054 (5.9531)\tPrec@1 85.000 (79.683)\tPrec@5 98.000 (98.268)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.3540 (5.9423)\tPrec@1 86.000 (80.118)\tPrec@5 99.000 (98.275)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.2847 (5.8747)\tPrec@1 83.000 (80.197)\tPrec@5 100.000 (98.377)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 5.2907 (5.8678)\tPrec@1 81.000 (80.169)\tPrec@5 100.000 (98.451)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6691 (5.8143)\tPrec@1 81.000 (80.469)\tPrec@5 98.000 (98.494)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.3953 (5.8508)\tPrec@1 88.000 (80.253)\tPrec@5 99.000 (98.440)\n",
      "val Results: Prec@1 80.160 Prec@5 98.430 Loss 5.89447\n",
      "val Class Accuracy: [0.899,0.965,0.817,0.687,0.809,0.749,0.819,0.729,0.753,0.789]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [203][0/97], lr: 0.00000\tTime 0.432 (0.432)\tData 0.247 (0.247)\tLoss 3.8395 (3.8395)\tPrec@1 85.938 (85.938)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [203][10/97], lr: 0.00000\tTime 0.321 (0.340)\tData 0.000 (0.037)\tLoss 1.0903 (2.5375)\tPrec@1 92.188 (88.494)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [203][20/97], lr: 0.00000\tTime 0.320 (0.333)\tData 0.000 (0.028)\tLoss 1.9029 (2.5887)\tPrec@1 89.062 (89.211)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [203][30/97], lr: 0.00000\tTime 0.323 (0.330)\tData 0.000 (0.024)\tLoss 1.8127 (2.8158)\tPrec@1 92.188 (88.861)\tPrec@5 100.000 (99.471)\n",
      "Epoch: [203][40/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.022)\tLoss 1.4538 (2.7681)\tPrec@1 90.625 (89.120)\tPrec@5 100.000 (99.486)\n",
      "Epoch: [203][50/97], lr: 0.00000\tTime 0.324 (0.329)\tData 0.000 (0.021)\tLoss 4.6609 (2.7785)\tPrec@1 86.719 (88.894)\tPrec@5 99.219 (99.494)\n",
      "Epoch: [203][60/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.021)\tLoss 1.7332 (2.7309)\tPrec@1 88.281 (89.011)\tPrec@5 99.219 (99.526)\n",
      "Epoch: [203][70/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.020)\tLoss 2.0279 (2.7171)\tPrec@1 89.062 (88.974)\tPrec@5 99.219 (99.527)\n",
      "Epoch: [203][80/97], lr: 0.00000\tTime 0.323 (0.327)\tData 0.000 (0.020)\tLoss 3.3363 (2.7093)\tPrec@1 87.500 (88.889)\tPrec@5 100.000 (99.508)\n",
      "Epoch: [203][90/97], lr: 0.00000\tTime 0.321 (0.327)\tData 0.000 (0.019)\tLoss 3.1363 (2.6878)\tPrec@1 88.281 (88.753)\tPrec@5 100.000 (99.502)\n",
      "Epoch: [203][96/97], lr: 0.00000\tTime 0.311 (0.327)\tData 0.000 (0.020)\tLoss 1.9159 (2.6291)\tPrec@1 88.136 (88.820)\tPrec@5 100.000 (99.516)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.313 (0.313)\tLoss 4.0140 (4.0140)\tPrec@1 83.000 (83.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.095)\tLoss 4.9131 (5.5092)\tPrec@1 78.000 (80.818)\tPrec@5 100.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 6.2048 (5.5939)\tPrec@1 79.000 (80.000)\tPrec@5 98.000 (98.476)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 6.8592 (5.7346)\tPrec@1 80.000 (79.839)\tPrec@5 97.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.3513 (5.7927)\tPrec@1 84.000 (79.902)\tPrec@5 98.000 (98.268)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.1859 (5.7760)\tPrec@1 87.000 (80.412)\tPrec@5 99.000 (98.275)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 7.9907 (5.7052)\tPrec@1 82.000 (80.426)\tPrec@5 99.000 (98.361)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 4.8685 (5.6950)\tPrec@1 80.000 (80.324)\tPrec@5 100.000 (98.451)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.4096 (5.6376)\tPrec@1 83.000 (80.580)\tPrec@5 98.000 (98.481)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.3904 (5.6820)\tPrec@1 88.000 (80.374)\tPrec@5 99.000 (98.440)\n",
      "val Results: Prec@1 80.270 Prec@5 98.450 Loss 5.72529\n",
      "val Class Accuracy: [0.911,0.959,0.804,0.670,0.804,0.742,0.837,0.755,0.751,0.794]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [204][0/97], lr: 0.00000\tTime 0.486 (0.486)\tData 0.224 (0.224)\tLoss 2.8793 (2.8793)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [204][10/97], lr: 0.00000\tTime 0.327 (0.346)\tData 0.000 (0.035)\tLoss 1.4018 (2.3277)\tPrec@1 89.062 (88.068)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [204][20/97], lr: 0.00000\tTime 0.324 (0.335)\tData 0.000 (0.026)\tLoss 1.4327 (2.4331)\tPrec@1 89.844 (88.504)\tPrec@5 100.000 (99.293)\n",
      "Epoch: [204][30/97], lr: 0.00000\tTime 0.320 (0.333)\tData 0.000 (0.023)\tLoss 2.4204 (2.4779)\tPrec@1 89.062 (88.584)\tPrec@5 99.219 (99.370)\n",
      "Epoch: [204][40/97], lr: 0.00000\tTime 0.320 (0.331)\tData 0.000 (0.022)\tLoss 1.1437 (2.3825)\tPrec@1 90.625 (88.986)\tPrec@5 99.219 (99.428)\n",
      "Epoch: [204][50/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.021)\tLoss 5.8954 (2.5353)\tPrec@1 84.375 (88.971)\tPrec@5 100.000 (99.510)\n",
      "Epoch: [204][60/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 1.3095 (2.5727)\tPrec@1 85.938 (88.819)\tPrec@5 97.656 (99.436)\n",
      "Epoch: [204][70/97], lr: 0.00000\tTime 0.319 (0.328)\tData 0.000 (0.020)\tLoss 1.8543 (2.5561)\tPrec@1 88.281 (88.985)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [204][80/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.019)\tLoss 2.6530 (2.5491)\tPrec@1 85.156 (88.976)\tPrec@5 99.219 (99.392)\n",
      "Epoch: [204][90/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.019)\tLoss 1.0240 (2.5998)\tPrec@1 91.406 (88.831)\tPrec@5 99.219 (99.399)\n",
      "Epoch: [204][96/97], lr: 0.00000\tTime 0.310 (0.327)\tData 0.000 (0.020)\tLoss 2.7442 (2.5942)\tPrec@1 88.983 (88.860)\tPrec@5 99.153 (99.412)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.254 (0.254)\tLoss 4.1398 (4.1398)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 4.9509 (5.5827)\tPrec@1 78.000 (80.909)\tPrec@5 100.000 (98.545)\n",
      "Test: [20/100]\tTime 0.074 (0.082)\tLoss 6.1818 (5.6577)\tPrec@1 79.000 (80.095)\tPrec@5 97.000 (98.476)\n",
      "Test: [30/100]\tTime 0.074 (0.079)\tLoss 6.8624 (5.8059)\tPrec@1 81.000 (79.903)\tPrec@5 97.000 (98.419)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 4.4360 (5.8651)\tPrec@1 83.000 (79.878)\tPrec@5 98.000 (98.268)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 5.2164 (5.8466)\tPrec@1 87.000 (80.431)\tPrec@5 99.000 (98.275)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.0902 (5.7782)\tPrec@1 81.000 (80.443)\tPrec@5 99.000 (98.361)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 5.0446 (5.7721)\tPrec@1 82.000 (80.437)\tPrec@5 100.000 (98.423)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.5529 (5.7188)\tPrec@1 82.000 (80.679)\tPrec@5 98.000 (98.457)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4083 (5.7581)\tPrec@1 88.000 (80.462)\tPrec@5 99.000 (98.396)\n",
      "val Results: Prec@1 80.330 Prec@5 98.390 Loss 5.80228\n",
      "val Class Accuracy: [0.906,0.961,0.804,0.679,0.809,0.743,0.841,0.750,0.752,0.788]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [205][0/97], lr: 0.00000\tTime 0.418 (0.418)\tData 0.233 (0.233)\tLoss 3.9988 (3.9988)\tPrec@1 84.375 (84.375)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [205][10/97], lr: 0.00000\tTime 0.320 (0.338)\tData 0.000 (0.036)\tLoss 1.1803 (2.7153)\tPrec@1 89.062 (88.423)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [205][20/97], lr: 0.00000\tTime 0.323 (0.331)\tData 0.000 (0.027)\tLoss 2.0351 (2.7347)\tPrec@1 88.281 (88.579)\tPrec@5 99.219 (99.330)\n",
      "Epoch: [205][30/97], lr: 0.00000\tTime 0.319 (0.329)\tData 0.000 (0.024)\tLoss 1.5696 (2.4983)\tPrec@1 88.281 (88.785)\tPrec@5 99.219 (99.370)\n",
      "Epoch: [205][40/97], lr: 0.00000\tTime 0.324 (0.328)\tData 0.000 (0.022)\tLoss 3.4116 (2.4825)\tPrec@1 88.281 (88.967)\tPrec@5 99.219 (99.390)\n",
      "Epoch: [205][50/97], lr: 0.00000\tTime 0.319 (0.327)\tData 0.000 (0.021)\tLoss 3.5167 (2.5140)\tPrec@1 85.938 (88.925)\tPrec@5 100.000 (99.387)\n",
      "Epoch: [205][60/97], lr: 0.00000\tTime 0.321 (0.327)\tData 0.000 (0.020)\tLoss 2.1635 (2.4524)\tPrec@1 88.281 (88.947)\tPrec@5 99.219 (99.360)\n",
      "Epoch: [205][70/97], lr: 0.00000\tTime 0.327 (0.327)\tData 0.000 (0.020)\tLoss 3.1605 (2.4579)\tPrec@1 87.500 (88.941)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [205][80/97], lr: 0.00000\tTime 0.320 (0.327)\tData 0.000 (0.020)\tLoss 2.2605 (2.4884)\tPrec@1 85.156 (88.870)\tPrec@5 99.219 (99.402)\n",
      "Epoch: [205][90/97], lr: 0.00000\tTime 0.320 (0.327)\tData 0.000 (0.019)\tLoss 2.1248 (2.4919)\tPrec@1 92.188 (89.045)\tPrec@5 100.000 (99.425)\n",
      "Epoch: [205][96/97], lr: 0.00000\tTime 0.314 (0.326)\tData 0.000 (0.020)\tLoss 2.6647 (2.5154)\tPrec@1 89.831 (89.005)\tPrec@5 100.000 (99.428)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.250 (0.250)\tLoss 4.0307 (4.0307)\tPrec@1 83.000 (83.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.074 (0.089)\tLoss 4.9576 (5.5802)\tPrec@1 78.000 (80.545)\tPrec@5 100.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.2580 (5.6765)\tPrec@1 79.000 (79.762)\tPrec@5 97.000 (98.429)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.9342 (5.8084)\tPrec@1 80.000 (79.645)\tPrec@5 97.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.4285 (5.8677)\tPrec@1 85.000 (79.805)\tPrec@5 98.000 (98.268)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.2626 (5.8515)\tPrec@1 86.000 (80.333)\tPrec@5 99.000 (98.275)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.1078 (5.7832)\tPrec@1 83.000 (80.410)\tPrec@5 100.000 (98.361)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 5.0580 (5.7760)\tPrec@1 80.000 (80.366)\tPrec@5 100.000 (98.437)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.6150 (5.7210)\tPrec@1 82.000 (80.667)\tPrec@5 98.000 (98.469)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 3.4608 (5.7646)\tPrec@1 88.000 (80.429)\tPrec@5 99.000 (98.429)\n",
      "val Results: Prec@1 80.280 Prec@5 98.420 Loss 5.80670\n",
      "val Class Accuracy: [0.912,0.960,0.807,0.695,0.802,0.737,0.828,0.750,0.748,0.789]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [206][0/97], lr: 0.00000\tTime 0.444 (0.444)\tData 0.243 (0.243)\tLoss 1.4124 (1.4124)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [206][10/97], lr: 0.00000\tTime 0.321 (0.340)\tData 0.000 (0.036)\tLoss 1.2572 (2.7613)\tPrec@1 91.406 (89.134)\tPrec@5 99.219 (99.574)\n",
      "Epoch: [206][20/97], lr: 0.00000\tTime 0.322 (0.333)\tData 0.000 (0.027)\tLoss 3.0350 (2.6035)\tPrec@1 85.938 (88.728)\tPrec@5 99.219 (99.256)\n",
      "Epoch: [206][30/97], lr: 0.00000\tTime 0.318 (0.331)\tData 0.000 (0.024)\tLoss 1.8672 (2.4254)\tPrec@1 90.625 (89.062)\tPrec@5 100.000 (99.420)\n",
      "Epoch: [206][40/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.022)\tLoss 1.4821 (2.3852)\tPrec@1 92.188 (89.253)\tPrec@5 100.000 (99.447)\n",
      "Epoch: [206][50/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.021)\tLoss 1.7412 (2.5079)\tPrec@1 91.406 (88.833)\tPrec@5 99.219 (99.387)\n",
      "Epoch: [206][60/97], lr: 0.00000\tTime 0.323 (0.328)\tData 0.000 (0.020)\tLoss 2.0955 (2.4052)\tPrec@1 88.281 (89.114)\tPrec@5 98.438 (99.411)\n",
      "Epoch: [206][70/97], lr: 0.00000\tTime 0.314 (0.328)\tData 0.000 (0.020)\tLoss 2.4702 (2.4042)\tPrec@1 92.188 (89.140)\tPrec@5 99.219 (99.406)\n",
      "Epoch: [206][80/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.020)\tLoss 2.9141 (2.4804)\tPrec@1 88.281 (89.140)\tPrec@5 100.000 (99.441)\n",
      "Epoch: [206][90/97], lr: 0.00000\tTime 0.322 (0.327)\tData 0.000 (0.019)\tLoss 1.7494 (2.4836)\tPrec@1 92.188 (89.234)\tPrec@5 100.000 (99.459)\n",
      "Epoch: [206][96/97], lr: 0.00000\tTime 0.313 (0.327)\tData 0.000 (0.020)\tLoss 1.6104 (2.4993)\tPrec@1 91.525 (89.175)\tPrec@5 100.000 (99.428)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.283 (0.283)\tLoss 4.0994 (4.0994)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 4.9843 (5.5528)\tPrec@1 78.000 (80.909)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.1651 (5.6344)\tPrec@1 79.000 (80.048)\tPrec@5 97.000 (98.429)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.9124 (5.7784)\tPrec@1 80.000 (79.806)\tPrec@5 97.000 (98.355)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.4461 (5.8412)\tPrec@1 84.000 (79.780)\tPrec@5 98.000 (98.195)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.2349 (5.8264)\tPrec@1 86.000 (80.294)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.0808 (5.7629)\tPrec@1 81.000 (80.328)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 4.9841 (5.7577)\tPrec@1 82.000 (80.338)\tPrec@5 100.000 (98.394)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.5488 (5.7016)\tPrec@1 81.000 (80.593)\tPrec@5 98.000 (98.444)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4449 (5.7417)\tPrec@1 88.000 (80.396)\tPrec@5 99.000 (98.385)\n",
      "val Results: Prec@1 80.250 Prec@5 98.380 Loss 5.78442\n",
      "val Class Accuracy: [0.905,0.958,0.799,0.689,0.811,0.748,0.833,0.737,0.752,0.793]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [207][0/97], lr: 0.00000\tTime 0.504 (0.504)\tData 0.275 (0.275)\tLoss 2.5952 (2.5952)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [207][10/97], lr: 0.00000\tTime 0.322 (0.347)\tData 0.000 (0.039)\tLoss 1.9592 (2.3823)\tPrec@1 92.188 (90.128)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [207][20/97], lr: 0.00000\tTime 0.328 (0.337)\tData 0.000 (0.029)\tLoss 3.1849 (2.2581)\tPrec@1 90.625 (89.918)\tPrec@5 99.219 (99.256)\n",
      "Epoch: [207][30/97], lr: 0.00000\tTime 0.320 (0.333)\tData 0.000 (0.025)\tLoss 3.3834 (2.3146)\tPrec@1 90.625 (89.894)\tPrec@5 98.438 (99.294)\n",
      "Epoch: [207][40/97], lr: 0.00000\tTime 0.320 (0.331)\tData 0.000 (0.023)\tLoss 2.0020 (2.4037)\tPrec@1 85.156 (89.577)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [207][50/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.022)\tLoss 2.4305 (2.4223)\tPrec@1 85.156 (89.277)\tPrec@5 99.219 (99.433)\n",
      "Epoch: [207][60/97], lr: 0.00000\tTime 0.324 (0.329)\tData 0.000 (0.021)\tLoss 5.7928 (2.4382)\tPrec@1 82.812 (89.306)\tPrec@5 99.219 (99.462)\n",
      "Epoch: [207][70/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.021)\tLoss 1.0084 (2.4078)\tPrec@1 93.750 (89.404)\tPrec@5 100.000 (99.417)\n",
      "Epoch: [207][80/97], lr: 0.00000\tTime 0.323 (0.328)\tData 0.000 (0.020)\tLoss 2.9217 (2.4128)\tPrec@1 87.500 (89.381)\tPrec@5 97.656 (99.383)\n",
      "Epoch: [207][90/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 2.6560 (2.5369)\tPrec@1 87.500 (89.037)\tPrec@5 100.000 (99.373)\n",
      "Epoch: [207][96/97], lr: 0.00000\tTime 0.313 (0.327)\tData 0.000 (0.020)\tLoss 1.9464 (2.5605)\tPrec@1 89.831 (88.997)\tPrec@5 100.000 (99.395)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.265 (0.265)\tLoss 4.0726 (4.0726)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 5.0340 (5.5740)\tPrec@1 78.000 (80.636)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.1995 (5.6501)\tPrec@1 79.000 (79.810)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.8988 (5.8014)\tPrec@1 81.000 (79.645)\tPrec@5 97.000 (98.258)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 4.4862 (5.8641)\tPrec@1 85.000 (79.829)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.3051 (5.8480)\tPrec@1 86.000 (80.314)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.0915 (5.7743)\tPrec@1 81.000 (80.344)\tPrec@5 100.000 (98.295)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 4.9657 (5.7686)\tPrec@1 81.000 (80.282)\tPrec@5 100.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.5441 (5.7112)\tPrec@1 82.000 (80.531)\tPrec@5 98.000 (98.420)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 3.4482 (5.7525)\tPrec@1 88.000 (80.319)\tPrec@5 99.000 (98.385)\n",
      "val Results: Prec@1 80.230 Prec@5 98.390 Loss 5.79709\n",
      "val Class Accuracy: [0.912,0.960,0.806,0.675,0.800,0.754,0.831,0.747,0.755,0.783]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [208][0/97], lr: 0.00000\tTime 0.447 (0.447)\tData 0.246 (0.246)\tLoss 2.7725 (2.7725)\tPrec@1 86.719 (86.719)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [208][10/97], lr: 0.00000\tTime 0.323 (0.343)\tData 0.000 (0.037)\tLoss 1.4423 (2.6670)\tPrec@1 90.625 (87.571)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [208][20/97], lr: 0.00000\tTime 0.319 (0.334)\tData 0.000 (0.027)\tLoss 2.2027 (2.4364)\tPrec@1 87.500 (88.244)\tPrec@5 99.219 (99.293)\n",
      "Epoch: [208][30/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.024)\tLoss 3.2700 (2.6358)\tPrec@1 88.281 (88.810)\tPrec@5 97.656 (99.294)\n",
      "Epoch: [208][40/97], lr: 0.00000\tTime 0.319 (0.330)\tData 0.000 (0.022)\tLoss 2.1845 (2.5711)\tPrec@1 90.625 (88.853)\tPrec@5 100.000 (99.295)\n",
      "Epoch: [208][50/97], lr: 0.00000\tTime 0.318 (0.329)\tData 0.000 (0.021)\tLoss 2.5625 (2.5516)\tPrec@1 88.281 (89.032)\tPrec@5 100.000 (99.341)\n",
      "Epoch: [208][60/97], lr: 0.00000\tTime 0.329 (0.329)\tData 0.000 (0.021)\tLoss 2.8313 (2.5165)\tPrec@1 89.062 (88.960)\tPrec@5 100.000 (99.347)\n",
      "Epoch: [208][70/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.020)\tLoss 1.4283 (2.4776)\tPrec@1 91.406 (88.908)\tPrec@5 99.219 (99.329)\n",
      "Epoch: [208][80/97], lr: 0.00000\tTime 0.327 (0.328)\tData 0.000 (0.020)\tLoss 2.3184 (2.4789)\tPrec@1 89.844 (88.976)\tPrec@5 99.219 (99.286)\n",
      "Epoch: [208][90/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.019)\tLoss 2.1306 (2.5043)\tPrec@1 89.062 (89.002)\tPrec@5 98.438 (99.287)\n",
      "Epoch: [208][96/97], lr: 0.00000\tTime 0.314 (0.327)\tData 0.000 (0.020)\tLoss 2.6587 (2.4931)\tPrec@1 88.136 (89.021)\tPrec@5 99.153 (99.299)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.293 (0.293)\tLoss 4.1212 (4.1212)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 4.9923 (5.6008)\tPrec@1 78.000 (80.273)\tPrec@5 99.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.1779 (5.6896)\tPrec@1 79.000 (79.571)\tPrec@5 97.000 (98.429)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.8857 (5.8330)\tPrec@1 80.000 (79.548)\tPrec@5 97.000 (98.387)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.4700 (5.8931)\tPrec@1 84.000 (79.561)\tPrec@5 98.000 (98.220)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.3172 (5.8815)\tPrec@1 85.000 (80.216)\tPrec@5 99.000 (98.235)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.0824 (5.8138)\tPrec@1 82.000 (80.262)\tPrec@5 100.000 (98.344)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.1118 (5.8110)\tPrec@1 81.000 (80.296)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6966 (5.7614)\tPrec@1 81.000 (80.593)\tPrec@5 98.000 (98.432)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4532 (5.7999)\tPrec@1 88.000 (80.363)\tPrec@5 99.000 (98.363)\n",
      "val Results: Prec@1 80.240 Prec@5 98.360 Loss 5.84062\n",
      "val Class Accuracy: [0.903,0.961,0.807,0.703,0.789,0.741,0.840,0.746,0.749,0.785]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [209][0/97], lr: 0.00000\tTime 0.491 (0.491)\tData 0.284 (0.284)\tLoss 2.4438 (2.4438)\tPrec@1 86.719 (86.719)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [209][10/97], lr: 0.00000\tTime 0.317 (0.345)\tData 0.000 (0.041)\tLoss 3.6534 (2.7995)\tPrec@1 89.062 (88.636)\tPrec@5 100.000 (99.716)\n",
      "Epoch: [209][20/97], lr: 0.00000\tTime 0.331 (0.336)\tData 0.000 (0.029)\tLoss 2.4354 (2.6515)\tPrec@1 88.281 (88.765)\tPrec@5 99.219 (99.516)\n",
      "Epoch: [209][30/97], lr: 0.00000\tTime 0.325 (0.332)\tData 0.000 (0.025)\tLoss 3.5349 (2.7525)\tPrec@1 88.281 (88.936)\tPrec@5 100.000 (99.546)\n",
      "Epoch: [209][40/97], lr: 0.00000\tTime 0.322 (0.331)\tData 0.000 (0.023)\tLoss 2.1466 (2.6956)\tPrec@1 89.844 (88.986)\tPrec@5 100.000 (99.505)\n",
      "Epoch: [209][50/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.022)\tLoss 3.0592 (2.6840)\tPrec@1 85.156 (88.894)\tPrec@5 99.219 (99.433)\n",
      "Epoch: [209][60/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.021)\tLoss 4.0301 (2.7268)\tPrec@1 82.031 (88.768)\tPrec@5 97.656 (99.360)\n",
      "Epoch: [209][70/97], lr: 0.00000\tTime 0.328 (0.329)\tData 0.000 (0.020)\tLoss 1.0141 (2.6807)\tPrec@1 92.969 (88.633)\tPrec@5 100.000 (99.351)\n",
      "Epoch: [209][80/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 4.6439 (2.6876)\tPrec@1 89.844 (88.696)\tPrec@5 99.219 (99.334)\n",
      "Epoch: [209][90/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.020)\tLoss 1.9909 (2.5842)\tPrec@1 87.500 (88.762)\tPrec@5 100.000 (99.322)\n",
      "Epoch: [209][96/97], lr: 0.00000\tTime 0.313 (0.328)\tData 0.000 (0.020)\tLoss 3.7189 (2.5863)\tPrec@1 84.746 (88.707)\tPrec@5 100.000 (99.323)\n",
      "Gated Network Weight Gate= Flip:0.54, Sc:0.46\n",
      "Test: [0/100]\tTime 0.275 (0.275)\tLoss 4.2288 (4.2288)\tPrec@1 82.000 (82.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 4.9854 (5.6741)\tPrec@1 78.000 (80.000)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.2591 (5.7510)\tPrec@1 78.000 (79.571)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.8972 (5.8979)\tPrec@1 80.000 (79.419)\tPrec@5 97.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.5538 (5.9520)\tPrec@1 85.000 (79.561)\tPrec@5 98.000 (98.195)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 5.3334 (5.9407)\tPrec@1 86.000 (80.118)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.2099 (5.8719)\tPrec@1 82.000 (80.230)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 5.1387 (5.8670)\tPrec@1 82.000 (80.197)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6771 (5.8154)\tPrec@1 82.000 (80.481)\tPrec@5 98.000 (98.457)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4776 (5.8545)\tPrec@1 88.000 (80.275)\tPrec@5 99.000 (98.407)\n",
      "val Results: Prec@1 80.170 Prec@5 98.400 Loss 5.89857\n",
      "val Class Accuracy: [0.904,0.963,0.807,0.692,0.814,0.736,0.822,0.748,0.748,0.783]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [210][0/97], lr: 0.00000\tTime 0.488 (0.488)\tData 0.259 (0.259)\tLoss 2.1736 (2.1736)\tPrec@1 86.719 (86.719)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [210][10/97], lr: 0.00000\tTime 0.325 (0.343)\tData 0.000 (0.037)\tLoss 0.9796 (2.0753)\tPrec@1 92.969 (89.560)\tPrec@5 100.000 (99.148)\n",
      "Epoch: [210][20/97], lr: 0.00000\tTime 0.321 (0.334)\tData 0.000 (0.027)\tLoss 3.7066 (2.2911)\tPrec@1 89.844 (89.509)\tPrec@5 100.000 (99.293)\n",
      "Epoch: [210][30/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.024)\tLoss 2.2755 (2.3374)\tPrec@1 89.062 (89.667)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [210][40/97], lr: 0.00000\tTime 0.320 (0.330)\tData 0.000 (0.023)\tLoss 5.0938 (2.4697)\tPrec@1 86.719 (89.444)\tPrec@5 100.000 (99.447)\n",
      "Epoch: [210][50/97], lr: 0.00000\tTime 0.319 (0.329)\tData 0.000 (0.021)\tLoss 4.2812 (2.4437)\tPrec@1 88.281 (89.445)\tPrec@5 100.000 (99.464)\n",
      "Epoch: [210][60/97], lr: 0.00000\tTime 0.326 (0.329)\tData 0.000 (0.021)\tLoss 3.4517 (2.4392)\tPrec@1 85.938 (89.472)\tPrec@5 99.219 (99.475)\n",
      "Epoch: [210][70/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 1.3221 (2.4609)\tPrec@1 89.062 (89.250)\tPrec@5 99.219 (99.494)\n",
      "Epoch: [210][80/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 4.5885 (2.5057)\tPrec@1 92.969 (89.207)\tPrec@5 99.219 (99.498)\n",
      "Epoch: [210][90/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.019)\tLoss 0.7420 (2.4612)\tPrec@1 94.531 (89.140)\tPrec@5 99.219 (99.485)\n",
      "Epoch: [210][96/97], lr: 0.00000\tTime 0.311 (0.327)\tData 0.000 (0.020)\tLoss 0.9850 (2.4902)\tPrec@1 92.373 (89.142)\tPrec@5 100.000 (99.484)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.346 (0.346)\tLoss 4.1009 (4.1009)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.098)\tLoss 5.0425 (5.6424)\tPrec@1 78.000 (80.455)\tPrec@5 100.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.086)\tLoss 6.1914 (5.7223)\tPrec@1 79.000 (79.714)\tPrec@5 97.000 (98.429)\n",
      "Test: [30/100]\tTime 0.074 (0.082)\tLoss 6.9311 (5.8609)\tPrec@1 81.000 (79.645)\tPrec@5 97.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 4.5508 (5.9172)\tPrec@1 84.000 (79.756)\tPrec@5 98.000 (98.220)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 5.2781 (5.9052)\tPrec@1 86.000 (80.196)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 8.1655 (5.8334)\tPrec@1 81.000 (80.213)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.1438 (5.8291)\tPrec@1 81.000 (80.197)\tPrec@5 100.000 (98.394)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.5939 (5.7763)\tPrec@1 82.000 (80.469)\tPrec@5 98.000 (98.432)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 3.4424 (5.8167)\tPrec@1 88.000 (80.231)\tPrec@5 99.000 (98.385)\n",
      "val Results: Prec@1 80.120 Prec@5 98.380 Loss 5.86144\n",
      "val Class Accuracy: [0.910,0.964,0.801,0.675,0.802,0.751,0.833,0.744,0.748,0.784]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [211][0/97], lr: 0.00000\tTime 0.494 (0.494)\tData 0.272 (0.272)\tLoss 2.6068 (2.6068)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [211][10/97], lr: 0.00000\tTime 0.321 (0.345)\tData 0.000 (0.039)\tLoss 2.6947 (2.3486)\tPrec@1 87.500 (88.068)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [211][20/97], lr: 0.00000\tTime 0.334 (0.336)\tData 0.000 (0.028)\tLoss 2.8741 (2.3848)\tPrec@1 91.406 (88.207)\tPrec@5 98.438 (99.256)\n",
      "Epoch: [211][30/97], lr: 0.00000\tTime 0.320 (0.332)\tData 0.000 (0.024)\tLoss 2.5381 (2.5017)\tPrec@1 91.406 (88.684)\tPrec@5 99.219 (99.320)\n",
      "Epoch: [211][40/97], lr: 0.00000\tTime 0.323 (0.331)\tData 0.000 (0.023)\tLoss 1.6239 (2.4562)\tPrec@1 96.094 (88.853)\tPrec@5 100.000 (99.371)\n",
      "Epoch: [211][50/97], lr: 0.00000\tTime 0.323 (0.330)\tData 0.000 (0.021)\tLoss 4.3661 (2.4893)\tPrec@1 85.938 (88.955)\tPrec@5 98.438 (99.295)\n",
      "Epoch: [211][60/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.021)\tLoss 0.8632 (2.4712)\tPrec@1 92.969 (88.870)\tPrec@5 99.219 (99.308)\n",
      "Epoch: [211][70/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 2.7006 (2.5511)\tPrec@1 91.406 (88.864)\tPrec@5 100.000 (99.318)\n",
      "Epoch: [211][80/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.020)\tLoss 1.3028 (2.5286)\tPrec@1 89.844 (88.628)\tPrec@5 99.219 (99.306)\n",
      "Epoch: [211][90/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.020)\tLoss 1.9930 (2.5408)\tPrec@1 89.844 (88.668)\tPrec@5 100.000 (99.322)\n",
      "Epoch: [211][96/97], lr: 0.00000\tTime 0.313 (0.327)\tData 0.000 (0.020)\tLoss 2.8153 (2.5324)\tPrec@1 88.983 (88.755)\tPrec@5 100.000 (99.339)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.287 (0.287)\tLoss 4.0170 (4.0170)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 4.8779 (5.5344)\tPrec@1 78.000 (80.455)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.2194 (5.6188)\tPrec@1 79.000 (79.762)\tPrec@5 97.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.8272 (5.7718)\tPrec@1 81.000 (79.484)\tPrec@5 97.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.4272 (5.8376)\tPrec@1 84.000 (79.610)\tPrec@5 98.000 (98.195)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.2935 (5.8225)\tPrec@1 86.000 (80.118)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.0192 (5.7514)\tPrec@1 81.000 (80.230)\tPrec@5 100.000 (98.344)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 4.9843 (5.7455)\tPrec@1 82.000 (80.211)\tPrec@5 100.000 (98.423)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.4818 (5.6896)\tPrec@1 83.000 (80.494)\tPrec@5 98.000 (98.457)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.3798 (5.7268)\tPrec@1 88.000 (80.286)\tPrec@5 99.000 (98.418)\n",
      "val Results: Prec@1 80.190 Prec@5 98.420 Loss 5.77099\n",
      "val Class Accuracy: [0.909,0.960,0.810,0.674,0.801,0.745,0.829,0.751,0.759,0.781]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [212][0/97], lr: 0.00000\tTime 0.469 (0.469)\tData 0.275 (0.275)\tLoss 3.5121 (3.5121)\tPrec@1 82.031 (82.031)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [212][10/97], lr: 0.00000\tTime 0.322 (0.343)\tData 0.000 (0.040)\tLoss 1.8594 (2.1817)\tPrec@1 91.406 (89.915)\tPrec@5 100.000 (99.716)\n",
      "Epoch: [212][20/97], lr: 0.00000\tTime 0.320 (0.334)\tData 0.000 (0.029)\tLoss 2.2380 (2.4495)\tPrec@1 88.281 (89.695)\tPrec@5 99.219 (99.554)\n",
      "Epoch: [212][30/97], lr: 0.00000\tTime 0.317 (0.332)\tData 0.000 (0.025)\tLoss 2.9568 (2.4097)\tPrec@1 88.281 (89.365)\tPrec@5 100.000 (99.622)\n",
      "Epoch: [212][40/97], lr: 0.00000\tTime 0.320 (0.330)\tData 0.000 (0.023)\tLoss 2.4045 (2.5182)\tPrec@1 89.844 (89.272)\tPrec@5 100.000 (99.619)\n",
      "Epoch: [212][50/97], lr: 0.00000\tTime 0.324 (0.329)\tData 0.000 (0.022)\tLoss 1.8472 (2.5035)\tPrec@1 87.500 (89.093)\tPrec@5 100.000 (99.617)\n",
      "Epoch: [212][60/97], lr: 0.00000\tTime 0.324 (0.328)\tData 0.000 (0.021)\tLoss 1.5610 (2.5759)\tPrec@1 88.281 (89.088)\tPrec@5 98.438 (99.590)\n",
      "Epoch: [212][70/97], lr: 0.00000\tTime 0.318 (0.328)\tData 0.000 (0.021)\tLoss 2.9077 (2.5817)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.516)\n",
      "Epoch: [212][80/97], lr: 0.00000\tTime 0.322 (0.327)\tData 0.000 (0.020)\tLoss 3.0094 (2.6206)\tPrec@1 83.594 (88.947)\tPrec@5 99.219 (99.518)\n",
      "Epoch: [212][90/97], lr: 0.00000\tTime 0.321 (0.327)\tData 0.000 (0.020)\tLoss 2.4364 (2.6019)\tPrec@1 89.844 (88.899)\tPrec@5 99.219 (99.519)\n",
      "Epoch: [212][96/97], lr: 0.00000\tTime 0.310 (0.327)\tData 0.000 (0.020)\tLoss 3.3425 (2.5741)\tPrec@1 88.136 (88.957)\tPrec@5 99.153 (99.516)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.261 (0.261)\tLoss 4.1914 (4.1914)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 5.1041 (5.6905)\tPrec@1 78.000 (80.273)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.074 (0.082)\tLoss 6.2721 (5.7631)\tPrec@1 78.000 (79.619)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.9982 (5.9099)\tPrec@1 80.000 (79.548)\tPrec@5 96.000 (98.323)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 4.6210 (5.9719)\tPrec@1 85.000 (79.659)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 5.4415 (5.9626)\tPrec@1 85.000 (80.059)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.2342 (5.8943)\tPrec@1 80.000 (80.016)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 5.1605 (5.8927)\tPrec@1 81.000 (80.141)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6816 (5.8390)\tPrec@1 82.000 (80.457)\tPrec@5 98.000 (98.432)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.5003 (5.8743)\tPrec@1 89.000 (80.253)\tPrec@5 99.000 (98.352)\n",
      "val Results: Prec@1 80.170 Prec@5 98.350 Loss 5.91724\n",
      "val Class Accuracy: [0.905,0.962,0.817,0.684,0.795,0.761,0.825,0.734,0.752,0.782]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [213][0/97], lr: 0.00000\tTime 0.471 (0.471)\tData 0.279 (0.279)\tLoss 2.4111 (2.4111)\tPrec@1 84.375 (84.375)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [213][10/97], lr: 0.00000\tTime 0.324 (0.342)\tData 0.000 (0.040)\tLoss 3.0353 (2.6876)\tPrec@1 89.062 (89.276)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [213][20/97], lr: 0.00000\tTime 0.332 (0.334)\tData 0.000 (0.029)\tLoss 3.8414 (2.6000)\tPrec@1 85.156 (88.207)\tPrec@5 100.000 (99.554)\n",
      "Epoch: [213][30/97], lr: 0.00000\tTime 0.319 (0.330)\tData 0.000 (0.025)\tLoss 2.4730 (2.4755)\tPrec@1 89.844 (88.936)\tPrec@5 100.000 (99.597)\n",
      "Epoch: [213][40/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.023)\tLoss 2.1746 (2.5561)\tPrec@1 91.406 (88.948)\tPrec@5 99.219 (99.486)\n",
      "Epoch: [213][50/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.022)\tLoss 3.2059 (2.5049)\tPrec@1 88.281 (89.001)\tPrec@5 100.000 (99.525)\n",
      "Epoch: [213][60/97], lr: 0.00000\tTime 0.322 (0.327)\tData 0.000 (0.021)\tLoss 1.4672 (2.4804)\tPrec@1 91.406 (89.075)\tPrec@5 100.000 (99.475)\n",
      "Epoch: [213][70/97], lr: 0.00000\tTime 0.324 (0.327)\tData 0.000 (0.021)\tLoss 3.4834 (2.5184)\tPrec@1 89.844 (89.162)\tPrec@5 98.438 (99.417)\n",
      "Epoch: [213][80/97], lr: 0.00000\tTime 0.321 (0.327)\tData 0.000 (0.020)\tLoss 1.3605 (2.5042)\tPrec@1 92.969 (89.111)\tPrec@5 99.219 (99.402)\n",
      "Epoch: [213][90/97], lr: 0.00000\tTime 0.320 (0.327)\tData 0.000 (0.020)\tLoss 3.0318 (2.4817)\tPrec@1 83.594 (88.856)\tPrec@5 98.438 (99.373)\n",
      "Epoch: [213][96/97], lr: 0.00000\tTime 0.311 (0.326)\tData 0.000 (0.020)\tLoss 2.9736 (2.4875)\tPrec@1 88.983 (88.909)\tPrec@5 100.000 (99.387)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.321 (0.321)\tLoss 4.1635 (4.1635)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.096)\tLoss 5.0257 (5.6438)\tPrec@1 78.000 (80.636)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 6.1895 (5.7022)\tPrec@1 79.000 (79.905)\tPrec@5 97.000 (98.429)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.8846 (5.8651)\tPrec@1 80.000 (79.581)\tPrec@5 97.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.5125 (5.9274)\tPrec@1 85.000 (79.732)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.4095 (5.9147)\tPrec@1 87.000 (80.196)\tPrec@5 99.000 (98.255)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.1745 (5.8467)\tPrec@1 81.000 (80.213)\tPrec@5 100.000 (98.393)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.0343 (5.8424)\tPrec@1 82.000 (80.239)\tPrec@5 100.000 (98.465)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.5724 (5.7861)\tPrec@1 83.000 (80.543)\tPrec@5 98.000 (98.506)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4853 (5.8248)\tPrec@1 88.000 (80.308)\tPrec@5 99.000 (98.473)\n",
      "val Results: Prec@1 80.220 Prec@5 98.470 Loss 5.87196\n",
      "val Class Accuracy: [0.910,0.961,0.805,0.674,0.810,0.757,0.828,0.746,0.754,0.777]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [214][0/97], lr: 0.00000\tTime 0.507 (0.507)\tData 0.270 (0.270)\tLoss 1.3867 (1.3867)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [214][10/97], lr: 0.00000\tTime 0.324 (0.348)\tData 0.000 (0.039)\tLoss 1.4945 (2.5211)\tPrec@1 90.625 (89.347)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [214][20/97], lr: 0.00000\tTime 0.323 (0.338)\tData 0.000 (0.029)\tLoss 6.0789 (2.7947)\tPrec@1 86.719 (88.988)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [214][30/97], lr: 0.00000\tTime 0.322 (0.334)\tData 0.000 (0.025)\tLoss 2.5149 (2.8092)\tPrec@1 88.281 (88.936)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [214][40/97], lr: 0.00000\tTime 0.321 (0.332)\tData 0.000 (0.023)\tLoss 1.5079 (2.6248)\tPrec@1 89.062 (89.234)\tPrec@5 99.219 (99.352)\n",
      "Epoch: [214][50/97], lr: 0.00000\tTime 0.320 (0.331)\tData 0.000 (0.022)\tLoss 3.8110 (2.6108)\tPrec@1 86.719 (89.231)\tPrec@5 100.000 (99.403)\n",
      "Epoch: [214][60/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.021)\tLoss 2.9498 (2.5472)\tPrec@1 87.500 (89.216)\tPrec@5 100.000 (99.411)\n",
      "Epoch: [214][70/97], lr: 0.00000\tTime 0.324 (0.329)\tData 0.000 (0.020)\tLoss 1.4590 (2.5397)\tPrec@1 89.062 (89.195)\tPrec@5 99.219 (99.417)\n",
      "Epoch: [214][80/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 1.1605 (2.4931)\tPrec@1 92.969 (89.111)\tPrec@5 100.000 (99.402)\n",
      "Epoch: [214][90/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.020)\tLoss 3.2753 (2.5483)\tPrec@1 85.156 (89.166)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [214][96/97], lr: 0.00000\tTime 0.313 (0.328)\tData 0.000 (0.020)\tLoss 2.5474 (2.5283)\tPrec@1 88.136 (89.175)\tPrec@5 99.153 (99.444)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.281 (0.281)\tLoss 4.1909 (4.1909)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 5.0390 (5.6614)\tPrec@1 78.000 (80.727)\tPrec@5 100.000 (98.636)\n",
      "Test: [20/100]\tTime 0.074 (0.083)\tLoss 6.2148 (5.7371)\tPrec@1 79.000 (79.762)\tPrec@5 97.000 (98.429)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.9093 (5.8859)\tPrec@1 80.000 (79.484)\tPrec@5 97.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.5685 (5.9410)\tPrec@1 84.000 (79.512)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.4076 (5.9255)\tPrec@1 86.000 (80.137)\tPrec@5 99.000 (98.255)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.1492 (5.8576)\tPrec@1 82.000 (80.230)\tPrec@5 100.000 (98.361)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 5.0678 (5.8564)\tPrec@1 80.000 (80.225)\tPrec@5 100.000 (98.437)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.7969 (5.8050)\tPrec@1 82.000 (80.568)\tPrec@5 98.000 (98.481)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.4835 (5.8447)\tPrec@1 88.000 (80.319)\tPrec@5 99.000 (98.451)\n",
      "val Results: Prec@1 80.220 Prec@5 98.440 Loss 5.88834\n",
      "val Class Accuracy: [0.911,0.962,0.793,0.710,0.809,0.740,0.819,0.742,0.754,0.782]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [215][0/97], lr: 0.00000\tTime 0.545 (0.545)\tData 0.321 (0.321)\tLoss 2.3754 (2.3754)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [215][10/97], lr: 0.00000\tTime 0.318 (0.349)\tData 0.000 (0.043)\tLoss 1.7968 (2.8184)\tPrec@1 89.062 (88.565)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [215][20/97], lr: 0.00000\tTime 0.331 (0.338)\tData 0.000 (0.031)\tLoss 2.0331 (2.6348)\tPrec@1 89.062 (88.504)\tPrec@5 99.219 (99.516)\n",
      "Epoch: [215][30/97], lr: 0.00000\tTime 0.319 (0.333)\tData 0.000 (0.026)\tLoss 2.4410 (2.5348)\tPrec@1 88.281 (88.634)\tPrec@5 99.219 (99.471)\n",
      "Epoch: [215][40/97], lr: 0.00000\tTime 0.322 (0.332)\tData 0.000 (0.024)\tLoss 2.6643 (2.4889)\tPrec@1 90.625 (88.929)\tPrec@5 100.000 (99.466)\n",
      "Epoch: [215][50/97], lr: 0.00000\tTime 0.323 (0.330)\tData 0.000 (0.022)\tLoss 1.6625 (2.4243)\tPrec@1 86.719 (88.955)\tPrec@5 100.000 (99.464)\n",
      "Epoch: [215][60/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.022)\tLoss 2.1969 (2.4431)\tPrec@1 89.844 (88.934)\tPrec@5 100.000 (99.462)\n",
      "Epoch: [215][70/97], lr: 0.00000\tTime 0.325 (0.329)\tData 0.000 (0.021)\tLoss 2.7648 (2.4006)\tPrec@1 89.844 (89.007)\tPrec@5 99.219 (99.428)\n",
      "Epoch: [215][80/97], lr: 0.00000\tTime 0.318 (0.328)\tData 0.000 (0.020)\tLoss 1.3631 (2.4104)\tPrec@1 89.844 (89.062)\tPrec@5 100.000 (99.460)\n",
      "Epoch: [215][90/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.020)\tLoss 3.5286 (2.3938)\tPrec@1 88.281 (89.088)\tPrec@5 100.000 (99.451)\n",
      "Epoch: [215][96/97], lr: 0.00000\tTime 0.312 (0.327)\tData 0.000 (0.021)\tLoss 2.5435 (2.4615)\tPrec@1 84.746 (88.876)\tPrec@5 100.000 (99.452)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.256 (0.256)\tLoss 3.9193 (3.9193)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.090)\tLoss 4.7951 (5.4636)\tPrec@1 78.000 (81.000)\tPrec@5 100.000 (98.364)\n",
      "Test: [20/100]\tTime 0.074 (0.082)\tLoss 6.0641 (5.5429)\tPrec@1 79.000 (80.095)\tPrec@5 97.000 (98.381)\n",
      "Test: [30/100]\tTime 0.074 (0.079)\tLoss 6.7442 (5.6990)\tPrec@1 81.000 (79.903)\tPrec@5 97.000 (98.290)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 4.4045 (5.7668)\tPrec@1 85.000 (79.951)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 5.1953 (5.7412)\tPrec@1 87.000 (80.451)\tPrec@5 99.000 (98.196)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.0144 (5.6708)\tPrec@1 81.000 (80.459)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 5.0085 (5.6584)\tPrec@1 80.000 (80.451)\tPrec@5 100.000 (98.394)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.3446 (5.6023)\tPrec@1 83.000 (80.679)\tPrec@5 98.000 (98.444)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.2304 (5.6346)\tPrec@1 88.000 (80.473)\tPrec@5 99.000 (98.418)\n",
      "val Results: Prec@1 80.320 Prec@5 98.420 Loss 5.68155\n",
      "val Class Accuracy: [0.898,0.963,0.810,0.670,0.804,0.756,0.836,0.738,0.766,0.791]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [216][0/97], lr: 0.00000\tTime 0.495 (0.495)\tData 0.270 (0.270)\tLoss 3.2156 (3.2156)\tPrec@1 87.500 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [216][10/97], lr: 0.00000\tTime 0.327 (0.355)\tData 0.000 (0.039)\tLoss 2.1443 (2.6360)\tPrec@1 92.969 (88.920)\tPrec@5 100.000 (99.148)\n",
      "Epoch: [216][20/97], lr: 0.00000\tTime 0.323 (0.341)\tData 0.000 (0.028)\tLoss 2.8109 (2.6011)\tPrec@1 87.500 (88.914)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [216][30/97], lr: 0.00000\tTime 0.321 (0.336)\tData 0.000 (0.025)\tLoss 1.4343 (2.4235)\tPrec@1 91.406 (89.163)\tPrec@5 100.000 (99.269)\n",
      "Epoch: [216][40/97], lr: 0.00000\tTime 0.329 (0.333)\tData 0.000 (0.023)\tLoss 1.8194 (2.4654)\tPrec@1 91.406 (88.929)\tPrec@5 100.000 (99.276)\n",
      "Epoch: [216][50/97], lr: 0.00000\tTime 0.322 (0.332)\tData 0.000 (0.022)\tLoss 3.1365 (2.5100)\tPrec@1 90.625 (89.108)\tPrec@5 100.000 (99.295)\n",
      "Epoch: [216][60/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.021)\tLoss 2.9423 (2.5121)\tPrec@1 92.188 (89.229)\tPrec@5 99.219 (99.308)\n",
      "Epoch: [216][70/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 1.6266 (2.5403)\tPrec@1 90.625 (89.206)\tPrec@5 99.219 (99.307)\n",
      "Epoch: [216][80/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.020)\tLoss 3.0200 (2.5714)\tPrec@1 89.062 (89.246)\tPrec@5 100.000 (99.354)\n",
      "Epoch: [216][90/97], lr: 0.00000\tTime 0.320 (0.329)\tData 0.000 (0.020)\tLoss 2.9328 (2.5802)\tPrec@1 85.156 (89.123)\tPrec@5 100.000 (99.390)\n",
      "Epoch: [216][96/97], lr: 0.00000\tTime 0.313 (0.328)\tData 0.000 (0.020)\tLoss 1.9276 (2.5912)\tPrec@1 89.831 (89.134)\tPrec@5 100.000 (99.412)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.293 (0.293)\tLoss 4.0581 (4.0581)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 4.7662 (5.5130)\tPrec@1 78.000 (81.091)\tPrec@5 100.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.1114 (5.5885)\tPrec@1 79.000 (80.048)\tPrec@5 97.000 (98.476)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.8094 (5.7334)\tPrec@1 81.000 (79.806)\tPrec@5 97.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.4067 (5.7938)\tPrec@1 84.000 (79.756)\tPrec@5 98.000 (98.195)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.1881 (5.7742)\tPrec@1 85.000 (80.235)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.0311 (5.7103)\tPrec@1 81.000 (80.262)\tPrec@5 99.000 (98.279)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 4.9668 (5.6995)\tPrec@1 81.000 (80.239)\tPrec@5 100.000 (98.366)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.4369 (5.6466)\tPrec@1 82.000 (80.469)\tPrec@5 98.000 (98.407)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.3037 (5.6818)\tPrec@1 88.000 (80.242)\tPrec@5 99.000 (98.363)\n",
      "val Results: Prec@1 80.130 Prec@5 98.360 Loss 5.72645\n",
      "val Class Accuracy: [0.896,0.957,0.808,0.676,0.806,0.742,0.840,0.740,0.756,0.792]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [217][0/97], lr: 0.00000\tTime 0.534 (0.534)\tData 0.322 (0.322)\tLoss 2.9976 (2.9976)\tPrec@1 87.500 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [217][10/97], lr: 0.00000\tTime 0.321 (0.344)\tData 0.000 (0.043)\tLoss 4.3005 (2.5986)\tPrec@1 85.156 (88.849)\tPrec@5 99.219 (99.077)\n",
      "Epoch: [217][20/97], lr: 0.00000\tTime 0.318 (0.335)\tData 0.000 (0.030)\tLoss 2.6847 (2.6793)\tPrec@1 85.156 (89.249)\tPrec@5 99.219 (99.293)\n",
      "Epoch: [217][30/97], lr: 0.00000\tTime 0.320 (0.332)\tData 0.000 (0.026)\tLoss 2.7060 (2.5939)\tPrec@1 85.156 (89.113)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [217][40/97], lr: 0.00000\tTime 0.326 (0.330)\tData 0.000 (0.024)\tLoss 1.9980 (2.6095)\tPrec@1 94.531 (89.425)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [217][50/97], lr: 0.00000\tTime 0.318 (0.329)\tData 0.000 (0.023)\tLoss 1.4261 (2.6372)\tPrec@1 92.188 (89.491)\tPrec@5 99.219 (99.433)\n",
      "Epoch: [217][60/97], lr: 0.00000\tTime 0.330 (0.328)\tData 0.000 (0.022)\tLoss 3.1181 (2.6001)\tPrec@1 86.719 (89.421)\tPrec@5 100.000 (99.385)\n",
      "Epoch: [217][70/97], lr: 0.00000\tTime 0.320 (0.327)\tData 0.000 (0.021)\tLoss 2.2293 (2.5813)\tPrec@1 92.969 (89.294)\tPrec@5 100.000 (99.384)\n",
      "Epoch: [217][80/97], lr: 0.00000\tTime 0.320 (0.327)\tData 0.000 (0.020)\tLoss 3.5955 (2.5750)\tPrec@1 87.500 (89.217)\tPrec@5 99.219 (99.402)\n",
      "Epoch: [217][90/97], lr: 0.00000\tTime 0.316 (0.327)\tData 0.000 (0.020)\tLoss 3.0281 (2.5504)\tPrec@1 91.406 (89.208)\tPrec@5 99.219 (99.433)\n",
      "Epoch: [217][96/97], lr: 0.00000\tTime 0.308 (0.326)\tData 0.000 (0.021)\tLoss 1.8382 (2.5414)\tPrec@1 90.678 (89.223)\tPrec@5 99.153 (99.444)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.291 (0.291)\tLoss 4.1067 (4.1067)\tPrec@1 82.000 (82.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 5.0328 (5.6513)\tPrec@1 78.000 (80.364)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.2523 (5.7262)\tPrec@1 78.000 (79.714)\tPrec@5 97.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.8915 (5.8780)\tPrec@1 81.000 (79.484)\tPrec@5 97.000 (98.258)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.5473 (5.9367)\tPrec@1 85.000 (79.512)\tPrec@5 98.000 (98.122)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.4399 (5.9208)\tPrec@1 85.000 (80.078)\tPrec@5 99.000 (98.157)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.1467 (5.8480)\tPrec@1 81.000 (80.213)\tPrec@5 100.000 (98.279)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.1712 (5.8437)\tPrec@1 81.000 (80.169)\tPrec@5 99.000 (98.352)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6668 (5.7919)\tPrec@1 82.000 (80.457)\tPrec@5 98.000 (98.395)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4394 (5.8266)\tPrec@1 89.000 (80.242)\tPrec@5 99.000 (98.407)\n",
      "val Results: Prec@1 80.160 Prec@5 98.410 Loss 5.87298\n",
      "val Class Accuracy: [0.909,0.964,0.805,0.691,0.805,0.742,0.810,0.753,0.761,0.776]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [218][0/97], lr: 0.00000\tTime 0.466 (0.466)\tData 0.239 (0.239)\tLoss 1.7260 (1.7260)\tPrec@1 87.500 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [218][10/97], lr: 0.00000\tTime 0.319 (0.341)\tData 0.000 (0.035)\tLoss 1.5886 (2.6043)\tPrec@1 92.969 (89.489)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [218][20/97], lr: 0.00000\tTime 0.326 (0.334)\tData 0.000 (0.027)\tLoss 1.2654 (2.5188)\tPrec@1 90.625 (89.211)\tPrec@5 100.000 (99.591)\n",
      "Epoch: [218][30/97], lr: 0.00000\tTime 0.328 (0.331)\tData 0.000 (0.023)\tLoss 3.8841 (2.5081)\tPrec@1 89.062 (89.340)\tPrec@5 98.438 (99.496)\n",
      "Epoch: [218][40/97], lr: 0.00000\tTime 0.318 (0.331)\tData 0.000 (0.022)\tLoss 2.5458 (2.4802)\tPrec@1 84.375 (89.043)\tPrec@5 99.219 (99.505)\n",
      "Epoch: [218][50/97], lr: 0.00000\tTime 0.319 (0.329)\tData 0.000 (0.021)\tLoss 4.4896 (2.7185)\tPrec@1 81.250 (88.802)\tPrec@5 97.656 (99.464)\n",
      "Epoch: [218][60/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 1.6037 (2.6526)\tPrec@1 88.281 (89.011)\tPrec@5 99.219 (99.488)\n",
      "Epoch: [218][70/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 4.1363 (2.5842)\tPrec@1 89.062 (89.173)\tPrec@5 98.438 (99.483)\n",
      "Epoch: [218][80/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.019)\tLoss 2.4162 (2.5761)\tPrec@1 85.938 (89.120)\tPrec@5 99.219 (99.412)\n",
      "Epoch: [218][90/97], lr: 0.00000\tTime 0.325 (0.328)\tData 0.000 (0.019)\tLoss 1.3445 (2.5305)\tPrec@1 93.750 (89.174)\tPrec@5 100.000 (99.433)\n",
      "Epoch: [218][96/97], lr: 0.00000\tTime 0.311 (0.328)\tData 0.000 (0.020)\tLoss 2.3244 (2.5579)\tPrec@1 86.441 (89.078)\tPrec@5 100.000 (99.428)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.309 (0.309)\tLoss 4.2065 (4.2065)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.095)\tLoss 4.9648 (5.6870)\tPrec@1 78.000 (80.455)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.1947 (5.7514)\tPrec@1 78.000 (79.762)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.8267 (5.9028)\tPrec@1 81.000 (79.548)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.5597 (5.9581)\tPrec@1 85.000 (79.634)\tPrec@5 98.000 (98.122)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.4266 (5.9476)\tPrec@1 86.000 (80.157)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.2177 (5.8775)\tPrec@1 81.000 (80.246)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.2162 (5.8738)\tPrec@1 82.000 (80.254)\tPrec@5 100.000 (98.394)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.6706 (5.8224)\tPrec@1 82.000 (80.556)\tPrec@5 98.000 (98.444)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.4198 (5.8566)\tPrec@1 89.000 (80.341)\tPrec@5 99.000 (98.407)\n",
      "val Results: Prec@1 80.240 Prec@5 98.400 Loss 5.90274\n",
      "val Class Accuracy: [0.906,0.964,0.806,0.688,0.818,0.739,0.820,0.749,0.757,0.777]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [219][0/97], lr: 0.00000\tTime 0.488 (0.488)\tData 0.258 (0.258)\tLoss 1.8449 (1.8449)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [219][10/97], lr: 0.00000\tTime 0.322 (0.344)\tData 0.000 (0.038)\tLoss 2.3165 (2.0809)\tPrec@1 88.281 (90.057)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [219][20/97], lr: 0.00000\tTime 0.326 (0.334)\tData 0.000 (0.028)\tLoss 4.1405 (2.4317)\tPrec@1 84.375 (89.658)\tPrec@5 99.219 (99.554)\n",
      "Epoch: [219][30/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.024)\tLoss 6.1774 (2.6335)\tPrec@1 88.281 (89.743)\tPrec@5 99.219 (99.546)\n",
      "Epoch: [219][40/97], lr: 0.00000\tTime 0.326 (0.329)\tData 0.000 (0.023)\tLoss 3.1679 (2.6048)\tPrec@1 92.188 (89.729)\tPrec@5 99.219 (99.524)\n",
      "Epoch: [219][50/97], lr: 0.00000\tTime 0.319 (0.328)\tData 0.000 (0.021)\tLoss 2.0257 (2.5134)\tPrec@1 90.625 (89.798)\tPrec@5 100.000 (99.525)\n",
      "Epoch: [219][60/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.021)\tLoss 3.8275 (2.4995)\tPrec@1 92.188 (89.921)\tPrec@5 100.000 (99.565)\n",
      "Epoch: [219][70/97], lr: 0.00000\tTime 0.319 (0.327)\tData 0.000 (0.020)\tLoss 2.9278 (2.5190)\tPrec@1 85.156 (89.701)\tPrec@5 99.219 (99.516)\n",
      "Epoch: [219][80/97], lr: 0.00000\tTime 0.322 (0.327)\tData 0.000 (0.020)\tLoss 1.8001 (2.4930)\tPrec@1 89.062 (89.670)\tPrec@5 100.000 (99.508)\n",
      "Epoch: [219][90/97], lr: 0.00000\tTime 0.320 (0.327)\tData 0.000 (0.020)\tLoss 2.1332 (2.4550)\tPrec@1 92.969 (89.663)\tPrec@5 99.219 (99.485)\n",
      "Epoch: [219][96/97], lr: 0.00000\tTime 0.309 (0.326)\tData 0.000 (0.020)\tLoss 3.5430 (2.4660)\tPrec@1 86.441 (89.602)\tPrec@5 100.000 (99.484)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.295 (0.295)\tLoss 4.0795 (4.0795)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.093)\tLoss 4.9047 (5.6313)\tPrec@1 78.000 (80.545)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.1965 (5.7089)\tPrec@1 78.000 (79.714)\tPrec@5 97.000 (98.381)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 6.8225 (5.8648)\tPrec@1 80.000 (79.452)\tPrec@5 97.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.5325 (5.9265)\tPrec@1 85.000 (79.415)\tPrec@5 98.000 (98.195)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.4097 (5.9121)\tPrec@1 87.000 (80.020)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.1292 (5.8413)\tPrec@1 81.000 (80.066)\tPrec@5 100.000 (98.295)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.2260 (5.8375)\tPrec@1 81.000 (80.028)\tPrec@5 100.000 (98.380)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6723 (5.7870)\tPrec@1 81.000 (80.321)\tPrec@5 98.000 (98.432)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.3563 (5.8190)\tPrec@1 89.000 (80.143)\tPrec@5 99.000 (98.396)\n",
      "val Results: Prec@1 80.080 Prec@5 98.390 Loss 5.86449\n",
      "val Class Accuracy: [0.898,0.964,0.811,0.684,0.804,0.744,0.821,0.748,0.760,0.774]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [220][0/97], lr: 0.00000\tTime 0.521 (0.521)\tData 0.319 (0.319)\tLoss 3.2159 (3.2159)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [220][10/97], lr: 0.00000\tTime 0.320 (0.349)\tData 0.000 (0.044)\tLoss 4.5944 (3.1986)\tPrec@1 86.719 (89.702)\tPrec@5 99.219 (99.361)\n",
      "Epoch: [220][20/97], lr: 0.00000\tTime 0.321 (0.337)\tData 0.000 (0.031)\tLoss 3.8790 (2.9276)\tPrec@1 82.812 (89.546)\tPrec@5 99.219 (99.330)\n",
      "Epoch: [220][30/97], lr: 0.00000\tTime 0.324 (0.333)\tData 0.000 (0.027)\tLoss 6.7012 (2.8308)\tPrec@1 87.500 (89.138)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [220][40/97], lr: 0.00000\tTime 0.323 (0.331)\tData 0.000 (0.024)\tLoss 4.8438 (2.6899)\tPrec@1 84.375 (88.948)\tPrec@5 100.000 (99.505)\n",
      "Epoch: [220][50/97], lr: 0.00000\tTime 0.323 (0.330)\tData 0.000 (0.023)\tLoss 2.0617 (2.6438)\tPrec@1 90.625 (88.940)\tPrec@5 99.219 (99.510)\n",
      "Epoch: [220][60/97], lr: 0.00000\tTime 0.339 (0.330)\tData 0.000 (0.022)\tLoss 3.4302 (2.7119)\tPrec@1 88.281 (88.781)\tPrec@5 100.000 (99.552)\n",
      "Epoch: [220][70/97], lr: 0.00000\tTime 0.327 (0.329)\tData 0.000 (0.021)\tLoss 1.5473 (2.6877)\tPrec@1 89.844 (88.974)\tPrec@5 100.000 (99.549)\n",
      "Epoch: [220][80/97], lr: 0.00000\tTime 0.344 (0.331)\tData 0.000 (0.021)\tLoss 3.6388 (2.6328)\tPrec@1 87.500 (89.014)\tPrec@5 97.656 (99.537)\n",
      "Epoch: [220][90/97], lr: 0.00000\tTime 0.332 (0.333)\tData 0.000 (0.020)\tLoss 2.1218 (2.5818)\tPrec@1 89.062 (89.011)\tPrec@5 100.000 (99.493)\n",
      "Epoch: [220][96/97], lr: 0.00000\tTime 0.317 (0.332)\tData 0.000 (0.021)\tLoss 2.0971 (2.5900)\tPrec@1 88.136 (88.965)\tPrec@5 100.000 (99.476)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.383 (0.383)\tLoss 4.2934 (4.2934)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.101)\tLoss 5.1159 (5.7901)\tPrec@1 78.000 (80.182)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.088)\tLoss 6.3000 (5.8606)\tPrec@1 78.000 (79.571)\tPrec@5 97.000 (98.429)\n",
      "Test: [30/100]\tTime 0.073 (0.083)\tLoss 6.9590 (6.0113)\tPrec@1 81.000 (79.387)\tPrec@5 97.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.081)\tLoss 4.6864 (6.0623)\tPrec@1 85.000 (79.512)\tPrec@5 98.000 (98.268)\n",
      "Test: [50/100]\tTime 0.073 (0.079)\tLoss 5.4985 (6.0550)\tPrec@1 87.000 (80.059)\tPrec@5 99.000 (98.255)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 8.3113 (5.9766)\tPrec@1 81.000 (80.180)\tPrec@5 100.000 (98.361)\n",
      "Test: [70/100]\tTime 0.073 (0.078)\tLoss 5.3160 (5.9775)\tPrec@1 81.000 (80.169)\tPrec@5 100.000 (98.437)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.8654 (5.9277)\tPrec@1 82.000 (80.444)\tPrec@5 98.000 (98.469)\n",
      "Test: [90/100]\tTime 0.073 (0.077)\tLoss 3.5401 (5.9664)\tPrec@1 88.000 (80.231)\tPrec@5 99.000 (98.418)\n",
      "val Results: Prec@1 80.120 Prec@5 98.430 Loss 6.01220\n",
      "val Class Accuracy: [0.911,0.964,0.809,0.687,0.804,0.741,0.825,0.751,0.748,0.772]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [221][0/97], lr: 0.00000\tTime 0.496 (0.496)\tData 0.274 (0.274)\tLoss 6.7711 (6.7711)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [221][10/97], lr: 0.00000\tTime 0.322 (0.347)\tData 0.000 (0.040)\tLoss 5.1246 (2.8119)\tPrec@1 90.625 (90.554)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [221][20/97], lr: 0.00000\tTime 0.332 (0.337)\tData 0.000 (0.029)\tLoss 2.6114 (2.6289)\tPrec@1 87.500 (89.509)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [221][30/97], lr: 0.00000\tTime 0.322 (0.333)\tData 0.000 (0.025)\tLoss 1.9018 (2.6788)\tPrec@1 90.625 (89.516)\tPrec@5 100.000 (99.471)\n",
      "Epoch: [221][40/97], lr: 0.00000\tTime 0.328 (0.332)\tData 0.000 (0.023)\tLoss 2.5038 (2.7372)\tPrec@1 89.062 (89.234)\tPrec@5 99.219 (99.447)\n",
      "Epoch: [221][50/97], lr: 0.00000\tTime 0.316 (0.330)\tData 0.000 (0.022)\tLoss 4.2572 (2.6729)\tPrec@1 88.281 (89.231)\tPrec@5 99.219 (99.464)\n",
      "Epoch: [221][60/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.021)\tLoss 2.2427 (2.6978)\tPrec@1 84.375 (89.037)\tPrec@5 100.000 (99.513)\n",
      "Epoch: [221][70/97], lr: 0.00000\tTime 0.320 (0.329)\tData 0.000 (0.020)\tLoss 2.1100 (2.6767)\tPrec@1 87.500 (88.908)\tPrec@5 100.000 (99.483)\n",
      "Epoch: [221][80/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.020)\tLoss 1.2291 (2.6518)\tPrec@1 91.406 (88.947)\tPrec@5 99.219 (99.518)\n",
      "Epoch: [221][90/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 2.7448 (2.6319)\tPrec@1 89.062 (88.959)\tPrec@5 99.219 (99.502)\n",
      "Epoch: [221][96/97], lr: 0.00000\tTime 0.311 (0.328)\tData 0.000 (0.020)\tLoss 1.7574 (2.6049)\tPrec@1 91.525 (88.981)\tPrec@5 99.153 (99.476)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.322 (0.322)\tLoss 4.1341 (4.1341)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.096)\tLoss 4.9804 (5.5889)\tPrec@1 78.000 (80.909)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.074 (0.085)\tLoss 6.1978 (5.6573)\tPrec@1 79.000 (80.000)\tPrec@5 97.000 (98.429)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 6.8924 (5.8146)\tPrec@1 80.000 (79.774)\tPrec@5 97.000 (98.355)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.4719 (5.8750)\tPrec@1 85.000 (79.878)\tPrec@5 98.000 (98.220)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.3366 (5.8607)\tPrec@1 87.000 (80.333)\tPrec@5 99.000 (98.235)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 8.0954 (5.7917)\tPrec@1 81.000 (80.361)\tPrec@5 100.000 (98.344)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.0888 (5.7863)\tPrec@1 82.000 (80.296)\tPrec@5 100.000 (98.423)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.5420 (5.7302)\tPrec@1 82.000 (80.568)\tPrec@5 98.000 (98.469)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4384 (5.7674)\tPrec@1 88.000 (80.341)\tPrec@5 99.000 (98.418)\n",
      "val Results: Prec@1 80.230 Prec@5 98.430 Loss 5.81024\n",
      "val Class Accuracy: [0.909,0.961,0.807,0.677,0.807,0.751,0.831,0.746,0.751,0.783]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [222][0/97], lr: 0.00000\tTime 0.507 (0.507)\tData 0.282 (0.282)\tLoss 1.5530 (1.5530)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [222][10/97], lr: 0.00000\tTime 0.326 (0.350)\tData 0.000 (0.040)\tLoss 2.8066 (2.5438)\tPrec@1 89.062 (89.560)\tPrec@5 99.219 (99.645)\n",
      "Epoch: [222][20/97], lr: 0.00000\tTime 0.324 (0.337)\tData 0.000 (0.029)\tLoss 1.2719 (2.5798)\tPrec@1 88.281 (88.839)\tPrec@5 100.000 (99.591)\n",
      "Epoch: [222][30/97], lr: 0.00000\tTime 0.324 (0.334)\tData 0.000 (0.025)\tLoss 2.3000 (2.4564)\tPrec@1 90.625 (89.012)\tPrec@5 100.000 (99.672)\n",
      "Epoch: [222][40/97], lr: 0.00000\tTime 0.334 (0.332)\tData 0.000 (0.023)\tLoss 2.3447 (2.3812)\tPrec@1 92.188 (89.348)\tPrec@5 99.219 (99.524)\n",
      "Epoch: [222][50/97], lr: 0.00000\tTime 0.327 (0.331)\tData 0.000 (0.022)\tLoss 1.0724 (2.4098)\tPrec@1 89.844 (89.338)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [222][60/97], lr: 0.00000\tTime 0.324 (0.330)\tData 0.000 (0.021)\tLoss 2.7668 (2.4988)\tPrec@1 85.938 (89.229)\tPrec@5 100.000 (99.449)\n",
      "Epoch: [222][70/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.021)\tLoss 3.2917 (2.4649)\tPrec@1 89.844 (89.338)\tPrec@5 100.000 (99.417)\n",
      "Epoch: [222][80/97], lr: 0.00000\tTime 0.331 (0.329)\tData 0.000 (0.020)\tLoss 1.9857 (2.4796)\tPrec@1 89.062 (89.323)\tPrec@5 100.000 (99.460)\n",
      "Epoch: [222][90/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.020)\tLoss 2.6786 (2.4665)\tPrec@1 86.719 (89.277)\tPrec@5 100.000 (99.476)\n",
      "Epoch: [222][96/97], lr: 0.00000\tTime 0.314 (0.329)\tData 0.000 (0.020)\tLoss 1.2523 (2.4813)\tPrec@1 94.068 (89.304)\tPrec@5 99.153 (99.468)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.285 (0.285)\tLoss 4.2412 (4.2412)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 5.1039 (5.7424)\tPrec@1 78.000 (80.091)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.2689 (5.8150)\tPrec@1 79.000 (79.429)\tPrec@5 97.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.9824 (5.9594)\tPrec@1 81.000 (79.355)\tPrec@5 97.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.6233 (6.0147)\tPrec@1 85.000 (79.561)\tPrec@5 98.000 (98.195)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.4591 (6.0085)\tPrec@1 85.000 (80.059)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.2420 (5.9353)\tPrec@1 80.000 (80.066)\tPrec@5 100.000 (98.295)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 5.2355 (5.9333)\tPrec@1 82.000 (80.085)\tPrec@5 100.000 (98.366)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.7808 (5.8839)\tPrec@1 82.000 (80.383)\tPrec@5 98.000 (98.407)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.5258 (5.9209)\tPrec@1 89.000 (80.165)\tPrec@5 99.000 (98.352)\n",
      "val Results: Prec@1 80.070 Prec@5 98.350 Loss 5.96467\n",
      "val Class Accuracy: [0.906,0.964,0.813,0.682,0.795,0.746,0.835,0.746,0.744,0.776]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [223][0/97], lr: 0.00000\tTime 0.467 (0.467)\tData 0.267 (0.267)\tLoss 1.8047 (1.8047)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [223][10/97], lr: 0.00000\tTime 0.319 (0.341)\tData 0.000 (0.038)\tLoss 2.1486 (2.6895)\tPrec@1 92.969 (88.778)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [223][20/97], lr: 0.00000\tTime 0.320 (0.334)\tData 0.000 (0.028)\tLoss 2.5836 (2.4057)\tPrec@1 90.625 (89.397)\tPrec@5 99.219 (99.405)\n",
      "Epoch: [223][30/97], lr: 0.00000\tTime 0.326 (0.331)\tData 0.000 (0.025)\tLoss 2.0629 (2.3978)\tPrec@1 89.062 (89.012)\tPrec@5 100.000 (99.446)\n",
      "Epoch: [223][40/97], lr: 0.00000\tTime 0.324 (0.330)\tData 0.000 (0.023)\tLoss 1.8510 (2.4112)\tPrec@1 93.750 (89.101)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [223][50/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.022)\tLoss 2.4884 (2.3909)\tPrec@1 83.594 (89.216)\tPrec@5 98.438 (99.433)\n",
      "Epoch: [223][60/97], lr: 0.00000\tTime 0.330 (0.328)\tData 0.000 (0.021)\tLoss 2.4131 (2.4374)\tPrec@1 91.406 (89.127)\tPrec@5 100.000 (99.462)\n",
      "Epoch: [223][70/97], lr: 0.00000\tTime 0.323 (0.328)\tData 0.000 (0.020)\tLoss 3.6385 (2.4779)\tPrec@1 92.188 (89.129)\tPrec@5 100.000 (99.472)\n",
      "Epoch: [223][80/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 3.3051 (2.4633)\tPrec@1 84.375 (88.976)\tPrec@5 97.656 (99.441)\n",
      "Epoch: [223][90/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 3.1439 (2.4165)\tPrec@1 89.062 (89.217)\tPrec@5 99.219 (99.476)\n",
      "Epoch: [223][96/97], lr: 0.00000\tTime 0.310 (0.328)\tData 0.000 (0.020)\tLoss 2.1615 (2.4419)\tPrec@1 88.983 (89.215)\tPrec@5 99.153 (99.460)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.326 (0.326)\tLoss 4.1364 (4.1364)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.096)\tLoss 4.9162 (5.6374)\tPrec@1 78.000 (80.909)\tPrec@5 100.000 (98.545)\n",
      "Test: [20/100]\tTime 0.074 (0.085)\tLoss 6.1639 (5.7072)\tPrec@1 79.000 (80.048)\tPrec@5 97.000 (98.429)\n",
      "Test: [30/100]\tTime 0.074 (0.082)\tLoss 6.8750 (5.8635)\tPrec@1 80.000 (79.839)\tPrec@5 97.000 (98.323)\n",
      "Test: [40/100]\tTime 0.074 (0.080)\tLoss 4.5989 (5.9251)\tPrec@1 85.000 (79.878)\tPrec@5 98.000 (98.195)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.4248 (5.9060)\tPrec@1 87.000 (80.373)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 8.1846 (5.8392)\tPrec@1 83.000 (80.475)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.1929 (5.8372)\tPrec@1 80.000 (80.465)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.6436 (5.7813)\tPrec@1 81.000 (80.728)\tPrec@5 98.000 (98.457)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.3798 (5.8126)\tPrec@1 88.000 (80.484)\tPrec@5 99.000 (98.440)\n",
      "val Results: Prec@1 80.330 Prec@5 98.450 Loss 5.85878\n",
      "val Class Accuracy: [0.915,0.962,0.806,0.693,0.807,0.751,0.828,0.729,0.764,0.778]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [224][0/97], lr: 0.00000\tTime 0.528 (0.528)\tData 0.300 (0.300)\tLoss 3.6526 (3.6526)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [224][10/97], lr: 0.00000\tTime 0.325 (0.354)\tData 0.000 (0.042)\tLoss 2.5337 (2.3683)\tPrec@1 92.969 (89.560)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [224][20/97], lr: 0.00000\tTime 0.330 (0.340)\tData 0.000 (0.030)\tLoss 4.2143 (2.4722)\tPrec@1 89.062 (89.211)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [224][30/97], lr: 0.00000\tTime 0.324 (0.335)\tData 0.000 (0.025)\tLoss 1.2234 (2.4114)\tPrec@1 88.281 (89.516)\tPrec@5 100.000 (99.521)\n",
      "Epoch: [224][40/97], lr: 0.00000\tTime 0.325 (0.335)\tData 0.000 (0.023)\tLoss 3.3029 (2.5154)\tPrec@1 89.844 (89.691)\tPrec@5 100.000 (99.447)\n",
      "Epoch: [224][50/97], lr: 0.00000\tTime 0.324 (0.333)\tData 0.000 (0.022)\tLoss 3.1235 (2.4603)\tPrec@1 90.625 (89.537)\tPrec@5 99.219 (99.387)\n",
      "Epoch: [224][60/97], lr: 0.00000\tTime 0.322 (0.332)\tData 0.000 (0.021)\tLoss 1.9969 (2.3840)\tPrec@1 88.281 (89.677)\tPrec@5 99.219 (99.398)\n",
      "Epoch: [224][70/97], lr: 0.00000\tTime 0.327 (0.332)\tData 0.000 (0.021)\tLoss 2.0381 (2.3446)\tPrec@1 90.625 (89.657)\tPrec@5 99.219 (99.373)\n",
      "Epoch: [224][80/97], lr: 0.00000\tTime 0.340 (0.331)\tData 0.000 (0.020)\tLoss 1.2314 (2.4134)\tPrec@1 92.969 (89.680)\tPrec@5 98.438 (99.363)\n",
      "Epoch: [224][90/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.020)\tLoss 3.9884 (2.4110)\tPrec@1 92.969 (89.672)\tPrec@5 99.219 (99.365)\n",
      "Epoch: [224][96/97], lr: 0.00000\tTime 0.313 (0.331)\tData 0.000 (0.020)\tLoss 2.0548 (2.4301)\tPrec@1 90.678 (89.763)\tPrec@5 100.000 (99.339)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.304 (0.304)\tLoss 4.1904 (4.1904)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.094)\tLoss 5.0069 (5.6940)\tPrec@1 78.000 (80.727)\tPrec@5 99.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.2666 (5.7761)\tPrec@1 79.000 (79.857)\tPrec@5 97.000 (98.476)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.9508 (5.9212)\tPrec@1 81.000 (79.806)\tPrec@5 97.000 (98.419)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.6315 (5.9772)\tPrec@1 84.000 (79.829)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.3744 (5.9668)\tPrec@1 86.000 (80.255)\tPrec@5 99.000 (98.235)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.2446 (5.8957)\tPrec@1 80.000 (80.262)\tPrec@5 99.000 (98.328)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.2621 (5.8961)\tPrec@1 81.000 (80.254)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6799 (5.8424)\tPrec@1 83.000 (80.531)\tPrec@5 98.000 (98.444)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.4854 (5.8795)\tPrec@1 89.000 (80.308)\tPrec@5 99.000 (98.407)\n",
      "val Results: Prec@1 80.200 Prec@5 98.410 Loss 5.92522\n",
      "val Class Accuracy: [0.909,0.964,0.813,0.668,0.807,0.747,0.843,0.743,0.748,0.778]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [225][0/97], lr: 0.00000\tTime 0.546 (0.546)\tData 0.324 (0.324)\tLoss 4.6076 (4.6076)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [225][10/97], lr: 0.00000\tTime 0.354 (0.359)\tData 0.000 (0.044)\tLoss 2.0734 (2.5164)\tPrec@1 92.188 (90.270)\tPrec@5 98.438 (99.290)\n",
      "Epoch: [225][20/97], lr: 0.00000\tTime 0.507 (0.399)\tData 0.001 (0.031)\tLoss 2.1216 (2.3049)\tPrec@1 89.062 (89.732)\tPrec@5 99.219 (99.293)\n",
      "Epoch: [225][30/97], lr: 0.00000\tTime 0.324 (0.394)\tData 0.000 (0.025)\tLoss 1.9195 (2.1987)\tPrec@1 92.188 (89.693)\tPrec@5 100.000 (99.496)\n",
      "Epoch: [225][40/97], lr: 0.00000\tTime 0.351 (0.379)\tData 0.000 (0.023)\tLoss 2.3521 (2.4411)\tPrec@1 88.281 (89.253)\tPrec@5 98.438 (99.466)\n",
      "Epoch: [225][50/97], lr: 0.00000\tTime 0.320 (0.369)\tData 0.000 (0.022)\tLoss 1.8558 (2.3825)\tPrec@1 89.062 (89.216)\tPrec@5 99.219 (99.494)\n",
      "Epoch: [225][60/97], lr: 0.00000\tTime 0.323 (0.362)\tData 0.000 (0.021)\tLoss 2.0837 (2.3803)\tPrec@1 90.625 (89.408)\tPrec@5 100.000 (99.488)\n",
      "Epoch: [225][70/97], lr: 0.00000\tTime 0.322 (0.356)\tData 0.000 (0.020)\tLoss 2.2459 (2.4018)\tPrec@1 86.719 (89.228)\tPrec@5 99.219 (99.428)\n",
      "Epoch: [225][80/97], lr: 0.00000\tTime 0.320 (0.352)\tData 0.000 (0.020)\tLoss 2.0811 (2.3996)\tPrec@1 87.500 (89.226)\tPrec@5 99.219 (99.431)\n",
      "Epoch: [225][90/97], lr: 0.00000\tTime 0.499 (0.354)\tData 0.000 (0.020)\tLoss 4.3015 (2.4439)\tPrec@1 86.719 (89.217)\tPrec@5 99.219 (99.408)\n",
      "Epoch: [225][96/97], lr: 0.00000\tTime 0.491 (0.359)\tData 0.000 (0.019)\tLoss 3.2242 (2.4443)\tPrec@1 88.136 (89.239)\tPrec@5 97.458 (99.404)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.595 (0.595)\tLoss 4.1145 (4.1145)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.075 (0.127)\tLoss 5.0079 (5.6665)\tPrec@1 78.000 (80.364)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.074 (0.103)\tLoss 6.2285 (5.7430)\tPrec@1 79.000 (79.810)\tPrec@5 97.000 (98.381)\n",
      "Test: [30/100]\tTime 0.074 (0.095)\tLoss 6.8998 (5.9042)\tPrec@1 81.000 (79.581)\tPrec@5 97.000 (98.355)\n",
      "Test: [40/100]\tTime 0.074 (0.093)\tLoss 4.5306 (5.9692)\tPrec@1 85.000 (79.683)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.074 (0.089)\tLoss 5.4975 (5.9600)\tPrec@1 86.000 (80.235)\tPrec@5 99.000 (98.255)\n",
      "Test: [60/100]\tTime 0.076 (0.087)\tLoss 8.1559 (5.8887)\tPrec@1 82.000 (80.328)\tPrec@5 100.000 (98.377)\n",
      "Test: [70/100]\tTime 0.076 (0.086)\tLoss 5.1973 (5.8901)\tPrec@1 82.000 (80.310)\tPrec@5 100.000 (98.437)\n",
      "Test: [80/100]\tTime 0.075 (0.085)\tLoss 6.7097 (5.8354)\tPrec@1 82.000 (80.605)\tPrec@5 98.000 (98.469)\n",
      "Test: [90/100]\tTime 0.077 (0.084)\tLoss 3.4664 (5.8707)\tPrec@1 88.000 (80.374)\tPrec@5 99.000 (98.440)\n",
      "val Results: Prec@1 80.260 Prec@5 98.430 Loss 5.91660\n",
      "val Class Accuracy: [0.911,0.964,0.810,0.683,0.800,0.741,0.834,0.755,0.756,0.772]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [226][0/97], lr: 0.00000\tTime 0.826 (0.826)\tData 0.479 (0.479)\tLoss 2.7712 (2.7712)\tPrec@1 87.500 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [226][10/97], lr: 0.00000\tTime 0.359 (0.435)\tData 0.000 (0.055)\tLoss 1.8133 (2.5914)\tPrec@1 85.938 (88.423)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [226][20/97], lr: 0.00000\tTime 0.366 (0.406)\tData 0.000 (0.037)\tLoss 4.3522 (2.4943)\tPrec@1 86.719 (88.951)\tPrec@5 100.000 (99.665)\n",
      "Epoch: [226][30/97], lr: 0.00000\tTime 0.322 (0.383)\tData 0.000 (0.030)\tLoss 1.8608 (2.2190)\tPrec@1 92.188 (89.390)\tPrec@5 100.000 (99.622)\n",
      "Epoch: [226][40/97], lr: 0.00000\tTime 0.320 (0.369)\tData 0.000 (0.027)\tLoss 4.9447 (2.4056)\tPrec@1 85.156 (89.215)\tPrec@5 99.219 (99.524)\n",
      "Epoch: [226][50/97], lr: 0.00000\tTime 0.324 (0.360)\tData 0.000 (0.025)\tLoss 2.7848 (2.4326)\tPrec@1 89.844 (89.062)\tPrec@5 100.000 (99.556)\n",
      "Epoch: [226][60/97], lr: 0.00000\tTime 0.319 (0.354)\tData 0.000 (0.024)\tLoss 1.3116 (2.4066)\tPrec@1 89.844 (89.075)\tPrec@5 100.000 (99.565)\n",
      "Epoch: [226][70/97], lr: 0.00000\tTime 0.327 (0.350)\tData 0.000 (0.023)\tLoss 1.8132 (2.3608)\tPrec@1 89.062 (89.173)\tPrec@5 100.000 (99.571)\n",
      "Epoch: [226][80/97], lr: 0.00000\tTime 0.331 (0.347)\tData 0.000 (0.022)\tLoss 4.0213 (2.3908)\tPrec@1 89.062 (89.226)\tPrec@5 99.219 (99.566)\n",
      "Epoch: [226][90/97], lr: 0.00000\tTime 0.322 (0.345)\tData 0.000 (0.021)\tLoss 3.7630 (2.4081)\tPrec@1 89.844 (89.054)\tPrec@5 100.000 (99.519)\n",
      "Epoch: [226][96/97], lr: 0.00000\tTime 0.328 (0.344)\tData 0.000 (0.022)\tLoss 2.6868 (2.4104)\tPrec@1 89.831 (89.070)\tPrec@5 97.458 (99.500)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.298 (0.298)\tLoss 4.0869 (4.0869)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.094)\tLoss 4.8815 (5.5472)\tPrec@1 78.000 (80.455)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.074 (0.084)\tLoss 6.2220 (5.6325)\tPrec@1 79.000 (79.667)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.8470 (5.7884)\tPrec@1 81.000 (79.581)\tPrec@5 97.000 (98.258)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.4591 (5.8474)\tPrec@1 85.000 (79.683)\tPrec@5 98.000 (98.171)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.3292 (5.8277)\tPrec@1 87.000 (80.255)\tPrec@5 99.000 (98.196)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.0599 (5.7600)\tPrec@1 82.000 (80.377)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.0088 (5.7578)\tPrec@1 81.000 (80.366)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.5649 (5.7011)\tPrec@1 82.000 (80.642)\tPrec@5 98.000 (98.444)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4178 (5.7376)\tPrec@1 88.000 (80.429)\tPrec@5 99.000 (98.418)\n",
      "val Results: Prec@1 80.320 Prec@5 98.420 Loss 5.78300\n",
      "val Class Accuracy: [0.914,0.961,0.805,0.691,0.807,0.739,0.826,0.748,0.761,0.780]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [227][0/97], lr: 0.00000\tTime 0.472 (0.472)\tData 0.248 (0.248)\tLoss 2.3015 (2.3015)\tPrec@1 85.156 (85.156)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [227][10/97], lr: 0.00000\tTime 0.318 (0.345)\tData 0.000 (0.037)\tLoss 2.1773 (2.5144)\tPrec@1 89.844 (88.565)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [227][20/97], lr: 0.00000\tTime 0.323 (0.336)\tData 0.000 (0.027)\tLoss 4.3569 (2.7085)\tPrec@1 87.500 (88.393)\tPrec@5 100.000 (99.368)\n",
      "Epoch: [227][30/97], lr: 0.00000\tTime 0.326 (0.333)\tData 0.000 (0.024)\tLoss 2.4907 (2.6014)\tPrec@1 87.500 (88.886)\tPrec@5 100.000 (99.521)\n",
      "Epoch: [227][40/97], lr: 0.00000\tTime 0.326 (0.331)\tData 0.000 (0.022)\tLoss 5.2638 (2.8181)\tPrec@1 80.469 (88.681)\tPrec@5 100.000 (99.466)\n",
      "Epoch: [227][50/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.021)\tLoss 3.2398 (2.6795)\tPrec@1 89.062 (88.925)\tPrec@5 99.219 (99.525)\n",
      "Epoch: [227][60/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.021)\tLoss 2.1052 (2.6326)\tPrec@1 93.750 (88.806)\tPrec@5 100.000 (99.475)\n",
      "Epoch: [227][70/97], lr: 0.00000\tTime 0.324 (0.329)\tData 0.000 (0.020)\tLoss 2.6284 (2.7115)\tPrec@1 89.062 (88.688)\tPrec@5 100.000 (99.450)\n",
      "Epoch: [227][80/97], lr: 0.00000\tTime 0.323 (0.328)\tData 0.000 (0.020)\tLoss 1.1178 (2.6131)\tPrec@1 90.625 (88.725)\tPrec@5 99.219 (99.421)\n",
      "Epoch: [227][90/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.020)\tLoss 1.8198 (2.5548)\tPrec@1 94.531 (88.942)\tPrec@5 99.219 (99.459)\n",
      "Epoch: [227][96/97], lr: 0.00000\tTime 0.309 (0.327)\tData 0.000 (0.020)\tLoss 5.1404 (2.6015)\tPrec@1 78.814 (88.876)\tPrec@5 100.000 (99.460)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.321 (0.321)\tLoss 4.2030 (4.2030)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.096)\tLoss 5.0957 (5.6828)\tPrec@1 78.000 (80.273)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 6.2539 (5.7562)\tPrec@1 79.000 (79.714)\tPrec@5 97.000 (98.429)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 6.9445 (5.9047)\tPrec@1 80.000 (79.516)\tPrec@5 97.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.5446 (5.9626)\tPrec@1 85.000 (79.585)\tPrec@5 98.000 (98.220)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.4500 (5.9528)\tPrec@1 87.000 (80.098)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.2045 (5.8853)\tPrec@1 81.000 (80.131)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.1890 (5.8822)\tPrec@1 81.000 (80.127)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.7120 (5.8264)\tPrec@1 82.000 (80.420)\tPrec@5 98.000 (98.457)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.5435 (5.8663)\tPrec@1 89.000 (80.220)\tPrec@5 99.000 (98.418)\n",
      "val Results: Prec@1 80.130 Prec@5 98.410 Loss 5.91072\n",
      "val Class Accuracy: [0.912,0.963,0.806,0.683,0.810,0.748,0.820,0.748,0.745,0.778]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [228][0/97], lr: 0.00000\tTime 0.437 (0.437)\tData 0.234 (0.234)\tLoss 3.8952 (3.8952)\tPrec@1 82.031 (82.031)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [228][10/97], lr: 0.00000\tTime 0.320 (0.340)\tData 0.000 (0.035)\tLoss 2.5438 (2.7557)\tPrec@1 90.625 (88.991)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [228][20/97], lr: 0.00000\tTime 0.322 (0.332)\tData 0.000 (0.027)\tLoss 1.6423 (2.4925)\tPrec@1 92.188 (89.695)\tPrec@5 100.000 (99.591)\n",
      "Epoch: [228][30/97], lr: 0.00000\tTime 0.322 (0.332)\tData 0.000 (0.023)\tLoss 3.4440 (2.6144)\tPrec@1 85.156 (89.315)\tPrec@5 97.656 (99.420)\n",
      "Epoch: [228][40/97], lr: 0.00000\tTime 0.327 (0.331)\tData 0.000 (0.022)\tLoss 1.3164 (2.6605)\tPrec@1 92.969 (89.177)\tPrec@5 100.000 (99.371)\n",
      "Epoch: [228][50/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.021)\tLoss 2.1147 (2.7086)\tPrec@1 88.281 (89.124)\tPrec@5 99.219 (99.326)\n",
      "Epoch: [228][60/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.020)\tLoss 1.3785 (2.6398)\tPrec@1 89.062 (89.088)\tPrec@5 99.219 (99.296)\n",
      "Epoch: [228][70/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.020)\tLoss 1.5077 (2.6111)\tPrec@1 89.062 (89.085)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [228][80/97], lr: 0.00000\tTime 0.318 (0.329)\tData 0.000 (0.019)\tLoss 2.9707 (2.5919)\tPrec@1 88.281 (89.178)\tPrec@5 99.219 (99.325)\n",
      "Epoch: [228][90/97], lr: 0.00000\tTime 0.320 (0.329)\tData 0.000 (0.019)\tLoss 1.5977 (2.6384)\tPrec@1 94.531 (89.123)\tPrec@5 99.219 (99.356)\n",
      "Epoch: [228][96/97], lr: 0.00000\tTime 0.312 (0.329)\tData 0.000 (0.020)\tLoss 2.9572 (2.6111)\tPrec@1 93.220 (89.207)\tPrec@5 100.000 (99.379)\n",
      "Gated Network Weight Gate= Flip:0.53, Sc:0.47\n",
      "Test: [0/100]\tTime 0.297 (0.297)\tLoss 4.1179 (4.1179)\tPrec@1 83.000 (83.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.074 (0.094)\tLoss 4.8946 (5.6173)\tPrec@1 78.000 (80.545)\tPrec@5 99.000 (98.182)\n",
      "Test: [20/100]\tTime 0.074 (0.084)\tLoss 6.2602 (5.6818)\tPrec@1 79.000 (79.905)\tPrec@5 97.000 (98.190)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.8834 (5.8373)\tPrec@1 81.000 (79.677)\tPrec@5 97.000 (98.194)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.5223 (5.9015)\tPrec@1 85.000 (79.683)\tPrec@5 98.000 (98.122)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.3878 (5.8903)\tPrec@1 86.000 (80.157)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.1246 (5.8220)\tPrec@1 83.000 (80.328)\tPrec@5 100.000 (98.295)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.1917 (5.8185)\tPrec@1 81.000 (80.225)\tPrec@5 100.000 (98.380)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.5391 (5.7636)\tPrec@1 83.000 (80.531)\tPrec@5 98.000 (98.432)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4233 (5.7962)\tPrec@1 89.000 (80.319)\tPrec@5 99.000 (98.418)\n",
      "val Results: Prec@1 80.210 Prec@5 98.420 Loss 5.84034\n",
      "val Class Accuracy: [0.907,0.962,0.818,0.675,0.813,0.738,0.828,0.741,0.759,0.780]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [229][0/97], lr: 0.00000\tTime 0.499 (0.499)\tData 0.271 (0.271)\tLoss 3.6436 (3.6436)\tPrec@1 86.719 (86.719)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [229][10/97], lr: 0.00000\tTime 0.328 (0.344)\tData 0.000 (0.038)\tLoss 3.2055 (2.6387)\tPrec@1 86.719 (88.849)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [229][20/97], lr: 0.00000\tTime 0.324 (0.339)\tData 0.000 (0.028)\tLoss 1.6806 (2.6332)\tPrec@1 89.844 (88.876)\tPrec@5 98.438 (99.368)\n",
      "Epoch: [229][30/97], lr: 0.00000\tTime 0.323 (0.335)\tData 0.000 (0.024)\tLoss 1.4612 (2.5427)\tPrec@1 91.406 (88.962)\tPrec@5 100.000 (99.446)\n",
      "Epoch: [229][40/97], lr: 0.00000\tTime 0.323 (0.333)\tData 0.000 (0.023)\tLoss 2.1843 (2.5619)\tPrec@1 90.625 (88.986)\tPrec@5 100.000 (99.409)\n",
      "Epoch: [229][50/97], lr: 0.00000\tTime 0.319 (0.332)\tData 0.000 (0.021)\tLoss 1.8666 (2.6045)\tPrec@1 92.188 (89.078)\tPrec@5 100.000 (99.418)\n",
      "Epoch: [229][60/97], lr: 0.00000\tTime 0.323 (0.331)\tData 0.000 (0.021)\tLoss 1.5994 (2.5267)\tPrec@1 92.188 (89.191)\tPrec@5 97.656 (99.360)\n",
      "Epoch: [229][70/97], lr: 0.00000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 2.5270 (2.5010)\tPrec@1 91.406 (89.327)\tPrec@5 100.000 (99.373)\n",
      "Epoch: [229][80/97], lr: 0.00000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 1.3324 (2.4810)\tPrec@1 92.188 (89.333)\tPrec@5 99.219 (99.363)\n",
      "Epoch: [229][90/97], lr: 0.00000\tTime 0.330 (0.331)\tData 0.000 (0.020)\tLoss 1.0449 (2.4701)\tPrec@1 90.625 (89.243)\tPrec@5 100.000 (99.390)\n",
      "Epoch: [229][96/97], lr: 0.00000\tTime 0.316 (0.331)\tData 0.000 (0.020)\tLoss 1.2338 (2.4877)\tPrec@1 90.678 (89.175)\tPrec@5 100.000 (99.387)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.311 (0.311)\tLoss 4.2246 (4.2246)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.095)\tLoss 5.0632 (5.7143)\tPrec@1 78.000 (80.273)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.074 (0.085)\tLoss 6.2659 (5.7822)\tPrec@1 78.000 (79.571)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.9542 (5.9416)\tPrec@1 80.000 (79.452)\tPrec@5 97.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.5880 (5.9977)\tPrec@1 85.000 (79.512)\tPrec@5 98.000 (98.195)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.5303 (5.9896)\tPrec@1 86.000 (80.000)\tPrec@5 99.000 (98.196)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.1962 (5.9178)\tPrec@1 82.000 (80.115)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.1530 (5.9183)\tPrec@1 82.000 (80.141)\tPrec@5 100.000 (98.394)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.7629 (5.8662)\tPrec@1 82.000 (80.444)\tPrec@5 98.000 (98.432)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.5538 (5.9039)\tPrec@1 89.000 (80.242)\tPrec@5 99.000 (98.396)\n",
      "val Results: Prec@1 80.140 Prec@5 98.410 Loss 5.94843\n",
      "val Class Accuracy: [0.912,0.964,0.807,0.690,0.800,0.744,0.823,0.750,0.749,0.775]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [230][0/97], lr: 0.00000\tTime 0.481 (0.481)\tData 0.275 (0.275)\tLoss 3.4318 (3.4318)\tPrec@1 90.625 (90.625)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [230][10/97], lr: 0.00000\tTime 0.324 (0.346)\tData 0.000 (0.039)\tLoss 1.6478 (2.5607)\tPrec@1 92.969 (90.341)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [230][20/97], lr: 0.00000\tTime 0.333 (0.336)\tData 0.000 (0.028)\tLoss 3.1468 (2.5718)\tPrec@1 82.031 (89.583)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [230][30/97], lr: 0.00000\tTime 0.322 (0.332)\tData 0.000 (0.025)\tLoss 1.7663 (2.5112)\tPrec@1 82.812 (89.390)\tPrec@5 100.000 (99.420)\n",
      "Epoch: [230][40/97], lr: 0.00000\tTime 0.318 (0.331)\tData 0.000 (0.023)\tLoss 2.0270 (2.5318)\tPrec@1 86.719 (88.967)\tPrec@5 98.438 (99.409)\n",
      "Epoch: [230][50/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.022)\tLoss 1.5068 (2.5081)\tPrec@1 91.406 (89.323)\tPrec@5 99.219 (99.403)\n",
      "Epoch: [230][60/97], lr: 0.00000\tTime 0.324 (0.329)\tData 0.000 (0.021)\tLoss 1.4135 (2.5056)\tPrec@1 86.719 (89.139)\tPrec@5 100.000 (99.398)\n",
      "Epoch: [230][70/97], lr: 0.00000\tTime 0.325 (0.329)\tData 0.000 (0.020)\tLoss 1.7241 (2.4884)\tPrec@1 92.188 (88.996)\tPrec@5 99.219 (99.362)\n",
      "Epoch: [230][80/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 3.8662 (2.4582)\tPrec@1 88.281 (89.024)\tPrec@5 98.438 (99.392)\n",
      "Epoch: [230][90/97], lr: 0.00000\tTime 0.324 (0.328)\tData 0.000 (0.020)\tLoss 2.2995 (2.5237)\tPrec@1 89.062 (88.908)\tPrec@5 100.000 (99.433)\n",
      "Epoch: [230][96/97], lr: 0.00000\tTime 0.312 (0.328)\tData 0.000 (0.020)\tLoss 2.1886 (2.5379)\tPrec@1 91.525 (88.989)\tPrec@5 100.000 (99.452)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.276 (0.276)\tLoss 4.1242 (4.1242)\tPrec@1 82.000 (82.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 4.9144 (5.6253)\tPrec@1 78.000 (80.364)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.074 (0.083)\tLoss 6.2828 (5.7102)\tPrec@1 78.000 (79.667)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 6.9025 (5.8516)\tPrec@1 81.000 (79.645)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.4889 (5.9101)\tPrec@1 85.000 (79.610)\tPrec@5 97.000 (98.049)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.4099 (5.8978)\tPrec@1 85.000 (80.118)\tPrec@5 99.000 (98.118)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.2067 (5.8308)\tPrec@1 82.000 (80.197)\tPrec@5 100.000 (98.246)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.1799 (5.8267)\tPrec@1 82.000 (80.169)\tPrec@5 99.000 (98.324)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6765 (5.7717)\tPrec@1 82.000 (80.481)\tPrec@5 98.000 (98.395)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4492 (5.8080)\tPrec@1 89.000 (80.286)\tPrec@5 99.000 (98.374)\n",
      "val Results: Prec@1 80.170 Prec@5 98.370 Loss 5.85040\n",
      "val Class Accuracy: [0.907,0.961,0.815,0.692,0.808,0.738,0.812,0.748,0.753,0.783]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [231][0/97], lr: 0.00000\tTime 0.569 (0.569)\tData 0.319 (0.319)\tLoss 2.9406 (2.9406)\tPrec@1 85.156 (85.156)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [231][10/97], lr: 0.00000\tTime 0.327 (0.358)\tData 0.000 (0.042)\tLoss 2.7085 (2.6293)\tPrec@1 85.938 (88.636)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [231][20/97], lr: 0.00000\tTime 0.319 (0.341)\tData 0.000 (0.030)\tLoss 1.0511 (2.4631)\tPrec@1 89.844 (88.914)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [231][30/97], lr: 0.00000\tTime 0.325 (0.336)\tData 0.000 (0.026)\tLoss 2.5165 (2.3880)\tPrec@1 89.844 (89.365)\tPrec@5 98.438 (99.546)\n",
      "Epoch: [231][40/97], lr: 0.00000\tTime 0.320 (0.333)\tData 0.000 (0.024)\tLoss 2.3019 (2.3878)\tPrec@1 89.844 (89.062)\tPrec@5 99.219 (99.505)\n",
      "Epoch: [231][50/97], lr: 0.00000\tTime 0.319 (0.332)\tData 0.000 (0.022)\tLoss 2.8537 (2.3817)\tPrec@1 89.062 (89.354)\tPrec@5 100.000 (99.525)\n",
      "Epoch: [231][60/97], lr: 0.00000\tTime 0.322 (0.332)\tData 0.000 (0.022)\tLoss 3.7083 (2.4627)\tPrec@1 87.500 (89.331)\tPrec@5 99.219 (99.526)\n",
      "Epoch: [231][70/97], lr: 0.00000\tTime 0.321 (0.331)\tData 0.000 (0.021)\tLoss 3.6820 (2.4723)\tPrec@1 85.938 (89.316)\tPrec@5 99.219 (99.505)\n",
      "Epoch: [231][80/97], lr: 0.00000\tTime 0.324 (0.330)\tData 0.000 (0.020)\tLoss 2.0032 (2.4660)\tPrec@1 87.500 (89.371)\tPrec@5 100.000 (99.537)\n",
      "Epoch: [231][90/97], lr: 0.00000\tTime 0.339 (0.332)\tData 0.000 (0.020)\tLoss 2.1746 (2.4319)\tPrec@1 91.406 (89.457)\tPrec@5 100.000 (99.519)\n",
      "Epoch: [231][96/97], lr: 0.00000\tTime 0.330 (0.334)\tData 0.000 (0.020)\tLoss 1.5451 (2.4350)\tPrec@1 89.831 (89.433)\tPrec@5 100.000 (99.532)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.449 (0.449)\tLoss 4.2227 (4.2227)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.108)\tLoss 5.0474 (5.7375)\tPrec@1 78.000 (80.091)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.074 (0.092)\tLoss 6.2663 (5.8244)\tPrec@1 79.000 (79.429)\tPrec@5 97.000 (98.381)\n",
      "Test: [30/100]\tTime 0.076 (0.089)\tLoss 6.9439 (5.9745)\tPrec@1 80.000 (79.258)\tPrec@5 97.000 (98.387)\n",
      "Test: [40/100]\tTime 0.083 (0.092)\tLoss 4.6741 (6.0287)\tPrec@1 85.000 (79.366)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.089 (0.091)\tLoss 5.4908 (6.0184)\tPrec@1 86.000 (80.059)\tPrec@5 99.000 (98.255)\n",
      "Test: [60/100]\tTime 0.077 (0.090)\tLoss 8.2182 (5.9459)\tPrec@1 81.000 (80.148)\tPrec@5 100.000 (98.377)\n",
      "Test: [70/100]\tTime 0.101 (0.089)\tLoss 5.2916 (5.9484)\tPrec@1 82.000 (80.211)\tPrec@5 100.000 (98.437)\n",
      "Test: [80/100]\tTime 0.093 (0.089)\tLoss 6.9151 (5.8998)\tPrec@1 81.000 (80.469)\tPrec@5 98.000 (98.469)\n",
      "Test: [90/100]\tTime 0.086 (0.089)\tLoss 3.5067 (5.9353)\tPrec@1 88.000 (80.231)\tPrec@5 99.000 (98.440)\n",
      "val Results: Prec@1 80.140 Prec@5 98.430 Loss 5.98012\n",
      "val Class Accuracy: [0.908,0.964,0.806,0.703,0.797,0.735,0.828,0.748,0.753,0.772]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [232][0/97], lr: 0.00000\tTime 1.246 (1.246)\tData 0.798 (0.798)\tLoss 3.6318 (3.6318)\tPrec@1 89.062 (89.062)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [232][10/97], lr: 0.00000\tTime 0.362 (0.485)\tData 0.000 (0.082)\tLoss 3.1622 (2.9250)\tPrec@1 88.281 (89.276)\tPrec@5 99.219 (99.361)\n",
      "Epoch: [232][20/97], lr: 0.00000\tTime 0.328 (0.416)\tData 0.000 (0.051)\tLoss 2.2959 (2.6606)\tPrec@1 92.969 (89.807)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [232][30/97], lr: 0.00000\tTime 0.320 (0.387)\tData 0.000 (0.040)\tLoss 1.8142 (2.5398)\tPrec@1 87.500 (89.340)\tPrec@5 97.656 (99.320)\n",
      "Epoch: [232][40/97], lr: 0.00000\tTime 0.318 (0.371)\tData 0.000 (0.034)\tLoss 1.2482 (2.5081)\tPrec@1 92.969 (89.348)\tPrec@5 100.000 (99.409)\n",
      "Epoch: [232][50/97], lr: 0.00000\tTime 0.319 (0.361)\tData 0.000 (0.031)\tLoss 5.3728 (2.4904)\tPrec@1 86.719 (89.369)\tPrec@5 99.219 (99.418)\n",
      "Epoch: [232][60/97], lr: 0.00000\tTime 0.322 (0.355)\tData 0.000 (0.029)\tLoss 2.1945 (2.4365)\tPrec@1 86.719 (89.357)\tPrec@5 100.000 (99.385)\n",
      "Epoch: [232][70/97], lr: 0.00000\tTime 0.321 (0.351)\tData 0.000 (0.027)\tLoss 2.3383 (2.4355)\tPrec@1 88.281 (89.360)\tPrec@5 98.438 (99.395)\n",
      "Epoch: [232][80/97], lr: 0.00000\tTime 0.325 (0.347)\tData 0.000 (0.026)\tLoss 3.0816 (2.4289)\tPrec@1 91.406 (89.554)\tPrec@5 100.000 (99.402)\n",
      "Epoch: [232][90/97], lr: 0.00000\tTime 0.317 (0.345)\tData 0.000 (0.025)\tLoss 3.2180 (2.4061)\tPrec@1 85.938 (89.569)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [232][96/97], lr: 0.00000\tTime 0.315 (0.343)\tData 0.000 (0.025)\tLoss 1.5826 (2.4295)\tPrec@1 90.678 (89.594)\tPrec@5 100.000 (99.460)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.331 (0.331)\tLoss 4.1141 (4.1141)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.097)\tLoss 5.0606 (5.6782)\tPrec@1 78.000 (80.636)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.086)\tLoss 6.2439 (5.7526)\tPrec@1 78.000 (79.714)\tPrec@5 98.000 (98.476)\n",
      "Test: [30/100]\tTime 0.073 (0.082)\tLoss 6.9911 (5.9030)\tPrec@1 80.000 (79.613)\tPrec@5 97.000 (98.355)\n",
      "Test: [40/100]\tTime 0.074 (0.080)\tLoss 4.5837 (5.9645)\tPrec@1 85.000 (79.659)\tPrec@5 98.000 (98.220)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 5.4856 (5.9526)\tPrec@1 86.000 (80.098)\tPrec@5 99.000 (98.255)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 8.1894 (5.8832)\tPrec@1 82.000 (80.115)\tPrec@5 100.000 (98.377)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.2126 (5.8816)\tPrec@1 79.000 (80.070)\tPrec@5 100.000 (98.451)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.6694 (5.8244)\tPrec@1 81.000 (80.370)\tPrec@5 98.000 (98.494)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4837 (5.8593)\tPrec@1 89.000 (80.165)\tPrec@5 99.000 (98.473)\n",
      "val Results: Prec@1 80.100 Prec@5 98.480 Loss 5.90340\n",
      "val Class Accuracy: [0.913,0.962,0.810,0.678,0.801,0.766,0.822,0.731,0.748,0.779]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [233][0/97], lr: 0.00000\tTime 0.493 (0.493)\tData 0.276 (0.276)\tLoss 1.6012 (1.6012)\tPrec@1 90.625 (90.625)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [233][10/97], lr: 0.00000\tTime 0.322 (0.344)\tData 0.000 (0.039)\tLoss 5.2262 (2.7112)\tPrec@1 91.406 (87.713)\tPrec@5 99.219 (99.077)\n",
      "Epoch: [233][20/97], lr: 0.00000\tTime 0.320 (0.334)\tData 0.000 (0.028)\tLoss 2.5585 (2.4861)\tPrec@1 83.594 (88.393)\tPrec@5 100.000 (99.144)\n",
      "Epoch: [233][30/97], lr: 0.00000\tTime 0.319 (0.330)\tData 0.000 (0.025)\tLoss 3.7691 (2.5832)\tPrec@1 91.406 (88.609)\tPrec@5 99.219 (99.294)\n",
      "Epoch: [233][40/97], lr: 0.00000\tTime 0.324 (0.329)\tData 0.000 (0.023)\tLoss 2.5310 (2.6515)\tPrec@1 91.406 (88.624)\tPrec@5 99.219 (99.295)\n",
      "Epoch: [233][50/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.022)\tLoss 1.4424 (2.5526)\tPrec@1 92.969 (88.833)\tPrec@5 100.000 (99.387)\n",
      "Epoch: [233][60/97], lr: 0.00000\tTime 0.327 (0.328)\tData 0.000 (0.021)\tLoss 3.0590 (2.6062)\tPrec@1 85.156 (88.678)\tPrec@5 100.000 (99.385)\n",
      "Epoch: [233][70/97], lr: 0.00000\tTime 0.325 (0.328)\tData 0.000 (0.020)\tLoss 4.0540 (2.5383)\tPrec@1 86.719 (88.963)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [233][80/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.020)\tLoss 1.8388 (2.5932)\tPrec@1 88.281 (88.899)\tPrec@5 98.438 (99.441)\n",
      "Epoch: [233][90/97], lr: 0.00000\tTime 0.320 (0.327)\tData 0.000 (0.020)\tLoss 3.3587 (2.5513)\tPrec@1 89.844 (89.123)\tPrec@5 99.219 (99.442)\n",
      "Epoch: [233][96/97], lr: 0.00000\tTime 0.313 (0.327)\tData 0.000 (0.020)\tLoss 2.3465 (2.5211)\tPrec@1 94.068 (89.223)\tPrec@5 100.000 (99.428)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.336 (0.336)\tLoss 4.0273 (4.0273)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.097)\tLoss 4.8966 (5.5911)\tPrec@1 78.000 (80.818)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.073 (0.086)\tLoss 6.0916 (5.6527)\tPrec@1 79.000 (79.905)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.082)\tLoss 6.7838 (5.8188)\tPrec@1 81.000 (79.710)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 4.5438 (5.8839)\tPrec@1 86.000 (79.805)\tPrec@5 98.000 (98.122)\n",
      "Test: [50/100]\tTime 0.073 (0.079)\tLoss 5.4033 (5.8671)\tPrec@1 86.000 (80.275)\tPrec@5 99.000 (98.098)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 8.0633 (5.7922)\tPrec@1 80.000 (80.295)\tPrec@5 100.000 (98.213)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.0870 (5.7894)\tPrec@1 81.000 (80.296)\tPrec@5 100.000 (98.310)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.5478 (5.7349)\tPrec@1 81.000 (80.531)\tPrec@5 98.000 (98.370)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.3157 (5.7647)\tPrec@1 88.000 (80.330)\tPrec@5 99.000 (98.374)\n",
      "val Results: Prec@1 80.250 Prec@5 98.380 Loss 5.81133\n",
      "val Class Accuracy: [0.904,0.964,0.808,0.673,0.795,0.761,0.833,0.746,0.767,0.774]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [234][0/97], lr: 0.00000\tTime 0.488 (0.488)\tData 0.284 (0.284)\tLoss 2.5579 (2.5579)\tPrec@1 85.938 (85.938)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [234][10/97], lr: 0.00000\tTime 0.322 (0.347)\tData 0.000 (0.040)\tLoss 2.6938 (2.7637)\tPrec@1 82.812 (88.849)\tPrec@5 98.438 (99.290)\n",
      "Epoch: [234][20/97], lr: 0.00000\tTime 0.320 (0.335)\tData 0.000 (0.029)\tLoss 6.1663 (2.6114)\tPrec@1 83.594 (89.286)\tPrec@5 99.219 (99.256)\n",
      "Epoch: [234][30/97], lr: 0.00000\tTime 0.323 (0.332)\tData 0.000 (0.025)\tLoss 1.5590 (2.4487)\tPrec@1 89.844 (89.617)\tPrec@5 100.000 (99.420)\n",
      "Epoch: [234][40/97], lr: 0.00000\tTime 0.327 (0.331)\tData 0.000 (0.023)\tLoss 2.2920 (2.5129)\tPrec@1 83.594 (89.177)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [234][50/97], lr: 0.00000\tTime 0.325 (0.330)\tData 0.000 (0.022)\tLoss 2.6185 (2.5471)\tPrec@1 89.062 (89.124)\tPrec@5 99.219 (99.449)\n",
      "Epoch: [234][60/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.021)\tLoss 2.8934 (2.5005)\tPrec@1 87.500 (89.152)\tPrec@5 99.219 (99.436)\n",
      "Epoch: [234][70/97], lr: 0.00000\tTime 0.325 (0.328)\tData 0.000 (0.021)\tLoss 1.7275 (2.5665)\tPrec@1 88.281 (89.074)\tPrec@5 100.000 (99.472)\n",
      "Epoch: [234][80/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.020)\tLoss 1.1844 (2.6180)\tPrec@1 89.844 (88.899)\tPrec@5 100.000 (99.508)\n",
      "Epoch: [234][90/97], lr: 0.00000\tTime 0.324 (0.328)\tData 0.000 (0.020)\tLoss 4.9874 (2.6087)\tPrec@1 86.719 (88.951)\tPrec@5 100.000 (99.528)\n",
      "Epoch: [234][96/97], lr: 0.00000\tTime 0.313 (0.327)\tData 0.000 (0.020)\tLoss 1.4743 (2.5768)\tPrec@1 90.678 (89.038)\tPrec@5 100.000 (99.492)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.308 (0.308)\tLoss 4.2150 (4.2150)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.095)\tLoss 5.0641 (5.7080)\tPrec@1 78.000 (80.455)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.074 (0.085)\tLoss 6.2732 (5.7851)\tPrec@1 78.000 (79.810)\tPrec@5 97.000 (98.429)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 6.9766 (5.9366)\tPrec@1 80.000 (79.548)\tPrec@5 97.000 (98.419)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.6564 (5.9917)\tPrec@1 85.000 (79.561)\tPrec@5 98.000 (98.293)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.5121 (5.9841)\tPrec@1 85.000 (79.980)\tPrec@5 99.000 (98.294)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.2044 (5.9143)\tPrec@1 81.000 (80.049)\tPrec@5 100.000 (98.410)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.2648 (5.9164)\tPrec@1 82.000 (80.085)\tPrec@5 100.000 (98.479)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.7926 (5.8644)\tPrec@1 82.000 (80.395)\tPrec@5 98.000 (98.519)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.5212 (5.8996)\tPrec@1 89.000 (80.176)\tPrec@5 99.000 (98.484)\n",
      "val Results: Prec@1 80.100 Prec@5 98.470 Loss 5.94321\n",
      "val Class Accuracy: [0.909,0.964,0.810,0.694,0.801,0.748,0.816,0.741,0.750,0.777]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [235][0/97], lr: 0.00000\tTime 0.464 (0.464)\tData 0.258 (0.258)\tLoss 2.0752 (2.0752)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [235][10/97], lr: 0.00000\tTime 0.324 (0.342)\tData 0.000 (0.038)\tLoss 2.2069 (2.9340)\tPrec@1 91.406 (89.205)\tPrec@5 99.219 (99.361)\n",
      "Epoch: [235][20/97], lr: 0.00000\tTime 0.323 (0.333)\tData 0.000 (0.028)\tLoss 3.6467 (2.5022)\tPrec@1 86.719 (89.509)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [235][30/97], lr: 0.00000\tTime 0.324 (0.331)\tData 0.000 (0.025)\tLoss 2.7000 (2.6389)\tPrec@1 88.281 (89.138)\tPrec@5 100.000 (99.345)\n",
      "Epoch: [235][40/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.023)\tLoss 2.3405 (2.6193)\tPrec@1 89.844 (89.177)\tPrec@5 99.219 (99.333)\n",
      "Epoch: [235][50/97], lr: 0.00000\tTime 0.324 (0.329)\tData 0.000 (0.022)\tLoss 1.6453 (2.5370)\tPrec@1 89.062 (89.323)\tPrec@5 99.219 (99.357)\n",
      "Epoch: [235][60/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.021)\tLoss 3.2589 (2.6190)\tPrec@1 89.062 (89.152)\tPrec@5 100.000 (99.360)\n",
      "Epoch: [235][70/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.020)\tLoss 4.3255 (2.5858)\tPrec@1 86.719 (89.118)\tPrec@5 99.219 (99.428)\n",
      "Epoch: [235][80/97], lr: 0.00000\tTime 0.325 (0.328)\tData 0.000 (0.020)\tLoss 1.4297 (2.5357)\tPrec@1 89.844 (89.255)\tPrec@5 99.219 (99.470)\n",
      "Epoch: [235][90/97], lr: 0.00000\tTime 0.321 (0.327)\tData 0.000 (0.020)\tLoss 2.3162 (2.4944)\tPrec@1 88.281 (89.354)\tPrec@5 99.219 (99.476)\n",
      "Epoch: [235][96/97], lr: 0.00000\tTime 0.313 (0.327)\tData 0.000 (0.020)\tLoss 2.9002 (2.5032)\tPrec@1 90.678 (89.296)\tPrec@5 99.153 (99.476)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.297 (0.297)\tLoss 4.2002 (4.2002)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.094)\tLoss 5.0073 (5.7516)\tPrec@1 78.000 (80.545)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.2519 (5.8277)\tPrec@1 78.000 (79.619)\tPrec@5 97.000 (98.429)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 6.9378 (5.9817)\tPrec@1 80.000 (79.355)\tPrec@5 97.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.6926 (6.0387)\tPrec@1 85.000 (79.415)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.5386 (6.0312)\tPrec@1 86.000 (79.961)\tPrec@5 99.000 (98.235)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.2480 (5.9597)\tPrec@1 81.000 (80.033)\tPrec@5 100.000 (98.344)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.3687 (5.9616)\tPrec@1 81.000 (80.056)\tPrec@5 100.000 (98.423)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.8095 (5.9117)\tPrec@1 81.000 (80.346)\tPrec@5 98.000 (98.457)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.4912 (5.9440)\tPrec@1 88.000 (80.121)\tPrec@5 99.000 (98.418)\n",
      "val Results: Prec@1 80.030 Prec@5 98.420 Loss 5.99066\n",
      "val Class Accuracy: [0.905,0.964,0.810,0.688,0.802,0.743,0.823,0.742,0.756,0.770]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [236][0/97], lr: 0.00000\tTime 0.495 (0.495)\tData 0.292 (0.292)\tLoss 5.2330 (5.2330)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [236][10/97], lr: 0.00000\tTime 0.321 (0.343)\tData 0.000 (0.041)\tLoss 1.7947 (3.1334)\tPrec@1 89.844 (88.920)\tPrec@5 100.000 (99.006)\n",
      "Epoch: [236][20/97], lr: 0.00000\tTime 0.320 (0.334)\tData 0.000 (0.030)\tLoss 4.7890 (2.8142)\tPrec@1 84.375 (89.100)\tPrec@5 98.438 (99.219)\n",
      "Epoch: [236][30/97], lr: 0.00000\tTime 0.321 (0.331)\tData 0.000 (0.026)\tLoss 2.4705 (2.4814)\tPrec@1 90.625 (89.693)\tPrec@5 99.219 (99.370)\n",
      "Epoch: [236][40/97], lr: 0.00000\tTime 0.325 (0.329)\tData 0.000 (0.023)\tLoss 4.3372 (2.6185)\tPrec@1 87.500 (89.101)\tPrec@5 100.000 (99.409)\n",
      "Epoch: [236][50/97], lr: 0.00000\tTime 0.318 (0.328)\tData 0.000 (0.022)\tLoss 2.5058 (2.5716)\tPrec@1 90.625 (88.955)\tPrec@5 97.656 (99.418)\n",
      "Epoch: [236][60/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.021)\tLoss 4.2553 (2.5544)\tPrec@1 81.250 (88.794)\tPrec@5 99.219 (99.398)\n",
      "Epoch: [236][70/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.021)\tLoss 2.2253 (2.5280)\tPrec@1 89.062 (88.919)\tPrec@5 99.219 (99.373)\n",
      "Epoch: [236][80/97], lr: 0.00000\tTime 0.321 (0.327)\tData 0.000 (0.020)\tLoss 2.5703 (2.5519)\tPrec@1 89.062 (89.072)\tPrec@5 100.000 (99.421)\n",
      "Epoch: [236][90/97], lr: 0.00000\tTime 0.320 (0.327)\tData 0.000 (0.020)\tLoss 1.5350 (2.4948)\tPrec@1 92.188 (89.157)\tPrec@5 99.219 (99.451)\n",
      "Epoch: [236][96/97], lr: 0.00000\tTime 0.316 (0.327)\tData 0.000 (0.020)\tLoss 1.0964 (2.4772)\tPrec@1 91.525 (89.199)\tPrec@5 98.305 (99.436)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.311 (0.311)\tLoss 4.2475 (4.2475)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.095)\tLoss 5.0903 (5.7129)\tPrec@1 78.000 (80.091)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 6.2546 (5.7907)\tPrec@1 79.000 (79.524)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 7.0110 (5.9456)\tPrec@1 80.000 (79.419)\tPrec@5 97.000 (98.323)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.6028 (6.0063)\tPrec@1 84.000 (79.415)\tPrec@5 98.000 (98.171)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.5326 (5.9993)\tPrec@1 86.000 (79.961)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.2069 (5.9315)\tPrec@1 80.000 (80.033)\tPrec@5 100.000 (98.295)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.1628 (5.9326)\tPrec@1 82.000 (80.056)\tPrec@5 100.000 (98.366)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.8371 (5.8779)\tPrec@1 82.000 (80.407)\tPrec@5 98.000 (98.407)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.5530 (5.9138)\tPrec@1 88.000 (80.187)\tPrec@5 99.000 (98.374)\n",
      "val Results: Prec@1 80.090 Prec@5 98.370 Loss 5.95668\n",
      "val Class Accuracy: [0.907,0.963,0.804,0.702,0.796,0.756,0.825,0.738,0.745,0.773]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [237][0/97], lr: 0.00000\tTime 0.489 (0.489)\tData 0.260 (0.260)\tLoss 2.1526 (2.1526)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [237][10/97], lr: 0.00000\tTime 0.321 (0.344)\tData 0.000 (0.038)\tLoss 2.9909 (2.6407)\tPrec@1 89.844 (89.205)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [237][20/97], lr: 0.00000\tTime 0.321 (0.335)\tData 0.000 (0.028)\tLoss 3.2170 (3.0596)\tPrec@1 91.406 (88.690)\tPrec@5 99.219 (99.368)\n",
      "Epoch: [237][30/97], lr: 0.00000\tTime 0.323 (0.331)\tData 0.000 (0.025)\tLoss 1.4440 (2.7662)\tPrec@1 92.969 (89.037)\tPrec@5 100.000 (99.420)\n",
      "Epoch: [237][40/97], lr: 0.00000\tTime 0.320 (0.330)\tData 0.000 (0.023)\tLoss 2.2111 (2.7895)\tPrec@1 93.750 (89.120)\tPrec@5 100.000 (99.466)\n",
      "Epoch: [237][50/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.022)\tLoss 1.4660 (2.5969)\tPrec@1 92.969 (89.200)\tPrec@5 99.219 (99.449)\n",
      "Epoch: [237][60/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.021)\tLoss 1.7860 (2.5145)\tPrec@1 91.406 (89.408)\tPrec@5 98.438 (99.385)\n",
      "Epoch: [237][70/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.020)\tLoss 2.6716 (2.5082)\tPrec@1 89.844 (89.393)\tPrec@5 100.000 (99.362)\n",
      "Epoch: [237][80/97], lr: 0.00000\tTime 0.320 (0.327)\tData 0.000 (0.020)\tLoss 1.6106 (2.4739)\tPrec@1 90.625 (89.400)\tPrec@5 98.438 (99.363)\n",
      "Epoch: [237][90/97], lr: 0.00000\tTime 0.321 (0.327)\tData 0.000 (0.020)\tLoss 4.0060 (2.4476)\tPrec@1 85.156 (89.354)\tPrec@5 99.219 (99.365)\n",
      "Epoch: [237][96/97], lr: 0.00000\tTime 0.312 (0.327)\tData 0.000 (0.020)\tLoss 2.0139 (2.4572)\tPrec@1 90.678 (89.247)\tPrec@5 99.153 (99.371)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.264 (0.264)\tLoss 4.1497 (4.1497)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.091)\tLoss 4.8967 (5.6267)\tPrec@1 78.000 (80.455)\tPrec@5 99.000 (98.182)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.2047 (5.6918)\tPrec@1 78.000 (79.905)\tPrec@5 98.000 (98.190)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 6.8370 (5.8529)\tPrec@1 82.000 (79.710)\tPrec@5 97.000 (98.129)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.5096 (5.9129)\tPrec@1 85.000 (79.732)\tPrec@5 98.000 (98.049)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.4644 (5.8987)\tPrec@1 86.000 (80.235)\tPrec@5 99.000 (98.098)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.2003 (5.8287)\tPrec@1 81.000 (80.311)\tPrec@5 100.000 (98.246)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 5.0923 (5.8280)\tPrec@1 82.000 (80.338)\tPrec@5 100.000 (98.338)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.5848 (5.7681)\tPrec@1 83.000 (80.630)\tPrec@5 98.000 (98.420)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.4099 (5.8016)\tPrec@1 89.000 (80.396)\tPrec@5 99.000 (98.407)\n",
      "val Results: Prec@1 80.310 Prec@5 98.410 Loss 5.84817\n",
      "val Class Accuracy: [0.914,0.962,0.806,0.682,0.817,0.745,0.818,0.749,0.764,0.774]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [238][0/97], lr: 0.00000\tTime 0.492 (0.492)\tData 0.271 (0.271)\tLoss 2.2318 (2.2318)\tPrec@1 85.938 (85.938)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [238][10/97], lr: 0.00000\tTime 0.320 (0.341)\tData 0.000 (0.039)\tLoss 2.8477 (2.5046)\tPrec@1 86.719 (89.844)\tPrec@5 99.219 (99.645)\n",
      "Epoch: [238][20/97], lr: 0.00000\tTime 0.320 (0.332)\tData 0.000 (0.029)\tLoss 1.6835 (2.4837)\tPrec@1 90.625 (89.546)\tPrec@5 100.000 (99.591)\n",
      "Epoch: [238][30/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.025)\tLoss 2.7005 (2.4736)\tPrec@1 92.188 (89.037)\tPrec@5 100.000 (99.521)\n",
      "Epoch: [238][40/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.023)\tLoss 1.9357 (2.5536)\tPrec@1 92.969 (89.196)\tPrec@5 99.219 (99.486)\n",
      "Epoch: [238][50/97], lr: 0.00000\tTime 0.326 (0.328)\tData 0.000 (0.022)\tLoss 2.5556 (2.4320)\tPrec@1 82.812 (89.231)\tPrec@5 100.000 (99.449)\n",
      "Epoch: [238][60/97], lr: 0.00000\tTime 0.338 (0.330)\tData 0.000 (0.021)\tLoss 4.9451 (2.4999)\tPrec@1 88.281 (89.203)\tPrec@5 98.438 (99.372)\n",
      "Epoch: [238][70/97], lr: 0.00000\tTime 0.342 (0.330)\tData 0.000 (0.020)\tLoss 1.7317 (2.5047)\tPrec@1 89.844 (89.173)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [238][80/97], lr: 0.00000\tTime 0.319 (0.330)\tData 0.000 (0.020)\tLoss 2.6954 (2.4778)\tPrec@1 91.406 (89.159)\tPrec@5 99.219 (99.421)\n",
      "Epoch: [238][90/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.020)\tLoss 2.7323 (2.4915)\tPrec@1 85.938 (89.080)\tPrec@5 100.000 (99.399)\n",
      "Epoch: [238][96/97], lr: 0.00000\tTime 0.313 (0.329)\tData 0.000 (0.020)\tLoss 2.7025 (2.4764)\tPrec@1 91.525 (89.215)\tPrec@5 100.000 (99.420)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.327 (0.327)\tLoss 3.9557 (3.9557)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.096)\tLoss 4.7279 (5.4634)\tPrec@1 78.000 (80.636)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 6.0632 (5.5211)\tPrec@1 78.000 (79.905)\tPrec@5 97.000 (98.238)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.7135 (5.7010)\tPrec@1 80.000 (79.677)\tPrec@5 96.000 (98.129)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.3592 (5.7717)\tPrec@1 86.000 (79.707)\tPrec@5 98.000 (98.024)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.3350 (5.7505)\tPrec@1 87.000 (80.216)\tPrec@5 99.000 (98.098)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 7.9798 (5.6856)\tPrec@1 81.000 (80.262)\tPrec@5 100.000 (98.246)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 4.9063 (5.6794)\tPrec@1 81.000 (80.324)\tPrec@5 100.000 (98.352)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.3218 (5.6185)\tPrec@1 83.000 (80.617)\tPrec@5 98.000 (98.420)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.2636 (5.6464)\tPrec@1 89.000 (80.440)\tPrec@5 99.000 (98.385)\n",
      "val Results: Prec@1 80.310 Prec@5 98.390 Loss 5.69380\n",
      "val Class Accuracy: [0.899,0.959,0.804,0.680,0.821,0.757,0.819,0.742,0.772,0.778]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [239][0/97], lr: 0.00000\tTime 0.469 (0.469)\tData 0.246 (0.246)\tLoss 3.2850 (3.2850)\tPrec@1 82.812 (82.812)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [239][10/97], lr: 0.00000\tTime 0.320 (0.343)\tData 0.000 (0.037)\tLoss 4.4805 (2.6705)\tPrec@1 85.938 (87.571)\tPrec@5 100.000 (99.716)\n",
      "Epoch: [239][20/97], lr: 0.00000\tTime 0.315 (0.333)\tData 0.000 (0.028)\tLoss 1.7988 (2.5774)\tPrec@1 88.281 (87.984)\tPrec@5 98.438 (99.479)\n",
      "Epoch: [239][30/97], lr: 0.00000\tTime 0.320 (0.330)\tData 0.000 (0.024)\tLoss 3.3369 (2.5787)\tPrec@1 81.250 (88.407)\tPrec@5 98.438 (99.395)\n",
      "Epoch: [239][40/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.022)\tLoss 1.6473 (2.4060)\tPrec@1 90.625 (88.758)\tPrec@5 99.219 (99.428)\n",
      "Epoch: [239][50/97], lr: 0.00000\tTime 0.323 (0.328)\tData 0.000 (0.021)\tLoss 1.9776 (2.4131)\tPrec@1 92.969 (89.154)\tPrec@5 98.438 (99.357)\n",
      "Epoch: [239][60/97], lr: 0.00000\tTime 0.322 (0.327)\tData 0.000 (0.021)\tLoss 2.6965 (2.4849)\tPrec@1 88.281 (89.229)\tPrec@5 99.219 (99.360)\n",
      "Epoch: [239][70/97], lr: 0.00000\tTime 0.321 (0.327)\tData 0.000 (0.020)\tLoss 2.1398 (2.4749)\tPrec@1 92.188 (89.151)\tPrec@5 100.000 (99.406)\n",
      "Epoch: [239][80/97], lr: 0.00000\tTime 0.324 (0.327)\tData 0.000 (0.020)\tLoss 5.5101 (2.4643)\tPrec@1 85.938 (89.053)\tPrec@5 98.438 (99.373)\n",
      "Epoch: [239][90/97], lr: 0.00000\tTime 0.319 (0.327)\tData 0.000 (0.019)\tLoss 1.8840 (2.4348)\tPrec@1 90.625 (89.183)\tPrec@5 100.000 (99.382)\n",
      "Epoch: [239][96/97], lr: 0.00000\tTime 0.313 (0.327)\tData 0.000 (0.020)\tLoss 0.8350 (2.4396)\tPrec@1 94.915 (89.376)\tPrec@5 99.153 (99.404)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.315 (0.315)\tLoss 4.1133 (4.1133)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.096)\tLoss 5.0073 (5.6464)\tPrec@1 78.000 (80.727)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 6.2176 (5.7174)\tPrec@1 78.000 (79.952)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 6.8387 (5.8896)\tPrec@1 81.000 (79.742)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.5096 (5.9516)\tPrec@1 85.000 (79.732)\tPrec@5 98.000 (98.098)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.5056 (5.9409)\tPrec@1 86.000 (80.235)\tPrec@5 99.000 (98.098)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.1462 (5.8702)\tPrec@1 81.000 (80.262)\tPrec@5 100.000 (98.246)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.1322 (5.8695)\tPrec@1 82.000 (80.211)\tPrec@5 99.000 (98.324)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.6434 (5.8146)\tPrec@1 82.000 (80.506)\tPrec@5 98.000 (98.407)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4297 (5.8484)\tPrec@1 89.000 (80.286)\tPrec@5 99.000 (98.385)\n",
      "val Results: Prec@1 80.230 Prec@5 98.390 Loss 5.89374\n",
      "val Class Accuracy: [0.906,0.964,0.809,0.681,0.814,0.755,0.808,0.755,0.759,0.772]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [240][0/97], lr: 0.00000\tTime 0.497 (0.497)\tData 0.296 (0.296)\tLoss 3.9655 (3.9655)\tPrec@1 85.156 (85.156)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [240][10/97], lr: 0.00000\tTime 0.325 (0.348)\tData 0.000 (0.042)\tLoss 1.8364 (2.3999)\tPrec@1 89.844 (90.057)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [240][20/97], lr: 0.00000\tTime 0.320 (0.337)\tData 0.000 (0.030)\tLoss 4.0620 (2.5522)\tPrec@1 85.938 (89.472)\tPrec@5 100.000 (99.554)\n",
      "Epoch: [240][30/97], lr: 0.00000\tTime 0.320 (0.332)\tData 0.000 (0.026)\tLoss 1.0405 (2.3691)\tPrec@1 90.625 (89.869)\tPrec@5 99.219 (99.521)\n",
      "Epoch: [240][40/97], lr: 0.00000\tTime 0.324 (0.331)\tData 0.000 (0.024)\tLoss 1.4817 (2.3770)\tPrec@1 88.281 (89.596)\tPrec@5 98.438 (99.543)\n",
      "Epoch: [240][50/97], lr: 0.00000\tTime 0.324 (0.330)\tData 0.000 (0.022)\tLoss 2.2890 (2.3369)\tPrec@1 90.625 (89.782)\tPrec@5 100.000 (99.556)\n",
      "Epoch: [240][60/97], lr: 0.00000\tTime 0.329 (0.335)\tData 0.000 (0.021)\tLoss 5.4834 (2.4247)\tPrec@1 88.281 (89.664)\tPrec@5 99.219 (99.488)\n",
      "Epoch: [240][70/97], lr: 0.00000\tTime 0.324 (0.333)\tData 0.000 (0.021)\tLoss 3.8968 (2.4499)\tPrec@1 86.719 (89.646)\tPrec@5 99.219 (99.483)\n",
      "Epoch: [240][80/97], lr: 0.00000\tTime 0.328 (0.333)\tData 0.000 (0.020)\tLoss 3.1207 (2.4188)\tPrec@1 86.719 (89.458)\tPrec@5 98.438 (99.460)\n",
      "Epoch: [240][90/97], lr: 0.00000\tTime 0.321 (0.332)\tData 0.000 (0.020)\tLoss 1.8395 (2.4002)\tPrec@1 92.188 (89.483)\tPrec@5 100.000 (99.468)\n",
      "Epoch: [240][96/97], lr: 0.00000\tTime 0.314 (0.332)\tData 0.000 (0.020)\tLoss 1.6385 (2.4126)\tPrec@1 91.525 (89.489)\tPrec@5 99.153 (99.476)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.303 (0.303)\tLoss 4.1747 (4.1747)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.095)\tLoss 4.9636 (5.6786)\tPrec@1 78.000 (80.182)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.074 (0.085)\tLoss 6.2444 (5.7624)\tPrec@1 78.000 (79.524)\tPrec@5 97.000 (98.381)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 6.8987 (5.9063)\tPrec@1 80.000 (79.484)\tPrec@5 97.000 (98.387)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.5971 (5.9615)\tPrec@1 85.000 (79.561)\tPrec@5 98.000 (98.268)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.3950 (5.9510)\tPrec@1 86.000 (80.078)\tPrec@5 99.000 (98.255)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.1904 (5.8805)\tPrec@1 81.000 (80.131)\tPrec@5 100.000 (98.377)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.2121 (5.8772)\tPrec@1 81.000 (80.127)\tPrec@5 100.000 (98.451)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.7096 (5.8248)\tPrec@1 82.000 (80.407)\tPrec@5 98.000 (98.481)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.4546 (5.8596)\tPrec@1 88.000 (80.198)\tPrec@5 99.000 (98.418)\n",
      "val Results: Prec@1 80.110 Prec@5 98.410 Loss 5.90383\n",
      "val Class Accuracy: [0.903,0.965,0.810,0.682,0.808,0.743,0.824,0.743,0.750,0.783]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [241][0/97], lr: 0.00000\tTime 0.486 (0.486)\tData 0.260 (0.260)\tLoss 2.2681 (2.2681)\tPrec@1 90.625 (90.625)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [241][10/97], lr: 0.00000\tTime 0.322 (0.341)\tData 0.000 (0.037)\tLoss 2.7192 (2.7426)\tPrec@1 89.062 (89.418)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [241][20/97], lr: 0.00000\tTime 0.320 (0.333)\tData 0.000 (0.028)\tLoss 3.6263 (2.6636)\tPrec@1 82.812 (89.025)\tPrec@5 99.219 (99.293)\n",
      "Epoch: [241][30/97], lr: 0.00000\tTime 0.320 (0.331)\tData 0.000 (0.024)\tLoss 3.3958 (2.5559)\tPrec@1 94.531 (89.315)\tPrec@5 100.000 (99.370)\n",
      "Epoch: [241][40/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.022)\tLoss 1.0828 (2.3946)\tPrec@1 92.188 (89.787)\tPrec@5 100.000 (99.447)\n",
      "Epoch: [241][50/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.021)\tLoss 5.6170 (2.5519)\tPrec@1 90.625 (89.752)\tPrec@5 99.219 (99.464)\n",
      "Epoch: [241][60/97], lr: 0.00000\tTime 0.321 (0.327)\tData 0.000 (0.021)\tLoss 3.6936 (2.5425)\tPrec@1 89.062 (89.613)\tPrec@5 99.219 (99.475)\n",
      "Epoch: [241][70/97], lr: 0.00000\tTime 0.320 (0.327)\tData 0.000 (0.020)\tLoss 2.1287 (2.5351)\tPrec@1 85.938 (89.503)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [241][80/97], lr: 0.00000\tTime 0.320 (0.327)\tData 0.000 (0.020)\tLoss 2.3685 (2.6181)\tPrec@1 91.406 (89.294)\tPrec@5 99.219 (99.383)\n",
      "Epoch: [241][90/97], lr: 0.00000\tTime 0.319 (0.326)\tData 0.000 (0.019)\tLoss 2.8431 (2.6327)\tPrec@1 87.500 (89.217)\tPrec@5 100.000 (99.399)\n",
      "Epoch: [241][96/97], lr: 0.00000\tTime 0.313 (0.326)\tData 0.000 (0.020)\tLoss 2.1710 (2.6003)\tPrec@1 88.983 (89.352)\tPrec@5 98.305 (99.412)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.287 (0.287)\tLoss 4.2137 (4.2137)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 5.0261 (5.6532)\tPrec@1 78.000 (80.182)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.3156 (5.7383)\tPrec@1 78.000 (79.619)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 6.9953 (5.8908)\tPrec@1 81.000 (79.516)\tPrec@5 97.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.5226 (5.9475)\tPrec@1 85.000 (79.512)\tPrec@5 98.000 (98.171)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.5080 (5.9446)\tPrec@1 85.000 (79.961)\tPrec@5 99.000 (98.196)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.1137 (5.8763)\tPrec@1 82.000 (79.967)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 4.9970 (5.8748)\tPrec@1 82.000 (79.986)\tPrec@5 99.000 (98.394)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6722 (5.8181)\tPrec@1 82.000 (80.333)\tPrec@5 98.000 (98.420)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.5754 (5.8573)\tPrec@1 90.000 (80.143)\tPrec@5 99.000 (98.374)\n",
      "val Results: Prec@1 80.090 Prec@5 98.370 Loss 5.89818\n",
      "val Class Accuracy: [0.906,0.960,0.816,0.688,0.799,0.748,0.817,0.752,0.741,0.782]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [242][0/97], lr: 0.00000\tTime 0.523 (0.523)\tData 0.278 (0.278)\tLoss 1.8104 (1.8104)\tPrec@1 92.969 (92.969)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [242][10/97], lr: 0.00000\tTime 0.320 (0.349)\tData 0.000 (0.039)\tLoss 3.1377 (2.6665)\tPrec@1 86.719 (89.560)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [242][20/97], lr: 0.00000\tTime 0.331 (0.338)\tData 0.000 (0.028)\tLoss 2.2715 (2.5138)\tPrec@1 92.188 (89.844)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [242][30/97], lr: 0.00000\tTime 0.319 (0.333)\tData 0.000 (0.024)\tLoss 1.7215 (2.6117)\tPrec@1 91.406 (89.667)\tPrec@5 100.000 (99.446)\n",
      "Epoch: [242][40/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.023)\tLoss 2.4345 (2.6835)\tPrec@1 87.500 (89.291)\tPrec@5 98.438 (99.352)\n",
      "Epoch: [242][50/97], lr: 0.00000\tTime 0.323 (0.330)\tData 0.000 (0.022)\tLoss 1.5871 (2.6766)\tPrec@1 92.188 (89.231)\tPrec@5 100.000 (99.387)\n",
      "Epoch: [242][60/97], lr: 0.00000\tTime 0.327 (0.329)\tData 0.000 (0.021)\tLoss 1.3589 (2.5952)\tPrec@1 85.156 (88.998)\tPrec@5 99.219 (99.372)\n",
      "Epoch: [242][70/97], lr: 0.00000\tTime 0.320 (0.329)\tData 0.000 (0.020)\tLoss 1.8457 (2.5662)\tPrec@1 89.844 (89.118)\tPrec@5 100.000 (99.406)\n",
      "Epoch: [242][80/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 1.9469 (2.5609)\tPrec@1 90.625 (89.062)\tPrec@5 99.219 (99.402)\n",
      "Epoch: [242][90/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 1.7892 (2.5095)\tPrec@1 92.188 (89.080)\tPrec@5 100.000 (99.416)\n",
      "Epoch: [242][96/97], lr: 0.00000\tTime 0.313 (0.328)\tData 0.000 (0.020)\tLoss 1.6614 (2.4834)\tPrec@1 94.915 (89.118)\tPrec@5 100.000 (99.412)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.287 (0.287)\tLoss 4.0920 (4.0920)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 4.9611 (5.6511)\tPrec@1 78.000 (80.727)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.074 (0.084)\tLoss 6.2033 (5.7229)\tPrec@1 78.000 (79.762)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.9031 (5.8852)\tPrec@1 80.000 (79.548)\tPrec@5 97.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.6090 (5.9498)\tPrec@1 85.000 (79.561)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.5176 (5.9376)\tPrec@1 85.000 (80.059)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.0682 (5.8675)\tPrec@1 81.000 (80.098)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 5.2277 (5.8702)\tPrec@1 82.000 (80.197)\tPrec@5 100.000 (98.380)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6957 (5.8156)\tPrec@1 82.000 (80.531)\tPrec@5 98.000 (98.420)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.4490 (5.8453)\tPrec@1 90.000 (80.330)\tPrec@5 99.000 (98.407)\n",
      "val Results: Prec@1 80.220 Prec@5 98.410 Loss 5.89053\n",
      "val Class Accuracy: [0.908,0.963,0.814,0.691,0.795,0.752,0.828,0.736,0.762,0.773]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [243][0/97], lr: 0.00000\tTime 0.504 (0.504)\tData 0.273 (0.273)\tLoss 2.1913 (2.1913)\tPrec@1 85.156 (85.156)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [243][10/97], lr: 0.00000\tTime 0.323 (0.344)\tData 0.000 (0.039)\tLoss 3.0761 (2.4771)\tPrec@1 91.406 (88.494)\tPrec@5 100.000 (99.787)\n",
      "Epoch: [243][20/97], lr: 0.00000\tTime 0.322 (0.335)\tData 0.000 (0.028)\tLoss 2.3709 (2.3571)\tPrec@1 86.719 (89.025)\tPrec@5 100.000 (99.777)\n",
      "Epoch: [243][30/97], lr: 0.00000\tTime 0.321 (0.331)\tData 0.000 (0.025)\tLoss 7.1603 (2.4958)\tPrec@1 85.156 (88.861)\tPrec@5 97.656 (99.597)\n",
      "Epoch: [243][40/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.023)\tLoss 2.8429 (2.4172)\tPrec@1 87.500 (88.796)\tPrec@5 99.219 (99.581)\n",
      "Epoch: [243][50/97], lr: 0.00000\tTime 0.317 (0.328)\tData 0.000 (0.022)\tLoss 2.1349 (2.4815)\tPrec@1 92.188 (88.710)\tPrec@5 100.000 (99.556)\n",
      "Epoch: [243][60/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.021)\tLoss 1.4454 (2.4866)\tPrec@1 89.844 (88.742)\tPrec@5 100.000 (99.501)\n",
      "Epoch: [243][70/97], lr: 0.00000\tTime 0.324 (0.327)\tData 0.000 (0.020)\tLoss 3.7152 (2.5004)\tPrec@1 90.625 (88.710)\tPrec@5 100.000 (99.461)\n",
      "Epoch: [243][80/97], lr: 0.00000\tTime 0.319 (0.327)\tData 0.000 (0.020)\tLoss 2.6461 (2.4976)\tPrec@1 88.281 (88.850)\tPrec@5 100.000 (99.470)\n",
      "Epoch: [243][90/97], lr: 0.00000\tTime 0.324 (0.326)\tData 0.000 (0.020)\tLoss 4.2307 (2.5306)\tPrec@1 92.188 (88.856)\tPrec@5 99.219 (99.468)\n",
      "Epoch: [243][96/97], lr: 0.00000\tTime 0.316 (0.326)\tData 0.000 (0.020)\tLoss 2.5307 (2.5486)\tPrec@1 86.441 (88.764)\tPrec@5 99.153 (99.460)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.249 (0.249)\tLoss 4.0276 (4.0276)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.089)\tLoss 4.8121 (5.5281)\tPrec@1 78.000 (80.545)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.074 (0.082)\tLoss 6.1710 (5.6102)\tPrec@1 78.000 (79.905)\tPrec@5 97.000 (98.381)\n",
      "Test: [30/100]\tTime 0.074 (0.079)\tLoss 6.8190 (5.7730)\tPrec@1 81.000 (79.742)\tPrec@5 97.000 (98.323)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 4.4849 (5.8385)\tPrec@1 86.000 (79.829)\tPrec@5 98.000 (98.195)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 5.4035 (5.8194)\tPrec@1 85.000 (80.314)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.074 (0.076)\tLoss 8.0280 (5.7507)\tPrec@1 83.000 (80.393)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 5.1066 (5.7487)\tPrec@1 82.000 (80.423)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.5219 (5.6882)\tPrec@1 83.000 (80.704)\tPrec@5 98.000 (98.457)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.3386 (5.7189)\tPrec@1 89.000 (80.484)\tPrec@5 99.000 (98.440)\n",
      "val Results: Prec@1 80.340 Prec@5 98.440 Loss 5.76479\n",
      "val Class Accuracy: [0.914,0.958,0.812,0.691,0.804,0.745,0.826,0.738,0.766,0.780]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [244][0/97], lr: 0.00000\tTime 0.469 (0.469)\tData 0.253 (0.253)\tLoss 3.1657 (3.1657)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [244][10/97], lr: 0.00000\tTime 0.321 (0.342)\tData 0.000 (0.038)\tLoss 1.3524 (2.9634)\tPrec@1 90.625 (87.784)\tPrec@5 97.656 (99.148)\n",
      "Epoch: [244][20/97], lr: 0.00000\tTime 0.320 (0.334)\tData 0.000 (0.028)\tLoss 2.8683 (2.7609)\tPrec@1 90.625 (88.728)\tPrec@5 99.219 (99.368)\n",
      "Epoch: [244][30/97], lr: 0.00000\tTime 0.321 (0.331)\tData 0.000 (0.024)\tLoss 3.3673 (2.5562)\tPrec@1 87.500 (88.962)\tPrec@5 98.438 (99.320)\n",
      "Epoch: [244][40/97], lr: 0.00000\tTime 0.320 (0.329)\tData 0.000 (0.023)\tLoss 2.2147 (2.5785)\tPrec@1 88.281 (88.872)\tPrec@5 100.000 (99.409)\n",
      "Epoch: [244][50/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.022)\tLoss 2.9641 (2.5213)\tPrec@1 85.938 (88.725)\tPrec@5 99.219 (99.418)\n",
      "Epoch: [244][60/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.021)\tLoss 3.7269 (2.5515)\tPrec@1 88.281 (88.704)\tPrec@5 98.438 (99.347)\n",
      "Epoch: [244][70/97], lr: 0.00000\tTime 0.321 (0.327)\tData 0.000 (0.020)\tLoss 2.4569 (2.6179)\tPrec@1 90.625 (88.644)\tPrec@5 100.000 (99.318)\n",
      "Epoch: [244][80/97], lr: 0.00000\tTime 0.325 (0.327)\tData 0.000 (0.020)\tLoss 1.0262 (2.5973)\tPrec@1 89.062 (88.696)\tPrec@5 100.000 (99.363)\n",
      "Epoch: [244][90/97], lr: 0.00000\tTime 0.322 (0.327)\tData 0.000 (0.020)\tLoss 3.0096 (2.5375)\tPrec@1 86.719 (88.822)\tPrec@5 98.438 (99.373)\n",
      "Epoch: [244][96/97], lr: 0.00000\tTime 0.318 (0.327)\tData 0.000 (0.020)\tLoss 1.8459 (2.5332)\tPrec@1 93.220 (88.844)\tPrec@5 100.000 (99.387)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.258 (0.258)\tLoss 4.1142 (4.1142)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 4.8957 (5.5871)\tPrec@1 78.000 (80.182)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.1632 (5.6642)\tPrec@1 79.000 (79.714)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.8394 (5.8293)\tPrec@1 81.000 (79.613)\tPrec@5 97.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.4893 (5.8934)\tPrec@1 85.000 (79.707)\tPrec@5 98.000 (98.195)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.4384 (5.8803)\tPrec@1 85.000 (80.235)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.0496 (5.8082)\tPrec@1 82.000 (80.361)\tPrec@5 99.000 (98.328)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 5.1045 (5.8050)\tPrec@1 82.000 (80.338)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.6363 (5.7498)\tPrec@1 82.000 (80.630)\tPrec@5 98.000 (98.457)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 3.3919 (5.7812)\tPrec@1 88.000 (80.418)\tPrec@5 99.000 (98.440)\n",
      "val Results: Prec@1 80.300 Prec@5 98.440 Loss 5.82635\n",
      "val Class Accuracy: [0.911,0.963,0.810,0.688,0.797,0.740,0.833,0.750,0.762,0.776]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [245][0/97], lr: 0.00000\tTime 0.472 (0.472)\tData 0.239 (0.239)\tLoss 2.8776 (2.8776)\tPrec@1 86.719 (86.719)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [245][10/97], lr: 0.00000\tTime 0.324 (0.344)\tData 0.000 (0.035)\tLoss 2.5490 (2.4197)\tPrec@1 89.844 (88.352)\tPrec@5 99.219 (99.006)\n",
      "Epoch: [245][20/97], lr: 0.00000\tTime 0.320 (0.334)\tData 0.000 (0.026)\tLoss 1.8047 (2.5154)\tPrec@1 89.062 (88.765)\tPrec@5 97.656 (99.182)\n",
      "Epoch: [245][30/97], lr: 0.00000\tTime 0.318 (0.331)\tData 0.000 (0.023)\tLoss 1.8717 (2.5643)\tPrec@1 90.625 (89.189)\tPrec@5 98.438 (99.294)\n",
      "Epoch: [245][40/97], lr: 0.00000\tTime 0.324 (0.329)\tData 0.000 (0.022)\tLoss 3.2836 (2.5311)\tPrec@1 88.281 (88.910)\tPrec@5 100.000 (99.352)\n",
      "Epoch: [245][50/97], lr: 0.00000\tTime 0.324 (0.329)\tData 0.000 (0.021)\tLoss 3.1945 (2.4259)\tPrec@1 85.938 (89.124)\tPrec@5 99.219 (99.403)\n",
      "Epoch: [245][60/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 1.1530 (2.4395)\tPrec@1 92.188 (88.973)\tPrec@5 100.000 (99.385)\n",
      "Epoch: [245][70/97], lr: 0.00000\tTime 0.321 (0.327)\tData 0.000 (0.020)\tLoss 6.1327 (2.5689)\tPrec@1 88.281 (88.842)\tPrec@5 100.000 (99.362)\n",
      "Epoch: [245][80/97], lr: 0.00000\tTime 0.322 (0.327)\tData 0.000 (0.019)\tLoss 3.1456 (2.5438)\tPrec@1 89.844 (88.908)\tPrec@5 99.219 (99.373)\n",
      "Epoch: [245][90/97], lr: 0.00000\tTime 0.324 (0.327)\tData 0.000 (0.019)\tLoss 2.3742 (2.5558)\tPrec@1 91.406 (88.908)\tPrec@5 98.438 (99.408)\n",
      "Epoch: [245][96/97], lr: 0.00000\tTime 0.314 (0.326)\tData 0.000 (0.020)\tLoss 3.3432 (2.5440)\tPrec@1 88.983 (88.901)\tPrec@5 100.000 (99.412)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.323 (0.323)\tLoss 4.2955 (4.2955)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.096)\tLoss 5.1201 (5.8176)\tPrec@1 78.000 (80.545)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 6.2349 (5.8752)\tPrec@1 79.000 (79.667)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.074 (0.082)\tLoss 7.0028 (6.0441)\tPrec@1 80.000 (79.484)\tPrec@5 96.000 (98.290)\n",
      "Test: [40/100]\tTime 0.074 (0.080)\tLoss 4.7161 (6.1072)\tPrec@1 84.000 (79.463)\tPrec@5 98.000 (98.122)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 5.6360 (6.1034)\tPrec@1 87.000 (79.980)\tPrec@5 99.000 (98.118)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 8.2517 (6.0342)\tPrec@1 80.000 (80.049)\tPrec@5 100.000 (98.246)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.4157 (6.0402)\tPrec@1 81.000 (80.113)\tPrec@5 100.000 (98.338)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.9707 (5.9914)\tPrec@1 80.000 (80.444)\tPrec@5 98.000 (98.383)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.5521 (6.0198)\tPrec@1 88.000 (80.242)\tPrec@5 99.000 (98.330)\n",
      "val Results: Prec@1 80.130 Prec@5 98.320 Loss 6.06334\n",
      "val Class Accuracy: [0.907,0.964,0.806,0.697,0.794,0.758,0.837,0.731,0.757,0.762]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [246][0/97], lr: 0.00000\tTime 0.476 (0.476)\tData 0.272 (0.272)\tLoss 3.3027 (3.3027)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [246][10/97], lr: 0.00000\tTime 0.321 (0.341)\tData 0.000 (0.039)\tLoss 1.4704 (2.3842)\tPrec@1 91.406 (90.341)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [246][20/97], lr: 0.00000\tTime 0.323 (0.333)\tData 0.000 (0.028)\tLoss 3.3458 (2.5792)\tPrec@1 87.500 (89.509)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [246][30/97], lr: 0.00000\tTime 0.322 (0.331)\tData 0.000 (0.025)\tLoss 2.1275 (2.5777)\tPrec@1 91.406 (89.567)\tPrec@5 98.438 (99.496)\n",
      "Epoch: [246][40/97], lr: 0.00000\tTime 0.320 (0.330)\tData 0.000 (0.023)\tLoss 2.1706 (2.4760)\tPrec@1 85.938 (89.386)\tPrec@5 100.000 (99.447)\n",
      "Epoch: [246][50/97], lr: 0.00000\tTime 0.320 (0.329)\tData 0.000 (0.022)\tLoss 1.5744 (2.4548)\tPrec@1 90.625 (89.491)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [246][60/97], lr: 0.00000\tTime 0.326 (0.328)\tData 0.000 (0.021)\tLoss 2.5041 (2.4434)\tPrec@1 85.156 (89.383)\tPrec@5 99.219 (99.462)\n",
      "Epoch: [246][70/97], lr: 0.00000\tTime 0.318 (0.328)\tData 0.000 (0.020)\tLoss 1.6663 (2.5571)\tPrec@1 91.406 (89.217)\tPrec@5 100.000 (99.439)\n",
      "Epoch: [246][80/97], lr: 0.00000\tTime 0.319 (0.328)\tData 0.000 (0.020)\tLoss 0.5164 (2.5113)\tPrec@1 95.312 (89.361)\tPrec@5 99.219 (99.460)\n",
      "Epoch: [246][90/97], lr: 0.00000\tTime 0.319 (0.327)\tData 0.000 (0.020)\tLoss 4.6748 (2.4878)\tPrec@1 88.281 (89.492)\tPrec@5 99.219 (99.485)\n",
      "Epoch: [246][96/97], lr: 0.00000\tTime 0.312 (0.327)\tData 0.000 (0.020)\tLoss 2.3085 (2.4918)\tPrec@1 88.983 (89.489)\tPrec@5 100.000 (99.484)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.343 (0.343)\tLoss 4.1932 (4.1932)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.098)\tLoss 4.9312 (5.7084)\tPrec@1 78.000 (80.727)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.086)\tLoss 6.2187 (5.7785)\tPrec@1 78.000 (79.857)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.074 (0.082)\tLoss 6.9018 (5.9404)\tPrec@1 81.000 (79.613)\tPrec@5 96.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 4.6389 (6.0026)\tPrec@1 84.000 (79.585)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.073 (0.079)\tLoss 5.5329 (5.9964)\tPrec@1 85.000 (80.098)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 8.1718 (5.9269)\tPrec@1 81.000 (80.197)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.3087 (5.9304)\tPrec@1 81.000 (80.197)\tPrec@5 100.000 (98.394)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.7633 (5.8787)\tPrec@1 81.000 (80.506)\tPrec@5 98.000 (98.432)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.4291 (5.9060)\tPrec@1 89.000 (80.264)\tPrec@5 99.000 (98.385)\n",
      "val Results: Prec@1 80.150 Prec@5 98.380 Loss 5.95042\n",
      "val Class Accuracy: [0.904,0.964,0.815,0.690,0.796,0.747,0.836,0.735,0.758,0.770]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [247][0/97], lr: 0.00000\tTime 0.477 (0.477)\tData 0.281 (0.281)\tLoss 4.7253 (4.7253)\tPrec@1 85.938 (85.938)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [247][10/97], lr: 0.00000\tTime 0.319 (0.340)\tData 0.000 (0.040)\tLoss 4.4933 (3.5828)\tPrec@1 88.281 (87.713)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [247][20/97], lr: 0.00000\tTime 0.321 (0.332)\tData 0.000 (0.029)\tLoss 1.9385 (2.8475)\tPrec@1 88.281 (88.467)\tPrec@5 99.219 (99.479)\n",
      "Epoch: [247][30/97], lr: 0.00000\tTime 0.319 (0.330)\tData 0.000 (0.025)\tLoss 1.8063 (2.5872)\tPrec@1 86.719 (88.911)\tPrec@5 100.000 (99.546)\n",
      "Epoch: [247][40/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.023)\tLoss 2.6023 (2.5226)\tPrec@1 89.844 (89.043)\tPrec@5 100.000 (99.505)\n",
      "Epoch: [247][50/97], lr: 0.00000\tTime 0.318 (0.328)\tData 0.000 (0.022)\tLoss 2.1568 (2.5223)\tPrec@1 89.844 (89.170)\tPrec@5 100.000 (99.525)\n",
      "Epoch: [247][60/97], lr: 0.00000\tTime 0.318 (0.327)\tData 0.000 (0.021)\tLoss 1.9752 (2.4783)\tPrec@1 89.062 (89.127)\tPrec@5 100.000 (99.552)\n",
      "Epoch: [247][70/97], lr: 0.00000\tTime 0.322 (0.327)\tData 0.000 (0.021)\tLoss 2.0185 (2.4552)\tPrec@1 90.625 (89.074)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [247][80/97], lr: 0.00000\tTime 0.320 (0.327)\tData 0.000 (0.020)\tLoss 2.6145 (2.4126)\tPrec@1 89.062 (89.091)\tPrec@5 99.219 (99.527)\n",
      "Epoch: [247][90/97], lr: 0.00000\tTime 0.317 (0.327)\tData 0.000 (0.020)\tLoss 3.1380 (2.4106)\tPrec@1 85.938 (89.208)\tPrec@5 100.000 (99.536)\n",
      "Epoch: [247][96/97], lr: 0.00000\tTime 0.310 (0.326)\tData 0.000 (0.020)\tLoss 4.2667 (2.4330)\tPrec@1 83.898 (89.247)\tPrec@5 97.458 (99.524)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.334 (0.334)\tLoss 4.0320 (4.0320)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.097)\tLoss 4.8001 (5.5646)\tPrec@1 78.000 (80.909)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.086)\tLoss 6.0637 (5.6409)\tPrec@1 79.000 (80.048)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.074 (0.082)\tLoss 6.7586 (5.8044)\tPrec@1 81.000 (79.871)\tPrec@5 97.000 (98.258)\n",
      "Test: [40/100]\tTime 0.074 (0.080)\tLoss 4.4519 (5.8659)\tPrec@1 85.000 (79.878)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 5.3725 (5.8514)\tPrec@1 87.000 (80.451)\tPrec@5 99.000 (98.137)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 8.0049 (5.7803)\tPrec@1 81.000 (80.508)\tPrec@5 100.000 (98.262)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.1071 (5.7765)\tPrec@1 82.000 (80.535)\tPrec@5 100.000 (98.338)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.5319 (5.7215)\tPrec@1 81.000 (80.753)\tPrec@5 98.000 (98.383)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 3.2802 (5.7515)\tPrec@1 88.000 (80.505)\tPrec@5 99.000 (98.363)\n",
      "val Results: Prec@1 80.380 Prec@5 98.360 Loss 5.79877\n",
      "val Class Accuracy: [0.908,0.964,0.802,0.684,0.801,0.741,0.841,0.756,0.767,0.774]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [248][0/97], lr: 0.00000\tTime 0.474 (0.474)\tData 0.269 (0.269)\tLoss 1.4956 (1.4956)\tPrec@1 92.188 (92.188)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [248][10/97], lr: 0.00000\tTime 0.323 (0.343)\tData 0.000 (0.039)\tLoss 4.9093 (2.4143)\tPrec@1 82.812 (88.778)\tPrec@5 99.219 (99.503)\n",
      "Epoch: [248][20/97], lr: 0.00000\tTime 0.321 (0.333)\tData 0.000 (0.028)\tLoss 2.0264 (2.2512)\tPrec@1 89.062 (89.621)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [248][30/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.025)\tLoss 3.0314 (2.2861)\tPrec@1 89.844 (89.693)\tPrec@5 100.000 (99.496)\n",
      "Epoch: [248][40/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.023)\tLoss 2.0102 (2.3043)\tPrec@1 87.500 (89.539)\tPrec@5 98.438 (99.428)\n",
      "Epoch: [248][50/97], lr: 0.00000\tTime 0.327 (0.328)\tData 0.000 (0.022)\tLoss 4.5664 (2.3387)\tPrec@1 86.719 (89.660)\tPrec@5 99.219 (99.449)\n",
      "Epoch: [248][60/97], lr: 0.00000\tTime 0.325 (0.328)\tData 0.000 (0.021)\tLoss 1.4162 (2.4070)\tPrec@1 89.844 (89.255)\tPrec@5 100.000 (99.424)\n",
      "Epoch: [248][70/97], lr: 0.00000\tTime 0.320 (0.327)\tData 0.000 (0.020)\tLoss 3.1188 (2.4793)\tPrec@1 89.844 (89.107)\tPrec@5 100.000 (99.461)\n",
      "Epoch: [248][80/97], lr: 0.00000\tTime 0.324 (0.327)\tData 0.000 (0.020)\tLoss 3.0610 (2.4578)\tPrec@1 88.281 (89.053)\tPrec@5 99.219 (99.441)\n",
      "Epoch: [248][90/97], lr: 0.00000\tTime 0.317 (0.327)\tData 0.000 (0.020)\tLoss 2.2678 (2.4643)\tPrec@1 86.719 (89.020)\tPrec@5 97.656 (99.442)\n",
      "Epoch: [248][96/97], lr: 0.00000\tTime 0.317 (0.326)\tData 0.000 (0.020)\tLoss 1.1013 (2.4569)\tPrec@1 91.525 (89.005)\tPrec@5 99.153 (99.444)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.325 (0.325)\tLoss 4.1656 (4.1656)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.096)\tLoss 4.9526 (5.6770)\tPrec@1 78.000 (80.182)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 6.2517 (5.7562)\tPrec@1 78.000 (79.476)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.9601 (5.9025)\tPrec@1 80.000 (79.419)\tPrec@5 97.000 (98.290)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.6102 (5.9594)\tPrec@1 84.000 (79.390)\tPrec@5 98.000 (98.171)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.4599 (5.9464)\tPrec@1 86.000 (79.980)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.1522 (5.8786)\tPrec@1 83.000 (80.098)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.2871 (5.8776)\tPrec@1 82.000 (80.141)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.7998 (5.8262)\tPrec@1 81.000 (80.444)\tPrec@5 98.000 (98.432)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4541 (5.8574)\tPrec@1 89.000 (80.242)\tPrec@5 99.000 (98.385)\n",
      "val Results: Prec@1 80.130 Prec@5 98.380 Loss 5.89965\n",
      "val Class Accuracy: [0.906,0.964,0.811,0.703,0.801,0.738,0.821,0.734,0.753,0.782]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [249][0/97], lr: 0.00000\tTime 0.452 (0.452)\tData 0.231 (0.231)\tLoss 1.3852 (1.3852)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [249][10/97], lr: 0.00000\tTime 0.319 (0.337)\tData 0.000 (0.035)\tLoss 3.3158 (2.2338)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [249][20/97], lr: 0.00000\tTime 0.320 (0.330)\tData 0.000 (0.027)\tLoss 1.7068 (2.1718)\tPrec@1 85.938 (88.951)\tPrec@5 100.000 (99.591)\n",
      "Epoch: [249][30/97], lr: 0.00000\tTime 0.320 (0.327)\tData 0.000 (0.024)\tLoss 3.6939 (2.2590)\tPrec@1 85.938 (88.609)\tPrec@5 100.000 (99.572)\n",
      "Epoch: [249][40/97], lr: 0.00000\tTime 0.321 (0.326)\tData 0.000 (0.022)\tLoss 2.4313 (2.3457)\tPrec@1 92.188 (88.948)\tPrec@5 98.438 (99.543)\n",
      "Epoch: [249][50/97], lr: 0.00000\tTime 0.322 (0.326)\tData 0.000 (0.021)\tLoss 2.3414 (2.3823)\tPrec@1 92.969 (89.200)\tPrec@5 100.000 (99.571)\n",
      "Epoch: [249][60/97], lr: 0.00000\tTime 0.320 (0.325)\tData 0.000 (0.020)\tLoss 1.8835 (2.3800)\tPrec@1 85.156 (89.229)\tPrec@5 97.656 (99.526)\n",
      "Epoch: [249][70/97], lr: 0.00000\tTime 0.322 (0.325)\tData 0.000 (0.020)\tLoss 1.7421 (2.4016)\tPrec@1 90.625 (89.173)\tPrec@5 100.000 (99.527)\n",
      "Epoch: [249][80/97], lr: 0.00000\tTime 0.320 (0.325)\tData 0.000 (0.019)\tLoss 1.5651 (2.4841)\tPrec@1 89.062 (89.043)\tPrec@5 100.000 (99.547)\n",
      "Epoch: [249][90/97], lr: 0.00000\tTime 0.321 (0.325)\tData 0.000 (0.019)\tLoss 1.9013 (2.5286)\tPrec@1 89.844 (89.011)\tPrec@5 99.219 (99.536)\n",
      "Epoch: [249][96/97], lr: 0.00000\tTime 0.313 (0.325)\tData 0.000 (0.020)\tLoss 1.8123 (2.5177)\tPrec@1 91.525 (88.989)\tPrec@5 100.000 (99.541)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.270 (0.270)\tLoss 4.0170 (4.0170)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.091)\tLoss 4.8369 (5.5228)\tPrec@1 78.000 (80.636)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.1574 (5.5978)\tPrec@1 78.000 (79.905)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 6.8437 (5.7583)\tPrec@1 80.000 (79.710)\tPrec@5 96.000 (98.290)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 4.4251 (5.8222)\tPrec@1 86.000 (79.707)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 5.3573 (5.8056)\tPrec@1 85.000 (80.176)\tPrec@5 99.000 (98.157)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.0098 (5.7392)\tPrec@1 82.000 (80.213)\tPrec@5 100.000 (98.295)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 5.0741 (5.7335)\tPrec@1 81.000 (80.254)\tPrec@5 100.000 (98.380)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.5253 (5.6763)\tPrec@1 81.000 (80.568)\tPrec@5 98.000 (98.407)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.3251 (5.7060)\tPrec@1 89.000 (80.385)\tPrec@5 99.000 (98.374)\n",
      "val Results: Prec@1 80.290 Prec@5 98.370 Loss 5.74791\n",
      "val Class Accuracy: [0.900,0.960,0.814,0.690,0.801,0.749,0.829,0.738,0.762,0.786]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [250][0/97], lr: 0.00000\tTime 0.484 (0.484)\tData 0.272 (0.272)\tLoss 1.0949 (1.0949)\tPrec@1 86.719 (86.719)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [250][10/97], lr: 0.00000\tTime 0.321 (0.341)\tData 0.000 (0.039)\tLoss 3.1208 (2.4823)\tPrec@1 85.938 (88.352)\tPrec@5 99.219 (99.148)\n",
      "Epoch: [250][20/97], lr: 0.00000\tTime 0.321 (0.333)\tData 0.000 (0.029)\tLoss 4.0356 (2.6085)\tPrec@1 90.625 (88.951)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [250][30/97], lr: 0.00000\tTime 0.319 (0.330)\tData 0.000 (0.025)\tLoss 2.6881 (2.5899)\tPrec@1 89.844 (89.088)\tPrec@5 100.000 (99.446)\n",
      "Epoch: [250][40/97], lr: 0.00000\tTime 0.323 (0.328)\tData 0.000 (0.023)\tLoss 2.5732 (2.7040)\tPrec@1 86.719 (89.043)\tPrec@5 100.000 (99.409)\n",
      "Epoch: [250][50/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.022)\tLoss 2.0525 (2.7155)\tPrec@1 90.625 (89.170)\tPrec@5 99.219 (99.464)\n",
      "Epoch: [250][60/97], lr: 0.00000\tTime 0.353 (0.331)\tData 0.000 (0.021)\tLoss 3.9977 (2.6474)\tPrec@1 89.844 (89.203)\tPrec@5 99.219 (99.501)\n",
      "Epoch: [250][70/97], lr: 0.00000\tTime 0.330 (0.335)\tData 0.000 (0.020)\tLoss 2.4904 (2.6450)\tPrec@1 87.500 (89.316)\tPrec@5 99.219 (99.450)\n",
      "Epoch: [250][80/97], lr: 0.00000\tTime 0.326 (0.336)\tData 0.000 (0.020)\tLoss 1.5282 (2.5927)\tPrec@1 88.281 (89.120)\tPrec@5 100.000 (99.383)\n",
      "Epoch: [250][90/97], lr: 0.00000\tTime 0.319 (0.335)\tData 0.000 (0.020)\tLoss 2.8694 (2.6131)\tPrec@1 89.062 (89.028)\tPrec@5 98.438 (99.365)\n",
      "Epoch: [250][96/97], lr: 0.00000\tTime 0.316 (0.334)\tData 0.000 (0.020)\tLoss 2.1817 (2.6115)\tPrec@1 86.441 (89.078)\tPrec@5 99.153 (99.347)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.279 (0.279)\tLoss 3.9674 (3.9674)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 4.7313 (5.4585)\tPrec@1 78.000 (80.727)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.1422 (5.5454)\tPrec@1 78.000 (79.857)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.7874 (5.7051)\tPrec@1 81.000 (79.645)\tPrec@5 97.000 (98.258)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.3705 (5.7716)\tPrec@1 86.000 (79.659)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.2937 (5.7512)\tPrec@1 84.000 (80.216)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 7.8969 (5.6841)\tPrec@1 82.000 (80.230)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 4.9468 (5.6778)\tPrec@1 81.000 (80.225)\tPrec@5 100.000 (98.394)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.4507 (5.6220)\tPrec@1 81.000 (80.531)\tPrec@5 98.000 (98.420)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.2766 (5.6521)\tPrec@1 89.000 (80.352)\tPrec@5 99.000 (98.374)\n",
      "val Results: Prec@1 80.260 Prec@5 98.380 Loss 5.69427\n",
      "val Class Accuracy: [0.898,0.958,0.815,0.690,0.794,0.743,0.831,0.744,0.765,0.788]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [251][0/97], lr: 0.00000\tTime 0.638 (0.638)\tData 0.338 (0.338)\tLoss 2.0268 (2.0268)\tPrec@1 91.406 (91.406)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [251][10/97], lr: 0.00000\tTime 0.323 (0.371)\tData 0.000 (0.043)\tLoss 1.8828 (2.5350)\tPrec@1 90.625 (89.205)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [251][20/97], lr: 0.00000\tTime 0.321 (0.350)\tData 0.000 (0.030)\tLoss 2.6332 (2.7064)\tPrec@1 92.188 (88.765)\tPrec@5 99.219 (99.628)\n",
      "Epoch: [251][30/97], lr: 0.00000\tTime 0.322 (0.344)\tData 0.000 (0.026)\tLoss 2.0492 (2.4985)\tPrec@1 90.625 (89.415)\tPrec@5 98.438 (99.496)\n",
      "Epoch: [251][40/97], lr: 0.00000\tTime 0.321 (0.341)\tData 0.000 (0.024)\tLoss 1.8604 (2.6132)\tPrec@1 88.281 (89.120)\tPrec@5 98.438 (99.486)\n",
      "Epoch: [251][50/97], lr: 0.00000\tTime 0.322 (0.339)\tData 0.000 (0.022)\tLoss 5.1716 (2.6419)\tPrec@1 86.719 (89.093)\tPrec@5 99.219 (99.494)\n",
      "Epoch: [251][60/97], lr: 0.00000\tTime 0.322 (0.339)\tData 0.000 (0.022)\tLoss 2.5205 (2.5582)\tPrec@1 88.281 (89.395)\tPrec@5 100.000 (99.501)\n",
      "Epoch: [251][70/97], lr: 0.00000\tTime 0.323 (0.338)\tData 0.000 (0.021)\tLoss 1.7733 (2.5766)\tPrec@1 90.625 (89.162)\tPrec@5 100.000 (99.472)\n",
      "Epoch: [251][80/97], lr: 0.00000\tTime 0.325 (0.337)\tData 0.000 (0.020)\tLoss 1.7762 (2.5328)\tPrec@1 92.188 (89.294)\tPrec@5 100.000 (99.498)\n",
      "Epoch: [251][90/97], lr: 0.00000\tTime 0.320 (0.337)\tData 0.000 (0.020)\tLoss 3.0967 (2.5996)\tPrec@1 88.281 (89.166)\tPrec@5 100.000 (99.502)\n",
      "Epoch: [251][96/97], lr: 0.00000\tTime 0.318 (0.336)\tData 0.000 (0.021)\tLoss 1.0681 (2.5838)\tPrec@1 92.373 (89.150)\tPrec@5 99.153 (99.508)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.332 (0.332)\tLoss 4.0540 (4.0540)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.097)\tLoss 4.8196 (5.5603)\tPrec@1 78.000 (80.455)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.086)\tLoss 6.0886 (5.6222)\tPrec@1 78.000 (79.810)\tPrec@5 97.000 (98.381)\n",
      "Test: [30/100]\tTime 0.075 (0.082)\tLoss 6.7751 (5.7895)\tPrec@1 80.000 (79.581)\tPrec@5 97.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 4.5116 (5.8557)\tPrec@1 86.000 (79.683)\tPrec@5 98.000 (98.171)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 5.4253 (5.8375)\tPrec@1 87.000 (80.255)\tPrec@5 99.000 (98.157)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 7.9631 (5.7623)\tPrec@1 80.000 (80.279)\tPrec@5 100.000 (98.279)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.0738 (5.7540)\tPrec@1 82.000 (80.324)\tPrec@5 100.000 (98.366)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.4512 (5.6958)\tPrec@1 82.000 (80.543)\tPrec@5 98.000 (98.407)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 3.2519 (5.7222)\tPrec@1 89.000 (80.319)\tPrec@5 99.000 (98.385)\n",
      "val Results: Prec@1 80.230 Prec@5 98.390 Loss 5.77022\n",
      "val Class Accuracy: [0.904,0.963,0.812,0.681,0.802,0.748,0.826,0.739,0.770,0.778]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [252][0/97], lr: 0.00000\tTime 0.641 (0.641)\tData 0.363 (0.363)\tLoss 2.0961 (2.0961)\tPrec@1 90.625 (90.625)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [252][10/97], lr: 0.00000\tTime 0.321 (0.368)\tData 0.000 (0.047)\tLoss 1.5290 (2.6120)\tPrec@1 93.750 (89.631)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [252][20/97], lr: 0.00000\tTime 0.323 (0.349)\tData 0.000 (0.033)\tLoss 3.1178 (2.4939)\tPrec@1 89.844 (89.025)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [252][30/97], lr: 0.00000\tTime 0.325 (0.343)\tData 0.000 (0.027)\tLoss 4.6040 (2.5147)\tPrec@1 86.719 (89.214)\tPrec@5 100.000 (99.572)\n",
      "Epoch: [252][40/97], lr: 0.00000\tTime 0.323 (0.341)\tData 0.000 (0.025)\tLoss 1.0652 (2.5549)\tPrec@1 93.750 (89.234)\tPrec@5 100.000 (99.524)\n",
      "Epoch: [252][50/97], lr: 0.00000\tTime 0.323 (0.339)\tData 0.000 (0.023)\tLoss 4.2091 (2.5880)\tPrec@1 84.375 (89.139)\tPrec@5 99.219 (99.525)\n",
      "Epoch: [252][60/97], lr: 0.00000\tTime 0.330 (0.338)\tData 0.000 (0.022)\tLoss 2.1792 (2.4489)\tPrec@1 89.844 (89.255)\tPrec@5 99.219 (99.526)\n",
      "Epoch: [252][70/97], lr: 0.00000\tTime 0.322 (0.337)\tData 0.000 (0.022)\tLoss 2.7393 (2.5026)\tPrec@1 88.281 (89.151)\tPrec@5 98.438 (99.527)\n",
      "Epoch: [252][80/97], lr: 0.00000\tTime 0.325 (0.337)\tData 0.000 (0.021)\tLoss 3.1526 (2.5259)\tPrec@1 88.281 (89.149)\tPrec@5 100.000 (99.537)\n",
      "Epoch: [252][90/97], lr: 0.00000\tTime 0.319 (0.337)\tData 0.000 (0.021)\tLoss 4.6097 (2.5541)\tPrec@1 88.281 (89.123)\tPrec@5 98.438 (99.528)\n",
      "Epoch: [252][96/97], lr: 0.00000\tTime 0.324 (0.336)\tData 0.000 (0.021)\tLoss 4.3586 (2.5363)\tPrec@1 89.831 (89.150)\tPrec@5 99.153 (99.500)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.360 (0.360)\tLoss 4.0451 (4.0451)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.099)\tLoss 4.8004 (5.5466)\tPrec@1 78.000 (80.727)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.087)\tLoss 6.0703 (5.6053)\tPrec@1 80.000 (80.048)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.074 (0.083)\tLoss 6.7604 (5.7714)\tPrec@1 80.000 (79.903)\tPrec@5 97.000 (98.258)\n",
      "Test: [40/100]\tTime 0.073 (0.081)\tLoss 4.4874 (5.8339)\tPrec@1 86.000 (79.951)\tPrec@5 98.000 (98.171)\n",
      "Test: [50/100]\tTime 0.075 (0.079)\tLoss 5.4177 (5.8108)\tPrec@1 87.000 (80.471)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.074 (0.079)\tLoss 8.0113 (5.7399)\tPrec@1 82.000 (80.557)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.075 (0.078)\tLoss 5.1153 (5.7333)\tPrec@1 80.000 (80.521)\tPrec@5 100.000 (98.394)\n",
      "Test: [80/100]\tTime 0.074 (0.078)\tLoss 6.5074 (5.6740)\tPrec@1 83.000 (80.765)\tPrec@5 98.000 (98.444)\n",
      "Test: [90/100]\tTime 0.075 (0.077)\tLoss 3.2701 (5.7020)\tPrec@1 89.000 (80.549)\tPrec@5 99.000 (98.418)\n",
      "val Results: Prec@1 80.400 Prec@5 98.440 Loss 5.74802\n",
      "val Class Accuracy: [0.910,0.961,0.807,0.685,0.812,0.751,0.827,0.735,0.770,0.782]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [253][0/97], lr: 0.00000\tTime 0.620 (0.620)\tData 0.340 (0.340)\tLoss 0.9949 (0.9949)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [253][10/97], lr: 0.00000\tTime 0.324 (0.371)\tData 0.000 (0.044)\tLoss 3.6817 (3.4802)\tPrec@1 88.281 (88.423)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [253][20/97], lr: 0.00000\tTime 0.322 (0.352)\tData 0.000 (0.031)\tLoss 1.6618 (2.6180)\tPrec@1 89.062 (89.323)\tPrec@5 99.219 (99.516)\n",
      "Epoch: [253][30/97], lr: 0.00000\tTime 0.324 (0.345)\tData 0.000 (0.027)\tLoss 3.1705 (2.6454)\tPrec@1 86.719 (88.861)\tPrec@5 100.000 (99.420)\n",
      "Epoch: [253][40/97], lr: 0.00000\tTime 0.321 (0.341)\tData 0.000 (0.024)\tLoss 2.0924 (2.6091)\tPrec@1 86.719 (88.796)\tPrec@5 97.656 (99.314)\n",
      "Epoch: [253][50/97], lr: 0.00000\tTime 0.321 (0.340)\tData 0.000 (0.023)\tLoss 2.5925 (2.5192)\tPrec@1 85.156 (89.017)\tPrec@5 98.438 (99.372)\n",
      "Epoch: [253][60/97], lr: 0.00000\tTime 0.322 (0.339)\tData 0.000 (0.022)\tLoss 2.3523 (2.5091)\tPrec@1 89.844 (89.191)\tPrec@5 100.000 (99.411)\n",
      "Epoch: [253][70/97], lr: 0.00000\tTime 0.324 (0.338)\tData 0.000 (0.021)\tLoss 2.5080 (2.5475)\tPrec@1 91.406 (89.173)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [253][80/97], lr: 0.00000\tTime 0.324 (0.338)\tData 0.000 (0.021)\tLoss 4.0779 (2.5953)\tPrec@1 85.938 (89.169)\tPrec@5 99.219 (99.402)\n",
      "Epoch: [253][90/97], lr: 0.00000\tTime 0.321 (0.337)\tData 0.000 (0.020)\tLoss 2.3119 (2.6324)\tPrec@1 89.844 (89.191)\tPrec@5 100.000 (99.408)\n",
      "Epoch: [253][96/97], lr: 0.00000\tTime 0.327 (0.337)\tData 0.000 (0.021)\tLoss 3.0559 (2.6192)\tPrec@1 92.373 (89.247)\tPrec@5 100.000 (99.404)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.418 (0.418)\tLoss 4.2047 (4.2047)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.105)\tLoss 5.0652 (5.7276)\tPrec@1 78.000 (80.091)\tPrec@5 99.000 (98.636)\n",
      "Test: [20/100]\tTime 0.074 (0.090)\tLoss 6.2878 (5.8069)\tPrec@1 78.000 (79.381)\tPrec@5 97.000 (98.429)\n",
      "Test: [30/100]\tTime 0.073 (0.085)\tLoss 6.9431 (5.9508)\tPrec@1 81.000 (79.419)\tPrec@5 97.000 (98.387)\n",
      "Test: [40/100]\tTime 0.073 (0.082)\tLoss 4.6816 (6.0007)\tPrec@1 85.000 (79.561)\tPrec@5 98.000 (98.268)\n",
      "Test: [50/100]\tTime 0.074 (0.081)\tLoss 5.5087 (5.9912)\tPrec@1 87.000 (80.118)\tPrec@5 99.000 (98.275)\n",
      "Test: [60/100]\tTime 0.075 (0.079)\tLoss 8.2165 (5.9161)\tPrec@1 81.000 (80.148)\tPrec@5 100.000 (98.377)\n",
      "Test: [70/100]\tTime 0.074 (0.079)\tLoss 5.3075 (5.9160)\tPrec@1 80.000 (80.155)\tPrec@5 100.000 (98.451)\n",
      "Test: [80/100]\tTime 0.073 (0.078)\tLoss 6.7846 (5.8654)\tPrec@1 82.000 (80.407)\tPrec@5 98.000 (98.481)\n",
      "Test: [90/100]\tTime 0.075 (0.078)\tLoss 3.4870 (5.8998)\tPrec@1 88.000 (80.187)\tPrec@5 99.000 (98.451)\n",
      "val Results: Prec@1 80.110 Prec@5 98.450 Loss 5.94407\n",
      "val Class Accuracy: [0.911,0.966,0.807,0.684,0.802,0.747,0.822,0.744,0.750,0.778]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [254][0/97], lr: 0.00000\tTime 0.538 (0.538)\tData 0.315 (0.315)\tLoss 3.5580 (3.5580)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [254][10/97], lr: 0.00000\tTime 0.321 (0.363)\tData 0.000 (0.042)\tLoss 1.1968 (2.4614)\tPrec@1 93.750 (90.625)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [254][20/97], lr: 0.00000\tTime 0.322 (0.349)\tData 0.000 (0.030)\tLoss 1.3693 (2.4696)\tPrec@1 92.188 (89.993)\tPrec@5 99.219 (99.293)\n",
      "Epoch: [254][30/97], lr: 0.00000\tTime 0.330 (0.343)\tData 0.000 (0.026)\tLoss 3.4478 (2.4405)\tPrec@1 92.969 (89.844)\tPrec@5 99.219 (99.320)\n",
      "Epoch: [254][40/97], lr: 0.00000\tTime 0.321 (0.340)\tData 0.000 (0.024)\tLoss 2.1738 (2.5411)\tPrec@1 89.062 (89.787)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [254][50/97], lr: 0.00000\tTime 0.328 (0.339)\tData 0.000 (0.022)\tLoss 1.8115 (2.5669)\tPrec@1 89.062 (89.813)\tPrec@5 99.219 (99.464)\n",
      "Epoch: [254][60/97], lr: 0.00000\tTime 0.329 (0.339)\tData 0.000 (0.022)\tLoss 4.7758 (2.5920)\tPrec@1 83.594 (89.767)\tPrec@5 99.219 (99.501)\n",
      "Epoch: [254][70/97], lr: 0.00000\tTime 0.324 (0.338)\tData 0.000 (0.021)\tLoss 2.2082 (2.5412)\tPrec@1 88.281 (89.569)\tPrec@5 99.219 (99.505)\n",
      "Epoch: [254][80/97], lr: 0.00000\tTime 0.323 (0.338)\tData 0.000 (0.020)\tLoss 2.1942 (2.5445)\tPrec@1 89.844 (89.410)\tPrec@5 100.000 (99.537)\n",
      "Epoch: [254][90/97], lr: 0.00000\tTime 0.321 (0.337)\tData 0.000 (0.020)\tLoss 2.1983 (2.5196)\tPrec@1 92.188 (89.483)\tPrec@5 99.219 (99.519)\n",
      "Epoch: [254][96/97], lr: 0.00000\tTime 0.326 (0.337)\tData 0.000 (0.021)\tLoss 1.7346 (2.5051)\tPrec@1 88.983 (89.594)\tPrec@5 100.000 (99.541)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.399 (0.399)\tLoss 4.0631 (4.0631)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.103)\tLoss 4.8710 (5.5546)\tPrec@1 78.000 (80.636)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.089)\tLoss 6.1577 (5.6328)\tPrec@1 79.000 (79.952)\tPrec@5 97.000 (98.381)\n",
      "Test: [30/100]\tTime 0.074 (0.084)\tLoss 6.8693 (5.7941)\tPrec@1 81.000 (79.774)\tPrec@5 97.000 (98.323)\n",
      "Test: [40/100]\tTime 0.074 (0.082)\tLoss 4.4721 (5.8571)\tPrec@1 85.000 (79.805)\tPrec@5 98.000 (98.195)\n",
      "Test: [50/100]\tTime 0.074 (0.080)\tLoss 5.3636 (5.8439)\tPrec@1 86.000 (80.275)\tPrec@5 99.000 (98.235)\n",
      "Test: [60/100]\tTime 0.074 (0.079)\tLoss 8.0174 (5.7751)\tPrec@1 82.000 (80.279)\tPrec@5 100.000 (98.344)\n",
      "Test: [70/100]\tTime 0.074 (0.078)\tLoss 5.0290 (5.7706)\tPrec@1 82.000 (80.296)\tPrec@5 100.000 (98.423)\n",
      "Test: [80/100]\tTime 0.075 (0.078)\tLoss 6.5311 (5.7153)\tPrec@1 81.000 (80.543)\tPrec@5 98.000 (98.444)\n",
      "Test: [90/100]\tTime 0.075 (0.078)\tLoss 3.3843 (5.7466)\tPrec@1 88.000 (80.341)\tPrec@5 99.000 (98.407)\n",
      "val Results: Prec@1 80.210 Prec@5 98.410 Loss 5.78951\n",
      "val Class Accuracy: [0.905,0.961,0.806,0.681,0.803,0.750,0.834,0.741,0.757,0.783]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [255][0/97], lr: 0.00000\tTime 0.649 (0.649)\tData 0.367 (0.367)\tLoss 1.2799 (1.2799)\tPrec@1 89.062 (89.062)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [255][10/97], lr: 0.00000\tTime 0.329 (0.381)\tData 0.000 (0.047)\tLoss 3.2625 (2.5468)\tPrec@1 86.719 (89.844)\tPrec@5 99.219 (99.574)\n",
      "Epoch: [255][20/97], lr: 0.00000\tTime 0.322 (0.357)\tData 0.000 (0.033)\tLoss 2.5762 (2.6403)\tPrec@1 89.844 (89.546)\tPrec@5 99.219 (99.554)\n",
      "Epoch: [255][30/97], lr: 0.00000\tTime 0.322 (0.348)\tData 0.000 (0.028)\tLoss 2.3697 (2.6661)\tPrec@1 86.719 (89.012)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [255][40/97], lr: 0.00000\tTime 0.324 (0.344)\tData 0.000 (0.025)\tLoss 1.3167 (2.6442)\tPrec@1 90.625 (89.310)\tPrec@5 100.000 (99.466)\n",
      "Epoch: [255][50/97], lr: 0.00000\tTime 0.338 (0.342)\tData 0.000 (0.023)\tLoss 2.8743 (2.5875)\tPrec@1 89.062 (89.308)\tPrec@5 99.219 (99.510)\n",
      "Epoch: [255][60/97], lr: 0.00000\tTime 0.325 (0.340)\tData 0.000 (0.022)\tLoss 1.3313 (2.5303)\tPrec@1 92.188 (89.139)\tPrec@5 99.219 (99.424)\n",
      "Epoch: [255][70/97], lr: 0.00000\tTime 0.328 (0.339)\tData 0.000 (0.022)\tLoss 4.1811 (2.5191)\tPrec@1 90.625 (89.316)\tPrec@5 100.000 (99.472)\n",
      "Epoch: [255][80/97], lr: 0.00000\tTime 0.390 (0.349)\tData 0.001 (0.021)\tLoss 2.4642 (2.4973)\tPrec@1 85.938 (89.265)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [255][90/97], lr: 0.00000\tTime 0.347 (0.350)\tData 0.000 (0.020)\tLoss 0.7334 (2.4956)\tPrec@1 95.312 (89.320)\tPrec@5 100.000 (99.468)\n",
      "Epoch: [255][96/97], lr: 0.00000\tTime 0.372 (0.352)\tData 0.000 (0.021)\tLoss 1.4542 (2.5007)\tPrec@1 91.525 (89.416)\tPrec@5 100.000 (99.484)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.470 (0.470)\tLoss 3.9376 (3.9376)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.076 (0.115)\tLoss 4.7220 (5.4834)\tPrec@1 78.000 (80.727)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.074 (0.096)\tLoss 6.0979 (5.5584)\tPrec@1 79.000 (80.000)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.074 (0.090)\tLoss 6.7481 (5.7111)\tPrec@1 81.000 (79.806)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.074 (0.086)\tLoss 4.4319 (5.7748)\tPrec@1 86.000 (79.927)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.075 (0.085)\tLoss 5.3182 (5.7546)\tPrec@1 86.000 (80.412)\tPrec@5 99.000 (98.196)\n",
      "Test: [60/100]\tTime 0.080 (0.084)\tLoss 7.9372 (5.6850)\tPrec@1 82.000 (80.426)\tPrec@5 100.000 (98.295)\n",
      "Test: [70/100]\tTime 0.075 (0.084)\tLoss 5.0618 (5.6758)\tPrec@1 81.000 (80.451)\tPrec@5 100.000 (98.394)\n",
      "Test: [80/100]\tTime 0.074 (0.083)\tLoss 6.4086 (5.6176)\tPrec@1 82.000 (80.667)\tPrec@5 98.000 (98.444)\n",
      "Test: [90/100]\tTime 0.075 (0.082)\tLoss 3.2236 (5.6458)\tPrec@1 89.000 (80.451)\tPrec@5 99.000 (98.418)\n",
      "val Results: Prec@1 80.320 Prec@5 98.420 Loss 5.68992\n",
      "val Class Accuracy: [0.902,0.960,0.811,0.677,0.806,0.749,0.831,0.740,0.770,0.786]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [256][0/97], lr: 0.00000\tTime 0.717 (0.717)\tData 0.404 (0.404)\tLoss 1.5057 (1.5057)\tPrec@1 86.719 (86.719)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [256][10/97], lr: 0.00000\tTime 0.323 (0.387)\tData 0.000 (0.050)\tLoss 2.1575 (2.2154)\tPrec@1 85.938 (88.565)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [256][20/97], lr: 0.00000\tTime 0.319 (0.359)\tData 0.000 (0.034)\tLoss 1.6686 (2.5430)\tPrec@1 92.188 (89.137)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [256][30/97], lr: 0.00000\tTime 0.325 (0.350)\tData 0.000 (0.029)\tLoss 1.5150 (2.5254)\tPrec@1 92.969 (88.861)\tPrec@5 100.000 (99.370)\n",
      "Epoch: [256][40/97], lr: 0.00000\tTime 0.322 (0.345)\tData 0.000 (0.026)\tLoss 1.1139 (2.4219)\tPrec@1 93.750 (89.120)\tPrec@5 99.219 (99.352)\n",
      "Epoch: [256][50/97], lr: 0.00000\tTime 0.323 (0.343)\tData 0.000 (0.024)\tLoss 1.8315 (2.5136)\tPrec@1 93.750 (89.017)\tPrec@5 100.000 (99.311)\n",
      "Epoch: [256][60/97], lr: 0.00000\tTime 0.323 (0.341)\tData 0.000 (0.023)\tLoss 1.9819 (2.4803)\tPrec@1 89.062 (88.909)\tPrec@5 99.219 (99.270)\n",
      "Epoch: [256][70/97], lr: 0.00000\tTime 0.324 (0.340)\tData 0.000 (0.022)\tLoss 2.1276 (2.5173)\tPrec@1 92.188 (89.206)\tPrec@5 100.000 (99.318)\n",
      "Epoch: [256][80/97], lr: 0.00000\tTime 0.328 (0.339)\tData 0.000 (0.021)\tLoss 2.3545 (2.5562)\tPrec@1 88.281 (89.091)\tPrec@5 98.438 (99.277)\n",
      "Epoch: [256][90/97], lr: 0.00000\tTime 0.322 (0.339)\tData 0.000 (0.021)\tLoss 1.0795 (2.5352)\tPrec@1 92.188 (89.020)\tPrec@5 100.000 (99.262)\n",
      "Epoch: [256][96/97], lr: 0.00000\tTime 0.326 (0.338)\tData 0.000 (0.021)\tLoss 1.4013 (2.4902)\tPrec@1 91.525 (89.094)\tPrec@5 100.000 (99.266)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.314 (0.314)\tLoss 4.0859 (4.0859)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.095)\tLoss 4.9230 (5.6003)\tPrec@1 77.000 (80.636)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.074 (0.085)\tLoss 6.1533 (5.6757)\tPrec@1 79.000 (79.905)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.075 (0.082)\tLoss 6.8931 (5.8375)\tPrec@1 81.000 (79.710)\tPrec@5 97.000 (98.194)\n",
      "Test: [40/100]\tTime 0.074 (0.080)\tLoss 4.5600 (5.9001)\tPrec@1 83.000 (79.732)\tPrec@5 98.000 (98.049)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 5.3524 (5.8870)\tPrec@1 86.000 (80.216)\tPrec@5 99.000 (98.078)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 8.0139 (5.8185)\tPrec@1 81.000 (80.262)\tPrec@5 99.000 (98.197)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.1567 (5.8196)\tPrec@1 81.000 (80.254)\tPrec@5 100.000 (98.282)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.5847 (5.7655)\tPrec@1 82.000 (80.531)\tPrec@5 98.000 (98.321)\n",
      "Test: [90/100]\tTime 0.075 (0.077)\tLoss 3.3954 (5.7951)\tPrec@1 88.000 (80.264)\tPrec@5 99.000 (98.330)\n",
      "val Results: Prec@1 80.130 Prec@5 98.330 Loss 5.84026\n",
      "val Class Accuracy: [0.906,0.962,0.805,0.675,0.797,0.750,0.846,0.741,0.755,0.776]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [257][0/97], lr: 0.00000\tTime 0.728 (0.728)\tData 0.413 (0.413)\tLoss 2.0019 (2.0019)\tPrec@1 90.625 (90.625)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [257][10/97], lr: 0.00000\tTime 0.321 (0.393)\tData 0.000 (0.050)\tLoss 2.7621 (2.7159)\tPrec@1 85.938 (89.418)\tPrec@5 98.438 (99.077)\n",
      "Epoch: [257][20/97], lr: 0.00000\tTime 0.321 (0.362)\tData 0.000 (0.034)\tLoss 0.6730 (2.3705)\tPrec@1 92.969 (89.881)\tPrec@5 100.000 (99.293)\n",
      "Epoch: [257][30/97], lr: 0.00000\tTime 0.318 (0.352)\tData 0.000 (0.029)\tLoss 1.3931 (2.3037)\tPrec@1 86.719 (89.567)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [257][40/97], lr: 0.00000\tTime 0.319 (0.347)\tData 0.000 (0.026)\tLoss 2.8683 (2.3245)\tPrec@1 89.062 (89.653)\tPrec@5 99.219 (99.352)\n",
      "Epoch: [257][50/97], lr: 0.00000\tTime 0.323 (0.344)\tData 0.000 (0.024)\tLoss 2.9986 (2.3051)\tPrec@1 88.281 (89.507)\tPrec@5 99.219 (99.357)\n",
      "Epoch: [257][60/97], lr: 0.00000\tTime 0.326 (0.342)\tData 0.000 (0.023)\tLoss 2.9079 (2.3702)\tPrec@1 89.844 (89.434)\tPrec@5 99.219 (99.372)\n",
      "Epoch: [257][70/97], lr: 0.00000\tTime 0.320 (0.342)\tData 0.000 (0.022)\tLoss 1.2105 (2.3712)\tPrec@1 88.281 (89.261)\tPrec@5 100.000 (99.439)\n",
      "Epoch: [257][80/97], lr: 0.00000\tTime 0.319 (0.341)\tData 0.000 (0.022)\tLoss 2.7353 (2.3291)\tPrec@1 89.844 (89.226)\tPrec@5 99.219 (99.402)\n",
      "Epoch: [257][90/97], lr: 0.00000\tTime 0.336 (0.342)\tData 0.000 (0.021)\tLoss 2.7936 (2.3403)\tPrec@1 94.531 (89.183)\tPrec@5 99.219 (99.416)\n",
      "Epoch: [257][96/97], lr: 0.00000\tTime 0.330 (0.342)\tData 0.000 (0.021)\tLoss 1.4708 (2.3805)\tPrec@1 86.441 (89.094)\tPrec@5 100.000 (99.404)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.426 (0.426)\tLoss 3.8698 (3.8698)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.106)\tLoss 4.6778 (5.4519)\tPrec@1 78.000 (80.727)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.073 (0.092)\tLoss 6.0699 (5.5272)\tPrec@1 79.000 (79.857)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.074 (0.087)\tLoss 6.7648 (5.6893)\tPrec@1 80.000 (79.677)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.075 (0.084)\tLoss 4.3804 (5.7567)\tPrec@1 86.000 (79.683)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.078 (0.085)\tLoss 5.2797 (5.7311)\tPrec@1 87.000 (80.275)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.074 (0.084)\tLoss 7.8938 (5.6639)\tPrec@1 82.000 (80.328)\tPrec@5 100.000 (98.295)\n",
      "Test: [70/100]\tTime 0.076 (0.084)\tLoss 5.0433 (5.6575)\tPrec@1 81.000 (80.338)\tPrec@5 100.000 (98.394)\n",
      "Test: [80/100]\tTime 0.074 (0.084)\tLoss 6.4374 (5.6034)\tPrec@1 81.000 (80.593)\tPrec@5 98.000 (98.444)\n",
      "Test: [90/100]\tTime 0.095 (0.084)\tLoss 3.2323 (5.6309)\tPrec@1 88.000 (80.396)\tPrec@5 99.000 (98.396)\n",
      "val Results: Prec@1 80.290 Prec@5 98.390 Loss 5.67506\n",
      "val Class Accuracy: [0.896,0.961,0.807,0.686,0.807,0.745,0.830,0.746,0.768,0.783]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [258][0/97], lr: 0.00000\tTime 1.432 (1.432)\tData 0.902 (0.902)\tLoss 1.8617 (1.8617)\tPrec@1 88.281 (88.281)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [258][10/97], lr: 0.00000\tTime 0.470 (0.572)\tData 0.000 (0.091)\tLoss 2.5374 (2.4349)\tPrec@1 86.719 (89.489)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [258][20/97], lr: 0.00000\tTime 0.497 (0.535)\tData 0.001 (0.054)\tLoss 2.8594 (2.4248)\tPrec@1 89.062 (89.546)\tPrec@5 99.219 (99.293)\n",
      "Epoch: [258][30/97], lr: 0.00000\tTime 0.408 (0.518)\tData 0.002 (0.040)\tLoss 3.4680 (2.5427)\tPrec@1 85.156 (89.491)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [258][40/97], lr: 0.00000\tTime 0.321 (0.478)\tData 0.000 (0.034)\tLoss 3.9810 (2.6409)\tPrec@1 88.281 (89.158)\tPrec@5 99.219 (99.276)\n",
      "Epoch: [258][50/97], lr: 0.00000\tTime 0.320 (0.450)\tData 0.000 (0.031)\tLoss 1.1257 (2.5471)\tPrec@1 90.625 (89.277)\tPrec@5 100.000 (99.341)\n",
      "Epoch: [258][60/97], lr: 0.00000\tTime 0.323 (0.431)\tData 0.000 (0.028)\tLoss 5.5296 (2.5208)\tPrec@1 85.156 (89.267)\tPrec@5 100.000 (99.411)\n",
      "Epoch: [258][70/97], lr: 0.00000\tTime 0.321 (0.417)\tData 0.000 (0.027)\tLoss 3.3606 (2.4818)\tPrec@1 89.062 (89.206)\tPrec@5 99.219 (99.472)\n",
      "Epoch: [258][80/97], lr: 0.00000\tTime 0.324 (0.407)\tData 0.000 (0.026)\tLoss 1.7691 (2.4634)\tPrec@1 89.844 (89.188)\tPrec@5 100.000 (99.402)\n",
      "Epoch: [258][90/97], lr: 0.00000\tTime 0.321 (0.399)\tData 0.000 (0.025)\tLoss 4.0211 (2.4217)\tPrec@1 87.500 (89.303)\tPrec@5 100.000 (99.373)\n",
      "Epoch: [258][96/97], lr: 0.00000\tTime 0.317 (0.395)\tData 0.000 (0.025)\tLoss 2.9403 (2.4279)\tPrec@1 90.678 (89.368)\tPrec@5 98.305 (99.379)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.307 (0.307)\tLoss 4.2077 (4.2077)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.095)\tLoss 4.9205 (5.7144)\tPrec@1 78.000 (80.545)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.074 (0.085)\tLoss 6.2225 (5.7835)\tPrec@1 79.000 (79.857)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.8877 (5.9425)\tPrec@1 80.000 (79.677)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.074 (0.080)\tLoss 4.6768 (5.9999)\tPrec@1 85.000 (79.707)\tPrec@5 98.000 (98.171)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.5931 (5.9909)\tPrec@1 86.000 (80.196)\tPrec@5 99.000 (98.196)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 8.1919 (5.9153)\tPrec@1 82.000 (80.377)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.2778 (5.9156)\tPrec@1 82.000 (80.352)\tPrec@5 100.000 (98.394)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.7924 (5.8605)\tPrec@1 81.000 (80.654)\tPrec@5 98.000 (98.444)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 3.4005 (5.8898)\tPrec@1 89.000 (80.429)\tPrec@5 99.000 (98.418)\n",
      "val Results: Prec@1 80.320 Prec@5 98.420 Loss 5.93564\n",
      "val Class Accuracy: [0.916,0.964,0.810,0.697,0.810,0.745,0.818,0.737,0.761,0.774]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [259][0/97], lr: 0.00000\tTime 1.039 (1.039)\tData 0.481 (0.481)\tLoss 3.4133 (3.4133)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [259][10/97], lr: 0.00000\tTime 0.325 (0.453)\tData 0.000 (0.053)\tLoss 1.7940 (3.2458)\tPrec@1 89.062 (87.784)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [259][20/97], lr: 0.00000\tTime 0.330 (0.401)\tData 0.000 (0.036)\tLoss 2.3163 (3.0524)\tPrec@1 92.188 (88.728)\tPrec@5 99.219 (99.293)\n",
      "Epoch: [259][30/97], lr: 0.00000\tTime 0.326 (0.380)\tData 0.000 (0.030)\tLoss 2.9804 (3.0031)\tPrec@1 93.750 (88.735)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [259][40/97], lr: 0.00000\tTime 0.323 (0.369)\tData 0.000 (0.027)\tLoss 1.1179 (2.8323)\tPrec@1 92.969 (89.177)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [259][50/97], lr: 0.00000\tTime 0.327 (0.363)\tData 0.000 (0.025)\tLoss 1.3518 (2.7338)\tPrec@1 94.531 (89.124)\tPrec@5 100.000 (99.372)\n",
      "Epoch: [259][60/97], lr: 0.00000\tTime 0.327 (0.359)\tData 0.000 (0.023)\tLoss 1.7701 (2.7393)\tPrec@1 91.406 (89.050)\tPrec@5 100.000 (99.385)\n",
      "Epoch: [259][70/97], lr: 0.00000\tTime 0.322 (0.355)\tData 0.000 (0.022)\tLoss 2.1723 (2.6189)\tPrec@1 89.844 (89.228)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [259][80/97], lr: 0.00000\tTime 0.322 (0.352)\tData 0.000 (0.022)\tLoss 2.0671 (2.6094)\tPrec@1 89.844 (89.313)\tPrec@5 99.219 (99.421)\n",
      "Epoch: [259][90/97], lr: 0.00000\tTime 0.327 (0.350)\tData 0.000 (0.021)\tLoss 3.3371 (2.6018)\tPrec@1 89.062 (89.329)\tPrec@5 99.219 (99.416)\n",
      "Epoch: [259][96/97], lr: 0.00000\tTime 0.316 (0.349)\tData 0.000 (0.022)\tLoss 1.5487 (2.6065)\tPrec@1 88.983 (89.183)\tPrec@5 100.000 (99.420)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.371 (0.371)\tLoss 4.1439 (4.1439)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.101)\tLoss 4.9249 (5.6916)\tPrec@1 78.000 (80.818)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.074 (0.088)\tLoss 6.2170 (5.7695)\tPrec@1 79.000 (79.857)\tPrec@5 97.000 (98.381)\n",
      "Test: [30/100]\tTime 0.074 (0.083)\tLoss 6.8790 (5.9290)\tPrec@1 81.000 (79.677)\tPrec@5 97.000 (98.323)\n",
      "Test: [40/100]\tTime 0.074 (0.081)\tLoss 4.6505 (5.9859)\tPrec@1 84.000 (79.683)\tPrec@5 98.000 (98.195)\n",
      "Test: [50/100]\tTime 0.074 (0.080)\tLoss 5.4705 (5.9737)\tPrec@1 85.000 (80.118)\tPrec@5 99.000 (98.196)\n",
      "Test: [60/100]\tTime 0.074 (0.079)\tLoss 8.1609 (5.8982)\tPrec@1 80.000 (80.180)\tPrec@5 99.000 (98.279)\n",
      "Test: [70/100]\tTime 0.074 (0.078)\tLoss 5.3647 (5.8995)\tPrec@1 81.000 (80.197)\tPrec@5 100.000 (98.366)\n",
      "Test: [80/100]\tTime 0.074 (0.078)\tLoss 6.7165 (5.8474)\tPrec@1 83.000 (80.481)\tPrec@5 98.000 (98.407)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 3.4158 (5.8766)\tPrec@1 88.000 (80.253)\tPrec@5 99.000 (98.385)\n",
      "val Results: Prec@1 80.190 Prec@5 98.390 Loss 5.92203\n",
      "val Class Accuracy: [0.907,0.964,0.815,0.671,0.800,0.742,0.843,0.747,0.756,0.774]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [260][0/97], lr: 0.00000\tTime 0.683 (0.683)\tData 0.372 (0.372)\tLoss 3.5177 (3.5177)\tPrec@1 85.156 (85.156)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [260][10/97], lr: 0.00000\tTime 0.323 (0.382)\tData 0.000 (0.046)\tLoss 2.3709 (2.5789)\tPrec@1 92.188 (88.494)\tPrec@5 100.000 (99.645)\n",
      "Epoch: [260][20/97], lr: 0.00000\tTime 0.321 (0.358)\tData 0.000 (0.032)\tLoss 3.0835 (2.4595)\tPrec@1 85.938 (88.839)\tPrec@5 98.438 (99.554)\n",
      "Epoch: [260][30/97], lr: 0.00000\tTime 0.320 (0.349)\tData 0.000 (0.027)\tLoss 1.7790 (2.5142)\tPrec@1 87.500 (88.861)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [260][40/97], lr: 0.00000\tTime 0.325 (0.345)\tData 0.000 (0.025)\tLoss 2.8788 (2.5481)\tPrec@1 90.625 (89.101)\tPrec@5 100.000 (99.352)\n",
      "Epoch: [260][50/97], lr: 0.00000\tTime 0.322 (0.344)\tData 0.000 (0.023)\tLoss 1.4375 (2.4532)\tPrec@1 88.281 (89.170)\tPrec@5 98.438 (99.249)\n",
      "Epoch: [260][60/97], lr: 0.00000\tTime 0.327 (0.343)\tData 0.000 (0.022)\tLoss 2.1112 (2.4844)\tPrec@1 92.188 (89.229)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [260][70/97], lr: 0.00000\tTime 0.326 (0.344)\tData 0.000 (0.022)\tLoss 2.9946 (2.4882)\tPrec@1 89.844 (89.239)\tPrec@5 99.219 (99.318)\n",
      "Epoch: [260][80/97], lr: 0.00000\tTime 0.337 (0.344)\tData 0.000 (0.021)\tLoss 1.1799 (2.4958)\tPrec@1 94.531 (89.265)\tPrec@5 99.219 (99.296)\n",
      "Epoch: [260][90/97], lr: 0.00000\tTime 0.363 (0.343)\tData 0.000 (0.020)\tLoss 2.4915 (2.5123)\tPrec@1 91.406 (89.269)\tPrec@5 100.000 (99.330)\n",
      "Epoch: [260][96/97], lr: 0.00000\tTime 0.387 (0.347)\tData 0.000 (0.021)\tLoss 2.1539 (2.4986)\tPrec@1 89.831 (89.336)\tPrec@5 100.000 (99.331)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.528 (0.528)\tLoss 4.0821 (4.0821)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.118)\tLoss 4.9002 (5.5800)\tPrec@1 78.000 (80.455)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.111 (0.101)\tLoss 6.2267 (5.6486)\tPrec@1 78.000 (79.714)\tPrec@5 98.000 (98.333)\n",
      "Test: [30/100]\tTime 0.088 (0.094)\tLoss 6.8379 (5.8200)\tPrec@1 81.000 (79.581)\tPrec@5 97.000 (98.258)\n",
      "Test: [40/100]\tTime 0.101 (0.092)\tLoss 4.4831 (5.8846)\tPrec@1 85.000 (79.707)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.094 (0.091)\tLoss 5.5037 (5.8715)\tPrec@1 86.000 (80.196)\tPrec@5 99.000 (98.157)\n",
      "Test: [60/100]\tTime 0.081 (0.090)\tLoss 8.0323 (5.8001)\tPrec@1 81.000 (80.197)\tPrec@5 100.000 (98.295)\n",
      "Test: [70/100]\tTime 0.075 (0.088)\tLoss 5.0707 (5.8004)\tPrec@1 81.000 (80.211)\tPrec@5 100.000 (98.380)\n",
      "Test: [80/100]\tTime 0.076 (0.087)\tLoss 6.5301 (5.7381)\tPrec@1 83.000 (80.519)\tPrec@5 98.000 (98.432)\n",
      "Test: [90/100]\tTime 0.075 (0.085)\tLoss 3.3834 (5.7683)\tPrec@1 89.000 (80.319)\tPrec@5 99.000 (98.429)\n",
      "val Results: Prec@1 80.240 Prec@5 98.410 Loss 5.81386\n",
      "val Class Accuracy: [0.915,0.961,0.811,0.675,0.800,0.756,0.826,0.743,0.763,0.774]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [261][0/97], lr: 0.00000\tTime 1.040 (1.040)\tData 0.586 (0.586)\tLoss 2.6230 (2.6230)\tPrec@1 87.500 (87.500)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [261][10/97], lr: 0.00000\tTime 0.334 (0.438)\tData 0.000 (0.063)\tLoss 1.6027 (2.4854)\tPrec@1 88.281 (88.707)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [261][20/97], lr: 0.00000\tTime 0.327 (0.388)\tData 0.000 (0.041)\tLoss 1.9678 (2.2425)\tPrec@1 92.969 (89.397)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [261][30/97], lr: 0.00000\tTime 0.327 (0.368)\tData 0.000 (0.033)\tLoss 3.7724 (2.1765)\tPrec@1 90.625 (89.768)\tPrec@5 100.000 (99.446)\n",
      "Epoch: [261][40/97], lr: 0.00000\tTime 0.320 (0.358)\tData 0.000 (0.029)\tLoss 1.5659 (2.1776)\tPrec@1 90.625 (89.806)\tPrec@5 100.000 (99.466)\n",
      "Epoch: [261][50/97], lr: 0.00000\tTime 0.324 (0.352)\tData 0.000 (0.027)\tLoss 2.8478 (2.2376)\tPrec@1 82.031 (89.338)\tPrec@5 98.438 (99.387)\n",
      "Epoch: [261][60/97], lr: 0.00000\tTime 0.322 (0.347)\tData 0.000 (0.025)\tLoss 2.0515 (2.2637)\tPrec@1 91.406 (89.511)\tPrec@5 100.000 (99.385)\n",
      "Epoch: [261][70/97], lr: 0.00000\tTime 0.323 (0.344)\tData 0.000 (0.024)\tLoss 1.3393 (2.2989)\tPrec@1 89.062 (89.514)\tPrec@5 100.000 (99.406)\n",
      "Epoch: [261][80/97], lr: 0.00000\tTime 0.338 (0.343)\tData 0.000 (0.023)\tLoss 2.3643 (2.3581)\tPrec@1 89.844 (89.400)\tPrec@5 96.875 (99.363)\n",
      "Epoch: [261][90/97], lr: 0.00000\tTime 0.333 (0.343)\tData 0.000 (0.022)\tLoss 1.8427 (2.3419)\tPrec@1 91.406 (89.329)\tPrec@5 100.000 (99.373)\n",
      "Epoch: [261][96/97], lr: 0.00000\tTime 0.318 (0.342)\tData 0.000 (0.023)\tLoss 3.4327 (2.3605)\tPrec@1 88.136 (89.296)\tPrec@5 100.000 (99.363)\n",
      "Gated Network Weight Gate= Flip:0.59, Sc:0.41\n",
      "Test: [0/100]\tTime 0.294 (0.294)\tLoss 3.9704 (3.9704)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 4.7488 (5.5188)\tPrec@1 78.000 (80.545)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.074 (0.084)\tLoss 6.1808 (5.5991)\tPrec@1 78.000 (79.762)\tPrec@5 98.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.7509 (5.7741)\tPrec@1 81.000 (79.516)\tPrec@5 97.000 (98.258)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.4132 (5.8402)\tPrec@1 86.000 (79.561)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.4760 (5.8198)\tPrec@1 86.000 (80.157)\tPrec@5 99.000 (98.157)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 7.9164 (5.7491)\tPrec@1 82.000 (80.246)\tPrec@5 100.000 (98.295)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.0426 (5.7478)\tPrec@1 82.000 (80.282)\tPrec@5 100.000 (98.380)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.4663 (5.6890)\tPrec@1 81.000 (80.543)\tPrec@5 98.000 (98.432)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.2990 (5.7169)\tPrec@1 89.000 (80.341)\tPrec@5 99.000 (98.407)\n",
      "val Results: Prec@1 80.240 Prec@5 98.400 Loss 5.76350\n",
      "val Class Accuracy: [0.905,0.959,0.806,0.691,0.810,0.743,0.809,0.755,0.773,0.773]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [262][0/97], lr: 0.00000\tTime 0.481 (0.481)\tData 0.266 (0.266)\tLoss 0.7396 (0.7396)\tPrec@1 92.969 (92.969)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [262][10/97], lr: 0.00000\tTime 0.322 (0.348)\tData 0.000 (0.039)\tLoss 3.4083 (2.5982)\tPrec@1 89.062 (90.057)\tPrec@5 99.219 (99.503)\n",
      "Epoch: [262][20/97], lr: 0.00000\tTime 0.323 (0.337)\tData 0.000 (0.028)\tLoss 4.8336 (2.8136)\tPrec@1 87.500 (88.653)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [262][30/97], lr: 0.00000\tTime 0.333 (0.335)\tData 0.000 (0.025)\tLoss 3.3065 (2.6954)\tPrec@1 92.188 (88.684)\tPrec@5 98.438 (99.370)\n",
      "Epoch: [262][40/97], lr: 0.00000\tTime 0.325 (0.334)\tData 0.000 (0.023)\tLoss 0.9847 (2.6684)\tPrec@1 87.500 (88.700)\tPrec@5 99.219 (99.447)\n",
      "Epoch: [262][50/97], lr: 0.00000\tTime 0.323 (0.332)\tData 0.000 (0.022)\tLoss 2.3895 (2.6115)\tPrec@1 89.062 (89.032)\tPrec@5 99.219 (99.479)\n",
      "Epoch: [262][60/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.021)\tLoss 1.6958 (2.6622)\tPrec@1 90.625 (88.832)\tPrec@5 100.000 (99.462)\n",
      "Epoch: [262][70/97], lr: 0.00000\tTime 0.325 (0.332)\tData 0.000 (0.020)\tLoss 1.7984 (2.5431)\tPrec@1 86.719 (88.952)\tPrec@5 99.219 (99.450)\n",
      "Epoch: [262][80/97], lr: 0.00000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 3.2639 (2.5830)\tPrec@1 87.500 (88.802)\tPrec@5 98.438 (99.392)\n",
      "Epoch: [262][90/97], lr: 0.00000\tTime 0.324 (0.331)\tData 0.000 (0.020)\tLoss 1.1971 (2.5691)\tPrec@1 93.750 (88.899)\tPrec@5 100.000 (99.382)\n",
      "Epoch: [262][96/97], lr: 0.00000\tTime 0.317 (0.330)\tData 0.000 (0.020)\tLoss 1.9835 (2.5530)\tPrec@1 89.831 (88.941)\tPrec@5 100.000 (99.412)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.317 (0.317)\tLoss 4.0396 (4.0396)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.096)\tLoss 4.8880 (5.5309)\tPrec@1 77.000 (80.364)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.074 (0.085)\tLoss 6.1209 (5.6153)\tPrec@1 79.000 (79.667)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.8557 (5.7746)\tPrec@1 80.000 (79.613)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.4212 (5.8390)\tPrec@1 83.000 (79.561)\tPrec@5 98.000 (98.073)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.3258 (5.8248)\tPrec@1 87.000 (80.098)\tPrec@5 99.000 (98.098)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 7.9466 (5.7591)\tPrec@1 81.000 (80.131)\tPrec@5 100.000 (98.213)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.0416 (5.7551)\tPrec@1 82.000 (80.239)\tPrec@5 100.000 (98.296)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.6055 (5.7036)\tPrec@1 81.000 (80.519)\tPrec@5 98.000 (98.333)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.3701 (5.7336)\tPrec@1 88.000 (80.319)\tPrec@5 99.000 (98.286)\n",
      "val Results: Prec@1 80.220 Prec@5 98.290 Loss 5.77434\n",
      "val Class Accuracy: [0.901,0.961,0.797,0.698,0.796,0.749,0.834,0.745,0.758,0.783]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [263][0/97], lr: 0.00000\tTime 0.566 (0.566)\tData 0.339 (0.339)\tLoss 1.3706 (1.3706)\tPrec@1 92.969 (92.969)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [263][10/97], lr: 0.00000\tTime 0.326 (0.353)\tData 0.000 (0.044)\tLoss 2.8950 (1.8607)\tPrec@1 89.062 (91.051)\tPrec@5 97.656 (99.219)\n",
      "Epoch: [263][20/97], lr: 0.00000\tTime 0.321 (0.339)\tData 0.000 (0.031)\tLoss 1.7626 (2.0246)\tPrec@1 91.406 (90.699)\tPrec@5 99.219 (99.442)\n",
      "Epoch: [263][30/97], lr: 0.00000\tTime 0.321 (0.334)\tData 0.000 (0.026)\tLoss 0.9816 (2.2430)\tPrec@1 92.188 (90.474)\tPrec@5 100.000 (99.546)\n",
      "Epoch: [263][40/97], lr: 0.00000\tTime 0.322 (0.331)\tData 0.000 (0.024)\tLoss 3.0638 (2.3604)\tPrec@1 91.406 (90.111)\tPrec@5 100.000 (99.562)\n",
      "Epoch: [263][50/97], lr: 0.00000\tTime 0.324 (0.330)\tData 0.000 (0.023)\tLoss 4.0787 (2.4578)\tPrec@1 89.062 (89.874)\tPrec@5 100.000 (99.571)\n",
      "Epoch: [263][60/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.022)\tLoss 2.3805 (2.4453)\tPrec@1 85.156 (89.498)\tPrec@5 98.438 (99.526)\n",
      "Epoch: [263][70/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.021)\tLoss 4.5081 (2.4937)\tPrec@1 86.719 (89.404)\tPrec@5 99.219 (99.549)\n",
      "Epoch: [263][80/97], lr: 0.00000\tTime 0.325 (0.329)\tData 0.000 (0.021)\tLoss 1.4058 (2.5261)\tPrec@1 90.625 (89.304)\tPrec@5 100.000 (99.518)\n",
      "Epoch: [263][90/97], lr: 0.00000\tTime 0.326 (0.329)\tData 0.000 (0.020)\tLoss 0.9418 (2.5137)\tPrec@1 92.188 (89.277)\tPrec@5 98.438 (99.502)\n",
      "Epoch: [263][96/97], lr: 0.00000\tTime 0.311 (0.328)\tData 0.000 (0.021)\tLoss 2.7295 (2.4901)\tPrec@1 88.136 (89.368)\tPrec@5 100.000 (99.500)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.329 (0.329)\tLoss 4.2385 (4.2385)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.097)\tLoss 4.9514 (5.7340)\tPrec@1 78.000 (80.455)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.086)\tLoss 6.2473 (5.7980)\tPrec@1 78.000 (79.762)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.074 (0.082)\tLoss 6.9099 (5.9608)\tPrec@1 81.000 (79.613)\tPrec@5 97.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 4.6821 (6.0199)\tPrec@1 84.000 (79.659)\tPrec@5 98.000 (98.171)\n",
      "Test: [50/100]\tTime 0.073 (0.079)\tLoss 5.5757 (6.0078)\tPrec@1 87.000 (80.176)\tPrec@5 99.000 (98.196)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 8.1941 (5.9356)\tPrec@1 82.000 (80.344)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.3043 (5.9384)\tPrec@1 82.000 (80.352)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.7735 (5.8832)\tPrec@1 81.000 (80.667)\tPrec@5 98.000 (98.444)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 3.4712 (5.9123)\tPrec@1 88.000 (80.407)\tPrec@5 99.000 (98.418)\n",
      "val Results: Prec@1 80.280 Prec@5 98.420 Loss 5.95970\n",
      "val Class Accuracy: [0.910,0.964,0.811,0.697,0.806,0.744,0.829,0.736,0.761,0.770]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [264][0/97], lr: 0.00000\tTime 0.520 (0.520)\tData 0.299 (0.299)\tLoss 0.9958 (0.9958)\tPrec@1 94.531 (94.531)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [264][10/97], lr: 0.00000\tTime 0.324 (0.350)\tData 0.000 (0.041)\tLoss 1.8492 (2.3892)\tPrec@1 92.969 (90.057)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [264][20/97], lr: 0.00000\tTime 0.326 (0.339)\tData 0.000 (0.030)\tLoss 3.2042 (2.6721)\tPrec@1 82.812 (88.356)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [264][30/97], lr: 0.00000\tTime 0.323 (0.334)\tData 0.000 (0.025)\tLoss 2.1426 (2.5298)\tPrec@1 86.719 (88.810)\tPrec@5 99.219 (99.395)\n",
      "Epoch: [264][40/97], lr: 0.00000\tTime 0.321 (0.333)\tData 0.000 (0.023)\tLoss 3.6807 (2.4321)\tPrec@1 88.281 (89.005)\tPrec@5 99.219 (99.409)\n",
      "Epoch: [264][50/97], lr: 0.00000\tTime 0.328 (0.332)\tData 0.000 (0.022)\tLoss 1.1342 (2.4288)\tPrec@1 92.188 (89.078)\tPrec@5 100.000 (99.418)\n",
      "Epoch: [264][60/97], lr: 0.00000\tTime 0.323 (0.332)\tData 0.000 (0.021)\tLoss 4.1771 (2.4478)\tPrec@1 89.062 (89.139)\tPrec@5 100.000 (99.411)\n",
      "Epoch: [264][70/97], lr: 0.00000\tTime 0.328 (0.331)\tData 0.000 (0.021)\tLoss 3.8153 (2.4973)\tPrec@1 89.844 (89.040)\tPrec@5 100.000 (99.406)\n",
      "Epoch: [264][80/97], lr: 0.00000\tTime 0.325 (0.331)\tData 0.000 (0.020)\tLoss 1.2278 (2.5342)\tPrec@1 89.844 (88.831)\tPrec@5 99.219 (99.421)\n",
      "Epoch: [264][90/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.020)\tLoss 2.3144 (2.4876)\tPrec@1 89.844 (89.062)\tPrec@5 99.219 (99.451)\n",
      "Epoch: [264][96/97], lr: 0.00000\tTime 0.316 (0.330)\tData 0.000 (0.020)\tLoss 3.0456 (2.4739)\tPrec@1 87.288 (89.046)\tPrec@5 99.153 (99.460)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.302 (0.302)\tLoss 4.0552 (4.0552)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.094)\tLoss 4.8677 (5.6046)\tPrec@1 78.000 (80.545)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.1909 (5.6767)\tPrec@1 78.000 (79.762)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.8436 (5.8409)\tPrec@1 81.000 (79.581)\tPrec@5 96.000 (98.258)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.5266 (5.9075)\tPrec@1 86.000 (79.585)\tPrec@5 98.000 (98.122)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.4828 (5.8946)\tPrec@1 85.000 (80.098)\tPrec@5 99.000 (98.157)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.1166 (5.8282)\tPrec@1 81.000 (80.213)\tPrec@5 100.000 (98.279)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.1779 (5.8294)\tPrec@1 82.000 (80.268)\tPrec@5 100.000 (98.366)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6183 (5.7720)\tPrec@1 80.000 (80.580)\tPrec@5 98.000 (98.407)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.3322 (5.7980)\tPrec@1 90.000 (80.363)\tPrec@5 99.000 (98.374)\n",
      "val Results: Prec@1 80.250 Prec@5 98.360 Loss 5.84287\n",
      "val Class Accuracy: [0.902,0.962,0.817,0.693,0.804,0.749,0.827,0.733,0.764,0.774]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [265][0/97], lr: 0.00000\tTime 0.518 (0.518)\tData 0.284 (0.284)\tLoss 2.1279 (2.1279)\tPrec@1 94.531 (94.531)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [265][10/97], lr: 0.00000\tTime 0.324 (0.346)\tData 0.000 (0.040)\tLoss 2.4169 (2.3787)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [265][20/97], lr: 0.00000\tTime 0.320 (0.337)\tData 0.000 (0.029)\tLoss 4.1141 (2.4300)\tPrec@1 84.375 (89.174)\tPrec@5 99.219 (99.330)\n",
      "Epoch: [265][30/97], lr: 0.00000\tTime 0.322 (0.333)\tData 0.000 (0.025)\tLoss 2.6155 (2.4786)\tPrec@1 91.406 (88.936)\tPrec@5 98.438 (99.294)\n",
      "Epoch: [265][40/97], lr: 0.00000\tTime 0.335 (0.332)\tData 0.000 (0.023)\tLoss 2.3323 (2.4660)\tPrec@1 88.281 (88.929)\tPrec@5 99.219 (99.371)\n",
      "Epoch: [265][50/97], lr: 0.00000\tTime 0.323 (0.331)\tData 0.000 (0.022)\tLoss 2.1092 (2.3855)\tPrec@1 88.281 (88.894)\tPrec@5 99.219 (99.433)\n",
      "Epoch: [265][60/97], lr: 0.00000\tTime 0.328 (0.331)\tData 0.000 (0.021)\tLoss 0.9804 (2.4693)\tPrec@1 91.406 (88.998)\tPrec@5 100.000 (99.462)\n",
      "Epoch: [265][70/97], lr: 0.00000\tTime 0.322 (0.331)\tData 0.000 (0.020)\tLoss 3.0376 (2.4755)\tPrec@1 92.969 (88.864)\tPrec@5 99.219 (99.428)\n",
      "Epoch: [265][80/97], lr: 0.00000\tTime 0.320 (0.330)\tData 0.000 (0.020)\tLoss 1.7752 (2.4855)\tPrec@1 87.500 (88.773)\tPrec@5 98.438 (99.412)\n",
      "Epoch: [265][90/97], lr: 0.00000\tTime 0.320 (0.330)\tData 0.000 (0.020)\tLoss 1.6608 (2.4408)\tPrec@1 90.625 (88.977)\tPrec@5 100.000 (99.451)\n",
      "Epoch: [265][96/97], lr: 0.00000\tTime 0.313 (0.330)\tData 0.000 (0.020)\tLoss 1.2847 (2.3919)\tPrec@1 93.220 (89.038)\tPrec@5 100.000 (99.460)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.290 (0.290)\tLoss 4.1811 (4.1811)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 4.9717 (5.6782)\tPrec@1 78.000 (80.182)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.2782 (5.7590)\tPrec@1 79.000 (79.714)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.9225 (5.9219)\tPrec@1 81.000 (79.581)\tPrec@5 97.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.5230 (5.9788)\tPrec@1 85.000 (79.634)\tPrec@5 98.000 (98.195)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.5039 (5.9737)\tPrec@1 86.000 (80.137)\tPrec@5 99.000 (98.196)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.1939 (5.9040)\tPrec@1 81.000 (80.164)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 5.1918 (5.9068)\tPrec@1 82.000 (80.141)\tPrec@5 100.000 (98.394)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.6773 (5.8520)\tPrec@1 82.000 (80.444)\tPrec@5 98.000 (98.432)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.4978 (5.8854)\tPrec@1 89.000 (80.209)\tPrec@5 99.000 (98.385)\n",
      "val Results: Prec@1 80.150 Prec@5 98.370 Loss 5.92970\n",
      "val Class Accuracy: [0.908,0.963,0.806,0.683,0.810,0.746,0.822,0.757,0.748,0.772]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [266][0/97], lr: 0.00000\tTime 0.450 (0.450)\tData 0.254 (0.254)\tLoss 1.8287 (1.8287)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [266][10/97], lr: 0.00000\tTime 0.323 (0.343)\tData 0.000 (0.038)\tLoss 1.8753 (2.5357)\tPrec@1 89.062 (88.991)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [266][20/97], lr: 0.00000\tTime 0.334 (0.335)\tData 0.000 (0.028)\tLoss 2.2285 (2.4749)\tPrec@1 88.281 (89.100)\tPrec@5 100.000 (99.330)\n",
      "Epoch: [266][30/97], lr: 0.00000\tTime 0.322 (0.332)\tData 0.000 (0.024)\tLoss 2.1920 (2.4596)\tPrec@1 90.625 (89.340)\tPrec@5 99.219 (99.294)\n",
      "Epoch: [266][40/97], lr: 0.00000\tTime 0.323 (0.331)\tData 0.000 (0.023)\tLoss 2.2159 (2.2757)\tPrec@1 87.500 (89.825)\tPrec@5 100.000 (99.409)\n",
      "Epoch: [266][50/97], lr: 0.00000\tTime 0.327 (0.330)\tData 0.000 (0.021)\tLoss 4.8739 (2.4818)\tPrec@1 85.938 (89.660)\tPrec@5 100.000 (99.341)\n",
      "Epoch: [266][60/97], lr: 0.00000\tTime 0.328 (0.330)\tData 0.000 (0.021)\tLoss 3.1486 (2.4591)\tPrec@1 89.844 (89.600)\tPrec@5 100.000 (99.372)\n",
      "Epoch: [266][70/97], lr: 0.00000\tTime 0.326 (0.330)\tData 0.000 (0.020)\tLoss 1.2627 (2.3914)\tPrec@1 95.312 (89.613)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [266][80/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.020)\tLoss 1.0588 (2.4167)\tPrec@1 93.750 (89.574)\tPrec@5 100.000 (99.431)\n",
      "Epoch: [266][90/97], lr: 0.00000\tTime 0.324 (0.330)\tData 0.000 (0.019)\tLoss 2.5893 (2.4039)\tPrec@1 87.500 (89.698)\tPrec@5 97.656 (99.425)\n",
      "Epoch: [266][96/97], lr: 0.00000\tTime 0.314 (0.329)\tData 0.000 (0.020)\tLoss 2.2157 (2.4170)\tPrec@1 87.288 (89.594)\tPrec@5 99.153 (99.420)\n",
      "Gated Network Weight Gate= Flip:0.54, Sc:0.46\n",
      "Test: [0/100]\tTime 0.305 (0.305)\tLoss 4.0571 (4.0571)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.095)\tLoss 4.8283 (5.6064)\tPrec@1 78.000 (80.636)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.1546 (5.6786)\tPrec@1 79.000 (79.905)\tPrec@5 97.000 (98.190)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.8485 (5.8414)\tPrec@1 80.000 (79.742)\tPrec@5 97.000 (98.194)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.5511 (5.9066)\tPrec@1 85.000 (79.805)\tPrec@5 98.000 (98.049)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.4578 (5.8912)\tPrec@1 87.000 (80.353)\tPrec@5 99.000 (98.078)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.0105 (5.8215)\tPrec@1 82.000 (80.410)\tPrec@5 100.000 (98.230)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.1912 (5.8200)\tPrec@1 82.000 (80.465)\tPrec@5 100.000 (98.310)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6698 (5.7658)\tPrec@1 81.000 (80.741)\tPrec@5 98.000 (98.346)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.3414 (5.7927)\tPrec@1 88.000 (80.538)\tPrec@5 99.000 (98.319)\n",
      "val Results: Prec@1 80.380 Prec@5 98.300 Loss 5.83732\n",
      "val Class Accuracy: [0.903,0.962,0.808,0.703,0.805,0.744,0.838,0.733,0.765,0.777]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [267][0/97], lr: 0.00000\tTime 0.492 (0.492)\tData 0.259 (0.259)\tLoss 2.7781 (2.7781)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [267][10/97], lr: 0.00000\tTime 0.334 (0.346)\tData 0.000 (0.037)\tLoss 0.9187 (2.7427)\tPrec@1 91.406 (90.554)\tPrec@5 99.219 (99.361)\n",
      "Epoch: [267][20/97], lr: 0.00000\tTime 0.319 (0.336)\tData 0.000 (0.027)\tLoss 3.0467 (2.6365)\tPrec@1 91.406 (90.104)\tPrec@5 100.000 (99.368)\n",
      "Epoch: [267][30/97], lr: 0.00000\tTime 0.320 (0.333)\tData 0.000 (0.024)\tLoss 2.6456 (2.6117)\tPrec@1 84.375 (89.541)\tPrec@5 99.219 (99.345)\n",
      "Epoch: [267][40/97], lr: 0.00000\tTime 0.325 (0.331)\tData 0.000 (0.022)\tLoss 2.6045 (2.5271)\tPrec@1 89.062 (89.405)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [267][50/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.021)\tLoss 2.3771 (2.5505)\tPrec@1 88.281 (89.415)\tPrec@5 100.000 (99.494)\n",
      "Epoch: [267][60/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 3.1176 (2.4693)\tPrec@1 85.156 (89.357)\tPrec@5 97.656 (99.488)\n",
      "Epoch: [267][70/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.020)\tLoss 3.3683 (2.4430)\tPrec@1 84.375 (89.316)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [267][80/97], lr: 0.00000\tTime 0.325 (0.329)\tData 0.000 (0.020)\tLoss 1.2690 (2.4870)\tPrec@1 92.969 (89.120)\tPrec@5 100.000 (99.518)\n",
      "Epoch: [267][90/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.019)\tLoss 2.6711 (2.4758)\tPrec@1 92.188 (89.311)\tPrec@5 99.219 (99.528)\n",
      "Epoch: [267][96/97], lr: 0.00000\tTime 0.315 (0.328)\tData 0.000 (0.020)\tLoss 3.2981 (2.5422)\tPrec@1 88.983 (89.279)\tPrec@5 99.153 (99.492)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.314 (0.314)\tLoss 4.1977 (4.1977)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.095)\tLoss 4.9743 (5.6660)\tPrec@1 78.000 (80.273)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 6.2452 (5.7386)\tPrec@1 78.000 (79.619)\tPrec@5 98.000 (98.476)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 6.8787 (5.9056)\tPrec@1 80.000 (79.452)\tPrec@5 97.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.5797 (5.9623)\tPrec@1 85.000 (79.585)\tPrec@5 98.000 (98.268)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.5543 (5.9519)\tPrec@1 86.000 (80.157)\tPrec@5 99.000 (98.255)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 8.0960 (5.8779)\tPrec@1 83.000 (80.279)\tPrec@5 100.000 (98.361)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.2110 (5.8787)\tPrec@1 80.000 (80.239)\tPrec@5 100.000 (98.437)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.7434 (5.8219)\tPrec@1 83.000 (80.593)\tPrec@5 98.000 (98.481)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4514 (5.8536)\tPrec@1 89.000 (80.385)\tPrec@5 99.000 (98.440)\n",
      "val Results: Prec@1 80.250 Prec@5 98.440 Loss 5.89872\n",
      "val Class Accuracy: [0.919,0.963,0.806,0.697,0.807,0.738,0.820,0.740,0.761,0.774]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [268][0/97], lr: 0.00000\tTime 0.553 (0.553)\tData 0.315 (0.315)\tLoss 1.2657 (1.2657)\tPrec@1 91.406 (91.406)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [268][10/97], lr: 0.00000\tTime 0.324 (0.355)\tData 0.000 (0.043)\tLoss 3.7186 (2.4422)\tPrec@1 84.375 (88.778)\tPrec@5 99.219 (99.503)\n",
      "Epoch: [268][20/97], lr: 0.00000\tTime 0.337 (0.341)\tData 0.000 (0.031)\tLoss 1.8146 (2.2778)\tPrec@1 89.062 (88.914)\tPrec@5 99.219 (99.442)\n",
      "Epoch: [268][30/97], lr: 0.00000\tTime 0.320 (0.336)\tData 0.000 (0.026)\tLoss 1.2202 (2.3053)\tPrec@1 92.969 (89.264)\tPrec@5 99.219 (99.496)\n",
      "Epoch: [268][40/97], lr: 0.00000\tTime 0.324 (0.334)\tData 0.000 (0.024)\tLoss 2.4031 (2.3027)\tPrec@1 91.406 (89.291)\tPrec@5 100.000 (99.466)\n",
      "Epoch: [268][50/97], lr: 0.00000\tTime 0.322 (0.333)\tData 0.000 (0.023)\tLoss 2.1077 (2.5730)\tPrec@1 89.844 (88.894)\tPrec@5 100.000 (99.449)\n",
      "Epoch: [268][60/97], lr: 0.00000\tTime 0.319 (0.332)\tData 0.000 (0.022)\tLoss 2.5647 (2.5736)\tPrec@1 86.719 (88.998)\tPrec@5 100.000 (99.462)\n",
      "Epoch: [268][70/97], lr: 0.00000\tTime 0.320 (0.332)\tData 0.000 (0.021)\tLoss 3.0080 (2.6181)\tPrec@1 89.844 (88.919)\tPrec@5 99.219 (99.450)\n",
      "Epoch: [268][80/97], lr: 0.00000\tTime 0.325 (0.332)\tData 0.000 (0.021)\tLoss 1.1280 (2.5252)\tPrec@1 92.188 (89.005)\tPrec@5 97.656 (99.460)\n",
      "Epoch: [268][90/97], lr: 0.00000\tTime 0.319 (0.332)\tData 0.000 (0.020)\tLoss 3.7650 (2.5035)\tPrec@1 88.281 (88.994)\tPrec@5 100.000 (99.451)\n",
      "Epoch: [268][96/97], lr: 0.00000\tTime 0.316 (0.331)\tData 0.000 (0.021)\tLoss 1.9960 (2.4683)\tPrec@1 88.136 (89.030)\tPrec@5 100.000 (99.420)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.275 (0.275)\tLoss 4.1920 (4.1920)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 4.9409 (5.6582)\tPrec@1 78.000 (80.455)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.1814 (5.7302)\tPrec@1 79.000 (79.857)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.8860 (5.8903)\tPrec@1 80.000 (79.710)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.6051 (5.9508)\tPrec@1 84.000 (79.561)\tPrec@5 98.000 (98.073)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.5081 (5.9377)\tPrec@1 87.000 (80.157)\tPrec@5 99.000 (98.118)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.0955 (5.8728)\tPrec@1 83.000 (80.262)\tPrec@5 100.000 (98.246)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 5.2577 (5.8718)\tPrec@1 82.000 (80.310)\tPrec@5 100.000 (98.338)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.7896 (5.8207)\tPrec@1 79.000 (80.568)\tPrec@5 98.000 (98.370)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.4212 (5.8481)\tPrec@1 89.000 (80.352)\tPrec@5 99.000 (98.363)\n",
      "val Results: Prec@1 80.240 Prec@5 98.360 Loss 5.89163\n",
      "val Class Accuracy: [0.907,0.962,0.806,0.708,0.805,0.741,0.824,0.733,0.760,0.778]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [269][0/97], lr: 0.00000\tTime 0.603 (0.603)\tData 0.388 (0.388)\tLoss 1.2538 (1.2538)\tPrec@1 93.750 (93.750)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [269][10/97], lr: 0.00000\tTime 0.324 (0.353)\tData 0.000 (0.049)\tLoss 1.3066 (2.5362)\tPrec@1 91.406 (90.199)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [269][20/97], lr: 0.00000\tTime 0.322 (0.339)\tData 0.000 (0.034)\tLoss 1.7469 (2.7220)\tPrec@1 92.188 (89.844)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [269][30/97], lr: 0.00000\tTime 0.322 (0.336)\tData 0.000 (0.028)\tLoss 3.0201 (2.5519)\tPrec@1 89.062 (89.642)\tPrec@5 99.219 (99.521)\n",
      "Epoch: [269][40/97], lr: 0.00000\tTime 0.328 (0.334)\tData 0.000 (0.025)\tLoss 2.8081 (2.4617)\tPrec@1 90.625 (90.244)\tPrec@5 99.219 (99.543)\n",
      "Epoch: [269][50/97], lr: 0.00000\tTime 0.324 (0.333)\tData 0.000 (0.024)\tLoss 1.2881 (2.3801)\tPrec@1 90.625 (90.104)\tPrec@5 99.219 (99.494)\n",
      "Epoch: [269][60/97], lr: 0.00000\tTime 0.320 (0.332)\tData 0.000 (0.023)\tLoss 2.4540 (2.4410)\tPrec@1 91.406 (90.010)\tPrec@5 99.219 (99.398)\n",
      "Epoch: [269][70/97], lr: 0.00000\tTime 0.319 (0.331)\tData 0.000 (0.022)\tLoss 3.7733 (2.4162)\tPrec@1 83.594 (89.888)\tPrec@5 96.875 (99.395)\n",
      "Epoch: [269][80/97], lr: 0.00000\tTime 0.327 (0.331)\tData 0.000 (0.021)\tLoss 3.4218 (2.4481)\tPrec@1 84.375 (89.834)\tPrec@5 98.438 (99.431)\n",
      "Epoch: [269][90/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.021)\tLoss 2.3805 (2.4666)\tPrec@1 88.281 (89.698)\tPrec@5 100.000 (99.425)\n",
      "Epoch: [269][96/97], lr: 0.00000\tTime 0.315 (0.330)\tData 0.000 (0.021)\tLoss 1.1638 (2.4562)\tPrec@1 90.678 (89.586)\tPrec@5 99.153 (99.395)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.296 (0.296)\tLoss 4.1974 (4.1974)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.094)\tLoss 5.0463 (5.7146)\tPrec@1 78.000 (80.636)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.074 (0.084)\tLoss 6.2151 (5.7788)\tPrec@1 78.000 (79.762)\tPrec@5 97.000 (98.381)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 6.8854 (5.9533)\tPrec@1 80.000 (79.548)\tPrec@5 97.000 (98.258)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.6155 (6.0192)\tPrec@1 85.000 (79.561)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.6316 (6.0138)\tPrec@1 86.000 (80.020)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.1842 (5.9423)\tPrec@1 81.000 (80.016)\tPrec@5 100.000 (98.295)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.2296 (5.9475)\tPrec@1 82.000 (80.085)\tPrec@5 100.000 (98.380)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.7343 (5.8917)\tPrec@1 82.000 (80.420)\tPrec@5 98.000 (98.432)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4636 (5.9217)\tPrec@1 89.000 (80.231)\tPrec@5 99.000 (98.407)\n",
      "val Results: Prec@1 80.170 Prec@5 98.400 Loss 5.96776\n",
      "val Class Accuracy: [0.909,0.963,0.810,0.683,0.800,0.762,0.822,0.743,0.758,0.767]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [270][0/97], lr: 0.00000\tTime 0.492 (0.492)\tData 0.285 (0.285)\tLoss 1.0335 (1.0335)\tPrec@1 92.188 (92.188)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [270][10/97], lr: 0.00000\tTime 0.323 (0.348)\tData 0.000 (0.041)\tLoss 2.9225 (2.7675)\tPrec@1 87.500 (89.205)\tPrec@5 99.219 (99.503)\n",
      "Epoch: [270][20/97], lr: 0.00000\tTime 0.323 (0.337)\tData 0.000 (0.029)\tLoss 2.8255 (2.8791)\tPrec@1 91.406 (88.914)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [270][30/97], lr: 0.00000\tTime 0.320 (0.333)\tData 0.000 (0.025)\tLoss 2.1062 (2.6836)\tPrec@1 89.844 (89.264)\tPrec@5 98.438 (99.370)\n",
      "Epoch: [270][40/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.023)\tLoss 4.9737 (2.7461)\tPrec@1 85.156 (88.948)\tPrec@5 100.000 (99.371)\n",
      "Epoch: [270][50/97], lr: 0.00000\tTime 0.325 (0.331)\tData 0.000 (0.022)\tLoss 2.5945 (2.6756)\tPrec@1 89.844 (89.185)\tPrec@5 100.000 (99.387)\n",
      "Epoch: [270][60/97], lr: 0.00000\tTime 0.324 (0.331)\tData 0.000 (0.021)\tLoss 3.9320 (2.7848)\tPrec@1 88.281 (89.229)\tPrec@5 99.219 (99.411)\n",
      "Epoch: [270][70/97], lr: 0.00000\tTime 0.324 (0.330)\tData 0.000 (0.021)\tLoss 3.4629 (2.8262)\tPrec@1 85.938 (89.261)\tPrec@5 100.000 (99.417)\n",
      "Epoch: [270][80/97], lr: 0.00000\tTime 0.324 (0.330)\tData 0.000 (0.020)\tLoss 0.8454 (2.7579)\tPrec@1 91.406 (89.265)\tPrec@5 98.438 (99.431)\n",
      "Epoch: [270][90/97], lr: 0.00000\tTime 0.327 (0.330)\tData 0.000 (0.020)\tLoss 1.9286 (2.6670)\tPrec@1 89.062 (89.174)\tPrec@5 99.219 (99.416)\n",
      "Epoch: [270][96/97], lr: 0.00000\tTime 0.313 (0.329)\tData 0.000 (0.020)\tLoss 1.1480 (2.6146)\tPrec@1 93.220 (89.175)\tPrec@5 100.000 (99.420)\n",
      "Gated Network Weight Gate= Flip:0.54, Sc:0.46\n",
      "Test: [0/100]\tTime 0.258 (0.258)\tLoss 4.1083 (4.1083)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 4.7708 (5.5934)\tPrec@1 78.000 (80.909)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.1187 (5.6515)\tPrec@1 79.000 (80.143)\tPrec@5 98.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.7421 (5.8362)\tPrec@1 81.000 (79.903)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 4.4703 (5.9049)\tPrec@1 85.000 (79.854)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.4902 (5.8937)\tPrec@1 86.000 (80.373)\tPrec@5 99.000 (98.137)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.0460 (5.8241)\tPrec@1 81.000 (80.393)\tPrec@5 100.000 (98.279)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 5.1673 (5.8262)\tPrec@1 82.000 (80.408)\tPrec@5 100.000 (98.366)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.4819 (5.7662)\tPrec@1 82.000 (80.667)\tPrec@5 98.000 (98.420)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 3.3371 (5.7913)\tPrec@1 88.000 (80.451)\tPrec@5 99.000 (98.396)\n",
      "val Results: Prec@1 80.350 Prec@5 98.380 Loss 5.83957\n",
      "val Class Accuracy: [0.905,0.960,0.809,0.673,0.811,0.748,0.841,0.744,0.776,0.768]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [271][0/97], lr: 0.00000\tTime 0.468 (0.468)\tData 0.231 (0.231)\tLoss 2.1979 (2.1979)\tPrec@1 92.188 (92.188)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [271][10/97], lr: 0.00000\tTime 0.323 (0.342)\tData 0.000 (0.035)\tLoss 2.8257 (2.2105)\tPrec@1 85.938 (87.997)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [271][20/97], lr: 0.00000\tTime 0.321 (0.333)\tData 0.000 (0.026)\tLoss 3.3709 (2.4114)\tPrec@1 85.156 (88.542)\tPrec@5 98.438 (99.405)\n",
      "Epoch: [271][30/97], lr: 0.00000\tTime 0.323 (0.332)\tData 0.000 (0.023)\tLoss 2.1405 (2.4496)\tPrec@1 90.625 (88.785)\tPrec@5 99.219 (99.294)\n",
      "Epoch: [271][40/97], lr: 0.00000\tTime 0.326 (0.330)\tData 0.000 (0.022)\tLoss 1.9821 (2.3176)\tPrec@1 93.750 (89.386)\tPrec@5 100.000 (99.390)\n",
      "Epoch: [271][50/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.021)\tLoss 1.2711 (2.3449)\tPrec@1 89.844 (89.216)\tPrec@5 98.438 (99.464)\n",
      "Epoch: [271][60/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.020)\tLoss 3.8899 (2.3365)\tPrec@1 85.938 (89.050)\tPrec@5 100.000 (99.436)\n",
      "Epoch: [271][70/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.020)\tLoss 3.0492 (2.3635)\tPrec@1 87.500 (89.007)\tPrec@5 100.000 (99.439)\n",
      "Epoch: [271][80/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.019)\tLoss 1.7628 (2.3927)\tPrec@1 91.406 (88.889)\tPrec@5 100.000 (99.421)\n",
      "Epoch: [271][90/97], lr: 0.00000\tTime 0.323 (0.328)\tData 0.000 (0.019)\tLoss 3.7238 (2.3553)\tPrec@1 92.969 (89.045)\tPrec@5 100.000 (99.425)\n",
      "Epoch: [271][96/97], lr: 0.00000\tTime 0.316 (0.328)\tData 0.000 (0.020)\tLoss 1.7371 (2.3599)\tPrec@1 92.373 (89.118)\tPrec@5 99.153 (99.428)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.281 (0.281)\tLoss 4.2783 (4.2783)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 4.9855 (5.7675)\tPrec@1 78.000 (80.455)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.2528 (5.8233)\tPrec@1 80.000 (79.905)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.9325 (5.9833)\tPrec@1 80.000 (79.581)\tPrec@5 97.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.6517 (6.0385)\tPrec@1 84.000 (79.537)\tPrec@5 98.000 (98.171)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 5.5896 (6.0335)\tPrec@1 85.000 (80.059)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.2445 (5.9661)\tPrec@1 82.000 (80.213)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 5.3987 (5.9683)\tPrec@1 82.000 (80.211)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.8422 (5.9156)\tPrec@1 81.000 (80.506)\tPrec@5 98.000 (98.469)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.5150 (5.9443)\tPrec@1 89.000 (80.275)\tPrec@5 99.000 (98.418)\n",
      "val Results: Prec@1 80.160 Prec@5 98.400 Loss 5.98838\n",
      "val Class Accuracy: [0.908,0.964,0.808,0.697,0.817,0.740,0.823,0.738,0.753,0.768]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [272][0/97], lr: 0.00000\tTime 0.461 (0.461)\tData 0.267 (0.267)\tLoss 2.4582 (2.4582)\tPrec@1 88.281 (88.281)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [272][10/97], lr: 0.00000\tTime 0.321 (0.347)\tData 0.000 (0.039)\tLoss 2.1715 (2.3706)\tPrec@1 89.844 (89.276)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [272][20/97], lr: 0.00000\tTime 0.333 (0.337)\tData 0.000 (0.028)\tLoss 1.8105 (2.3926)\tPrec@1 87.500 (88.839)\tPrec@5 99.219 (99.516)\n",
      "Epoch: [272][30/97], lr: 0.00000\tTime 0.326 (0.334)\tData 0.000 (0.025)\tLoss 1.8181 (2.2755)\tPrec@1 85.156 (89.062)\tPrec@5 100.000 (99.622)\n",
      "Epoch: [272][40/97], lr: 0.00000\tTime 0.323 (0.332)\tData 0.000 (0.023)\tLoss 1.6961 (2.3962)\tPrec@1 92.188 (88.853)\tPrec@5 100.000 (99.600)\n",
      "Epoch: [272][50/97], lr: 0.00000\tTime 0.321 (0.331)\tData 0.000 (0.022)\tLoss 2.8715 (2.4153)\tPrec@1 88.281 (88.894)\tPrec@5 100.000 (99.571)\n",
      "Epoch: [272][60/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.021)\tLoss 1.5675 (2.3925)\tPrec@1 89.062 (88.665)\tPrec@5 99.219 (99.577)\n",
      "Epoch: [272][70/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.020)\tLoss 2.9695 (2.4934)\tPrec@1 86.719 (88.633)\tPrec@5 100.000 (99.615)\n",
      "Epoch: [272][80/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 4.9988 (2.5749)\tPrec@1 85.938 (88.532)\tPrec@5 99.219 (99.595)\n",
      "Epoch: [272][90/97], lr: 0.00000\tTime 0.319 (0.329)\tData 0.000 (0.020)\tLoss 1.7059 (2.4923)\tPrec@1 88.281 (88.762)\tPrec@5 100.000 (99.631)\n",
      "Epoch: [272][96/97], lr: 0.00000\tTime 0.309 (0.328)\tData 0.000 (0.020)\tLoss 1.5557 (2.5110)\tPrec@1 90.678 (88.796)\tPrec@5 99.153 (99.605)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.282 (0.282)\tLoss 4.1442 (4.1442)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.092)\tLoss 4.9021 (5.6597)\tPrec@1 78.000 (80.727)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.074 (0.083)\tLoss 6.1570 (5.7199)\tPrec@1 79.000 (80.000)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 6.8726 (5.8891)\tPrec@1 81.000 (79.774)\tPrec@5 97.000 (98.290)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.5789 (5.9522)\tPrec@1 86.000 (79.829)\tPrec@5 98.000 (98.171)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.4990 (5.9419)\tPrec@1 87.000 (80.314)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.0867 (5.8713)\tPrec@1 80.000 (80.377)\tPrec@5 100.000 (98.295)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.2567 (5.8729)\tPrec@1 81.000 (80.366)\tPrec@5 100.000 (98.366)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6908 (5.8181)\tPrec@1 82.000 (80.642)\tPrec@5 98.000 (98.395)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4203 (5.8460)\tPrec@1 88.000 (80.385)\tPrec@5 99.000 (98.363)\n",
      "val Results: Prec@1 80.250 Prec@5 98.350 Loss 5.89138\n",
      "val Class Accuracy: [0.910,0.964,0.810,0.680,0.799,0.746,0.842,0.741,0.762,0.771]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [273][0/97], lr: 0.00000\tTime 0.437 (0.437)\tData 0.235 (0.235)\tLoss 2.5127 (2.5127)\tPrec@1 86.719 (86.719)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [273][10/97], lr: 0.00000\tTime 0.324 (0.340)\tData 0.000 (0.036)\tLoss 1.0753 (2.4519)\tPrec@1 90.625 (89.986)\tPrec@5 99.219 (99.574)\n",
      "Epoch: [273][20/97], lr: 0.00000\tTime 0.324 (0.334)\tData 0.000 (0.027)\tLoss 1.2458 (2.7527)\tPrec@1 91.406 (89.025)\tPrec@5 99.219 (99.368)\n",
      "Epoch: [273][30/97], lr: 0.00000\tTime 0.324 (0.333)\tData 0.000 (0.024)\tLoss 1.3136 (2.4574)\tPrec@1 94.531 (89.592)\tPrec@5 100.000 (99.420)\n",
      "Epoch: [273][40/97], lr: 0.00000\tTime 0.323 (0.332)\tData 0.000 (0.022)\tLoss 0.5265 (2.3955)\tPrec@1 95.312 (89.272)\tPrec@5 99.219 (99.409)\n",
      "Epoch: [273][50/97], lr: 0.00000\tTime 0.319 (0.331)\tData 0.000 (0.021)\tLoss 1.2183 (2.3448)\tPrec@1 90.625 (89.124)\tPrec@5 98.438 (99.464)\n",
      "Epoch: [273][60/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.020)\tLoss 4.2038 (2.3842)\tPrec@1 84.375 (89.024)\tPrec@5 100.000 (99.449)\n",
      "Epoch: [273][70/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 1.6916 (2.3834)\tPrec@1 88.281 (89.206)\tPrec@5 100.000 (99.461)\n",
      "Epoch: [273][80/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.020)\tLoss 2.7159 (2.3629)\tPrec@1 92.969 (89.304)\tPrec@5 99.219 (99.460)\n",
      "Epoch: [273][90/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.019)\tLoss 3.1216 (2.4116)\tPrec@1 88.281 (89.372)\tPrec@5 100.000 (99.493)\n",
      "Epoch: [273][96/97], lr: 0.00000\tTime 0.327 (0.328)\tData 0.000 (0.020)\tLoss 1.2727 (2.4161)\tPrec@1 89.831 (89.376)\tPrec@5 99.153 (99.444)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.268 (0.268)\tLoss 4.1622 (4.1622)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.091)\tLoss 4.9726 (5.7505)\tPrec@1 78.000 (80.545)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.2560 (5.8238)\tPrec@1 79.000 (79.714)\tPrec@5 97.000 (98.381)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 6.9589 (5.9692)\tPrec@1 81.000 (79.677)\tPrec@5 97.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.7175 (6.0283)\tPrec@1 85.000 (79.732)\tPrec@5 98.000 (98.195)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 5.5478 (6.0173)\tPrec@1 86.000 (80.235)\tPrec@5 99.000 (98.196)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.2495 (5.9445)\tPrec@1 82.000 (80.295)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 5.4370 (5.9460)\tPrec@1 80.000 (80.310)\tPrec@5 100.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.8653 (5.8953)\tPrec@1 80.000 (80.568)\tPrec@5 98.000 (98.407)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4292 (5.9229)\tPrec@1 88.000 (80.330)\tPrec@5 99.000 (98.385)\n",
      "val Results: Prec@1 80.210 Prec@5 98.390 Loss 5.96744\n",
      "val Class Accuracy: [0.910,0.965,0.809,0.688,0.799,0.751,0.838,0.733,0.754,0.774]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [274][0/97], lr: 0.00000\tTime 0.466 (0.466)\tData 0.246 (0.246)\tLoss 3.4705 (3.4705)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [274][10/97], lr: 0.00000\tTime 0.319 (0.341)\tData 0.000 (0.037)\tLoss 3.2203 (2.1866)\tPrec@1 86.719 (88.636)\tPrec@5 99.219 (99.432)\n",
      "Epoch: [274][20/97], lr: 0.00000\tTime 0.335 (0.333)\tData 0.000 (0.027)\tLoss 4.0222 (2.3371)\tPrec@1 87.500 (88.951)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [274][30/97], lr: 0.00000\tTime 0.318 (0.330)\tData 0.000 (0.024)\tLoss 1.1873 (2.3466)\tPrec@1 92.188 (89.239)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [274][40/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.022)\tLoss 1.9456 (2.5010)\tPrec@1 92.188 (89.139)\tPrec@5 100.000 (99.276)\n",
      "Epoch: [274][50/97], lr: 0.00000\tTime 0.319 (0.328)\tData 0.000 (0.021)\tLoss 2.7032 (2.4638)\tPrec@1 85.938 (89.124)\tPrec@5 99.219 (99.188)\n",
      "Epoch: [274][60/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 2.0242 (2.4234)\tPrec@1 90.625 (89.139)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [274][70/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 1.1159 (2.3917)\tPrec@1 90.625 (89.261)\tPrec@5 100.000 (99.296)\n",
      "Epoch: [274][80/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 1.9268 (2.4023)\tPrec@1 92.188 (89.381)\tPrec@5 99.219 (99.334)\n",
      "Epoch: [274][90/97], lr: 0.00000\tTime 0.316 (0.327)\tData 0.000 (0.019)\tLoss 3.7257 (2.4287)\tPrec@1 86.719 (89.277)\tPrec@5 100.000 (99.339)\n",
      "Epoch: [274][96/97], lr: 0.00000\tTime 0.316 (0.327)\tData 0.000 (0.020)\tLoss 1.3114 (2.4081)\tPrec@1 88.983 (89.247)\tPrec@5 100.000 (99.363)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.325 (0.325)\tLoss 4.2255 (4.2255)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.096)\tLoss 4.9608 (5.7916)\tPrec@1 78.000 (81.000)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 6.1639 (5.8437)\tPrec@1 80.000 (80.095)\tPrec@5 98.000 (98.429)\n",
      "Test: [30/100]\tTime 0.074 (0.082)\tLoss 6.8386 (6.0012)\tPrec@1 81.000 (79.968)\tPrec@5 97.000 (98.323)\n",
      "Test: [40/100]\tTime 0.074 (0.080)\tLoss 4.7426 (6.0572)\tPrec@1 85.000 (80.024)\tPrec@5 98.000 (98.220)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 5.6104 (6.0483)\tPrec@1 87.000 (80.490)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 8.2838 (5.9719)\tPrec@1 82.000 (80.574)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.4900 (5.9733)\tPrec@1 79.000 (80.507)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.8634 (5.9213)\tPrec@1 82.000 (80.728)\tPrec@5 98.000 (98.457)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4407 (5.9489)\tPrec@1 89.000 (80.484)\tPrec@5 99.000 (98.429)\n",
      "val Results: Prec@1 80.350 Prec@5 98.430 Loss 5.99609\n",
      "val Class Accuracy: [0.914,0.968,0.809,0.677,0.811,0.750,0.838,0.736,0.762,0.770]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [275][0/97], lr: 0.00000\tTime 0.462 (0.462)\tData 0.267 (0.267)\tLoss 3.2666 (3.2666)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [275][10/97], lr: 0.00000\tTime 0.327 (0.342)\tData 0.000 (0.039)\tLoss 1.6594 (2.2976)\tPrec@1 92.188 (89.062)\tPrec@5 99.219 (99.574)\n",
      "Epoch: [275][20/97], lr: 0.00000\tTime 0.321 (0.334)\tData 0.000 (0.028)\tLoss 3.2133 (2.6164)\tPrec@1 85.156 (88.542)\tPrec@5 100.000 (99.591)\n",
      "Epoch: [275][30/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.025)\tLoss 3.9631 (2.5777)\tPrec@1 82.812 (88.861)\tPrec@5 98.438 (99.572)\n",
      "Epoch: [275][40/97], lr: 0.00000\tTime 0.328 (0.331)\tData 0.000 (0.023)\tLoss 3.5372 (2.5347)\tPrec@1 86.719 (88.739)\tPrec@5 100.000 (99.581)\n",
      "Epoch: [275][50/97], lr: 0.00000\tTime 0.325 (0.330)\tData 0.000 (0.022)\tLoss 3.5837 (2.5302)\tPrec@1 85.938 (88.634)\tPrec@5 100.000 (99.586)\n",
      "Epoch: [275][60/97], lr: 0.00000\tTime 0.325 (0.330)\tData 0.000 (0.021)\tLoss 2.0365 (2.5091)\tPrec@1 90.625 (88.589)\tPrec@5 99.219 (99.552)\n",
      "Epoch: [275][70/97], lr: 0.00000\tTime 0.320 (0.329)\tData 0.000 (0.020)\tLoss 2.8340 (2.6140)\tPrec@1 89.062 (88.578)\tPrec@5 99.219 (99.527)\n",
      "Epoch: [275][80/97], lr: 0.00000\tTime 0.320 (0.329)\tData 0.000 (0.020)\tLoss 3.0215 (2.5489)\tPrec@1 89.844 (88.773)\tPrec@5 99.219 (99.479)\n",
      "Epoch: [275][90/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.020)\tLoss 3.7042 (2.5250)\tPrec@1 89.062 (88.994)\tPrec@5 99.219 (99.468)\n",
      "Epoch: [275][96/97], lr: 0.00000\tTime 0.314 (0.328)\tData 0.000 (0.020)\tLoss 2.0263 (2.5048)\tPrec@1 91.525 (88.973)\tPrec@5 100.000 (99.460)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.270 (0.270)\tLoss 4.2002 (4.2002)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.091)\tLoss 4.9289 (5.6681)\tPrec@1 78.000 (80.545)\tPrec@5 99.000 (98.182)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.3024 (5.7369)\tPrec@1 80.000 (79.857)\tPrec@5 98.000 (98.190)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 6.9025 (5.9048)\tPrec@1 81.000 (79.710)\tPrec@5 97.000 (98.194)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 4.5733 (5.9692)\tPrec@1 84.000 (79.707)\tPrec@5 98.000 (98.098)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 5.5540 (5.9612)\tPrec@1 86.000 (80.118)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.2183 (5.8917)\tPrec@1 81.000 (80.180)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 5.1585 (5.8932)\tPrec@1 82.000 (80.183)\tPrec@5 100.000 (98.394)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.6041 (5.8314)\tPrec@1 83.000 (80.519)\tPrec@5 98.000 (98.432)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.4631 (5.8611)\tPrec@1 90.000 (80.286)\tPrec@5 99.000 (98.407)\n",
      "val Results: Prec@1 80.190 Prec@5 98.390 Loss 5.90615\n",
      "val Class Accuracy: [0.913,0.961,0.818,0.672,0.812,0.751,0.824,0.735,0.759,0.774]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [276][0/97], lr: 0.00000\tTime 0.450 (0.450)\tData 0.226 (0.226)\tLoss 3.1017 (3.1017)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [276][10/97], lr: 0.00000\tTime 0.324 (0.344)\tData 0.000 (0.035)\tLoss 1.7487 (2.1036)\tPrec@1 90.625 (90.128)\tPrec@5 98.438 (99.574)\n",
      "Epoch: [276][20/97], lr: 0.00000\tTime 0.331 (0.336)\tData 0.000 (0.027)\tLoss 1.1841 (2.3509)\tPrec@1 88.281 (88.914)\tPrec@5 99.219 (99.554)\n",
      "Epoch: [276][30/97], lr: 0.00000\tTime 0.324 (0.333)\tData 0.000 (0.023)\tLoss 1.8010 (2.5168)\tPrec@1 89.844 (88.281)\tPrec@5 100.000 (99.496)\n",
      "Epoch: [276][40/97], lr: 0.00000\tTime 0.322 (0.332)\tData 0.000 (0.022)\tLoss 4.5696 (2.6984)\tPrec@1 86.719 (88.205)\tPrec@5 100.000 (99.543)\n",
      "Epoch: [276][50/97], lr: 0.00000\tTime 0.322 (0.331)\tData 0.000 (0.021)\tLoss 2.8592 (2.5804)\tPrec@1 87.500 (88.725)\tPrec@5 100.000 (99.602)\n",
      "Epoch: [276][60/97], lr: 0.00000\tTime 0.320 (0.330)\tData 0.000 (0.020)\tLoss 2.0640 (2.5082)\tPrec@1 92.188 (89.037)\tPrec@5 100.000 (99.616)\n",
      "Epoch: [276][70/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.020)\tLoss 3.3780 (2.5088)\tPrec@1 84.375 (89.085)\tPrec@5 99.219 (99.582)\n",
      "Epoch: [276][80/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.019)\tLoss 5.2933 (2.5986)\tPrec@1 78.906 (88.899)\tPrec@5 98.438 (99.566)\n",
      "Epoch: [276][90/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.019)\tLoss 2.9900 (2.5544)\tPrec@1 89.844 (88.951)\tPrec@5 100.000 (99.579)\n",
      "Epoch: [276][96/97], lr: 0.00000\tTime 0.314 (0.328)\tData 0.000 (0.020)\tLoss 2.9051 (2.5354)\tPrec@1 84.746 (88.997)\tPrec@5 99.153 (99.557)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.261 (0.261)\tLoss 4.1869 (4.1869)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 4.9285 (5.7003)\tPrec@1 78.000 (80.636)\tPrec@5 99.000 (98.182)\n",
      "Test: [20/100]\tTime 0.074 (0.082)\tLoss 6.2286 (5.7638)\tPrec@1 78.000 (79.857)\tPrec@5 97.000 (98.190)\n",
      "Test: [30/100]\tTime 0.074 (0.079)\tLoss 6.8472 (5.9378)\tPrec@1 81.000 (79.677)\tPrec@5 97.000 (98.194)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 4.6010 (6.0039)\tPrec@1 86.000 (79.659)\tPrec@5 98.000 (98.098)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 5.5822 (5.9985)\tPrec@1 85.000 (80.137)\tPrec@5 99.000 (98.137)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.1314 (5.9264)\tPrec@1 81.000 (80.197)\tPrec@5 100.000 (98.295)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 5.2679 (5.9260)\tPrec@1 82.000 (80.211)\tPrec@5 100.000 (98.380)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6803 (5.8708)\tPrec@1 81.000 (80.519)\tPrec@5 98.000 (98.407)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4047 (5.8953)\tPrec@1 89.000 (80.275)\tPrec@5 99.000 (98.363)\n",
      "val Results: Prec@1 80.170 Prec@5 98.350 Loss 5.93988\n",
      "val Class Accuracy: [0.901,0.963,0.818,0.684,0.799,0.748,0.834,0.740,0.763,0.767]\n",
      "Best Prec@1: 80.420\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [277][0/97], lr: 0.00000\tTime 0.553 (0.553)\tData 0.313 (0.313)\tLoss 2.2316 (2.2316)\tPrec@1 91.406 (91.406)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [277][10/97], lr: 0.00000\tTime 0.324 (0.349)\tData 0.000 (0.043)\tLoss 1.5782 (2.4183)\tPrec@1 85.938 (89.489)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [277][20/97], lr: 0.00000\tTime 0.321 (0.337)\tData 0.000 (0.030)\tLoss 2.0135 (2.5431)\tPrec@1 88.281 (89.435)\tPrec@5 99.219 (99.405)\n",
      "Epoch: [277][30/97], lr: 0.00000\tTime 0.323 (0.333)\tData 0.000 (0.026)\tLoss 1.9431 (2.4995)\tPrec@1 89.844 (89.239)\tPrec@5 99.219 (99.446)\n",
      "Epoch: [277][40/97], lr: 0.00000\tTime 0.328 (0.332)\tData 0.000 (0.024)\tLoss 3.0620 (2.4258)\tPrec@1 89.062 (89.348)\tPrec@5 100.000 (99.505)\n",
      "Epoch: [277][50/97], lr: 0.00000\tTime 0.320 (0.331)\tData 0.000 (0.022)\tLoss 3.1578 (2.4782)\tPrec@1 89.844 (89.507)\tPrec@5 99.219 (99.510)\n",
      "Epoch: [277][60/97], lr: 0.00000\tTime 0.320 (0.330)\tData 0.000 (0.022)\tLoss 2.8636 (2.4933)\tPrec@1 87.500 (89.395)\tPrec@5 100.000 (99.552)\n",
      "Epoch: [277][70/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.021)\tLoss 2.4683 (2.4502)\tPrec@1 89.844 (89.580)\tPrec@5 100.000 (99.538)\n",
      "Epoch: [277][80/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 1.2031 (2.3862)\tPrec@1 90.625 (89.689)\tPrec@5 99.219 (99.547)\n",
      "Epoch: [277][90/97], lr: 0.00000\tTime 0.320 (0.329)\tData 0.000 (0.020)\tLoss 1.8647 (2.3776)\tPrec@1 92.969 (89.775)\tPrec@5 100.000 (99.545)\n",
      "Epoch: [277][96/97], lr: 0.00000\tTime 0.317 (0.328)\tData 0.000 (0.021)\tLoss 2.1754 (2.3924)\tPrec@1 89.831 (89.747)\tPrec@5 100.000 (99.524)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.264 (0.264)\tLoss 4.1165 (4.1165)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 4.8517 (5.5474)\tPrec@1 78.000 (80.727)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.2066 (5.6266)\tPrec@1 79.000 (80.048)\tPrec@5 98.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.079)\tLoss 6.8919 (5.7942)\tPrec@1 81.000 (79.871)\tPrec@5 97.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.4812 (5.8567)\tPrec@1 85.000 (79.951)\tPrec@5 98.000 (98.171)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 5.4092 (5.8406)\tPrec@1 87.000 (80.392)\tPrec@5 99.000 (98.196)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 8.0220 (5.7742)\tPrec@1 82.000 (80.443)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 4.9927 (5.7737)\tPrec@1 82.000 (80.507)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.5380 (5.7129)\tPrec@1 82.000 (80.802)\tPrec@5 98.000 (98.444)\n",
      "Test: [90/100]\tTime 0.074 (0.075)\tLoss 3.4282 (5.7472)\tPrec@1 89.000 (80.549)\tPrec@5 99.000 (98.407)\n",
      "val Results: Prec@1 80.440 Prec@5 98.400 Loss 5.79146\n",
      "val Class Accuracy: [0.915,0.960,0.809,0.683,0.808,0.751,0.836,0.738,0.759,0.785]\n",
      "Best Prec@1: 80.440\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [278][0/97], lr: 0.00000\tTime 0.459 (0.459)\tData 0.235 (0.235)\tLoss 1.7392 (1.7392)\tPrec@1 87.500 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [278][10/97], lr: 0.00000\tTime 0.325 (0.346)\tData 0.000 (0.036)\tLoss 1.6498 (2.2618)\tPrec@1 86.719 (88.849)\tPrec@5 98.438 (99.716)\n",
      "Epoch: [278][20/97], lr: 0.00000\tTime 0.335 (0.336)\tData 0.000 (0.027)\tLoss 1.5880 (2.2559)\tPrec@1 91.406 (89.881)\tPrec@5 100.000 (99.628)\n",
      "Epoch: [278][30/97], lr: 0.00000\tTime 0.325 (0.334)\tData 0.000 (0.024)\tLoss 3.0252 (2.5184)\tPrec@1 90.625 (89.718)\tPrec@5 99.219 (99.622)\n",
      "Epoch: [278][40/97], lr: 0.00000\tTime 0.323 (0.333)\tData 0.000 (0.022)\tLoss 1.7897 (2.4234)\tPrec@1 92.188 (89.748)\tPrec@5 99.219 (99.638)\n",
      "Epoch: [278][50/97], lr: 0.00000\tTime 0.323 (0.331)\tData 0.000 (0.021)\tLoss 1.8650 (2.4779)\tPrec@1 91.406 (89.476)\tPrec@5 99.219 (99.586)\n",
      "Epoch: [278][60/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.020)\tLoss 3.3433 (2.5024)\tPrec@1 89.062 (89.331)\tPrec@5 99.219 (99.565)\n",
      "Epoch: [278][70/97], lr: 0.00000\tTime 0.324 (0.330)\tData 0.000 (0.020)\tLoss 1.9522 (2.5063)\tPrec@1 91.406 (89.338)\tPrec@5 100.000 (99.549)\n",
      "Epoch: [278][80/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.020)\tLoss 1.7410 (2.5343)\tPrec@1 89.844 (89.246)\tPrec@5 99.219 (99.556)\n",
      "Epoch: [278][90/97], lr: 0.00000\tTime 0.320 (0.329)\tData 0.000 (0.019)\tLoss 2.7648 (2.5379)\tPrec@1 90.625 (89.269)\tPrec@5 100.000 (99.579)\n",
      "Epoch: [278][96/97], lr: 0.00000\tTime 0.309 (0.328)\tData 0.000 (0.020)\tLoss 3.9213 (2.5489)\tPrec@1 90.678 (89.223)\tPrec@5 98.305 (99.557)\n",
      "Gated Network Weight Gate= Flip:0.59, Sc:0.41\n",
      "Test: [0/100]\tTime 0.257 (0.257)\tLoss 4.1332 (4.1332)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 4.8874 (5.6690)\tPrec@1 78.000 (80.636)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.1923 (5.7369)\tPrec@1 78.000 (79.857)\tPrec@5 97.000 (98.381)\n",
      "Test: [30/100]\tTime 0.074 (0.079)\tLoss 6.8592 (5.8957)\tPrec@1 80.000 (79.645)\tPrec@5 97.000 (98.323)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 4.6381 (5.9558)\tPrec@1 85.000 (79.659)\tPrec@5 98.000 (98.220)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 5.5281 (5.9447)\tPrec@1 84.000 (80.059)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.1078 (5.8728)\tPrec@1 81.000 (80.082)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 5.2782 (5.8723)\tPrec@1 82.000 (80.141)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6782 (5.8196)\tPrec@1 81.000 (80.432)\tPrec@5 98.000 (98.444)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.3850 (5.8457)\tPrec@1 89.000 (80.220)\tPrec@5 99.000 (98.396)\n",
      "val Results: Prec@1 80.150 Prec@5 98.400 Loss 5.89028\n",
      "val Class Accuracy: [0.901,0.964,0.812,0.685,0.799,0.752,0.827,0.737,0.759,0.779]\n",
      "Best Prec@1: 80.440\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [279][0/97], lr: 0.00000\tTime 0.492 (0.492)\tData 0.282 (0.282)\tLoss 3.7382 (3.7382)\tPrec@1 85.938 (85.938)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [279][10/97], lr: 0.00000\tTime 0.330 (0.347)\tData 0.000 (0.040)\tLoss 1.8101 (2.7950)\tPrec@1 86.719 (88.423)\tPrec@5 100.000 (99.574)\n",
      "Epoch: [279][20/97], lr: 0.00000\tTime 0.318 (0.336)\tData 0.000 (0.029)\tLoss 0.8081 (2.5098)\tPrec@1 94.531 (89.509)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [279][30/97], lr: 0.00000\tTime 0.326 (0.333)\tData 0.000 (0.025)\tLoss 2.2812 (2.5675)\tPrec@1 92.969 (89.315)\tPrec@5 100.000 (99.496)\n",
      "Epoch: [279][40/97], lr: 0.00000\tTime 0.359 (0.333)\tData 0.000 (0.023)\tLoss 1.3938 (2.6244)\tPrec@1 88.281 (88.929)\tPrec@5 100.000 (99.466)\n",
      "Epoch: [279][50/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.022)\tLoss 2.2770 (2.6005)\tPrec@1 85.156 (89.017)\tPrec@5 96.094 (99.479)\n",
      "Epoch: [279][60/97], lr: 0.00000\tTime 0.322 (0.331)\tData 0.000 (0.021)\tLoss 1.5454 (2.5912)\tPrec@1 88.281 (89.152)\tPrec@5 99.219 (99.436)\n",
      "Epoch: [279][70/97], lr: 0.00000\tTime 0.320 (0.330)\tData 0.000 (0.020)\tLoss 2.9657 (2.5747)\tPrec@1 87.500 (89.107)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [279][80/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.020)\tLoss 2.5770 (2.5068)\tPrec@1 88.281 (89.130)\tPrec@5 99.219 (99.363)\n",
      "Epoch: [279][90/97], lr: 0.00000\tTime 0.324 (0.329)\tData 0.000 (0.020)\tLoss 1.9911 (2.4864)\tPrec@1 88.281 (89.140)\tPrec@5 100.000 (99.390)\n",
      "Epoch: [279][96/97], lr: 0.00000\tTime 0.312 (0.329)\tData 0.000 (0.020)\tLoss 3.2185 (2.4722)\tPrec@1 93.220 (89.287)\tPrec@5 100.000 (99.428)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.264 (0.264)\tLoss 3.8703 (3.8703)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.091)\tLoss 4.5631 (5.4552)\tPrec@1 78.000 (80.636)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 5.9879 (5.5284)\tPrec@1 79.000 (79.952)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 6.6329 (5.7015)\tPrec@1 82.000 (79.968)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.3304 (5.7661)\tPrec@1 85.000 (80.000)\tPrec@5 98.000 (98.122)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.2824 (5.7424)\tPrec@1 86.000 (80.569)\tPrec@5 99.000 (98.157)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 7.8921 (5.6679)\tPrec@1 81.000 (80.492)\tPrec@5 100.000 (98.279)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 4.9884 (5.6576)\tPrec@1 81.000 (80.493)\tPrec@5 100.000 (98.366)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.3755 (5.6045)\tPrec@1 83.000 (80.765)\tPrec@5 98.000 (98.420)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.1582 (5.6304)\tPrec@1 88.000 (80.538)\tPrec@5 99.000 (98.385)\n",
      "val Results: Prec@1 80.460 Prec@5 98.390 Loss 5.67610\n",
      "val Class Accuracy: [0.895,0.964,0.804,0.672,0.814,0.744,0.838,0.761,0.775,0.779]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [280][0/97], lr: 0.00000\tTime 0.449 (0.449)\tData 0.259 (0.259)\tLoss 1.2256 (1.2256)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [280][10/97], lr: 0.00000\tTime 0.320 (0.342)\tData 0.000 (0.038)\tLoss 1.7024 (1.7320)\tPrec@1 89.062 (90.980)\tPrec@5 99.219 (99.645)\n",
      "Epoch: [280][20/97], lr: 0.00000\tTime 0.345 (0.334)\tData 0.000 (0.028)\tLoss 1.6381 (2.0861)\tPrec@1 92.188 (90.141)\tPrec@5 99.219 (99.516)\n",
      "Epoch: [280][30/97], lr: 0.00000\tTime 0.322 (0.331)\tData 0.000 (0.024)\tLoss 1.2569 (2.2744)\tPrec@1 88.281 (89.819)\tPrec@5 99.219 (99.521)\n",
      "Epoch: [280][40/97], lr: 0.00000\tTime 0.326 (0.331)\tData 0.000 (0.023)\tLoss 3.4080 (2.2955)\tPrec@1 91.406 (90.111)\tPrec@5 100.000 (99.581)\n",
      "Epoch: [280][50/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.022)\tLoss 1.8607 (2.4770)\tPrec@1 92.969 (89.936)\tPrec@5 99.219 (99.510)\n",
      "Epoch: [280][60/97], lr: 0.00000\tTime 0.324 (0.329)\tData 0.000 (0.021)\tLoss 1.5445 (2.5278)\tPrec@1 90.625 (89.767)\tPrec@5 100.000 (99.577)\n",
      "Epoch: [280][70/97], lr: 0.00000\tTime 0.325 (0.329)\tData 0.000 (0.020)\tLoss 3.4294 (2.6370)\tPrec@1 87.500 (89.503)\tPrec@5 100.000 (99.549)\n",
      "Epoch: [280][80/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.020)\tLoss 2.9529 (2.5676)\tPrec@1 88.281 (89.400)\tPrec@5 99.219 (99.518)\n",
      "Epoch: [280][90/97], lr: 0.00000\tTime 0.316 (0.329)\tData 0.000 (0.020)\tLoss 4.7478 (2.5827)\tPrec@1 86.719 (89.397)\tPrec@5 99.219 (99.493)\n",
      "Epoch: [280][96/97], lr: 0.00000\tTime 0.314 (0.328)\tData 0.000 (0.020)\tLoss 1.7385 (2.5569)\tPrec@1 89.831 (89.328)\tPrec@5 100.000 (99.492)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.287 (0.287)\tLoss 4.0043 (4.0043)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.093)\tLoss 4.7094 (5.5743)\tPrec@1 78.000 (80.818)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.074 (0.084)\tLoss 6.0300 (5.6363)\tPrec@1 79.000 (79.952)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 6.7184 (5.8135)\tPrec@1 81.000 (79.806)\tPrec@5 97.000 (98.194)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.5068 (5.8862)\tPrec@1 86.000 (79.805)\tPrec@5 98.000 (98.073)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.4710 (5.8689)\tPrec@1 87.000 (80.314)\tPrec@5 99.000 (98.098)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 7.9845 (5.7967)\tPrec@1 81.000 (80.361)\tPrec@5 100.000 (98.230)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.1765 (5.7938)\tPrec@1 82.000 (80.423)\tPrec@5 100.000 (98.324)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.5429 (5.7381)\tPrec@1 80.000 (80.654)\tPrec@5 98.000 (98.383)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.2219 (5.7594)\tPrec@1 89.000 (80.451)\tPrec@5 99.000 (98.330)\n",
      "val Results: Prec@1 80.310 Prec@5 98.320 Loss 5.80851\n",
      "val Class Accuracy: [0.900,0.961,0.805,0.684,0.805,0.751,0.840,0.739,0.778,0.768]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [281][0/97], lr: 0.00000\tTime 0.588 (0.588)\tData 0.370 (0.370)\tLoss 2.0247 (2.0247)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [281][10/97], lr: 0.00000\tTime 0.324 (0.361)\tData 0.000 (0.047)\tLoss 2.0255 (2.4467)\tPrec@1 86.719 (88.210)\tPrec@5 98.438 (99.574)\n",
      "Epoch: [281][20/97], lr: 0.00000\tTime 0.325 (0.343)\tData 0.000 (0.033)\tLoss 4.6028 (2.4514)\tPrec@1 91.406 (89.174)\tPrec@5 98.438 (99.665)\n",
      "Epoch: [281][30/97], lr: 0.00000\tTime 0.323 (0.338)\tData 0.000 (0.028)\tLoss 1.5273 (2.5225)\tPrec@1 90.625 (88.735)\tPrec@5 99.219 (99.597)\n",
      "Epoch: [281][40/97], lr: 0.00000\tTime 0.323 (0.335)\tData 0.000 (0.025)\tLoss 5.6284 (2.5141)\tPrec@1 90.625 (89.120)\tPrec@5 100.000 (99.562)\n",
      "Epoch: [281][50/97], lr: 0.00000\tTime 0.322 (0.333)\tData 0.000 (0.024)\tLoss 3.4705 (2.4818)\tPrec@1 87.500 (88.710)\tPrec@5 100.000 (99.540)\n",
      "Epoch: [281][60/97], lr: 0.00000\tTime 0.322 (0.332)\tData 0.000 (0.023)\tLoss 2.2976 (2.4374)\tPrec@1 89.062 (88.768)\tPrec@5 97.656 (99.526)\n",
      "Epoch: [281][70/97], lr: 0.00000\tTime 0.320 (0.331)\tData 0.000 (0.022)\tLoss 0.9427 (2.4439)\tPrec@1 93.750 (88.688)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [281][80/97], lr: 0.00000\tTime 0.324 (0.331)\tData 0.000 (0.021)\tLoss 2.3341 (2.4710)\tPrec@1 90.625 (88.628)\tPrec@5 98.438 (99.489)\n",
      "Epoch: [281][90/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.021)\tLoss 3.9097 (2.5243)\tPrec@1 87.500 (88.685)\tPrec@5 98.438 (99.493)\n",
      "Epoch: [281][96/97], lr: 0.00000\tTime 0.323 (0.330)\tData 0.000 (0.021)\tLoss 1.9760 (2.5015)\tPrec@1 86.441 (88.780)\tPrec@5 100.000 (99.484)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.296 (0.296)\tLoss 4.1208 (4.1208)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.094)\tLoss 4.9426 (5.7045)\tPrec@1 78.000 (80.727)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.2179 (5.7718)\tPrec@1 78.000 (79.857)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.8884 (5.9292)\tPrec@1 80.000 (79.581)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.6458 (5.9874)\tPrec@1 85.000 (79.634)\tPrec@5 98.000 (98.122)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.5348 (5.9768)\tPrec@1 85.000 (80.137)\tPrec@5 99.000 (98.157)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.1871 (5.9036)\tPrec@1 82.000 (80.213)\tPrec@5 100.000 (98.279)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.3510 (5.9026)\tPrec@1 81.000 (80.254)\tPrec@5 100.000 (98.366)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.7585 (5.8494)\tPrec@1 81.000 (80.556)\tPrec@5 98.000 (98.407)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.4023 (5.8766)\tPrec@1 89.000 (80.330)\tPrec@5 99.000 (98.374)\n",
      "val Results: Prec@1 80.200 Prec@5 98.380 Loss 5.92168\n",
      "val Class Accuracy: [0.907,0.965,0.812,0.692,0.806,0.745,0.821,0.734,0.763,0.775]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [282][0/97], lr: 0.00000\tTime 0.468 (0.468)\tData 0.241 (0.241)\tLoss 1.7838 (1.7838)\tPrec@1 91.406 (91.406)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [282][10/97], lr: 0.00000\tTime 0.323 (0.340)\tData 0.000 (0.036)\tLoss 1.7984 (2.8067)\tPrec@1 85.938 (88.778)\tPrec@5 98.438 (99.361)\n",
      "Epoch: [282][20/97], lr: 0.00000\tTime 0.323 (0.333)\tData 0.000 (0.027)\tLoss 2.8184 (2.4775)\tPrec@1 87.500 (89.918)\tPrec@5 97.656 (99.330)\n",
      "Epoch: [282][30/97], lr: 0.00000\tTime 0.318 (0.332)\tData 0.000 (0.024)\tLoss 2.0873 (2.6273)\tPrec@1 89.844 (89.617)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [282][40/97], lr: 0.00000\tTime 0.321 (0.331)\tData 0.000 (0.023)\tLoss 3.6339 (2.5591)\tPrec@1 85.156 (89.444)\tPrec@5 100.000 (99.352)\n",
      "Epoch: [282][50/97], lr: 0.00000\tTime 0.317 (0.330)\tData 0.000 (0.021)\tLoss 2.8242 (2.6305)\tPrec@1 89.062 (89.323)\tPrec@5 100.000 (99.403)\n",
      "Epoch: [282][60/97], lr: 0.00000\tTime 0.327 (0.330)\tData 0.000 (0.021)\tLoss 2.9324 (2.7843)\tPrec@1 87.500 (89.139)\tPrec@5 100.000 (99.411)\n",
      "Epoch: [282][70/97], lr: 0.00000\tTime 0.325 (0.330)\tData 0.000 (0.020)\tLoss 2.3255 (2.6868)\tPrec@1 85.938 (89.239)\tPrec@5 100.000 (99.417)\n",
      "Epoch: [282][80/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.020)\tLoss 1.7979 (2.6557)\tPrec@1 88.281 (89.207)\tPrec@5 98.438 (99.373)\n",
      "Epoch: [282][90/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.020)\tLoss 1.9720 (2.5504)\tPrec@1 90.625 (89.337)\tPrec@5 100.000 (99.433)\n",
      "Epoch: [282][96/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.020)\tLoss 6.0311 (2.5685)\tPrec@1 83.898 (89.199)\tPrec@5 98.305 (99.412)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.376 (0.376)\tLoss 4.1165 (4.1165)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.101)\tLoss 4.9315 (5.6338)\tPrec@1 78.000 (80.545)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.076 (0.088)\tLoss 6.1835 (5.7172)\tPrec@1 78.000 (79.667)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.074 (0.084)\tLoss 6.9110 (5.8795)\tPrec@1 80.000 (79.516)\tPrec@5 97.000 (98.258)\n",
      "Test: [40/100]\tTime 0.073 (0.081)\tLoss 4.5682 (5.9378)\tPrec@1 84.000 (79.415)\tPrec@5 98.000 (98.122)\n",
      "Test: [50/100]\tTime 0.074 (0.080)\tLoss 5.5130 (5.9268)\tPrec@1 84.000 (79.941)\tPrec@5 99.000 (98.137)\n",
      "Test: [60/100]\tTime 0.074 (0.079)\tLoss 8.0290 (5.8562)\tPrec@1 82.000 (80.016)\tPrec@5 100.000 (98.262)\n",
      "Test: [70/100]\tTime 0.073 (0.078)\tLoss 5.2108 (5.8527)\tPrec@1 81.000 (80.056)\tPrec@5 100.000 (98.352)\n",
      "Test: [80/100]\tTime 0.074 (0.078)\tLoss 6.7471 (5.8022)\tPrec@1 81.000 (80.370)\tPrec@5 98.000 (98.395)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 3.4282 (5.8322)\tPrec@1 89.000 (80.165)\tPrec@5 99.000 (98.385)\n",
      "val Results: Prec@1 80.100 Prec@5 98.390 Loss 5.87408\n",
      "val Class Accuracy: [0.900,0.964,0.809,0.697,0.795,0.752,0.818,0.742,0.756,0.777]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [283][0/97], lr: 0.00000\tTime 0.511 (0.511)\tData 0.317 (0.317)\tLoss 2.5582 (2.5582)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [283][10/97], lr: 0.00000\tTime 0.324 (0.347)\tData 0.000 (0.044)\tLoss 2.5072 (2.3250)\tPrec@1 89.844 (89.915)\tPrec@5 98.438 (99.361)\n",
      "Epoch: [283][20/97], lr: 0.00000\tTime 0.322 (0.336)\tData 0.000 (0.031)\tLoss 1.5039 (2.2771)\tPrec@1 90.625 (89.509)\tPrec@5 99.219 (99.479)\n",
      "Epoch: [283][30/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.026)\tLoss 1.4367 (2.3317)\tPrec@1 88.281 (89.642)\tPrec@5 98.438 (99.446)\n",
      "Epoch: [283][40/97], lr: 0.00000\tTime 0.331 (0.330)\tData 0.000 (0.024)\tLoss 2.7232 (2.4818)\tPrec@1 86.719 (89.482)\tPrec@5 100.000 (99.505)\n",
      "Epoch: [283][50/97], lr: 0.00000\tTime 0.326 (0.330)\tData 0.000 (0.023)\tLoss 4.5619 (2.5398)\tPrec@1 86.719 (89.047)\tPrec@5 100.000 (99.525)\n",
      "Epoch: [283][60/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.022)\tLoss 0.7889 (2.4964)\tPrec@1 93.750 (89.101)\tPrec@5 100.000 (99.513)\n",
      "Epoch: [283][70/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.021)\tLoss 2.7392 (2.5236)\tPrec@1 85.156 (88.721)\tPrec@5 99.219 (99.527)\n",
      "Epoch: [283][80/97], lr: 0.00000\tTime 0.320 (0.329)\tData 0.000 (0.021)\tLoss 0.7486 (2.5511)\tPrec@1 94.531 (88.850)\tPrec@5 100.000 (99.498)\n",
      "Epoch: [283][90/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 2.5066 (2.4938)\tPrec@1 86.719 (88.908)\tPrec@5 99.219 (99.485)\n",
      "Epoch: [283][96/97], lr: 0.00000\tTime 0.309 (0.329)\tData 0.000 (0.021)\tLoss 3.1763 (2.5202)\tPrec@1 87.288 (88.860)\tPrec@5 99.153 (99.476)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.289 (0.289)\tLoss 4.0684 (4.0684)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.093)\tLoss 4.8481 (5.5754)\tPrec@1 78.000 (81.000)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.1469 (5.6392)\tPrec@1 79.000 (80.143)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.8159 (5.8064)\tPrec@1 81.000 (79.871)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.4999 (5.8708)\tPrec@1 86.000 (79.902)\tPrec@5 98.000 (98.122)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.4165 (5.8545)\tPrec@1 86.000 (80.412)\tPrec@5 99.000 (98.137)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.0128 (5.7873)\tPrec@1 82.000 (80.508)\tPrec@5 100.000 (98.279)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 5.2234 (5.7864)\tPrec@1 81.000 (80.507)\tPrec@5 100.000 (98.366)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.5405 (5.7286)\tPrec@1 81.000 (80.716)\tPrec@5 98.000 (98.420)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.3457 (5.7554)\tPrec@1 89.000 (80.505)\tPrec@5 99.000 (98.396)\n",
      "val Results: Prec@1 80.380 Prec@5 98.390 Loss 5.80087\n",
      "val Class Accuracy: [0.906,0.961,0.808,0.681,0.811,0.748,0.838,0.740,0.769,0.776]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [284][0/97], lr: 0.00000\tTime 0.489 (0.489)\tData 0.285 (0.285)\tLoss 1.2359 (1.2359)\tPrec@1 91.406 (91.406)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [284][10/97], lr: 0.00000\tTime 0.320 (0.343)\tData 0.000 (0.041)\tLoss 3.1980 (2.0443)\tPrec@1 89.062 (90.980)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [284][20/97], lr: 0.00000\tTime 0.332 (0.334)\tData 0.000 (0.029)\tLoss 5.3756 (2.4260)\tPrec@1 86.719 (90.030)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [284][30/97], lr: 0.00000\tTime 0.321 (0.332)\tData 0.000 (0.025)\tLoss 4.4310 (2.6160)\tPrec@1 89.062 (89.768)\tPrec@5 100.000 (99.269)\n",
      "Epoch: [284][40/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.023)\tLoss 1.8894 (2.4368)\tPrec@1 88.281 (89.806)\tPrec@5 98.438 (99.352)\n",
      "Epoch: [284][50/97], lr: 0.00000\tTime 0.320 (0.330)\tData 0.000 (0.022)\tLoss 3.2797 (2.5133)\tPrec@1 87.500 (89.874)\tPrec@5 98.438 (99.403)\n",
      "Epoch: [284][60/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.021)\tLoss 1.8300 (2.5562)\tPrec@1 92.969 (89.639)\tPrec@5 98.438 (99.372)\n",
      "Epoch: [284][70/97], lr: 0.00000\tTime 0.318 (0.329)\tData 0.000 (0.021)\tLoss 5.3731 (2.5954)\tPrec@1 85.156 (89.514)\tPrec@5 98.438 (99.384)\n",
      "Epoch: [284][80/97], lr: 0.00000\tTime 0.323 (0.328)\tData 0.000 (0.020)\tLoss 2.0532 (2.6026)\tPrec@1 88.281 (89.525)\tPrec@5 98.438 (99.402)\n",
      "Epoch: [284][90/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.020)\tLoss 1.4429 (2.6257)\tPrec@1 90.625 (89.440)\tPrec@5 100.000 (99.416)\n",
      "Epoch: [284][96/97], lr: 0.00000\tTime 0.320 (0.327)\tData 0.000 (0.020)\tLoss 1.2469 (2.5909)\tPrec@1 90.678 (89.408)\tPrec@5 100.000 (99.395)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.293 (0.293)\tLoss 4.1275 (4.1275)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 4.9404 (5.6411)\tPrec@1 78.000 (80.182)\tPrec@5 99.000 (98.545)\n",
      "Test: [20/100]\tTime 0.074 (0.084)\tLoss 6.2819 (5.7178)\tPrec@1 78.000 (79.619)\tPrec@5 97.000 (98.381)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 6.9220 (5.8722)\tPrec@1 80.000 (79.452)\tPrec@5 97.000 (98.290)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.5567 (5.9295)\tPrec@1 85.000 (79.537)\tPrec@5 98.000 (98.171)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.4754 (5.9188)\tPrec@1 85.000 (80.039)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.0998 (5.8473)\tPrec@1 83.000 (80.115)\tPrec@5 100.000 (98.295)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.2017 (5.8474)\tPrec@1 80.000 (80.113)\tPrec@5 99.000 (98.366)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6502 (5.7909)\tPrec@1 82.000 (80.407)\tPrec@5 98.000 (98.420)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4311 (5.8226)\tPrec@1 89.000 (80.209)\tPrec@5 99.000 (98.396)\n",
      "val Results: Prec@1 80.100 Prec@5 98.400 Loss 5.86542\n",
      "val Class Accuracy: [0.909,0.961,0.805,0.684,0.812,0.745,0.814,0.745,0.755,0.780]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [285][0/97], lr: 0.00000\tTime 0.481 (0.481)\tData 0.258 (0.258)\tLoss 2.8011 (2.8011)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [285][10/97], lr: 0.00000\tTime 0.327 (0.344)\tData 0.000 (0.038)\tLoss 2.4675 (2.5340)\tPrec@1 87.500 (88.849)\tPrec@5 99.219 (99.858)\n",
      "Epoch: [285][20/97], lr: 0.00000\tTime 0.320 (0.334)\tData 0.000 (0.028)\tLoss 1.5794 (2.5345)\tPrec@1 92.969 (89.509)\tPrec@5 99.219 (99.479)\n",
      "Epoch: [285][30/97], lr: 0.00000\tTime 0.322 (0.333)\tData 0.000 (0.024)\tLoss 2.0553 (2.4059)\tPrec@1 92.969 (89.970)\tPrec@5 99.219 (99.446)\n",
      "Epoch: [285][40/97], lr: 0.00000\tTime 0.334 (0.331)\tData 0.000 (0.023)\tLoss 2.1574 (2.4619)\tPrec@1 87.500 (89.748)\tPrec@5 99.219 (99.409)\n",
      "Epoch: [285][50/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.021)\tLoss 0.9160 (2.4269)\tPrec@1 92.188 (89.277)\tPrec@5 100.000 (99.403)\n",
      "Epoch: [285][60/97], lr: 0.00000\tTime 0.330 (0.330)\tData 0.000 (0.021)\tLoss 5.4779 (2.4590)\tPrec@1 89.062 (89.267)\tPrec@5 99.219 (99.424)\n",
      "Epoch: [285][70/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 2.0534 (2.4818)\tPrec@1 90.625 (89.206)\tPrec@5 100.000 (99.406)\n",
      "Epoch: [285][80/97], lr: 0.00000\tTime 0.328 (0.329)\tData 0.000 (0.020)\tLoss 1.8760 (2.4193)\tPrec@1 89.844 (89.371)\tPrec@5 100.000 (99.421)\n",
      "Epoch: [285][90/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.019)\tLoss 1.6032 (2.4488)\tPrec@1 93.750 (89.243)\tPrec@5 100.000 (99.425)\n",
      "Epoch: [285][96/97], lr: 0.00000\tTime 0.312 (0.329)\tData 0.000 (0.020)\tLoss 3.1911 (2.4641)\tPrec@1 87.288 (89.231)\tPrec@5 99.153 (99.436)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.279 (0.279)\tLoss 4.1098 (4.1098)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.092)\tLoss 4.8556 (5.5874)\tPrec@1 78.000 (80.545)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.074 (0.083)\tLoss 6.2271 (5.6637)\tPrec@1 78.000 (79.857)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 6.8750 (5.8273)\tPrec@1 80.000 (79.613)\tPrec@5 97.000 (98.258)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.4825 (5.8915)\tPrec@1 85.000 (79.634)\tPrec@5 98.000 (98.122)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 5.4655 (5.8806)\tPrec@1 86.000 (80.157)\tPrec@5 99.000 (98.157)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.0719 (5.8144)\tPrec@1 81.000 (80.164)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 5.1842 (5.8132)\tPrec@1 82.000 (80.183)\tPrec@5 100.000 (98.394)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.5626 (5.7544)\tPrec@1 81.000 (80.481)\tPrec@5 98.000 (98.444)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.4151 (5.7831)\tPrec@1 89.000 (80.275)\tPrec@5 99.000 (98.407)\n",
      "val Results: Prec@1 80.170 Prec@5 98.400 Loss 5.82650\n",
      "val Class Accuracy: [0.906,0.960,0.813,0.685,0.808,0.747,0.822,0.741,0.759,0.776]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [286][0/97], lr: 0.00000\tTime 0.537 (0.537)\tData 0.291 (0.291)\tLoss 1.8442 (1.8442)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [286][10/97], lr: 0.00000\tTime 0.320 (0.348)\tData 0.000 (0.040)\tLoss 2.6576 (2.3060)\tPrec@1 90.625 (89.773)\tPrec@5 98.438 (99.716)\n",
      "Epoch: [286][20/97], lr: 0.00000\tTime 0.329 (0.336)\tData 0.000 (0.029)\tLoss 1.1469 (2.3695)\tPrec@1 91.406 (89.509)\tPrec@5 100.000 (99.628)\n",
      "Epoch: [286][30/97], lr: 0.00000\tTime 0.324 (0.333)\tData 0.000 (0.025)\tLoss 4.5394 (2.3604)\tPrec@1 85.156 (89.516)\tPrec@5 99.219 (99.521)\n",
      "Epoch: [286][40/97], lr: 0.00000\tTime 0.321 (0.332)\tData 0.000 (0.023)\tLoss 2.9363 (2.3800)\tPrec@1 90.625 (89.482)\tPrec@5 100.000 (99.409)\n",
      "Epoch: [286][50/97], lr: 0.00000\tTime 0.323 (0.331)\tData 0.000 (0.022)\tLoss 1.6981 (2.3302)\tPrec@1 92.969 (89.645)\tPrec@5 98.438 (99.295)\n",
      "Epoch: [286][60/97], lr: 0.00000\tTime 0.323 (0.330)\tData 0.000 (0.021)\tLoss 2.3224 (2.3495)\tPrec@1 88.281 (89.421)\tPrec@5 96.875 (99.296)\n",
      "Epoch: [286][70/97], lr: 0.00000\tTime 0.323 (0.330)\tData 0.000 (0.020)\tLoss 5.2410 (2.5148)\tPrec@1 89.062 (89.206)\tPrec@5 99.219 (99.274)\n",
      "Epoch: [286][80/97], lr: 0.00000\tTime 0.323 (0.330)\tData 0.000 (0.020)\tLoss 2.9895 (2.4950)\tPrec@1 90.625 (89.304)\tPrec@5 100.000 (99.315)\n",
      "Epoch: [286][90/97], lr: 0.00000\tTime 0.317 (0.330)\tData 0.000 (0.020)\tLoss 3.0304 (2.5235)\tPrec@1 91.406 (89.483)\tPrec@5 97.656 (99.313)\n",
      "Epoch: [286][96/97], lr: 0.00000\tTime 0.326 (0.329)\tData 0.000 (0.020)\tLoss 3.4028 (2.5186)\tPrec@1 87.288 (89.481)\tPrec@5 99.153 (99.339)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.288 (0.288)\tLoss 4.0026 (4.0026)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.093)\tLoss 4.7515 (5.5077)\tPrec@1 78.000 (80.636)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.0826 (5.5685)\tPrec@1 78.000 (79.905)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.7362 (5.7519)\tPrec@1 81.000 (79.677)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.3849 (5.8216)\tPrec@1 86.000 (79.707)\tPrec@5 98.000 (98.098)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.4515 (5.8048)\tPrec@1 86.000 (80.275)\tPrec@5 99.000 (98.098)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 7.8888 (5.7359)\tPrec@1 81.000 (80.246)\tPrec@5 100.000 (98.230)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 4.9532 (5.7319)\tPrec@1 82.000 (80.282)\tPrec@5 100.000 (98.338)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.4568 (5.6739)\tPrec@1 81.000 (80.543)\tPrec@5 98.000 (98.407)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.2906 (5.7009)\tPrec@1 89.000 (80.352)\tPrec@5 99.000 (98.385)\n",
      "val Results: Prec@1 80.250 Prec@5 98.370 Loss 5.74589\n",
      "val Class Accuracy: [0.900,0.958,0.802,0.688,0.816,0.751,0.812,0.753,0.773,0.772]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [287][0/97], lr: 0.00000\tTime 0.491 (0.491)\tData 0.275 (0.275)\tLoss 3.2143 (3.2143)\tPrec@1 85.156 (85.156)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [287][10/97], lr: 0.00000\tTime 0.327 (0.344)\tData 0.000 (0.040)\tLoss 2.8287 (2.2815)\tPrec@1 89.844 (88.281)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [287][20/97], lr: 0.00000\tTime 0.321 (0.336)\tData 0.000 (0.029)\tLoss 1.6014 (2.2196)\tPrec@1 90.625 (88.467)\tPrec@5 100.000 (99.554)\n",
      "Epoch: [287][30/97], lr: 0.00000\tTime 0.322 (0.333)\tData 0.000 (0.025)\tLoss 1.0637 (2.2052)\tPrec@1 92.188 (88.684)\tPrec@5 99.219 (99.496)\n",
      "Epoch: [287][40/97], lr: 0.00000\tTime 0.341 (0.331)\tData 0.000 (0.023)\tLoss 3.2628 (2.2638)\tPrec@1 85.156 (88.815)\tPrec@5 100.000 (99.524)\n",
      "Epoch: [287][50/97], lr: 0.00000\tTime 0.322 (0.331)\tData 0.000 (0.022)\tLoss 4.1503 (2.3041)\tPrec@1 87.500 (88.817)\tPrec@5 99.219 (99.449)\n",
      "Epoch: [287][60/97], lr: 0.00000\tTime 0.323 (0.330)\tData 0.000 (0.021)\tLoss 1.8995 (2.3861)\tPrec@1 92.188 (88.768)\tPrec@5 99.219 (99.424)\n",
      "Epoch: [287][70/97], lr: 0.00000\tTime 0.320 (0.329)\tData 0.000 (0.020)\tLoss 1.3810 (2.4327)\tPrec@1 95.312 (88.644)\tPrec@5 98.438 (99.428)\n",
      "Epoch: [287][80/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 4.1645 (2.4395)\tPrec@1 85.938 (88.870)\tPrec@5 99.219 (99.460)\n",
      "Epoch: [287][90/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 5.0646 (2.4705)\tPrec@1 89.062 (88.925)\tPrec@5 100.000 (99.502)\n",
      "Epoch: [287][96/97], lr: 0.00000\tTime 0.312 (0.329)\tData 0.000 (0.020)\tLoss 3.6372 (2.4526)\tPrec@1 88.136 (88.909)\tPrec@5 99.153 (99.492)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.380 (0.380)\tLoss 3.9732 (3.9732)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.101)\tLoss 4.7338 (5.4879)\tPrec@1 78.000 (80.455)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.088)\tLoss 6.1560 (5.5755)\tPrec@1 79.000 (79.952)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.083)\tLoss 6.8000 (5.7403)\tPrec@1 81.000 (79.742)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.074 (0.081)\tLoss 4.3712 (5.8037)\tPrec@1 86.000 (79.756)\tPrec@5 98.000 (98.122)\n",
      "Test: [50/100]\tTime 0.073 (0.079)\tLoss 5.3534 (5.7859)\tPrec@1 86.000 (80.392)\tPrec@5 99.000 (98.137)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 7.9138 (5.7181)\tPrec@1 83.000 (80.459)\tPrec@5 100.000 (98.262)\n",
      "Test: [70/100]\tTime 0.073 (0.078)\tLoss 5.0017 (5.7137)\tPrec@1 82.000 (80.451)\tPrec@5 100.000 (98.352)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.4974 (5.6554)\tPrec@1 82.000 (80.741)\tPrec@5 98.000 (98.407)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 3.3380 (5.6861)\tPrec@1 89.000 (80.516)\tPrec@5 99.000 (98.363)\n",
      "val Results: Prec@1 80.390 Prec@5 98.360 Loss 5.72954\n",
      "val Class Accuracy: [0.908,0.960,0.806,0.693,0.811,0.736,0.825,0.752,0.764,0.784]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [288][0/97], lr: 0.00000\tTime 0.505 (0.505)\tData 0.275 (0.275)\tLoss 2.2295 (2.2295)\tPrec@1 88.281 (88.281)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [288][10/97], lr: 0.00000\tTime 0.322 (0.344)\tData 0.000 (0.039)\tLoss 1.6182 (2.6139)\tPrec@1 89.844 (88.849)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [288][20/97], lr: 0.00000\tTime 0.334 (0.335)\tData 0.000 (0.028)\tLoss 1.4593 (2.4609)\tPrec@1 90.625 (88.914)\tPrec@5 100.000 (99.144)\n",
      "Epoch: [288][30/97], lr: 0.00000\tTime 0.323 (0.332)\tData 0.000 (0.025)\tLoss 3.3513 (2.4428)\tPrec@1 90.625 (89.113)\tPrec@5 99.219 (99.244)\n",
      "Epoch: [288][40/97], lr: 0.00000\tTime 0.324 (0.331)\tData 0.000 (0.023)\tLoss 2.9789 (2.3654)\tPrec@1 90.625 (89.215)\tPrec@5 98.438 (99.295)\n",
      "Epoch: [288][50/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.022)\tLoss 2.4555 (2.4663)\tPrec@1 83.594 (89.001)\tPrec@5 99.219 (99.326)\n",
      "Epoch: [288][60/97], lr: 0.00000\tTime 0.324 (0.330)\tData 0.000 (0.021)\tLoss 4.4633 (2.5030)\tPrec@1 83.594 (88.870)\tPrec@5 100.000 (99.321)\n",
      "Epoch: [288][70/97], lr: 0.00000\tTime 0.327 (0.329)\tData 0.000 (0.020)\tLoss 1.9927 (2.4683)\tPrec@1 88.281 (88.897)\tPrec@5 100.000 (99.384)\n",
      "Epoch: [288][80/97], lr: 0.00000\tTime 0.319 (0.329)\tData 0.000 (0.020)\tLoss 4.8204 (2.4344)\tPrec@1 87.500 (88.889)\tPrec@5 98.438 (99.373)\n",
      "Epoch: [288][90/97], lr: 0.00000\tTime 0.318 (0.329)\tData 0.000 (0.020)\tLoss 1.8627 (2.4428)\tPrec@1 90.625 (89.002)\tPrec@5 99.219 (99.408)\n",
      "Epoch: [288][96/97], lr: 0.00000\tTime 0.313 (0.328)\tData 0.000 (0.020)\tLoss 4.4260 (2.4983)\tPrec@1 88.136 (88.989)\tPrec@5 98.305 (99.420)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.375 (0.375)\tLoss 4.1246 (4.1246)\tPrec@1 83.000 (83.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.101)\tLoss 4.8668 (5.6101)\tPrec@1 78.000 (80.182)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.074 (0.088)\tLoss 6.2710 (5.6838)\tPrec@1 78.000 (79.571)\tPrec@5 98.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.083)\tLoss 6.8659 (5.8446)\tPrec@1 81.000 (79.581)\tPrec@5 97.000 (98.323)\n",
      "Test: [40/100]\tTime 0.074 (0.081)\tLoss 4.5258 (5.9068)\tPrec@1 85.000 (79.707)\tPrec@5 98.000 (98.244)\n",
      "Test: [50/100]\tTime 0.074 (0.080)\tLoss 5.5326 (5.8950)\tPrec@1 85.000 (80.196)\tPrec@5 99.000 (98.235)\n",
      "Test: [60/100]\tTime 0.074 (0.079)\tLoss 8.0462 (5.8211)\tPrec@1 83.000 (80.328)\tPrec@5 100.000 (98.361)\n",
      "Test: [70/100]\tTime 0.074 (0.078)\tLoss 5.2518 (5.8199)\tPrec@1 80.000 (80.254)\tPrec@5 100.000 (98.437)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.5913 (5.7593)\tPrec@1 84.000 (80.568)\tPrec@5 98.000 (98.481)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 3.3853 (5.7892)\tPrec@1 89.000 (80.341)\tPrec@5 99.000 (98.473)\n",
      "val Results: Prec@1 80.210 Prec@5 98.460 Loss 5.83363\n",
      "val Class Accuracy: [0.920,0.961,0.813,0.681,0.804,0.734,0.826,0.745,0.760,0.777]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [289][0/97], lr: 0.00000\tTime 0.481 (0.481)\tData 0.245 (0.245)\tLoss 1.6494 (1.6494)\tPrec@1 88.281 (88.281)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [289][10/97], lr: 0.00000\tTime 0.323 (0.343)\tData 0.000 (0.037)\tLoss 3.1971 (2.1970)\tPrec@1 86.719 (88.210)\tPrec@5 97.656 (99.148)\n",
      "Epoch: [289][20/97], lr: 0.00000\tTime 0.321 (0.334)\tData 0.000 (0.027)\tLoss 1.7628 (2.4538)\tPrec@1 89.844 (88.839)\tPrec@5 99.219 (99.405)\n",
      "Epoch: [289][30/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.024)\tLoss 2.2002 (2.5457)\tPrec@1 90.625 (88.936)\tPrec@5 100.000 (99.395)\n",
      "Epoch: [289][40/97], lr: 0.00000\tTime 0.341 (0.329)\tData 0.000 (0.022)\tLoss 3.5008 (2.4841)\tPrec@1 87.500 (88.948)\tPrec@5 99.219 (99.409)\n",
      "Epoch: [289][50/97], lr: 0.00000\tTime 0.315 (0.329)\tData 0.000 (0.021)\tLoss 1.9660 (2.3910)\tPrec@1 89.062 (89.400)\tPrec@5 100.000 (99.418)\n",
      "Epoch: [289][60/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.021)\tLoss 5.2630 (2.5108)\tPrec@1 87.500 (89.447)\tPrec@5 98.438 (99.411)\n",
      "Epoch: [289][70/97], lr: 0.00000\tTime 0.320 (0.329)\tData 0.000 (0.020)\tLoss 2.7201 (2.5449)\tPrec@1 88.281 (89.250)\tPrec@5 99.219 (99.428)\n",
      "Epoch: [289][80/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.020)\tLoss 1.6101 (2.5189)\tPrec@1 90.625 (89.361)\tPrec@5 99.219 (99.460)\n",
      "Epoch: [289][90/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.019)\tLoss 3.2992 (2.4773)\tPrec@1 87.500 (89.526)\tPrec@5 98.438 (99.459)\n",
      "Epoch: [289][96/97], lr: 0.00000\tTime 0.314 (0.328)\tData 0.000 (0.020)\tLoss 3.0313 (2.4857)\tPrec@1 82.203 (89.424)\tPrec@5 100.000 (99.476)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.350 (0.350)\tLoss 4.1686 (4.1686)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.098)\tLoss 4.9367 (5.6514)\tPrec@1 78.000 (80.818)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.086)\tLoss 6.1684 (5.7113)\tPrec@1 79.000 (80.048)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.082)\tLoss 6.8650 (5.8824)\tPrec@1 80.000 (79.806)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 4.6111 (5.9445)\tPrec@1 85.000 (79.829)\tPrec@5 98.000 (98.098)\n",
      "Test: [50/100]\tTime 0.073 (0.079)\tLoss 5.5472 (5.9296)\tPrec@1 87.000 (80.314)\tPrec@5 99.000 (98.098)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 8.0866 (5.8576)\tPrec@1 81.000 (80.311)\tPrec@5 100.000 (98.230)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.2242 (5.8594)\tPrec@1 80.000 (80.366)\tPrec@5 100.000 (98.310)\n",
      "Test: [80/100]\tTime 0.073 (0.077)\tLoss 6.6891 (5.8041)\tPrec@1 82.000 (80.642)\tPrec@5 98.000 (98.358)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.3749 (5.8302)\tPrec@1 88.000 (80.440)\tPrec@5 99.000 (98.352)\n",
      "val Results: Prec@1 80.330 Prec@5 98.360 Loss 5.87673\n",
      "val Class Accuracy: [0.912,0.964,0.806,0.688,0.802,0.752,0.832,0.737,0.766,0.774]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [290][0/97], lr: 0.00000\tTime 0.485 (0.485)\tData 0.267 (0.267)\tLoss 3.5625 (3.5625)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [290][10/97], lr: 0.00000\tTime 0.323 (0.347)\tData 0.000 (0.039)\tLoss 1.1257 (2.1579)\tPrec@1 90.625 (88.636)\tPrec@5 99.219 (99.503)\n",
      "Epoch: [290][20/97], lr: 0.00000\tTime 0.333 (0.336)\tData 0.000 (0.029)\tLoss 2.7006 (2.2639)\tPrec@1 85.156 (88.170)\tPrec@5 99.219 (99.405)\n",
      "Epoch: [290][30/97], lr: 0.00000\tTime 0.326 (0.333)\tData 0.000 (0.025)\tLoss 2.1413 (2.3673)\tPrec@1 85.938 (88.357)\tPrec@5 98.438 (99.395)\n",
      "Epoch: [290][40/97], lr: 0.00000\tTime 0.324 (0.333)\tData 0.000 (0.023)\tLoss 2.0185 (2.4395)\tPrec@1 95.312 (89.005)\tPrec@5 100.000 (99.486)\n",
      "Epoch: [290][50/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.022)\tLoss 2.9612 (2.4593)\tPrec@1 90.625 (88.986)\tPrec@5 99.219 (99.510)\n",
      "Epoch: [290][60/97], lr: 0.00000\tTime 0.320 (0.331)\tData 0.000 (0.021)\tLoss 1.7487 (2.4282)\tPrec@1 90.625 (89.280)\tPrec@5 100.000 (99.513)\n",
      "Epoch: [290][70/97], lr: 0.00000\tTime 0.328 (0.331)\tData 0.000 (0.020)\tLoss 2.3658 (2.4014)\tPrec@1 92.188 (89.360)\tPrec@5 99.219 (99.516)\n",
      "Epoch: [290][80/97], lr: 0.00000\tTime 0.325 (0.330)\tData 0.000 (0.020)\tLoss 2.5117 (2.4347)\tPrec@1 91.406 (89.448)\tPrec@5 99.219 (99.518)\n",
      "Epoch: [290][90/97], lr: 0.00000\tTime 0.320 (0.330)\tData 0.000 (0.020)\tLoss 3.4295 (2.4233)\tPrec@1 85.156 (89.397)\tPrec@5 98.438 (99.528)\n",
      "Epoch: [290][96/97], lr: 0.00000\tTime 0.315 (0.329)\tData 0.000 (0.020)\tLoss 2.5342 (2.4084)\tPrec@1 84.746 (89.376)\tPrec@5 99.153 (99.541)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.317 (0.317)\tLoss 4.0342 (4.0342)\tPrec@1 81.000 (81.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.096)\tLoss 4.7786 (5.4699)\tPrec@1 78.000 (80.636)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 6.2480 (5.5481)\tPrec@1 78.000 (80.000)\tPrec@5 97.000 (98.238)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.8425 (5.7193)\tPrec@1 80.000 (79.677)\tPrec@5 97.000 (98.194)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.3697 (5.7904)\tPrec@1 85.000 (79.659)\tPrec@5 98.000 (98.122)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.4103 (5.7741)\tPrec@1 85.000 (80.137)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 7.9683 (5.7098)\tPrec@1 83.000 (80.213)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 4.9034 (5.7094)\tPrec@1 81.000 (80.239)\tPrec@5 100.000 (98.394)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.4247 (5.6433)\tPrec@1 81.000 (80.568)\tPrec@5 98.000 (98.481)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.3469 (5.6737)\tPrec@1 89.000 (80.385)\tPrec@5 99.000 (98.451)\n",
      "val Results: Prec@1 80.290 Prec@5 98.430 Loss 5.71747\n",
      "val Class Accuracy: [0.910,0.955,0.807,0.684,0.818,0.747,0.815,0.738,0.766,0.789]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [291][0/97], lr: 0.00000\tTime 0.496 (0.496)\tData 0.269 (0.269)\tLoss 2.6344 (2.6344)\tPrec@1 84.375 (84.375)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [291][10/97], lr: 0.00000\tTime 0.322 (0.346)\tData 0.000 (0.039)\tLoss 3.5822 (2.6690)\tPrec@1 85.938 (87.074)\tPrec@5 99.219 (99.574)\n",
      "Epoch: [291][20/97], lr: 0.00000\tTime 0.324 (0.335)\tData 0.000 (0.028)\tLoss 4.7420 (2.6760)\tPrec@1 87.500 (87.686)\tPrec@5 99.219 (99.293)\n",
      "Epoch: [291][30/97], lr: 0.00000\tTime 0.327 (0.334)\tData 0.000 (0.024)\tLoss 3.5239 (2.4610)\tPrec@1 89.844 (88.231)\tPrec@5 100.000 (99.370)\n",
      "Epoch: [291][40/97], lr: 0.00000\tTime 0.342 (0.333)\tData 0.000 (0.023)\tLoss 4.1849 (2.4653)\tPrec@1 86.719 (88.529)\tPrec@5 98.438 (99.371)\n",
      "Epoch: [291][50/97], lr: 0.00000\tTime 0.318 (0.331)\tData 0.000 (0.021)\tLoss 2.4312 (2.4472)\tPrec@1 89.844 (88.756)\tPrec@5 99.219 (99.433)\n",
      "Epoch: [291][60/97], lr: 0.00000\tTime 0.321 (0.331)\tData 0.000 (0.021)\tLoss 1.2930 (2.4354)\tPrec@1 92.969 (89.101)\tPrec@5 100.000 (99.488)\n",
      "Epoch: [291][70/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.020)\tLoss 3.1488 (2.4910)\tPrec@1 89.062 (89.162)\tPrec@5 100.000 (99.450)\n",
      "Epoch: [291][80/97], lr: 0.00000\tTime 0.323 (0.330)\tData 0.000 (0.020)\tLoss 1.7642 (2.4668)\tPrec@1 89.844 (89.217)\tPrec@5 100.000 (99.450)\n",
      "Epoch: [291][90/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.019)\tLoss 2.2797 (2.4711)\tPrec@1 87.500 (89.045)\tPrec@5 99.219 (99.442)\n",
      "Epoch: [291][96/97], lr: 0.00000\tTime 0.312 (0.329)\tData 0.000 (0.020)\tLoss 2.5400 (2.4632)\tPrec@1 89.831 (89.046)\tPrec@5 99.153 (99.452)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.397 (0.397)\tLoss 4.2647 (4.2647)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.103)\tLoss 4.8829 (5.7036)\tPrec@1 78.000 (80.364)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.089)\tLoss 6.1636 (5.7718)\tPrec@1 79.000 (79.857)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.084)\tLoss 6.8476 (5.9409)\tPrec@1 81.000 (79.710)\tPrec@5 97.000 (98.258)\n",
      "Test: [40/100]\tTime 0.073 (0.081)\tLoss 4.5812 (5.9984)\tPrec@1 84.000 (79.756)\tPrec@5 98.000 (98.195)\n",
      "Test: [50/100]\tTime 0.074 (0.080)\tLoss 5.4983 (5.9905)\tPrec@1 86.000 (80.353)\tPrec@5 99.000 (98.196)\n",
      "Test: [60/100]\tTime 0.074 (0.079)\tLoss 8.0702 (5.9182)\tPrec@1 82.000 (80.426)\tPrec@5 99.000 (98.295)\n",
      "Test: [70/100]\tTime 0.074 (0.078)\tLoss 5.2668 (5.9185)\tPrec@1 81.000 (80.394)\tPrec@5 100.000 (98.380)\n",
      "Test: [80/100]\tTime 0.074 (0.078)\tLoss 6.7731 (5.8683)\tPrec@1 80.000 (80.617)\tPrec@5 98.000 (98.420)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 3.4697 (5.8977)\tPrec@1 88.000 (80.385)\tPrec@5 99.000 (98.374)\n",
      "val Results: Prec@1 80.250 Prec@5 98.380 Loss 5.94091\n",
      "val Class Accuracy: [0.907,0.964,0.804,0.694,0.811,0.735,0.838,0.751,0.753,0.768]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [292][0/97], lr: 0.00000\tTime 0.435 (0.435)\tData 0.232 (0.232)\tLoss 1.7438 (1.7438)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [292][10/97], lr: 0.00000\tTime 0.323 (0.341)\tData 0.000 (0.035)\tLoss 0.8396 (2.5046)\tPrec@1 92.188 (88.707)\tPrec@5 99.219 (99.290)\n",
      "Epoch: [292][20/97], lr: 0.00000\tTime 0.327 (0.333)\tData 0.000 (0.027)\tLoss 1.9159 (2.5542)\tPrec@1 90.625 (89.286)\tPrec@5 99.219 (99.405)\n",
      "Epoch: [292][30/97], lr: 0.00000\tTime 0.320 (0.331)\tData 0.000 (0.024)\tLoss 1.9056 (2.4327)\tPrec@1 88.281 (89.415)\tPrec@5 100.000 (99.345)\n",
      "Epoch: [292][40/97], lr: 0.00000\tTime 0.320 (0.331)\tData 0.000 (0.022)\tLoss 1.5872 (2.4467)\tPrec@1 91.406 (89.177)\tPrec@5 99.219 (99.371)\n",
      "Epoch: [292][50/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.021)\tLoss 2.3808 (2.4925)\tPrec@1 89.844 (89.277)\tPrec@5 99.219 (99.387)\n",
      "Epoch: [292][60/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.020)\tLoss 2.6447 (2.5122)\tPrec@1 91.406 (89.139)\tPrec@5 98.438 (99.398)\n",
      "Epoch: [292][70/97], lr: 0.00000\tTime 0.324 (0.329)\tData 0.000 (0.020)\tLoss 0.9564 (2.4984)\tPrec@1 96.094 (89.239)\tPrec@5 99.219 (99.406)\n",
      "Epoch: [292][80/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 3.5860 (2.5501)\tPrec@1 87.500 (89.130)\tPrec@5 100.000 (99.421)\n",
      "Epoch: [292][90/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.019)\tLoss 2.3687 (2.5189)\tPrec@1 88.281 (89.054)\tPrec@5 99.219 (99.425)\n",
      "Epoch: [292][96/97], lr: 0.00000\tTime 0.314 (0.329)\tData 0.000 (0.020)\tLoss 3.6576 (2.5250)\tPrec@1 89.831 (89.086)\tPrec@5 100.000 (99.436)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.290 (0.290)\tLoss 4.1618 (4.1618)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.093)\tLoss 4.8405 (5.5607)\tPrec@1 78.000 (80.636)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.074 (0.084)\tLoss 6.2234 (5.6390)\tPrec@1 79.000 (80.048)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.8943 (5.7994)\tPrec@1 81.000 (79.839)\tPrec@5 97.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.4656 (5.8568)\tPrec@1 84.000 (79.902)\tPrec@5 98.000 (98.171)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.3516 (5.8478)\tPrec@1 86.000 (80.314)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.0030 (5.7840)\tPrec@1 82.000 (80.311)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 5.0457 (5.7831)\tPrec@1 82.000 (80.366)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.4971 (5.7250)\tPrec@1 83.000 (80.642)\tPrec@5 98.000 (98.420)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4569 (5.7580)\tPrec@1 88.000 (80.374)\tPrec@5 99.000 (98.385)\n",
      "val Results: Prec@1 80.260 Prec@5 98.390 Loss 5.80029\n",
      "val Class Accuracy: [0.908,0.959,0.806,0.676,0.811,0.747,0.841,0.743,0.749,0.786]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [293][0/97], lr: 0.00000\tTime 0.513 (0.513)\tData 0.314 (0.314)\tLoss 2.7199 (2.7199)\tPrec@1 87.500 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [293][10/97], lr: 0.00000\tTime 0.321 (0.348)\tData 0.000 (0.043)\tLoss 3.1136 (2.8376)\tPrec@1 92.188 (88.778)\tPrec@5 100.000 (99.432)\n",
      "Epoch: [293][20/97], lr: 0.00000\tTime 0.319 (0.336)\tData 0.000 (0.030)\tLoss 3.5307 (2.5909)\tPrec@1 82.031 (88.430)\tPrec@5 100.000 (99.591)\n",
      "Epoch: [293][30/97], lr: 0.00000\tTime 0.321 (0.332)\tData 0.000 (0.026)\tLoss 1.5576 (2.4383)\tPrec@1 89.844 (88.911)\tPrec@5 98.438 (99.572)\n",
      "Epoch: [293][40/97], lr: 0.00000\tTime 0.342 (0.331)\tData 0.000 (0.024)\tLoss 4.1178 (2.3386)\tPrec@1 85.938 (89.043)\tPrec@5 99.219 (99.524)\n",
      "Epoch: [293][50/97], lr: 0.00000\tTime 0.322 (0.331)\tData 0.000 (0.023)\tLoss 0.7324 (2.3497)\tPrec@1 96.875 (89.430)\tPrec@5 100.000 (99.540)\n",
      "Epoch: [293][60/97], lr: 0.00000\tTime 0.319 (0.330)\tData 0.000 (0.022)\tLoss 2.4763 (2.4985)\tPrec@1 92.188 (89.421)\tPrec@5 98.438 (99.539)\n",
      "Epoch: [293][70/97], lr: 0.00000\tTime 0.326 (0.329)\tData 0.000 (0.021)\tLoss 1.9611 (2.4079)\tPrec@1 91.406 (89.514)\tPrec@5 100.000 (99.505)\n",
      "Epoch: [293][80/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.020)\tLoss 3.3386 (2.4395)\tPrec@1 85.156 (89.284)\tPrec@5 97.656 (99.489)\n",
      "Epoch: [293][90/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 1.3116 (2.4395)\tPrec@1 91.406 (89.191)\tPrec@5 99.219 (99.485)\n",
      "Epoch: [293][96/97], lr: 0.00000\tTime 0.315 (0.328)\tData 0.000 (0.021)\tLoss 2.3980 (2.4712)\tPrec@1 91.525 (89.167)\tPrec@5 100.000 (99.468)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.369 (0.369)\tLoss 4.1803 (4.1803)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.100)\tLoss 4.8163 (5.6612)\tPrec@1 78.000 (80.182)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.073 (0.087)\tLoss 6.2922 (5.7341)\tPrec@1 79.000 (79.619)\tPrec@5 97.000 (98.238)\n",
      "Test: [30/100]\tTime 0.074 (0.083)\tLoss 6.8874 (5.8964)\tPrec@1 80.000 (79.452)\tPrec@5 97.000 (98.258)\n",
      "Test: [40/100]\tTime 0.073 (0.081)\tLoss 4.5708 (5.9576)\tPrec@1 86.000 (79.512)\tPrec@5 98.000 (98.122)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 5.5361 (5.9459)\tPrec@1 85.000 (80.020)\tPrec@5 99.000 (98.157)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 8.1353 (5.8790)\tPrec@1 82.000 (80.115)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.074 (0.078)\tLoss 5.2662 (5.8802)\tPrec@1 82.000 (80.099)\tPrec@5 99.000 (98.380)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.6537 (5.8244)\tPrec@1 80.000 (80.444)\tPrec@5 98.000 (98.444)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 3.4162 (5.8494)\tPrec@1 90.000 (80.220)\tPrec@5 99.000 (98.396)\n",
      "val Results: Prec@1 80.120 Prec@5 98.380 Loss 5.89291\n",
      "val Class Accuracy: [0.903,0.961,0.818,0.694,0.816,0.737,0.815,0.736,0.759,0.773]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [294][0/97], lr: 0.00000\tTime 0.440 (0.440)\tData 0.248 (0.248)\tLoss 3.0538 (3.0538)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [294][10/97], lr: 0.00000\tTime 0.323 (0.342)\tData 0.000 (0.037)\tLoss 1.0879 (2.3942)\tPrec@1 91.406 (87.997)\tPrec@5 99.219 (99.503)\n",
      "Epoch: [294][20/97], lr: 0.00000\tTime 0.337 (0.334)\tData 0.000 (0.028)\tLoss 2.9363 (2.7638)\tPrec@1 86.719 (87.723)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [294][30/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.024)\tLoss 2.8402 (2.5540)\tPrec@1 92.969 (88.836)\tPrec@5 98.438 (99.521)\n",
      "Epoch: [294][40/97], lr: 0.00000\tTime 0.324 (0.330)\tData 0.000 (0.022)\tLoss 2.8595 (2.4417)\tPrec@1 89.844 (89.139)\tPrec@5 96.875 (99.428)\n",
      "Epoch: [294][50/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.021)\tLoss 1.7925 (2.4596)\tPrec@1 89.844 (89.292)\tPrec@5 97.656 (99.464)\n",
      "Epoch: [294][60/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.021)\tLoss 2.0828 (2.4845)\tPrec@1 84.375 (89.203)\tPrec@5 98.438 (99.436)\n",
      "Epoch: [294][70/97], lr: 0.00000\tTime 0.327 (0.329)\tData 0.000 (0.020)\tLoss 1.5328 (2.5049)\tPrec@1 89.844 (89.162)\tPrec@5 100.000 (99.450)\n",
      "Epoch: [294][80/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 1.7488 (2.6101)\tPrec@1 88.281 (88.735)\tPrec@5 99.219 (99.412)\n",
      "Epoch: [294][90/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.019)\tLoss 2.7736 (2.5835)\tPrec@1 89.844 (88.899)\tPrec@5 100.000 (99.408)\n",
      "Epoch: [294][96/97], lr: 0.00000\tTime 0.314 (0.328)\tData 0.000 (0.020)\tLoss 3.1539 (2.5770)\tPrec@1 88.136 (88.860)\tPrec@5 100.000 (99.428)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.370 (0.370)\tLoss 4.2145 (4.2145)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.101)\tLoss 4.9199 (5.6258)\tPrec@1 78.000 (80.455)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.073 (0.088)\tLoss 6.2638 (5.7036)\tPrec@1 78.000 (79.905)\tPrec@5 98.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.083)\tLoss 6.9003 (5.8748)\tPrec@1 81.000 (79.742)\tPrec@5 97.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.081)\tLoss 4.5342 (5.9335)\tPrec@1 84.000 (79.707)\tPrec@5 98.000 (98.171)\n",
      "Test: [50/100]\tTime 0.073 (0.079)\tLoss 5.5083 (5.9264)\tPrec@1 86.000 (80.176)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 8.0563 (5.8541)\tPrec@1 82.000 (80.262)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.073 (0.078)\tLoss 5.0704 (5.8546)\tPrec@1 82.000 (80.310)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.073 (0.077)\tLoss 6.5978 (5.7942)\tPrec@1 84.000 (80.630)\tPrec@5 98.000 (98.457)\n",
      "Test: [90/100]\tTime 0.073 (0.077)\tLoss 3.5057 (5.8284)\tPrec@1 88.000 (80.341)\tPrec@5 99.000 (98.418)\n",
      "val Results: Prec@1 80.260 Prec@5 98.420 Loss 5.87245\n",
      "val Class Accuracy: [0.914,0.961,0.812,0.675,0.810,0.745,0.833,0.746,0.753,0.777]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [295][0/97], lr: 0.00000\tTime 0.496 (0.496)\tData 0.263 (0.263)\tLoss 8.7196 (8.7196)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [295][10/97], lr: 0.00000\tTime 0.323 (0.347)\tData 0.000 (0.038)\tLoss 2.4852 (3.1920)\tPrec@1 83.594 (88.920)\tPrec@5 100.000 (99.787)\n",
      "Epoch: [295][20/97], lr: 0.00000\tTime 0.320 (0.335)\tData 0.000 (0.028)\tLoss 7.4863 (2.9973)\tPrec@1 92.969 (88.728)\tPrec@5 98.438 (99.665)\n",
      "Epoch: [295][30/97], lr: 0.00000\tTime 0.324 (0.337)\tData 0.000 (0.027)\tLoss 1.5253 (2.5775)\tPrec@1 92.188 (89.289)\tPrec@5 99.219 (99.572)\n",
      "Epoch: [295][40/97], lr: 0.00000\tTime 0.334 (0.334)\tData 0.000 (0.025)\tLoss 1.2474 (2.5605)\tPrec@1 89.844 (89.005)\tPrec@5 100.000 (99.486)\n",
      "Epoch: [295][50/97], lr: 0.00000\tTime 0.320 (0.332)\tData 0.000 (0.023)\tLoss 2.4446 (2.6416)\tPrec@1 89.062 (89.017)\tPrec@5 99.219 (99.449)\n",
      "Epoch: [295][60/97], lr: 0.00000\tTime 0.321 (0.332)\tData 0.000 (0.022)\tLoss 5.2483 (2.6904)\tPrec@1 87.500 (88.960)\tPrec@5 98.438 (99.488)\n",
      "Epoch: [295][70/97], lr: 0.00000\tTime 0.321 (0.331)\tData 0.000 (0.022)\tLoss 3.5352 (2.6950)\tPrec@1 84.375 (88.897)\tPrec@5 99.219 (99.516)\n",
      "Epoch: [295][80/97], lr: 0.00000\tTime 0.320 (0.330)\tData 0.000 (0.021)\tLoss 1.6939 (2.6837)\tPrec@1 93.750 (88.821)\tPrec@5 100.000 (99.489)\n",
      "Epoch: [295][90/97], lr: 0.00000\tTime 0.320 (0.330)\tData 0.000 (0.021)\tLoss 1.9319 (2.5710)\tPrec@1 88.281 (88.891)\tPrec@5 99.219 (99.493)\n",
      "Epoch: [295][96/97], lr: 0.00000\tTime 0.315 (0.329)\tData 0.000 (0.021)\tLoss 0.9522 (2.5653)\tPrec@1 93.220 (88.892)\tPrec@5 100.000 (99.500)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.261 (0.261)\tLoss 4.1448 (4.1448)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.090)\tLoss 4.8508 (5.6009)\tPrec@1 78.000 (80.636)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.075 (0.082)\tLoss 6.2350 (5.6699)\tPrec@1 78.000 (79.905)\tPrec@5 97.000 (98.238)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.8966 (5.8400)\tPrec@1 80.000 (79.645)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.5162 (5.9033)\tPrec@1 86.000 (79.610)\tPrec@5 98.000 (98.122)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 5.5272 (5.8899)\tPrec@1 86.000 (80.157)\tPrec@5 99.000 (98.157)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.0101 (5.8232)\tPrec@1 82.000 (80.230)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 5.0923 (5.8219)\tPrec@1 82.000 (80.296)\tPrec@5 100.000 (98.394)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6297 (5.7644)\tPrec@1 80.000 (80.593)\tPrec@5 98.000 (98.444)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.3914 (5.7906)\tPrec@1 89.000 (80.385)\tPrec@5 99.000 (98.418)\n",
      "val Results: Prec@1 80.260 Prec@5 98.410 Loss 5.83347\n",
      "val Class Accuracy: [0.904,0.960,0.809,0.696,0.813,0.751,0.816,0.735,0.762,0.780]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [296][0/97], lr: 0.00000\tTime 0.443 (0.443)\tData 0.242 (0.242)\tLoss 2.7981 (2.7981)\tPrec@1 87.500 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [296][10/97], lr: 0.00000\tTime 0.327 (0.343)\tData 0.000 (0.037)\tLoss 3.8505 (2.8473)\tPrec@1 88.281 (87.429)\tPrec@5 100.000 (99.290)\n",
      "Epoch: [296][20/97], lr: 0.00000\tTime 0.338 (0.334)\tData 0.000 (0.027)\tLoss 2.1686 (2.7073)\tPrec@1 91.406 (89.100)\tPrec@5 100.000 (99.330)\n",
      "Epoch: [296][30/97], lr: 0.00000\tTime 0.320 (0.331)\tData 0.000 (0.024)\tLoss 1.4076 (2.7164)\tPrec@1 93.750 (88.911)\tPrec@5 100.000 (99.370)\n",
      "Epoch: [296][40/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.022)\tLoss 2.6123 (2.5804)\tPrec@1 85.938 (89.253)\tPrec@5 99.219 (99.390)\n",
      "Epoch: [296][50/97], lr: 0.00000\tTime 0.320 (0.329)\tData 0.000 (0.021)\tLoss 1.7308 (2.4337)\tPrec@1 88.281 (89.369)\tPrec@5 99.219 (99.341)\n",
      "Epoch: [296][60/97], lr: 0.00000\tTime 0.325 (0.329)\tData 0.000 (0.021)\tLoss 2.2606 (2.4797)\tPrec@1 87.500 (89.255)\tPrec@5 100.000 (99.372)\n",
      "Epoch: [296][70/97], lr: 0.00000\tTime 0.325 (0.329)\tData 0.000 (0.020)\tLoss 1.6394 (2.5955)\tPrec@1 86.719 (89.184)\tPrec@5 100.000 (99.351)\n",
      "Epoch: [296][80/97], lr: 0.00000\tTime 0.328 (0.329)\tData 0.000 (0.020)\tLoss 2.7575 (2.4961)\tPrec@1 86.719 (89.255)\tPrec@5 100.000 (99.354)\n",
      "Epoch: [296][90/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.019)\tLoss 2.5844 (2.5334)\tPrec@1 90.625 (89.303)\tPrec@5 100.000 (99.373)\n",
      "Epoch: [296][96/97], lr: 0.00000\tTime 0.312 (0.328)\tData 0.000 (0.020)\tLoss 4.0544 (2.5264)\tPrec@1 89.831 (89.296)\tPrec@5 100.000 (99.404)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.324 (0.324)\tLoss 4.1618 (4.1618)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.096)\tLoss 4.8471 (5.6355)\tPrec@1 78.000 (80.727)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 6.1498 (5.7014)\tPrec@1 79.000 (79.952)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.082)\tLoss 6.8444 (5.8767)\tPrec@1 80.000 (79.806)\tPrec@5 97.000 (98.258)\n",
      "Test: [40/100]\tTime 0.074 (0.080)\tLoss 4.5277 (5.9417)\tPrec@1 86.000 (79.829)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.073 (0.079)\tLoss 5.5843 (5.9313)\tPrec@1 87.000 (80.392)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 8.0829 (5.8653)\tPrec@1 82.000 (80.459)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.2796 (5.8678)\tPrec@1 82.000 (80.479)\tPrec@5 100.000 (98.394)\n",
      "Test: [80/100]\tTime 0.073 (0.077)\tLoss 6.7124 (5.8091)\tPrec@1 81.000 (80.765)\tPrec@5 98.000 (98.457)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.3892 (5.8360)\tPrec@1 89.000 (80.527)\tPrec@5 99.000 (98.451)\n",
      "val Results: Prec@1 80.420 Prec@5 98.430 Loss 5.88060\n",
      "val Class Accuracy: [0.912,0.960,0.811,0.694,0.806,0.752,0.831,0.739,0.767,0.770]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [297][0/97], lr: 0.00000\tTime 0.453 (0.453)\tData 0.239 (0.239)\tLoss 3.1976 (3.1976)\tPrec@1 85.938 (85.938)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [297][10/97], lr: 0.00000\tTime 0.325 (0.341)\tData 0.000 (0.036)\tLoss 4.5351 (2.4228)\tPrec@1 89.844 (87.997)\tPrec@5 99.219 (99.148)\n",
      "Epoch: [297][20/97], lr: 0.00000\tTime 0.320 (0.333)\tData 0.000 (0.027)\tLoss 2.0249 (2.4258)\tPrec@1 89.062 (87.984)\tPrec@5 99.219 (99.182)\n",
      "Epoch: [297][30/97], lr: 0.00000\tTime 0.322 (0.331)\tData 0.000 (0.024)\tLoss 1.8434 (2.3352)\tPrec@1 89.062 (88.357)\tPrec@5 98.438 (99.244)\n",
      "Epoch: [297][40/97], lr: 0.00000\tTime 0.328 (0.329)\tData 0.000 (0.022)\tLoss 2.0133 (2.5726)\tPrec@1 92.969 (88.472)\tPrec@5 100.000 (99.314)\n",
      "Epoch: [297][50/97], lr: 0.00000\tTime 0.332 (0.328)\tData 0.000 (0.021)\tLoss 2.4250 (2.5798)\tPrec@1 88.281 (88.572)\tPrec@5 100.000 (99.311)\n",
      "Epoch: [297][60/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.020)\tLoss 1.4537 (2.6020)\tPrec@1 91.406 (88.653)\tPrec@5 100.000 (99.411)\n",
      "Epoch: [297][70/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 2.5876 (2.5937)\tPrec@1 85.156 (88.699)\tPrec@5 100.000 (99.417)\n",
      "Epoch: [297][80/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.019)\tLoss 2.5559 (2.5648)\tPrec@1 85.156 (88.706)\tPrec@5 99.219 (99.421)\n",
      "Epoch: [297][90/97], lr: 0.00000\tTime 0.320 (0.327)\tData 0.000 (0.019)\tLoss 1.4903 (2.5821)\tPrec@1 89.844 (88.822)\tPrec@5 100.000 (99.442)\n",
      "Epoch: [297][96/97], lr: 0.00000\tTime 0.314 (0.327)\tData 0.000 (0.020)\tLoss 2.2093 (2.5376)\tPrec@1 84.746 (88.909)\tPrec@5 100.000 (99.436)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.294 (0.294)\tLoss 4.0314 (4.0314)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 4.7541 (5.5233)\tPrec@1 78.000 (80.909)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.0930 (5.5936)\tPrec@1 79.000 (80.095)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 6.7845 (5.7629)\tPrec@1 80.000 (79.871)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.4485 (5.8270)\tPrec@1 85.000 (79.780)\tPrec@5 98.000 (98.122)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.4594 (5.8134)\tPrec@1 85.000 (80.294)\tPrec@5 99.000 (98.137)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 7.9063 (5.7461)\tPrec@1 83.000 (80.393)\tPrec@5 100.000 (98.262)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.0858 (5.7424)\tPrec@1 82.000 (80.437)\tPrec@5 100.000 (98.352)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.5504 (5.6846)\tPrec@1 81.000 (80.728)\tPrec@5 98.000 (98.407)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.2956 (5.7121)\tPrec@1 89.000 (80.495)\tPrec@5 99.000 (98.363)\n",
      "val Results: Prec@1 80.380 Prec@5 98.370 Loss 5.75615\n",
      "val Class Accuracy: [0.904,0.958,0.805,0.696,0.811,0.751,0.826,0.738,0.766,0.783]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [298][0/97], lr: 0.00000\tTime 0.463 (0.463)\tData 0.255 (0.255)\tLoss 1.4447 (1.4447)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [298][10/97], lr: 0.00000\tTime 0.324 (0.342)\tData 0.000 (0.037)\tLoss 1.8226 (2.5301)\tPrec@1 91.406 (89.489)\tPrec@5 100.000 (99.787)\n",
      "Epoch: [298][20/97], lr: 0.00000\tTime 0.327 (0.334)\tData 0.000 (0.028)\tLoss 1.7527 (2.2663)\tPrec@1 92.188 (89.435)\tPrec@5 100.000 (99.814)\n",
      "Epoch: [298][30/97], lr: 0.00000\tTime 0.325 (0.331)\tData 0.000 (0.024)\tLoss 2.3650 (2.2521)\tPrec@1 87.500 (89.163)\tPrec@5 100.000 (99.798)\n",
      "Epoch: [298][40/97], lr: 0.00000\tTime 0.322 (0.331)\tData 0.000 (0.022)\tLoss 1.6642 (2.2710)\tPrec@1 92.188 (89.310)\tPrec@5 100.000 (99.714)\n",
      "Epoch: [298][50/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.021)\tLoss 1.9155 (2.3033)\tPrec@1 87.500 (89.185)\tPrec@5 100.000 (99.663)\n",
      "Epoch: [298][60/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.021)\tLoss 3.0640 (2.3581)\tPrec@1 89.844 (89.306)\tPrec@5 100.000 (99.629)\n",
      "Epoch: [298][70/97], lr: 0.00000\tTime 0.324 (0.329)\tData 0.000 (0.020)\tLoss 1.8158 (2.3357)\tPrec@1 87.500 (89.239)\tPrec@5 100.000 (99.593)\n",
      "Epoch: [298][80/97], lr: 0.00000\tTime 0.324 (0.329)\tData 0.000 (0.020)\tLoss 1.2918 (2.2994)\tPrec@1 88.281 (89.371)\tPrec@5 99.219 (99.624)\n",
      "Epoch: [298][90/97], lr: 0.00000\tTime 0.320 (0.329)\tData 0.000 (0.019)\tLoss 3.7646 (2.3441)\tPrec@1 92.188 (89.432)\tPrec@5 98.438 (99.605)\n",
      "Epoch: [298][96/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.020)\tLoss 2.0166 (2.3373)\tPrec@1 87.288 (89.312)\tPrec@5 100.000 (99.589)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.291 (0.291)\tLoss 4.1738 (4.1738)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.093)\tLoss 4.9563 (5.6730)\tPrec@1 78.000 (80.364)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.074 (0.084)\tLoss 6.2741 (5.7571)\tPrec@1 79.000 (79.905)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.9550 (5.9171)\tPrec@1 81.000 (79.645)\tPrec@5 97.000 (98.323)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.5777 (5.9739)\tPrec@1 85.000 (79.732)\tPrec@5 98.000 (98.195)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.4987 (5.9693)\tPrec@1 86.000 (80.216)\tPrec@5 99.000 (98.235)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.1585 (5.9023)\tPrec@1 82.000 (80.180)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 5.2257 (5.9055)\tPrec@1 82.000 (80.169)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6380 (5.8494)\tPrec@1 82.000 (80.481)\tPrec@5 98.000 (98.444)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4977 (5.8814)\tPrec@1 88.000 (80.242)\tPrec@5 99.000 (98.407)\n",
      "val Results: Prec@1 80.160 Prec@5 98.400 Loss 5.92309\n",
      "val Class Accuracy: [0.904,0.961,0.813,0.676,0.804,0.756,0.832,0.749,0.743,0.778]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [299][0/97], lr: 0.00000\tTime 0.479 (0.479)\tData 0.282 (0.282)\tLoss 2.1595 (2.1595)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [299][10/97], lr: 0.00000\tTime 0.324 (0.341)\tData 0.000 (0.040)\tLoss 1.2765 (1.9331)\tPrec@1 92.188 (89.986)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [299][20/97], lr: 0.00000\tTime 0.321 (0.332)\tData 0.000 (0.029)\tLoss 1.3611 (2.0401)\tPrec@1 89.844 (89.435)\tPrec@5 99.219 (99.479)\n",
      "Epoch: [299][30/97], lr: 0.00000\tTime 0.329 (0.331)\tData 0.000 (0.025)\tLoss 1.5705 (2.1473)\tPrec@1 89.844 (89.264)\tPrec@5 99.219 (99.496)\n",
      "Epoch: [299][40/97], lr: 0.00000\tTime 0.337 (0.330)\tData 0.000 (0.023)\tLoss 1.5893 (2.2117)\tPrec@1 91.406 (89.291)\tPrec@5 100.000 (99.466)\n",
      "Epoch: [299][50/97], lr: 0.00000\tTime 0.320 (0.329)\tData 0.000 (0.022)\tLoss 1.3881 (2.1947)\tPrec@1 91.406 (89.507)\tPrec@5 99.219 (99.433)\n",
      "Epoch: [299][60/97], lr: 0.00000\tTime 0.325 (0.329)\tData 0.000 (0.021)\tLoss 2.4684 (2.2683)\tPrec@1 90.625 (89.434)\tPrec@5 100.000 (99.449)\n",
      "Epoch: [299][70/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.020)\tLoss 1.6418 (2.2564)\tPrec@1 89.062 (89.470)\tPrec@5 100.000 (99.483)\n",
      "Epoch: [299][80/97], lr: 0.00000\tTime 0.324 (0.328)\tData 0.000 (0.020)\tLoss 2.6312 (2.3054)\tPrec@1 88.281 (89.323)\tPrec@5 100.000 (99.489)\n",
      "Epoch: [299][90/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.020)\tLoss 3.2082 (2.3957)\tPrec@1 87.500 (89.260)\tPrec@5 99.219 (99.468)\n",
      "Epoch: [299][96/97], lr: 0.00000\tTime 0.316 (0.328)\tData 0.000 (0.020)\tLoss 2.9797 (2.4327)\tPrec@1 90.678 (89.142)\tPrec@5 99.153 (99.484)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.283 (0.283)\tLoss 4.0081 (4.0081)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.092)\tLoss 4.7036 (5.5022)\tPrec@1 78.000 (80.364)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.1999 (5.5859)\tPrec@1 78.000 (79.762)\tPrec@5 97.000 (98.238)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 6.7652 (5.7597)\tPrec@1 80.000 (79.645)\tPrec@5 97.000 (98.194)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.3685 (5.8287)\tPrec@1 86.000 (79.732)\tPrec@5 98.000 (98.098)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.4722 (5.8125)\tPrec@1 85.000 (80.333)\tPrec@5 99.000 (98.118)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 7.9018 (5.7457)\tPrec@1 83.000 (80.410)\tPrec@5 100.000 (98.262)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 4.9873 (5.7462)\tPrec@1 81.000 (80.394)\tPrec@5 100.000 (98.352)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.5459 (5.6832)\tPrec@1 81.000 (80.691)\tPrec@5 98.000 (98.407)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.3419 (5.7113)\tPrec@1 89.000 (80.473)\tPrec@5 99.000 (98.385)\n",
      "val Results: Prec@1 80.340 Prec@5 98.370 Loss 5.75427\n",
      "val Class Accuracy: [0.910,0.958,0.807,0.706,0.808,0.733,0.820,0.746,0.770,0.776]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [300][0/97], lr: 0.00000\tTime 0.500 (0.500)\tData 0.279 (0.279)\tLoss 2.4237 (2.4237)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [300][10/97], lr: 0.00000\tTime 0.321 (0.343)\tData 0.000 (0.039)\tLoss 1.9263 (2.5552)\tPrec@1 89.844 (88.849)\tPrec@5 99.219 (99.574)\n",
      "Epoch: [300][20/97], lr: 0.00000\tTime 0.359 (0.335)\tData 0.000 (0.029)\tLoss 2.2096 (2.6935)\tPrec@1 88.281 (89.249)\tPrec@5 99.219 (99.591)\n",
      "Epoch: [300][30/97], lr: 0.00000\tTime 0.320 (0.331)\tData 0.000 (0.025)\tLoss 2.1793 (2.5098)\tPrec@1 86.719 (89.264)\tPrec@5 100.000 (99.521)\n",
      "Epoch: [300][40/97], lr: 0.00000\tTime 0.325 (0.331)\tData 0.000 (0.023)\tLoss 2.7084 (2.4917)\tPrec@1 89.062 (89.196)\tPrec@5 99.219 (99.505)\n",
      "Epoch: [300][50/97], lr: 0.00000\tTime 0.318 (0.330)\tData 0.000 (0.022)\tLoss 1.4504 (2.4539)\tPrec@1 90.625 (89.262)\tPrec@5 100.000 (99.525)\n",
      "Epoch: [300][60/97], lr: 0.00000\tTime 0.324 (0.331)\tData 0.000 (0.021)\tLoss 4.1030 (2.5532)\tPrec@1 89.062 (89.152)\tPrec@5 98.438 (99.526)\n",
      "Epoch: [300][70/97], lr: 0.00000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 2.5863 (2.5299)\tPrec@1 89.844 (89.371)\tPrec@5 99.219 (99.505)\n",
      "Epoch: [300][80/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.020)\tLoss 2.2031 (2.5469)\tPrec@1 85.156 (89.226)\tPrec@5 99.219 (99.537)\n",
      "Epoch: [300][90/97], lr: 0.00000\tTime 0.319 (0.330)\tData 0.000 (0.020)\tLoss 2.0347 (2.5546)\tPrec@1 90.625 (89.217)\tPrec@5 100.000 (99.536)\n",
      "Epoch: [300][96/97], lr: 0.00000\tTime 0.311 (0.329)\tData 0.000 (0.020)\tLoss 2.1375 (2.5743)\tPrec@1 86.441 (89.158)\tPrec@5 99.153 (99.516)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.304 (0.304)\tLoss 4.3929 (4.3929)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.094)\tLoss 5.1304 (5.8818)\tPrec@1 79.000 (80.909)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.3475 (5.9348)\tPrec@1 78.000 (79.952)\tPrec@5 98.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.9677 (6.1136)\tPrec@1 81.000 (79.677)\tPrec@5 97.000 (98.258)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.7072 (6.1716)\tPrec@1 85.000 (79.634)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.8221 (6.1773)\tPrec@1 86.000 (80.137)\tPrec@5 99.000 (98.157)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.2809 (6.1039)\tPrec@1 81.000 (80.164)\tPrec@5 100.000 (98.262)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.3866 (6.1157)\tPrec@1 82.000 (80.113)\tPrec@5 100.000 (98.352)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.8631 (6.0588)\tPrec@1 81.000 (80.444)\tPrec@5 98.000 (98.395)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.6270 (6.0887)\tPrec@1 90.000 (80.209)\tPrec@5 99.000 (98.352)\n",
      "val Results: Prec@1 80.150 Prec@5 98.340 Loss 6.13294\n",
      "val Class Accuracy: [0.913,0.966,0.817,0.677,0.807,0.752,0.826,0.752,0.749,0.756]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [301][0/97], lr: 0.00000\tTime 0.470 (0.470)\tData 0.264 (0.264)\tLoss 1.6356 (1.6356)\tPrec@1 86.719 (86.719)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [301][10/97], lr: 0.00000\tTime 0.322 (0.345)\tData 0.000 (0.038)\tLoss 4.8170 (2.6765)\tPrec@1 85.938 (87.855)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [301][20/97], lr: 0.00000\tTime 0.321 (0.334)\tData 0.000 (0.028)\tLoss 0.8178 (2.6352)\tPrec@1 91.406 (88.914)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [301][30/97], lr: 0.00000\tTime 0.322 (0.333)\tData 0.000 (0.025)\tLoss 3.0265 (2.6587)\tPrec@1 90.625 (88.936)\tPrec@5 99.219 (99.446)\n",
      "Epoch: [301][40/97], lr: 0.00000\tTime 0.316 (0.332)\tData 0.000 (0.023)\tLoss 2.8331 (2.6491)\tPrec@1 87.500 (89.101)\tPrec@5 99.219 (99.428)\n",
      "Epoch: [301][50/97], lr: 0.00000\tTime 0.321 (0.331)\tData 0.000 (0.021)\tLoss 3.1581 (2.8079)\tPrec@1 91.406 (88.680)\tPrec@5 99.219 (99.449)\n",
      "Epoch: [301][60/97], lr: 0.00000\tTime 0.322 (0.331)\tData 0.000 (0.021)\tLoss 2.4013 (2.6776)\tPrec@1 84.375 (88.640)\tPrec@5 99.219 (99.462)\n",
      "Epoch: [301][70/97], lr: 0.00000\tTime 0.319 (0.330)\tData 0.000 (0.020)\tLoss 2.0531 (2.5977)\tPrec@1 92.188 (88.941)\tPrec@5 100.000 (99.505)\n",
      "Epoch: [301][80/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.020)\tLoss 2.4519 (2.6329)\tPrec@1 86.719 (88.937)\tPrec@5 98.438 (99.518)\n",
      "Epoch: [301][90/97], lr: 0.00000\tTime 0.324 (0.330)\tData 0.000 (0.019)\tLoss 2.0779 (2.6351)\tPrec@1 93.750 (89.062)\tPrec@5 99.219 (99.502)\n",
      "Epoch: [301][96/97], lr: 0.00000\tTime 0.317 (0.329)\tData 0.000 (0.020)\tLoss 1.3343 (2.6047)\tPrec@1 86.441 (89.150)\tPrec@5 100.000 (99.476)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.268 (0.268)\tLoss 4.0141 (4.0141)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 4.7767 (5.4713)\tPrec@1 78.000 (80.909)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 6.1136 (5.5552)\tPrec@1 79.000 (80.095)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 6.7869 (5.7259)\tPrec@1 80.000 (79.871)\tPrec@5 97.000 (98.258)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 4.3533 (5.7931)\tPrec@1 84.000 (79.829)\tPrec@5 98.000 (98.171)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 5.3636 (5.7771)\tPrec@1 86.000 (80.392)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 7.8676 (5.7123)\tPrec@1 83.000 (80.443)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 4.9406 (5.7063)\tPrec@1 82.000 (80.465)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.5178 (5.6470)\tPrec@1 82.000 (80.765)\tPrec@5 98.000 (98.457)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.2940 (5.6773)\tPrec@1 89.000 (80.571)\tPrec@5 99.000 (98.429)\n",
      "val Results: Prec@1 80.460 Prec@5 98.420 Loss 5.71980\n",
      "val Class Accuracy: [0.907,0.955,0.803,0.694,0.808,0.750,0.833,0.743,0.764,0.789]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [302][0/97], lr: 0.00000\tTime 0.506 (0.506)\tData 0.283 (0.283)\tLoss 1.6989 (1.6989)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [302][10/97], lr: 0.00000\tTime 0.326 (0.346)\tData 0.000 (0.040)\tLoss 2.6033 (1.9417)\tPrec@1 92.188 (88.920)\tPrec@5 99.219 (99.645)\n",
      "Epoch: [302][20/97], lr: 0.00000\tTime 0.338 (0.337)\tData 0.000 (0.029)\tLoss 1.7871 (2.2913)\tPrec@1 90.625 (89.472)\tPrec@5 99.219 (99.405)\n",
      "Epoch: [302][30/97], lr: 0.00000\tTime 0.326 (0.333)\tData 0.000 (0.025)\tLoss 2.2546 (2.3814)\tPrec@1 88.281 (89.567)\tPrec@5 98.438 (99.420)\n",
      "Epoch: [302][40/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.023)\tLoss 2.9991 (2.6438)\tPrec@1 92.969 (89.196)\tPrec@5 99.219 (99.428)\n",
      "Epoch: [302][50/97], lr: 0.00000\tTime 0.322 (0.332)\tData 0.000 (0.022)\tLoss 3.3162 (2.5242)\tPrec@1 89.844 (89.553)\tPrec@5 100.000 (99.449)\n",
      "Epoch: [302][60/97], lr: 0.00000\tTime 0.319 (0.331)\tData 0.000 (0.021)\tLoss 2.0896 (2.6152)\tPrec@1 87.500 (89.344)\tPrec@5 100.000 (99.501)\n",
      "Epoch: [302][70/97], lr: 0.00000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 3.9402 (2.5843)\tPrec@1 89.062 (89.338)\tPrec@5 99.219 (99.505)\n",
      "Epoch: [302][80/97], lr: 0.00000\tTime 0.323 (0.330)\tData 0.000 (0.020)\tLoss 3.6771 (2.6494)\tPrec@1 91.406 (89.333)\tPrec@5 100.000 (99.518)\n",
      "Epoch: [302][90/97], lr: 0.00000\tTime 0.325 (0.330)\tData 0.000 (0.020)\tLoss 3.2326 (2.6604)\tPrec@1 89.062 (89.389)\tPrec@5 100.000 (99.502)\n",
      "Epoch: [302][96/97], lr: 0.00000\tTime 0.315 (0.330)\tData 0.000 (0.020)\tLoss 1.9191 (2.6498)\tPrec@1 89.831 (89.336)\tPrec@5 99.153 (99.492)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.360 (0.360)\tLoss 4.1745 (4.1745)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.100)\tLoss 4.8351 (5.6273)\tPrec@1 78.000 (80.273)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.074 (0.087)\tLoss 6.2528 (5.7068)\tPrec@1 78.000 (79.714)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.083)\tLoss 6.8548 (5.8686)\tPrec@1 80.000 (79.548)\tPrec@5 97.000 (98.290)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 4.5439 (5.9228)\tPrec@1 85.000 (79.659)\tPrec@5 98.000 (98.220)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 5.4929 (5.9132)\tPrec@1 86.000 (80.176)\tPrec@5 99.000 (98.196)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 8.0180 (5.8410)\tPrec@1 83.000 (80.230)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.1440 (5.8384)\tPrec@1 82.000 (80.296)\tPrec@5 100.000 (98.394)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.6390 (5.7813)\tPrec@1 83.000 (80.630)\tPrec@5 98.000 (98.444)\n",
      "Test: [90/100]\tTime 0.073 (0.077)\tLoss 3.4207 (5.8129)\tPrec@1 89.000 (80.385)\tPrec@5 99.000 (98.418)\n",
      "val Results: Prec@1 80.270 Prec@5 98.420 Loss 5.85702\n",
      "val Class Accuracy: [0.911,0.960,0.808,0.694,0.815,0.735,0.823,0.743,0.758,0.780]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [303][0/97], lr: 0.00000\tTime 0.453 (0.453)\tData 0.247 (0.247)\tLoss 2.3463 (2.3463)\tPrec@1 86.719 (86.719)\tPrec@5 98.438 (98.438)\n",
      "Epoch: [303][10/97], lr: 0.00000\tTime 0.319 (0.340)\tData 0.000 (0.037)\tLoss 1.7825 (2.6960)\tPrec@1 90.625 (90.057)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [303][20/97], lr: 0.00000\tTime 0.321 (0.332)\tData 0.000 (0.027)\tLoss 2.4062 (2.3172)\tPrec@1 86.719 (89.807)\tPrec@5 99.219 (99.442)\n",
      "Epoch: [303][30/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.024)\tLoss 1.8658 (2.4938)\tPrec@1 94.531 (89.617)\tPrec@5 98.438 (99.446)\n",
      "Epoch: [303][40/97], lr: 0.00000\tTime 0.331 (0.329)\tData 0.000 (0.022)\tLoss 2.5569 (2.4913)\tPrec@1 89.062 (89.405)\tPrec@5 100.000 (99.409)\n",
      "Epoch: [303][50/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.021)\tLoss 4.7000 (2.6071)\tPrec@1 92.188 (89.292)\tPrec@5 100.000 (99.449)\n",
      "Epoch: [303][60/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.020)\tLoss 2.2449 (2.5785)\tPrec@1 93.750 (89.242)\tPrec@5 99.219 (99.398)\n",
      "Epoch: [303][70/97], lr: 0.00000\tTime 0.318 (0.328)\tData 0.000 (0.020)\tLoss 1.3510 (2.4972)\tPrec@1 90.625 (89.470)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [303][80/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.020)\tLoss 2.3477 (2.4638)\tPrec@1 84.375 (89.400)\tPrec@5 99.219 (99.421)\n",
      "Epoch: [303][90/97], lr: 0.00000\tTime 0.324 (0.327)\tData 0.000 (0.019)\tLoss 4.2960 (2.5109)\tPrec@1 86.719 (89.311)\tPrec@5 100.000 (99.408)\n",
      "Epoch: [303][96/97], lr: 0.00000\tTime 0.313 (0.327)\tData 0.000 (0.020)\tLoss 3.9054 (2.5337)\tPrec@1 94.068 (89.312)\tPrec@5 100.000 (99.436)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.372 (0.372)\tLoss 4.3799 (4.3799)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.101)\tLoss 5.1177 (5.8284)\tPrec@1 79.000 (80.545)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.088)\tLoss 6.3324 (5.9027)\tPrec@1 80.000 (79.857)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.073 (0.083)\tLoss 7.0924 (6.0600)\tPrec@1 81.000 (79.645)\tPrec@5 97.000 (98.355)\n",
      "Test: [40/100]\tTime 0.074 (0.081)\tLoss 4.7755 (6.1127)\tPrec@1 84.000 (79.634)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 5.6317 (6.1148)\tPrec@1 87.000 (80.098)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 8.2661 (6.0466)\tPrec@1 81.000 (80.131)\tPrec@5 100.000 (98.295)\n",
      "Test: [70/100]\tTime 0.074 (0.078)\tLoss 5.3918 (6.0565)\tPrec@1 82.000 (80.155)\tPrec@5 100.000 (98.380)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.8921 (6.0009)\tPrec@1 82.000 (80.469)\tPrec@5 98.000 (98.407)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 3.6177 (6.0325)\tPrec@1 87.000 (80.176)\tPrec@5 99.000 (98.352)\n",
      "val Results: Prec@1 80.080 Prec@5 98.340 Loss 6.07504\n",
      "val Class Accuracy: [0.908,0.963,0.811,0.688,0.807,0.751,0.843,0.732,0.734,0.771]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [304][0/97], lr: 0.00000\tTime 0.540 (0.540)\tData 0.305 (0.305)\tLoss 1.6605 (1.6605)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [304][10/97], lr: 0.00000\tTime 0.324 (0.353)\tData 0.000 (0.042)\tLoss 4.0128 (2.6724)\tPrec@1 87.500 (88.139)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [304][20/97], lr: 0.00000\tTime 0.331 (0.339)\tData 0.000 (0.030)\tLoss 2.7433 (2.5187)\tPrec@1 89.844 (88.542)\tPrec@5 100.000 (99.405)\n",
      "Epoch: [304][30/97], lr: 0.00000\tTime 0.320 (0.335)\tData 0.000 (0.026)\tLoss 1.2491 (2.6484)\tPrec@1 90.625 (88.735)\tPrec@5 99.219 (99.370)\n",
      "Epoch: [304][40/97], lr: 0.00000\tTime 0.323 (0.333)\tData 0.000 (0.024)\tLoss 1.6312 (2.5628)\tPrec@1 90.625 (88.720)\tPrec@5 100.000 (99.352)\n",
      "Epoch: [304][50/97], lr: 0.00000\tTime 0.318 (0.332)\tData 0.000 (0.022)\tLoss 2.8330 (2.4748)\tPrec@1 90.625 (88.863)\tPrec@5 100.000 (99.372)\n",
      "Epoch: [304][60/97], lr: 0.00000\tTime 0.323 (0.331)\tData 0.000 (0.022)\tLoss 2.2368 (2.3640)\tPrec@1 89.844 (89.255)\tPrec@5 99.219 (99.385)\n",
      "Epoch: [304][70/97], lr: 0.00000\tTime 0.328 (0.330)\tData 0.000 (0.021)\tLoss 2.8137 (2.4102)\tPrec@1 95.312 (89.316)\tPrec@5 100.000 (99.428)\n",
      "Epoch: [304][80/97], lr: 0.00000\tTime 0.320 (0.330)\tData 0.000 (0.020)\tLoss 1.7666 (2.3763)\tPrec@1 90.625 (89.284)\tPrec@5 100.000 (99.441)\n",
      "Epoch: [304][90/97], lr: 0.00000\tTime 0.318 (0.330)\tData 0.000 (0.020)\tLoss 2.1973 (2.3880)\tPrec@1 88.281 (89.380)\tPrec@5 100.000 (99.433)\n",
      "Epoch: [304][96/97], lr: 0.00000\tTime 0.313 (0.329)\tData 0.000 (0.021)\tLoss 3.6059 (2.4006)\tPrec@1 88.983 (89.360)\tPrec@5 100.000 (99.436)\n",
      "Gated Network Weight Gate= Flip:0.53, Sc:0.47\n",
      "Test: [0/100]\tTime 0.305 (0.305)\tLoss 4.1231 (4.1231)\tPrec@1 85.000 (85.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.094)\tLoss 4.9754 (5.7008)\tPrec@1 78.000 (80.818)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.2299 (5.7733)\tPrec@1 79.000 (80.048)\tPrec@5 97.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.9095 (5.9371)\tPrec@1 81.000 (79.806)\tPrec@5 97.000 (98.323)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.6227 (5.9949)\tPrec@1 85.000 (79.805)\tPrec@5 98.000 (98.195)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.5665 (5.9888)\tPrec@1 86.000 (80.275)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.1458 (5.9157)\tPrec@1 81.000 (80.246)\tPrec@5 100.000 (98.295)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.3316 (5.9195)\tPrec@1 81.000 (80.239)\tPrec@5 100.000 (98.380)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6593 (5.8649)\tPrec@1 82.000 (80.506)\tPrec@5 98.000 (98.420)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.4564 (5.8951)\tPrec@1 88.000 (80.275)\tPrec@5 99.000 (98.396)\n",
      "val Results: Prec@1 80.190 Prec@5 98.400 Loss 5.94070\n",
      "val Class Accuracy: [0.905,0.965,0.810,0.669,0.802,0.759,0.836,0.749,0.750,0.774]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [305][0/97], lr: 0.00000\tTime 0.515 (0.515)\tData 0.287 (0.287)\tLoss 2.0514 (2.0514)\tPrec@1 91.406 (91.406)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [305][10/97], lr: 0.00000\tTime 0.322 (0.345)\tData 0.000 (0.040)\tLoss 2.3626 (2.2283)\tPrec@1 89.062 (89.915)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [305][20/97], lr: 0.00000\tTime 0.318 (0.334)\tData 0.000 (0.029)\tLoss 1.2282 (2.2485)\tPrec@1 93.750 (89.769)\tPrec@5 100.000 (99.554)\n",
      "Epoch: [305][30/97], lr: 0.00000\tTime 0.325 (0.332)\tData 0.000 (0.025)\tLoss 2.2879 (2.3570)\tPrec@1 91.406 (89.315)\tPrec@5 98.438 (99.496)\n",
      "Epoch: [305][40/97], lr: 0.00000\tTime 0.328 (0.330)\tData 0.000 (0.023)\tLoss 1.9752 (2.3864)\tPrec@1 92.188 (89.158)\tPrec@5 100.000 (99.486)\n",
      "Epoch: [305][50/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.022)\tLoss 1.1117 (2.4323)\tPrec@1 91.406 (89.231)\tPrec@5 98.438 (99.464)\n",
      "Epoch: [305][60/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.021)\tLoss 2.4438 (2.4752)\tPrec@1 91.406 (89.229)\tPrec@5 100.000 (99.424)\n",
      "Epoch: [305][70/97], lr: 0.00000\tTime 0.323 (0.328)\tData 0.000 (0.021)\tLoss 1.6259 (2.4859)\tPrec@1 91.406 (89.140)\tPrec@5 100.000 (99.417)\n",
      "Epoch: [305][80/97], lr: 0.00000\tTime 0.319 (0.328)\tData 0.000 (0.020)\tLoss 1.8410 (2.4646)\tPrec@1 89.062 (89.091)\tPrec@5 99.219 (99.383)\n",
      "Epoch: [305][90/97], lr: 0.00000\tTime 0.326 (0.328)\tData 0.000 (0.020)\tLoss 3.3638 (2.5061)\tPrec@1 90.625 (89.002)\tPrec@5 100.000 (99.399)\n",
      "Epoch: [305][96/97], lr: 0.00000\tTime 0.313 (0.328)\tData 0.000 (0.020)\tLoss 4.8716 (2.5013)\tPrec@1 83.051 (89.046)\tPrec@5 99.153 (99.412)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.298 (0.298)\tLoss 4.0843 (4.0843)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.095)\tLoss 4.8650 (5.5679)\tPrec@1 78.000 (80.273)\tPrec@5 99.000 (98.182)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 6.2702 (5.6531)\tPrec@1 78.000 (79.714)\tPrec@5 97.000 (98.238)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 6.9089 (5.8209)\tPrec@1 80.000 (79.452)\tPrec@5 97.000 (98.258)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.4687 (5.8855)\tPrec@1 85.000 (79.512)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.5087 (5.8784)\tPrec@1 85.000 (80.039)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 7.9668 (5.8125)\tPrec@1 82.000 (80.131)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.0174 (5.8136)\tPrec@1 82.000 (80.225)\tPrec@5 100.000 (98.394)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.5797 (5.7529)\tPrec@1 82.000 (80.580)\tPrec@5 98.000 (98.432)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4455 (5.7835)\tPrec@1 89.000 (80.308)\tPrec@5 99.000 (98.407)\n",
      "val Results: Prec@1 80.240 Prec@5 98.390 Loss 5.82440\n",
      "val Class Accuracy: [0.905,0.960,0.811,0.697,0.806,0.748,0.821,0.738,0.757,0.781]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [306][0/97], lr: 0.00000\tTime 0.441 (0.441)\tData 0.242 (0.242)\tLoss 2.9359 (2.9359)\tPrec@1 87.500 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [306][10/97], lr: 0.00000\tTime 0.322 (0.340)\tData 0.000 (0.036)\tLoss 2.2219 (2.5171)\tPrec@1 85.938 (88.352)\tPrec@5 100.000 (99.148)\n",
      "Epoch: [306][20/97], lr: 0.00000\tTime 0.331 (0.333)\tData 0.000 (0.027)\tLoss 1.7391 (2.4879)\tPrec@1 92.188 (88.728)\tPrec@5 100.000 (99.293)\n",
      "Epoch: [306][30/97], lr: 0.00000\tTime 0.324 (0.331)\tData 0.000 (0.024)\tLoss 2.5992 (2.3324)\tPrec@1 89.844 (88.911)\tPrec@5 99.219 (99.294)\n",
      "Epoch: [306][40/97], lr: 0.00000\tTime 0.329 (0.332)\tData 0.000 (0.022)\tLoss 2.2683 (2.4076)\tPrec@1 89.062 (88.929)\tPrec@5 100.000 (99.371)\n",
      "Epoch: [306][50/97], lr: 0.00000\tTime 0.322 (0.331)\tData 0.000 (0.021)\tLoss 6.7966 (2.5387)\tPrec@1 85.938 (88.909)\tPrec@5 100.000 (99.387)\n",
      "Epoch: [306][60/97], lr: 0.00000\tTime 0.324 (0.330)\tData 0.000 (0.020)\tLoss 2.9766 (2.4881)\tPrec@1 89.844 (89.062)\tPrec@5 100.000 (99.411)\n",
      "Epoch: [306][70/97], lr: 0.00000\tTime 0.331 (0.330)\tData 0.000 (0.020)\tLoss 1.1153 (2.4892)\tPrec@1 88.281 (88.985)\tPrec@5 100.000 (99.384)\n",
      "Epoch: [306][80/97], lr: 0.00000\tTime 0.324 (0.330)\tData 0.000 (0.019)\tLoss 3.4044 (2.4958)\tPrec@1 92.188 (88.899)\tPrec@5 96.875 (99.383)\n",
      "Epoch: [306][90/97], lr: 0.00000\tTime 0.319 (0.329)\tData 0.000 (0.019)\tLoss 4.5107 (2.5042)\tPrec@1 87.500 (88.899)\tPrec@5 98.438 (99.382)\n",
      "Epoch: [306][96/97], lr: 0.00000\tTime 0.325 (0.329)\tData 0.000 (0.020)\tLoss 2.1205 (2.5286)\tPrec@1 89.831 (88.804)\tPrec@5 99.153 (99.371)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.341 (0.341)\tLoss 4.0102 (4.0102)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.098)\tLoss 4.8940 (5.5218)\tPrec@1 78.000 (80.636)\tPrec@5 99.000 (98.455)\n",
      "Test: [20/100]\tTime 0.073 (0.086)\tLoss 6.1558 (5.5954)\tPrec@1 79.000 (79.952)\tPrec@5 97.000 (98.381)\n",
      "Test: [30/100]\tTime 0.073 (0.082)\tLoss 6.8467 (5.7703)\tPrec@1 81.000 (79.806)\tPrec@5 97.000 (98.258)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 4.4536 (5.8390)\tPrec@1 85.000 (79.902)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.073 (0.079)\tLoss 5.4309 (5.8250)\tPrec@1 85.000 (80.275)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 7.9359 (5.7561)\tPrec@1 82.000 (80.295)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.0188 (5.7542)\tPrec@1 82.000 (80.352)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.073 (0.077)\tLoss 6.4828 (5.6938)\tPrec@1 82.000 (80.630)\tPrec@5 98.000 (98.432)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.3717 (5.7234)\tPrec@1 88.000 (80.407)\tPrec@5 99.000 (98.407)\n",
      "val Results: Prec@1 80.300 Prec@5 98.410 Loss 5.76575\n",
      "val Class Accuracy: [0.911,0.960,0.810,0.682,0.798,0.756,0.836,0.736,0.761,0.780]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [307][0/97], lr: 0.00000\tTime 0.535 (0.535)\tData 0.307 (0.307)\tLoss 3.1871 (3.1871)\tPrec@1 91.406 (91.406)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [307][10/97], lr: 0.00000\tTime 0.324 (0.351)\tData 0.000 (0.042)\tLoss 2.5676 (2.0720)\tPrec@1 82.812 (89.560)\tPrec@5 100.000 (99.716)\n",
      "Epoch: [307][20/97], lr: 0.00000\tTime 0.319 (0.338)\tData 0.000 (0.030)\tLoss 2.8022 (2.1965)\tPrec@1 87.500 (89.621)\tPrec@5 100.000 (99.628)\n",
      "Epoch: [307][30/97], lr: 0.00000\tTime 0.324 (0.336)\tData 0.000 (0.026)\tLoss 3.0809 (2.3660)\tPrec@1 92.188 (89.441)\tPrec@5 99.219 (99.521)\n",
      "Epoch: [307][40/97], lr: 0.00000\tTime 0.311 (0.334)\tData 0.000 (0.024)\tLoss 2.5825 (2.4201)\tPrec@1 85.156 (89.234)\tPrec@5 99.219 (99.524)\n",
      "Epoch: [307][50/97], lr: 0.00000\tTime 0.323 (0.332)\tData 0.000 (0.022)\tLoss 2.0235 (2.5470)\tPrec@1 87.500 (89.216)\tPrec@5 99.219 (99.571)\n",
      "Epoch: [307][60/97], lr: 0.00000\tTime 0.318 (0.331)\tData 0.000 (0.021)\tLoss 1.6108 (2.5069)\tPrec@1 90.625 (89.165)\tPrec@5 100.000 (99.577)\n",
      "Epoch: [307][70/97], lr: 0.00000\tTime 0.319 (0.330)\tData 0.000 (0.021)\tLoss 2.6522 (2.4937)\tPrec@1 89.062 (89.085)\tPrec@5 98.438 (99.527)\n",
      "Epoch: [307][80/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.020)\tLoss 3.9919 (2.5139)\tPrec@1 87.500 (89.072)\tPrec@5 100.000 (99.537)\n",
      "Epoch: [307][90/97], lr: 0.00000\tTime 0.318 (0.329)\tData 0.000 (0.020)\tLoss 1.1986 (2.5339)\tPrec@1 93.750 (89.097)\tPrec@5 100.000 (99.536)\n",
      "Epoch: [307][96/97], lr: 0.00000\tTime 0.309 (0.329)\tData 0.000 (0.020)\tLoss 2.6951 (2.5056)\tPrec@1 89.831 (89.142)\tPrec@5 100.000 (99.524)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.295 (0.295)\tLoss 4.0763 (4.0763)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.093)\tLoss 4.7168 (5.5750)\tPrec@1 78.000 (80.636)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.074 (0.084)\tLoss 6.1255 (5.6422)\tPrec@1 79.000 (79.905)\tPrec@5 97.000 (98.238)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.7755 (5.8140)\tPrec@1 81.000 (79.774)\tPrec@5 97.000 (98.194)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.5063 (5.8787)\tPrec@1 86.000 (79.854)\tPrec@5 98.000 (98.073)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.5065 (5.8666)\tPrec@1 86.000 (80.275)\tPrec@5 99.000 (98.157)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.0193 (5.7970)\tPrec@1 81.000 (80.230)\tPrec@5 100.000 (98.295)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.0958 (5.7938)\tPrec@1 82.000 (80.282)\tPrec@5 100.000 (98.380)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.4939 (5.7372)\tPrec@1 81.000 (80.556)\tPrec@5 98.000 (98.420)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.2838 (5.7601)\tPrec@1 89.000 (80.330)\tPrec@5 99.000 (98.374)\n",
      "val Results: Prec@1 80.240 Prec@5 98.370 Loss 5.80613\n",
      "val Class Accuracy: [0.898,0.963,0.810,0.676,0.813,0.754,0.832,0.738,0.766,0.774]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [308][0/97], lr: 0.00000\tTime 0.452 (0.452)\tData 0.231 (0.231)\tLoss 3.2834 (3.2834)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [308][10/97], lr: 0.00000\tTime 0.319 (0.340)\tData 0.000 (0.036)\tLoss 1.2070 (2.6985)\tPrec@1 92.188 (90.980)\tPrec@5 99.219 (99.645)\n",
      "Epoch: [308][20/97], lr: 0.00000\tTime 0.328 (0.332)\tData 0.000 (0.027)\tLoss 2.5706 (2.4104)\tPrec@1 91.406 (90.737)\tPrec@5 99.219 (99.702)\n",
      "Epoch: [308][30/97], lr: 0.00000\tTime 0.322 (0.331)\tData 0.000 (0.024)\tLoss 2.3172 (2.3071)\tPrec@1 92.969 (90.600)\tPrec@5 99.219 (99.622)\n",
      "Epoch: [308][40/97], lr: 0.00000\tTime 0.323 (0.331)\tData 0.000 (0.022)\tLoss 1.1372 (2.4407)\tPrec@1 95.312 (90.244)\tPrec@5 100.000 (99.619)\n",
      "Epoch: [308][50/97], lr: 0.00000\tTime 0.333 (0.330)\tData 0.000 (0.021)\tLoss 2.0012 (2.3801)\tPrec@1 89.844 (90.058)\tPrec@5 100.000 (99.571)\n",
      "Epoch: [308][60/97], lr: 0.00000\tTime 0.324 (0.329)\tData 0.000 (0.020)\tLoss 2.6579 (2.4180)\tPrec@1 92.188 (89.780)\tPrec@5 100.000 (99.539)\n",
      "Epoch: [308][70/97], lr: 0.00000\tTime 0.326 (0.329)\tData 0.000 (0.020)\tLoss 3.2171 (2.4214)\tPrec@1 87.500 (89.646)\tPrec@5 98.438 (99.549)\n",
      "Epoch: [308][80/97], lr: 0.00000\tTime 0.324 (0.329)\tData 0.000 (0.020)\tLoss 3.6580 (2.4303)\tPrec@1 89.844 (89.612)\tPrec@5 100.000 (99.576)\n",
      "Epoch: [308][90/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.019)\tLoss 2.7674 (2.4401)\tPrec@1 88.281 (89.526)\tPrec@5 99.219 (99.528)\n",
      "Epoch: [308][96/97], lr: 0.00000\tTime 0.315 (0.328)\tData 0.000 (0.020)\tLoss 3.4909 (2.4890)\tPrec@1 88.136 (89.376)\tPrec@5 100.000 (99.541)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.265 (0.265)\tLoss 3.9935 (3.9935)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 4.7375 (5.5009)\tPrec@1 78.000 (80.545)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.074 (0.083)\tLoss 6.1398 (5.5772)\tPrec@1 79.000 (79.905)\tPrec@5 97.000 (98.238)\n",
      "Test: [30/100]\tTime 0.074 (0.080)\tLoss 6.7774 (5.7478)\tPrec@1 81.000 (79.677)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 4.3971 (5.8131)\tPrec@1 86.000 (79.732)\tPrec@5 98.000 (98.146)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.4067 (5.7952)\tPrec@1 86.000 (80.235)\tPrec@5 99.000 (98.196)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 7.9287 (5.7278)\tPrec@1 82.000 (80.295)\tPrec@5 100.000 (98.311)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 4.9767 (5.7238)\tPrec@1 82.000 (80.324)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.5186 (5.6659)\tPrec@1 81.000 (80.630)\tPrec@5 98.000 (98.444)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.3298 (5.6940)\tPrec@1 88.000 (80.396)\tPrec@5 99.000 (98.407)\n",
      "val Results: Prec@1 80.280 Prec@5 98.400 Loss 5.73594\n",
      "val Class Accuracy: [0.900,0.958,0.807,0.689,0.811,0.747,0.825,0.741,0.765,0.785]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [309][0/97], lr: 0.00000\tTime 0.487 (0.487)\tData 0.288 (0.288)\tLoss 3.1488 (3.1488)\tPrec@1 90.625 (90.625)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [309][10/97], lr: 0.00000\tTime 0.324 (0.342)\tData 0.000 (0.040)\tLoss 1.8346 (2.2286)\tPrec@1 89.062 (90.128)\tPrec@5 99.219 (99.645)\n",
      "Epoch: [309][20/97], lr: 0.00000\tTime 0.322 (0.334)\tData 0.000 (0.029)\tLoss 2.6618 (2.5405)\tPrec@1 90.625 (89.397)\tPrec@5 100.000 (99.591)\n",
      "Epoch: [309][30/97], lr: 0.00000\tTime 0.320 (0.331)\tData 0.000 (0.025)\tLoss 2.2055 (2.5593)\tPrec@1 89.062 (89.138)\tPrec@5 98.438 (99.597)\n",
      "Epoch: [309][40/97], lr: 0.00000\tTime 0.340 (0.330)\tData 0.000 (0.023)\tLoss 2.6034 (2.5936)\tPrec@1 89.844 (89.139)\tPrec@5 100.000 (99.600)\n",
      "Epoch: [309][50/97], lr: 0.00000\tTime 0.323 (0.330)\tData 0.000 (0.022)\tLoss 1.7286 (2.5255)\tPrec@1 84.375 (88.925)\tPrec@5 98.438 (99.540)\n",
      "Epoch: [309][60/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.021)\tLoss 2.3418 (2.5323)\tPrec@1 89.062 (89.242)\tPrec@5 99.219 (99.513)\n",
      "Epoch: [309][70/97], lr: 0.00000\tTime 0.319 (0.329)\tData 0.000 (0.021)\tLoss 2.9184 (2.5383)\tPrec@1 92.188 (89.184)\tPrec@5 100.000 (99.516)\n",
      "Epoch: [309][80/97], lr: 0.00000\tTime 0.323 (0.328)\tData 0.000 (0.020)\tLoss 2.4984 (2.5689)\tPrec@1 90.625 (89.198)\tPrec@5 98.438 (99.508)\n",
      "Epoch: [309][90/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 1.8248 (2.5410)\tPrec@1 89.844 (89.200)\tPrec@5 100.000 (99.502)\n",
      "Epoch: [309][96/97], lr: 0.00000\tTime 0.315 (0.328)\tData 0.000 (0.020)\tLoss 3.8423 (2.5227)\tPrec@1 88.136 (89.207)\tPrec@5 100.000 (99.508)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.255 (0.255)\tLoss 4.2081 (4.2081)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.090)\tLoss 4.9803 (5.7052)\tPrec@1 78.000 (80.455)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 6.2681 (5.7850)\tPrec@1 78.000 (79.857)\tPrec@5 97.000 (98.333)\n",
      "Test: [30/100]\tTime 0.074 (0.079)\tLoss 6.9379 (5.9523)\tPrec@1 81.000 (79.613)\tPrec@5 97.000 (98.290)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 4.6381 (6.0093)\tPrec@1 85.000 (79.659)\tPrec@5 98.000 (98.195)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 5.6015 (6.0088)\tPrec@1 84.000 (80.078)\tPrec@5 99.000 (98.235)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.1307 (5.9364)\tPrec@1 81.000 (80.066)\tPrec@5 100.000 (98.328)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 5.2745 (5.9401)\tPrec@1 81.000 (80.099)\tPrec@5 100.000 (98.408)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.7015 (5.8858)\tPrec@1 82.000 (80.383)\tPrec@5 98.000 (98.432)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4899 (5.9144)\tPrec@1 89.000 (80.132)\tPrec@5 99.000 (98.396)\n",
      "val Results: Prec@1 80.070 Prec@5 98.400 Loss 5.95848\n",
      "val Class Accuracy: [0.904,0.964,0.815,0.673,0.800,0.756,0.831,0.744,0.749,0.771]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [310][0/97], lr: 0.00000\tTime 0.434 (0.434)\tData 0.250 (0.250)\tLoss 1.4281 (1.4281)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [310][10/97], lr: 0.00000\tTime 0.320 (0.338)\tData 0.000 (0.038)\tLoss 3.1666 (2.2607)\tPrec@1 89.844 (89.631)\tPrec@5 100.000 (99.361)\n",
      "Epoch: [310][20/97], lr: 0.00000\tTime 0.353 (0.332)\tData 0.000 (0.028)\tLoss 5.2708 (2.4656)\tPrec@1 85.156 (89.025)\tPrec@5 98.438 (99.330)\n",
      "Epoch: [310][30/97], lr: 0.00000\tTime 0.320 (0.331)\tData 0.000 (0.024)\tLoss 2.5297 (2.4888)\tPrec@1 89.062 (88.407)\tPrec@5 98.438 (99.269)\n",
      "Epoch: [310][40/97], lr: 0.00000\tTime 0.323 (0.330)\tData 0.000 (0.022)\tLoss 2.1051 (2.4759)\tPrec@1 89.062 (88.643)\tPrec@5 99.219 (99.333)\n",
      "Epoch: [310][50/97], lr: 0.00000\tTime 0.319 (0.329)\tData 0.000 (0.021)\tLoss 3.4217 (2.5790)\tPrec@1 89.844 (88.603)\tPrec@5 100.000 (99.326)\n",
      "Epoch: [310][60/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.021)\tLoss 2.2979 (2.4825)\tPrec@1 92.969 (88.755)\tPrec@5 99.219 (99.385)\n",
      "Epoch: [310][70/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.020)\tLoss 1.5491 (2.4394)\tPrec@1 89.062 (88.908)\tPrec@5 98.438 (99.384)\n",
      "Epoch: [310][80/97], lr: 0.00000\tTime 0.323 (0.328)\tData 0.000 (0.020)\tLoss 3.1415 (2.4174)\tPrec@1 85.938 (88.918)\tPrec@5 100.000 (99.392)\n",
      "Epoch: [310][90/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.019)\tLoss 2.3557 (2.4239)\tPrec@1 89.844 (88.865)\tPrec@5 97.656 (99.339)\n",
      "Epoch: [310][96/97], lr: 0.00000\tTime 0.314 (0.328)\tData 0.000 (0.020)\tLoss 1.6304 (2.4066)\tPrec@1 90.678 (88.868)\tPrec@5 100.000 (99.339)\n",
      "Gated Network Weight Gate= Flip:0.58, Sc:0.42\n",
      "Test: [0/100]\tTime 0.287 (0.287)\tLoss 4.0029 (4.0029)\tPrec@1 81.000 (81.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 4.7235 (5.4582)\tPrec@1 78.000 (80.273)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.1029 (5.5405)\tPrec@1 79.000 (79.762)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 6.7727 (5.7177)\tPrec@1 80.000 (79.645)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 4.3568 (5.7886)\tPrec@1 85.000 (79.683)\tPrec@5 98.000 (98.098)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 5.4063 (5.7710)\tPrec@1 86.000 (80.333)\tPrec@5 99.000 (98.098)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 7.8134 (5.7049)\tPrec@1 83.000 (80.393)\tPrec@5 100.000 (98.246)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 4.9160 (5.7006)\tPrec@1 82.000 (80.479)\tPrec@5 100.000 (98.338)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.4926 (5.6427)\tPrec@1 79.000 (80.753)\tPrec@5 98.000 (98.370)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.2997 (5.6701)\tPrec@1 89.000 (80.549)\tPrec@5 99.000 (98.352)\n",
      "val Results: Prec@1 80.450 Prec@5 98.360 Loss 5.71289\n",
      "val Class Accuracy: [0.906,0.957,0.809,0.704,0.797,0.744,0.835,0.739,0.771,0.783]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [311][0/97], lr: 0.00000\tTime 0.502 (0.502)\tData 0.287 (0.287)\tLoss 1.6966 (1.6966)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [311][10/97], lr: 0.00000\tTime 0.324 (0.344)\tData 0.000 (0.041)\tLoss 1.6743 (2.4533)\tPrec@1 88.281 (89.986)\tPrec@5 98.438 (99.645)\n",
      "Epoch: [311][20/97], lr: 0.00000\tTime 0.322 (0.335)\tData 0.000 (0.029)\tLoss 1.6243 (2.4410)\tPrec@1 91.406 (89.137)\tPrec@5 100.000 (99.591)\n",
      "Epoch: [311][30/97], lr: 0.00000\tTime 0.324 (0.333)\tData 0.000 (0.025)\tLoss 2.1574 (2.5910)\tPrec@1 89.844 (88.634)\tPrec@5 98.438 (99.496)\n",
      "Epoch: [311][40/97], lr: 0.00000\tTime 0.359 (0.333)\tData 0.000 (0.023)\tLoss 2.4010 (2.5292)\tPrec@1 88.281 (88.929)\tPrec@5 100.000 (99.524)\n",
      "Epoch: [311][50/97], lr: 0.00000\tTime 0.321 (0.332)\tData 0.000 (0.022)\tLoss 1.5771 (2.4715)\tPrec@1 94.531 (88.909)\tPrec@5 100.000 (99.556)\n",
      "Epoch: [311][60/97], lr: 0.00000\tTime 0.327 (0.331)\tData 0.000 (0.021)\tLoss 1.5191 (2.4562)\tPrec@1 86.719 (88.845)\tPrec@5 99.219 (99.552)\n",
      "Epoch: [311][70/97], lr: 0.00000\tTime 0.323 (0.330)\tData 0.000 (0.021)\tLoss 1.8500 (2.4118)\tPrec@1 92.188 (88.974)\tPrec@5 99.219 (99.527)\n",
      "Epoch: [311][80/97], lr: 0.00000\tTime 0.323 (0.330)\tData 0.000 (0.020)\tLoss 2.2055 (2.4097)\tPrec@1 90.625 (89.062)\tPrec@5 97.656 (99.470)\n",
      "Epoch: [311][90/97], lr: 0.00000\tTime 0.323 (0.330)\tData 0.000 (0.020)\tLoss 2.5440 (2.3696)\tPrec@1 87.500 (89.174)\tPrec@5 99.219 (99.502)\n",
      "Epoch: [311][96/97], lr: 0.00000\tTime 0.310 (0.329)\tData 0.000 (0.020)\tLoss 5.7719 (2.4180)\tPrec@1 92.373 (89.110)\tPrec@5 99.153 (99.516)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.338 (0.338)\tLoss 4.1898 (4.1898)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.097)\tLoss 5.0199 (5.6235)\tPrec@1 79.000 (80.818)\tPrec@5 99.000 (98.182)\n",
      "Test: [20/100]\tTime 0.073 (0.086)\tLoss 6.2461 (5.6897)\tPrec@1 79.000 (80.238)\tPrec@5 97.000 (98.190)\n",
      "Test: [30/100]\tTime 0.073 (0.082)\tLoss 6.9520 (5.8666)\tPrec@1 80.000 (79.903)\tPrec@5 97.000 (98.129)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 4.4571 (5.9274)\tPrec@1 84.000 (79.927)\tPrec@5 98.000 (98.049)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 5.5114 (5.9167)\tPrec@1 87.000 (80.412)\tPrec@5 99.000 (98.118)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 8.0560 (5.8505)\tPrec@1 82.000 (80.410)\tPrec@5 100.000 (98.246)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.0342 (5.8553)\tPrec@1 80.000 (80.366)\tPrec@5 100.000 (98.352)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.6169 (5.7921)\tPrec@1 82.000 (80.691)\tPrec@5 98.000 (98.420)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 3.5722 (5.8272)\tPrec@1 89.000 (80.462)\tPrec@5 99.000 (98.385)\n",
      "val Results: Prec@1 80.340 Prec@5 98.390 Loss 5.86983\n",
      "val Class Accuracy: [0.917,0.960,0.792,0.690,0.823,0.754,0.828,0.743,0.747,0.780]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [312][0/97], lr: 0.00000\tTime 0.467 (0.467)\tData 0.269 (0.269)\tLoss 1.6183 (1.6183)\tPrec@1 90.625 (90.625)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [312][10/97], lr: 0.00000\tTime 0.328 (0.347)\tData 0.000 (0.039)\tLoss 2.9804 (2.4861)\tPrec@1 91.406 (89.347)\tPrec@5 100.000 (99.716)\n",
      "Epoch: [312][20/97], lr: 0.00000\tTime 0.333 (0.336)\tData 0.000 (0.028)\tLoss 1.4424 (2.2986)\tPrec@1 92.188 (89.658)\tPrec@5 100.000 (99.777)\n",
      "Epoch: [312][30/97], lr: 0.00000\tTime 0.327 (0.334)\tData 0.000 (0.025)\tLoss 2.4396 (2.1595)\tPrec@1 87.500 (89.693)\tPrec@5 99.219 (99.672)\n",
      "Epoch: [312][40/97], lr: 0.00000\tTime 0.328 (0.333)\tData 0.000 (0.023)\tLoss 2.9212 (2.3131)\tPrec@1 89.844 (89.653)\tPrec@5 100.000 (99.619)\n",
      "Epoch: [312][50/97], lr: 0.00000\tTime 0.323 (0.332)\tData 0.000 (0.022)\tLoss 1.7786 (2.3796)\tPrec@1 90.625 (89.415)\tPrec@5 100.000 (99.556)\n",
      "Epoch: [312][60/97], lr: 0.00000\tTime 0.319 (0.332)\tData 0.000 (0.021)\tLoss 4.7728 (2.4510)\tPrec@1 84.375 (89.306)\tPrec@5 97.656 (99.513)\n",
      "Epoch: [312][70/97], lr: 0.00000\tTime 0.322 (0.331)\tData 0.000 (0.020)\tLoss 1.5101 (2.4183)\tPrec@1 89.844 (89.074)\tPrec@5 97.656 (99.461)\n",
      "Epoch: [312][80/97], lr: 0.00000\tTime 0.321 (0.331)\tData 0.000 (0.020)\tLoss 1.6651 (2.4888)\tPrec@1 89.844 (88.947)\tPrec@5 100.000 (99.479)\n",
      "Epoch: [312][90/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.020)\tLoss 1.3531 (2.4393)\tPrec@1 90.625 (88.985)\tPrec@5 100.000 (99.468)\n",
      "Epoch: [312][96/97], lr: 0.00000\tTime 0.313 (0.330)\tData 0.000 (0.020)\tLoss 1.6234 (2.4353)\tPrec@1 92.373 (89.021)\tPrec@5 99.153 (99.468)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.369 (0.369)\tLoss 3.9847 (3.9847)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.101)\tLoss 4.7785 (5.5008)\tPrec@1 78.000 (80.636)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.088)\tLoss 6.1674 (5.5903)\tPrec@1 79.000 (79.905)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.083)\tLoss 6.8436 (5.7538)\tPrec@1 80.000 (79.710)\tPrec@5 97.000 (98.194)\n",
      "Test: [40/100]\tTime 0.073 (0.081)\tLoss 4.4134 (5.8196)\tPrec@1 86.000 (79.756)\tPrec@5 98.000 (98.098)\n",
      "Test: [50/100]\tTime 0.073 (0.079)\tLoss 5.4350 (5.8071)\tPrec@1 83.000 (80.353)\tPrec@5 99.000 (98.118)\n",
      "Test: [60/100]\tTime 0.074 (0.078)\tLoss 7.8995 (5.7413)\tPrec@1 83.000 (80.377)\tPrec@5 100.000 (98.262)\n",
      "Test: [70/100]\tTime 0.074 (0.078)\tLoss 5.0901 (5.7414)\tPrec@1 82.000 (80.423)\tPrec@5 100.000 (98.352)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.5682 (5.6836)\tPrec@1 81.000 (80.691)\tPrec@5 98.000 (98.395)\n",
      "Test: [90/100]\tTime 0.073 (0.077)\tLoss 3.3486 (5.7124)\tPrec@1 89.000 (80.462)\tPrec@5 99.000 (98.363)\n",
      "val Results: Prec@1 80.360 Prec@5 98.340 Loss 5.75448\n",
      "val Class Accuracy: [0.909,0.958,0.808,0.702,0.800,0.738,0.830,0.745,0.764,0.782]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [313][0/97], lr: 0.00000\tTime 0.501 (0.501)\tData 0.299 (0.299)\tLoss 2.0076 (2.0076)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [313][10/97], lr: 0.00000\tTime 0.327 (0.345)\tData 0.000 (0.042)\tLoss 2.1826 (2.7202)\tPrec@1 88.281 (88.423)\tPrec@5 99.219 (99.574)\n",
      "Epoch: [313][20/97], lr: 0.00000\tTime 0.320 (0.336)\tData 0.000 (0.030)\tLoss 1.8724 (2.5334)\tPrec@1 92.188 (88.988)\tPrec@5 100.000 (99.591)\n",
      "Epoch: [313][30/97], lr: 0.00000\tTime 0.323 (0.333)\tData 0.000 (0.026)\tLoss 3.3289 (2.5764)\tPrec@1 89.062 (89.163)\tPrec@5 100.000 (99.597)\n",
      "Epoch: [313][40/97], lr: 0.00000\tTime 0.323 (0.331)\tData 0.000 (0.024)\tLoss 1.6236 (2.4454)\tPrec@1 89.844 (89.177)\tPrec@5 98.438 (99.428)\n",
      "Epoch: [313][50/97], lr: 0.00000\tTime 0.319 (0.330)\tData 0.000 (0.022)\tLoss 2.1744 (2.4854)\tPrec@1 82.812 (89.062)\tPrec@5 98.438 (99.403)\n",
      "Epoch: [313][60/97], lr: 0.00000\tTime 0.327 (0.329)\tData 0.000 (0.021)\tLoss 0.4821 (2.3560)\tPrec@1 95.312 (89.331)\tPrec@5 100.000 (99.436)\n",
      "Epoch: [313][70/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.021)\tLoss 1.7711 (2.4178)\tPrec@1 89.062 (89.338)\tPrec@5 99.219 (99.417)\n",
      "Epoch: [313][80/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.020)\tLoss 1.9165 (2.4567)\tPrec@1 90.625 (89.265)\tPrec@5 98.438 (99.402)\n",
      "Epoch: [313][90/97], lr: 0.00000\tTime 0.324 (0.328)\tData 0.000 (0.020)\tLoss 3.3804 (2.4068)\tPrec@1 85.156 (89.414)\tPrec@5 98.438 (99.425)\n",
      "Epoch: [313][96/97], lr: 0.00000\tTime 0.313 (0.328)\tData 0.000 (0.020)\tLoss 2.5481 (2.4385)\tPrec@1 86.441 (89.312)\tPrec@5 100.000 (99.428)\n",
      "Gated Network Weight Gate= Flip:0.56, Sc:0.44\n",
      "Test: [0/100]\tTime 0.365 (0.365)\tLoss 4.0946 (4.0946)\tPrec@1 84.000 (84.000)\tPrec@5 99.000 (99.000)\n",
      "Test: [10/100]\tTime 0.073 (0.100)\tLoss 4.7424 (5.5650)\tPrec@1 77.000 (79.909)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.073 (0.087)\tLoss 6.2849 (5.6588)\tPrec@1 77.000 (79.381)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.083)\tLoss 6.8215 (5.8221)\tPrec@1 80.000 (79.290)\tPrec@5 97.000 (98.258)\n",
      "Test: [40/100]\tTime 0.074 (0.081)\tLoss 4.4190 (5.8787)\tPrec@1 85.000 (79.439)\tPrec@5 98.000 (98.195)\n",
      "Test: [50/100]\tTime 0.073 (0.079)\tLoss 5.4341 (5.8681)\tPrec@1 85.000 (80.020)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 7.9970 (5.7962)\tPrec@1 83.000 (80.148)\tPrec@5 100.000 (98.344)\n",
      "Test: [70/100]\tTime 0.074 (0.078)\tLoss 5.1021 (5.7926)\tPrec@1 82.000 (80.141)\tPrec@5 100.000 (98.423)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.5761 (5.7349)\tPrec@1 83.000 (80.481)\tPrec@5 98.000 (98.469)\n",
      "Test: [90/100]\tTime 0.074 (0.077)\tLoss 3.3578 (5.7653)\tPrec@1 89.000 (80.242)\tPrec@5 99.000 (98.451)\n",
      "val Results: Prec@1 80.160 Prec@5 98.450 Loss 5.80832\n",
      "val Class Accuracy: [0.909,0.960,0.813,0.695,0.804,0.718,0.827,0.756,0.755,0.779]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [314][0/97], lr: 0.00000\tTime 0.456 (0.456)\tData 0.263 (0.263)\tLoss 2.5780 (2.5780)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [314][10/97], lr: 0.00000\tTime 0.321 (0.341)\tData 0.000 (0.038)\tLoss 3.0028 (2.2686)\tPrec@1 88.281 (89.205)\tPrec@5 100.000 (99.219)\n",
      "Epoch: [314][20/97], lr: 0.00000\tTime 0.340 (0.333)\tData 0.000 (0.028)\tLoss 1.3462 (2.3044)\tPrec@1 91.406 (89.174)\tPrec@5 100.000 (99.182)\n",
      "Epoch: [314][30/97], lr: 0.00000\tTime 0.336 (0.331)\tData 0.000 (0.025)\tLoss 2.5036 (2.4415)\tPrec@1 87.500 (89.012)\tPrec@5 100.000 (99.345)\n",
      "Epoch: [314][40/97], lr: 0.00000\tTime 0.327 (0.330)\tData 0.000 (0.023)\tLoss 1.7762 (2.5031)\tPrec@1 90.625 (89.062)\tPrec@5 99.219 (99.333)\n",
      "Epoch: [314][50/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.022)\tLoss 1.0003 (2.4380)\tPrec@1 93.750 (89.461)\tPrec@5 100.000 (99.403)\n",
      "Epoch: [314][60/97], lr: 0.00000\tTime 0.327 (0.329)\tData 0.000 (0.021)\tLoss 2.1254 (2.4812)\tPrec@1 89.062 (89.395)\tPrec@5 99.219 (99.424)\n",
      "Epoch: [314][70/97], lr: 0.00000\tTime 0.323 (0.329)\tData 0.000 (0.020)\tLoss 1.4758 (2.5204)\tPrec@1 90.625 (89.250)\tPrec@5 98.438 (99.406)\n",
      "Epoch: [314][80/97], lr: 0.00000\tTime 0.325 (0.329)\tData 0.000 (0.020)\tLoss 1.0357 (2.4774)\tPrec@1 89.062 (89.342)\tPrec@5 98.438 (99.412)\n",
      "Epoch: [314][90/97], lr: 0.00000\tTime 0.324 (0.329)\tData 0.000 (0.020)\tLoss 1.9022 (2.4771)\tPrec@1 91.406 (89.406)\tPrec@5 100.000 (99.451)\n",
      "Epoch: [314][96/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.020)\tLoss 1.5110 (2.4640)\tPrec@1 88.136 (89.392)\tPrec@5 99.153 (99.460)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.361 (0.361)\tLoss 4.1533 (4.1533)\tPrec@1 82.000 (82.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.100)\tLoss 4.8710 (5.6331)\tPrec@1 78.000 (80.091)\tPrec@5 99.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.087)\tLoss 6.2276 (5.7192)\tPrec@1 79.000 (79.476)\tPrec@5 97.000 (98.476)\n",
      "Test: [30/100]\tTime 0.073 (0.083)\tLoss 6.9432 (5.8838)\tPrec@1 79.000 (79.387)\tPrec@5 97.000 (98.419)\n",
      "Test: [40/100]\tTime 0.073 (0.081)\tLoss 4.5280 (5.9446)\tPrec@1 84.000 (79.463)\tPrec@5 98.000 (98.268)\n",
      "Test: [50/100]\tTime 0.073 (0.079)\tLoss 5.4766 (5.9362)\tPrec@1 87.000 (80.118)\tPrec@5 99.000 (98.275)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 7.9928 (5.8704)\tPrec@1 82.000 (80.197)\tPrec@5 100.000 (98.393)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.1954 (5.8729)\tPrec@1 82.000 (80.239)\tPrec@5 100.000 (98.451)\n",
      "Test: [80/100]\tTime 0.073 (0.077)\tLoss 6.7827 (5.8224)\tPrec@1 81.000 (80.519)\tPrec@5 98.000 (98.481)\n",
      "Test: [90/100]\tTime 0.073 (0.077)\tLoss 3.4672 (5.8513)\tPrec@1 88.000 (80.275)\tPrec@5 99.000 (98.440)\n",
      "val Results: Prec@1 80.170 Prec@5 98.420 Loss 5.89259\n",
      "val Class Accuracy: [0.904,0.962,0.804,0.713,0.797,0.731,0.835,0.743,0.752,0.776]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [315][0/97], lr: 0.00000\tTime 0.483 (0.483)\tData 0.266 (0.266)\tLoss 1.7986 (1.7986)\tPrec@1 87.500 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [315][10/97], lr: 0.00000\tTime 0.322 (0.346)\tData 0.000 (0.039)\tLoss 1.8864 (2.0767)\tPrec@1 89.062 (89.631)\tPrec@5 99.219 (99.361)\n",
      "Epoch: [315][20/97], lr: 0.00000\tTime 0.321 (0.334)\tData 0.000 (0.028)\tLoss 2.4376 (2.2620)\tPrec@1 91.406 (89.025)\tPrec@5 99.219 (99.256)\n",
      "Epoch: [315][30/97], lr: 0.00000\tTime 0.322 (0.330)\tData 0.000 (0.025)\tLoss 2.4146 (2.1999)\tPrec@1 86.719 (89.012)\tPrec@5 99.219 (99.294)\n",
      "Epoch: [315][40/97], lr: 0.00000\tTime 0.324 (0.329)\tData 0.000 (0.023)\tLoss 1.7552 (2.3986)\tPrec@1 89.844 (89.367)\tPrec@5 100.000 (99.371)\n",
      "Epoch: [315][50/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.022)\tLoss 3.3197 (2.4821)\tPrec@1 85.938 (89.139)\tPrec@5 99.219 (99.387)\n",
      "Epoch: [315][60/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.021)\tLoss 2.5055 (2.4565)\tPrec@1 89.844 (89.344)\tPrec@5 100.000 (99.424)\n",
      "Epoch: [315][70/97], lr: 0.00000\tTime 0.324 (0.328)\tData 0.000 (0.020)\tLoss 1.6944 (2.4451)\tPrec@1 90.625 (89.217)\tPrec@5 100.000 (99.450)\n",
      "Epoch: [315][80/97], lr: 0.00000\tTime 0.326 (0.328)\tData 0.000 (0.020)\tLoss 4.3071 (2.4930)\tPrec@1 82.812 (89.111)\tPrec@5 99.219 (99.450)\n",
      "Epoch: [315][90/97], lr: 0.00000\tTime 0.323 (0.328)\tData 0.000 (0.020)\tLoss 1.5685 (2.5070)\tPrec@1 92.969 (89.183)\tPrec@5 99.219 (99.451)\n",
      "Epoch: [315][96/97], lr: 0.00000\tTime 0.312 (0.328)\tData 0.000 (0.020)\tLoss 5.0028 (2.5533)\tPrec@1 83.051 (89.038)\tPrec@5 98.305 (99.436)\n",
      "Gated Network Weight Gate= Flip:0.55, Sc:0.45\n",
      "Test: [0/100]\tTime 0.299 (0.299)\tLoss 4.1533 (4.1533)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.094)\tLoss 4.9312 (5.6335)\tPrec@1 78.000 (80.636)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.074 (0.084)\tLoss 6.2313 (5.7150)\tPrec@1 79.000 (79.857)\tPrec@5 97.000 (98.286)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.9054 (5.8813)\tPrec@1 81.000 (79.613)\tPrec@5 97.000 (98.226)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.5529 (5.9429)\tPrec@1 85.000 (79.732)\tPrec@5 98.000 (98.122)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.5056 (5.9348)\tPrec@1 86.000 (80.176)\tPrec@5 99.000 (98.137)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 8.0836 (5.8652)\tPrec@1 81.000 (80.131)\tPrec@5 100.000 (98.279)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.1492 (5.8663)\tPrec@1 82.000 (80.183)\tPrec@5 100.000 (98.366)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 6.6762 (5.8100)\tPrec@1 82.000 (80.519)\tPrec@5 98.000 (98.407)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.4549 (5.8404)\tPrec@1 88.000 (80.275)\tPrec@5 99.000 (98.363)\n",
      "val Results: Prec@1 80.200 Prec@5 98.350 Loss 5.88423\n",
      "val Class Accuracy: [0.907,0.961,0.807,0.685,0.802,0.753,0.833,0.746,0.749,0.777]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [316][0/97], lr: 0.00000\tTime 0.438 (0.438)\tData 0.233 (0.233)\tLoss 1.4649 (1.4649)\tPrec@1 89.844 (89.844)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [316][10/97], lr: 0.00000\tTime 0.323 (0.342)\tData 0.000 (0.035)\tLoss 1.9036 (2.6633)\tPrec@1 91.406 (89.773)\tPrec@5 100.000 (99.716)\n",
      "Epoch: [316][20/97], lr: 0.00000\tTime 0.338 (0.334)\tData 0.000 (0.026)\tLoss 2.1870 (2.5948)\tPrec@1 86.719 (89.397)\tPrec@5 97.656 (99.405)\n",
      "Epoch: [316][30/97], lr: 0.00000\tTime 0.324 (0.332)\tData 0.000 (0.023)\tLoss 2.4316 (2.5351)\tPrec@1 89.844 (89.390)\tPrec@5 100.000 (99.420)\n",
      "Epoch: [316][40/97], lr: 0.00000\tTime 0.323 (0.332)\tData 0.000 (0.022)\tLoss 1.5472 (2.6342)\tPrec@1 91.406 (88.986)\tPrec@5 99.219 (99.371)\n",
      "Epoch: [316][50/97], lr: 0.00000\tTime 0.327 (0.331)\tData 0.000 (0.021)\tLoss 5.3391 (2.5788)\tPrec@1 87.500 (89.032)\tPrec@5 99.219 (99.387)\n",
      "Epoch: [316][60/97], lr: 0.00000\tTime 0.326 (0.331)\tData 0.000 (0.020)\tLoss 1.6835 (2.6524)\tPrec@1 88.281 (88.755)\tPrec@5 99.219 (99.360)\n",
      "Epoch: [316][70/97], lr: 0.00000\tTime 0.320 (0.330)\tData 0.000 (0.020)\tLoss 5.0286 (2.6769)\tPrec@1 86.719 (88.853)\tPrec@5 100.000 (99.362)\n",
      "Epoch: [316][80/97], lr: 0.00000\tTime 0.326 (0.330)\tData 0.000 (0.019)\tLoss 1.9522 (2.6122)\tPrec@1 85.156 (88.937)\tPrec@5 99.219 (99.392)\n",
      "Epoch: [316][90/97], lr: 0.00000\tTime 0.320 (0.330)\tData 0.000 (0.019)\tLoss 1.8737 (2.5998)\tPrec@1 88.281 (88.951)\tPrec@5 98.438 (99.399)\n",
      "Epoch: [316][96/97], lr: 0.00000\tTime 0.312 (0.329)\tData 0.000 (0.020)\tLoss 2.3949 (2.5746)\tPrec@1 85.593 (88.933)\tPrec@5 100.000 (99.387)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.365 (0.365)\tLoss 3.8934 (3.8934)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.100)\tLoss 4.5526 (5.3908)\tPrec@1 78.000 (81.000)\tPrec@5 99.000 (98.636)\n",
      "Test: [20/100]\tTime 0.073 (0.087)\tLoss 6.0997 (5.4874)\tPrec@1 79.000 (80.000)\tPrec@5 97.000 (98.524)\n",
      "Test: [30/100]\tTime 0.073 (0.083)\tLoss 6.7487 (5.6414)\tPrec@1 81.000 (79.903)\tPrec@5 97.000 (98.355)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 4.3115 (5.7015)\tPrec@1 84.000 (79.854)\tPrec@5 98.000 (98.220)\n",
      "Test: [50/100]\tTime 0.073 (0.079)\tLoss 5.2017 (5.6815)\tPrec@1 85.000 (80.353)\tPrec@5 99.000 (98.255)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 7.8171 (5.6135)\tPrec@1 83.000 (80.410)\tPrec@5 99.000 (98.344)\n",
      "Test: [70/100]\tTime 0.074 (0.078)\tLoss 4.9243 (5.6034)\tPrec@1 80.000 (80.423)\tPrec@5 100.000 (98.423)\n",
      "Test: [80/100]\tTime 0.073 (0.077)\tLoss 6.3460 (5.5450)\tPrec@1 83.000 (80.691)\tPrec@5 98.000 (98.457)\n",
      "Test: [90/100]\tTime 0.073 (0.077)\tLoss 3.2041 (5.5750)\tPrec@1 88.000 (80.484)\tPrec@5 99.000 (98.396)\n",
      "val Results: Prec@1 80.350 Prec@5 98.420 Loss 5.61659\n",
      "val Class Accuracy: [0.903,0.959,0.809,0.677,0.799,0.736,0.846,0.745,0.766,0.795]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [317][0/97], lr: 0.00000\tTime 0.506 (0.506)\tData 0.274 (0.274)\tLoss 2.1615 (2.1615)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [317][10/97], lr: 0.00000\tTime 0.336 (0.350)\tData 0.000 (0.039)\tLoss 3.3577 (2.2463)\tPrec@1 89.062 (89.844)\tPrec@5 98.438 (99.645)\n",
      "Epoch: [317][20/97], lr: 0.00000\tTime 0.318 (0.339)\tData 0.000 (0.028)\tLoss 1.4184 (2.3322)\tPrec@1 92.969 (90.030)\tPrec@5 100.000 (99.591)\n",
      "Epoch: [317][30/97], lr: 0.00000\tTime 0.324 (0.334)\tData 0.000 (0.025)\tLoss 2.8530 (2.3301)\tPrec@1 90.625 (89.214)\tPrec@5 99.219 (99.622)\n",
      "Epoch: [317][40/97], lr: 0.00000\tTime 0.326 (0.332)\tData 0.000 (0.023)\tLoss 1.9931 (2.2759)\tPrec@1 94.531 (88.948)\tPrec@5 100.000 (99.562)\n",
      "Epoch: [317][50/97], lr: 0.00000\tTime 0.322 (0.331)\tData 0.000 (0.022)\tLoss 2.9464 (2.2604)\tPrec@1 85.156 (89.001)\tPrec@5 100.000 (99.556)\n",
      "Epoch: [317][60/97], lr: 0.00000\tTime 0.323 (0.331)\tData 0.000 (0.021)\tLoss 4.0425 (2.3209)\tPrec@1 85.156 (88.960)\tPrec@5 99.219 (99.526)\n",
      "Epoch: [317][70/97], lr: 0.00000\tTime 0.321 (0.330)\tData 0.000 (0.020)\tLoss 1.9335 (2.3319)\tPrec@1 89.062 (88.743)\tPrec@5 100.000 (99.505)\n",
      "Epoch: [317][80/97], lr: 0.00000\tTime 0.327 (0.330)\tData 0.000 (0.020)\tLoss 3.0104 (2.3967)\tPrec@1 82.812 (88.571)\tPrec@5 99.219 (99.518)\n",
      "Epoch: [317][90/97], lr: 0.00000\tTime 0.324 (0.330)\tData 0.000 (0.020)\tLoss 2.6689 (2.4239)\tPrec@1 85.938 (88.556)\tPrec@5 99.219 (99.493)\n",
      "Epoch: [317][96/97], lr: 0.00000\tTime 0.320 (0.329)\tData 0.000 (0.020)\tLoss 2.8876 (2.4496)\tPrec@1 92.373 (88.562)\tPrec@5 100.000 (99.468)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.321 (0.321)\tLoss 3.9629 (3.9629)\tPrec@1 83.000 (83.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.074 (0.097)\tLoss 4.7998 (5.4757)\tPrec@1 78.000 (80.636)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.073 (0.086)\tLoss 6.0708 (5.5490)\tPrec@1 79.000 (79.810)\tPrec@5 97.000 (98.238)\n",
      "Test: [30/100]\tTime 0.073 (0.082)\tLoss 6.8007 (5.7185)\tPrec@1 81.000 (79.742)\tPrec@5 97.000 (98.161)\n",
      "Test: [40/100]\tTime 0.074 (0.080)\tLoss 4.4104 (5.7864)\tPrec@1 85.000 (79.829)\tPrec@5 98.000 (98.073)\n",
      "Test: [50/100]\tTime 0.074 (0.079)\tLoss 5.3326 (5.7701)\tPrec@1 85.000 (80.294)\tPrec@5 99.000 (98.098)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 7.8571 (5.7012)\tPrec@1 81.000 (80.262)\tPrec@5 100.000 (98.230)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 4.9996 (5.6953)\tPrec@1 82.000 (80.324)\tPrec@5 100.000 (98.310)\n",
      "Test: [80/100]\tTime 0.074 (0.077)\tLoss 6.4448 (5.6382)\tPrec@1 81.000 (80.605)\tPrec@5 98.000 (98.358)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.2682 (5.6643)\tPrec@1 88.000 (80.374)\tPrec@5 99.000 (98.330)\n",
      "val Results: Prec@1 80.260 Prec@5 98.340 Loss 5.70724\n",
      "val Class Accuracy: [0.898,0.960,0.804,0.682,0.797,0.755,0.842,0.740,0.761,0.787]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [318][0/97], lr: 0.00000\tTime 0.458 (0.458)\tData 0.248 (0.248)\tLoss 4.6211 (4.6211)\tPrec@1 85.938 (85.938)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [318][10/97], lr: 0.00000\tTime 0.321 (0.342)\tData 0.000 (0.036)\tLoss 3.5924 (2.6561)\tPrec@1 89.062 (88.068)\tPrec@5 99.219 (99.574)\n",
      "Epoch: [318][20/97], lr: 0.00000\tTime 0.349 (0.335)\tData 0.000 (0.027)\tLoss 1.3859 (2.5048)\tPrec@1 91.406 (88.802)\tPrec@5 100.000 (99.665)\n",
      "Epoch: [318][30/97], lr: 0.00000\tTime 0.325 (0.332)\tData 0.000 (0.024)\tLoss 1.7370 (2.7402)\tPrec@1 87.500 (88.584)\tPrec@5 99.219 (99.546)\n",
      "Epoch: [318][40/97], lr: 0.00000\tTime 0.320 (0.331)\tData 0.000 (0.022)\tLoss 3.0288 (2.6056)\tPrec@1 88.281 (88.891)\tPrec@5 100.000 (99.543)\n",
      "Epoch: [318][50/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.021)\tLoss 2.8207 (2.6290)\tPrec@1 89.844 (89.093)\tPrec@5 99.219 (99.464)\n",
      "Epoch: [318][60/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 1.9027 (2.6398)\tPrec@1 89.844 (89.255)\tPrec@5 99.219 (99.501)\n",
      "Epoch: [318][70/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.020)\tLoss 1.6484 (2.6364)\tPrec@1 88.281 (89.162)\tPrec@5 100.000 (99.494)\n",
      "Epoch: [318][80/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.020)\tLoss 1.2984 (2.6075)\tPrec@1 89.844 (89.072)\tPrec@5 100.000 (99.498)\n",
      "Epoch: [318][90/97], lr: 0.00000\tTime 0.321 (0.328)\tData 0.000 (0.019)\tLoss 2.2335 (2.5993)\tPrec@1 89.062 (89.054)\tPrec@5 99.219 (99.493)\n",
      "Epoch: [318][96/97], lr: 0.00000\tTime 0.318 (0.328)\tData 0.000 (0.020)\tLoss 2.2950 (2.5500)\tPrec@1 89.831 (89.150)\tPrec@5 100.000 (99.508)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.316 (0.316)\tLoss 4.1840 (4.1840)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.095)\tLoss 4.7338 (5.6256)\tPrec@1 78.000 (80.545)\tPrec@5 99.000 (98.364)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 6.2233 (5.7056)\tPrec@1 80.000 (79.952)\tPrec@5 97.000 (98.238)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 6.8495 (5.8681)\tPrec@1 80.000 (79.839)\tPrec@5 97.000 (98.194)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 4.4791 (5.9219)\tPrec@1 85.000 (79.878)\tPrec@5 98.000 (98.171)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 5.4845 (5.9164)\tPrec@1 86.000 (80.510)\tPrec@5 99.000 (98.216)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 8.0434 (5.8504)\tPrec@1 83.000 (80.557)\tPrec@5 100.000 (98.344)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 5.2221 (5.8507)\tPrec@1 82.000 (80.507)\tPrec@5 100.000 (98.423)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.6736 (5.7954)\tPrec@1 81.000 (80.765)\tPrec@5 98.000 (98.457)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 3.4627 (5.8245)\tPrec@1 89.000 (80.505)\tPrec@5 99.000 (98.407)\n",
      "val Results: Prec@1 80.380 Prec@5 98.400 Loss 5.86785\n",
      "val Class Accuracy: [0.908,0.961,0.804,0.703,0.825,0.723,0.832,0.750,0.753,0.779]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "cifar_train_lorot-E.py:470: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gn_softmax = nn.Softmax()(gn_output.mean(dim=0))\n",
      "Epoch: [319][0/97], lr: 0.00000\tTime 0.463 (0.463)\tData 0.254 (0.254)\tLoss 3.5550 (3.5550)\tPrec@1 87.500 (87.500)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [319][10/97], lr: 0.00000\tTime 0.323 (0.342)\tData 0.000 (0.037)\tLoss 2.3262 (3.3548)\tPrec@1 86.719 (87.358)\tPrec@5 100.000 (99.503)\n",
      "Epoch: [319][20/97], lr: 0.00000\tTime 0.324 (0.333)\tData 0.000 (0.027)\tLoss 2.5491 (2.9129)\tPrec@1 90.625 (88.207)\tPrec@5 99.219 (99.591)\n",
      "Epoch: [319][30/97], lr: 0.00000\tTime 0.319 (0.330)\tData 0.000 (0.024)\tLoss 3.3697 (2.6362)\tPrec@1 88.281 (88.760)\tPrec@5 99.219 (99.572)\n",
      "Epoch: [319][40/97], lr: 0.00000\tTime 0.345 (0.330)\tData 0.000 (0.022)\tLoss 2.3447 (2.4875)\tPrec@1 89.062 (88.891)\tPrec@5 98.438 (99.543)\n",
      "Epoch: [319][50/97], lr: 0.00000\tTime 0.322 (0.329)\tData 0.000 (0.021)\tLoss 1.7534 (2.6129)\tPrec@1 92.969 (88.756)\tPrec@5 99.219 (99.510)\n",
      "Epoch: [319][60/97], lr: 0.00000\tTime 0.321 (0.329)\tData 0.000 (0.020)\tLoss 2.1488 (2.6025)\tPrec@1 87.500 (88.742)\tPrec@5 100.000 (99.539)\n",
      "Epoch: [319][70/97], lr: 0.00000\tTime 0.320 (0.329)\tData 0.000 (0.020)\tLoss 3.1071 (2.6206)\tPrec@1 85.156 (88.622)\tPrec@5 98.438 (99.505)\n",
      "Epoch: [319][80/97], lr: 0.00000\tTime 0.322 (0.328)\tData 0.000 (0.020)\tLoss 2.5057 (2.5754)\tPrec@1 86.719 (88.889)\tPrec@5 99.219 (99.518)\n",
      "Epoch: [319][90/97], lr: 0.00000\tTime 0.320 (0.328)\tData 0.000 (0.019)\tLoss 2.5110 (2.5427)\tPrec@1 91.406 (89.020)\tPrec@5 98.438 (99.511)\n",
      "Epoch: [319][96/97], lr: 0.00000\tTime 0.316 (0.328)\tData 0.000 (0.020)\tLoss 2.3725 (2.5474)\tPrec@1 88.136 (88.892)\tPrec@5 100.000 (99.524)\n",
      "Gated Network Weight Gate= Flip:0.57, Sc:0.43\n",
      "Test: [0/100]\tTime 0.298 (0.298)\tLoss 4.0678 (4.0678)\tPrec@1 84.000 (84.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/100]\tTime 0.073 (0.094)\tLoss 4.7488 (5.5520)\tPrec@1 78.000 (81.000)\tPrec@5 99.000 (98.273)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 6.0991 (5.6231)\tPrec@1 80.000 (80.238)\tPrec@5 97.000 (98.238)\n",
      "Test: [30/100]\tTime 0.074 (0.081)\tLoss 6.7967 (5.7973)\tPrec@1 81.000 (80.065)\tPrec@5 97.000 (98.194)\n",
      "Test: [40/100]\tTime 0.074 (0.079)\tLoss 4.4710 (5.8618)\tPrec@1 86.000 (80.122)\tPrec@5 98.000 (98.098)\n",
      "Test: [50/100]\tTime 0.074 (0.078)\tLoss 5.4408 (5.8491)\tPrec@1 86.000 (80.471)\tPrec@5 99.000 (98.176)\n",
      "Test: [60/100]\tTime 0.074 (0.077)\tLoss 7.9819 (5.7786)\tPrec@1 81.000 (80.393)\tPrec@5 100.000 (98.295)\n",
      "Test: [70/100]\tTime 0.074 (0.077)\tLoss 5.1174 (5.7763)\tPrec@1 82.000 (80.437)\tPrec@5 100.000 (98.380)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 6.5203 (5.7184)\tPrec@1 81.000 (80.691)\tPrec@5 98.000 (98.432)\n",
      "Test: [90/100]\tTime 0.074 (0.076)\tLoss 3.2758 (5.7426)\tPrec@1 89.000 (80.429)\tPrec@5 99.000 (98.374)\n",
      "val Results: Prec@1 80.310 Prec@5 98.380 Loss 5.78697\n",
      "val Class Accuracy: [0.902,0.961,0.808,0.680,0.811,0.752,0.840,0.738,0.762,0.777]\n",
      "Best Prec@1: 80.460\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"cifar_train_lorot-E.py\", line 656, in <module>\n",
      "    main()\n",
      "  File \"cifar_train_lorot-E.py\", line 181, in main\n",
      "    main_worker(args.gpu, ngpus_per_node, args)\n",
      "  File \"cifar_train_lorot-E.py\", line 337, in main_worker\n",
      "    effective_num = 1.0 - np.power(betas[idx], cls_num_list)\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "!python cifar_train_lorot-E.py --gpu 0 --imb_type exp --exp_str flipscLdamDrw --imb_factor 0.01 --loss_type LDAM --train_rule DRW -g -m \"fliplr sc\" --resume \"/media/aristo/Data A/documents/kuliah/Project/Localizable-Rotation/Imbalanced/checkpoint/cifar10_resnet32_LDAM_DRW_exp_0.01_flipscLdamDrw/ckpt.pth.tar\" --epochs 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar_train_lorot-E.py:175: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.\n",
      "  warnings.warn(\n",
      "Use GPU: 0 for training\n",
      "=> creating model 'resnet32'\n",
      "num_trans : 16\n",
      "num_flipped : 4\n",
      "num shuffled channel : 24\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[5000, 2997, 1796, 1077, 645, 387, 232, 139, 83, 50]\n",
      "/media/aristo/Data A/documents/kuliah/Project/Localizable-Rotation/Imbalanced/losses.py:49: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/TensorCompare.cpp:402.)\n",
      "  output = torch.where(index, x_m, x)\n",
      "Epoch: [0][0/97], lr: 0.00200\tTime 2.554 (2.554)\tData 0.168 (0.168)\tLoss 13.8618 (13.8618)\tPrec@1 3.125 (3.125)\tPrec@5 28.125 (28.125)\n",
      "Epoch: [0][10/97], lr: 0.00200\tTime 0.350 (0.531)\tData 0.000 (0.016)\tLoss 9.5048 (9.8691)\tPrec@1 35.156 (29.048)\tPrec@5 75.781 (77.202)\n",
      "Epoch: [0][20/97], lr: 0.00200\tTime 0.343 (0.448)\tData 0.000 (0.016)\tLoss 7.9449 (8.9049)\tPrec@1 46.094 (35.528)\tPrec@5 76.562 (81.287)\n",
      "Epoch: [0][30/97], lr: 0.00200\tTime 0.350 (0.416)\tData 0.000 (0.016)\tLoss 7.5453 (8.5232)\tPrec@1 45.312 (36.618)\tPrec@5 83.594 (83.997)\n",
      "Epoch: [0][40/97], lr: 0.00200\tTime 0.344 (0.399)\tData 0.001 (0.016)\tLoss 7.8433 (8.3265)\tPrec@1 39.844 (37.538)\tPrec@5 86.719 (85.290)\n",
      "Epoch: [0][50/97], lr: 0.00200\tTime 0.350 (0.394)\tData 0.000 (0.016)\tLoss 8.1087 (8.1337)\tPrec@1 41.406 (38.036)\tPrec@5 90.625 (86.795)\n",
      "Epoch: [0][60/97], lr: 0.00200\tTime 0.353 (0.388)\tData 0.000 (0.016)\tLoss 7.2709 (8.0655)\tPrec@1 37.500 (38.384)\tPrec@5 95.312 (87.013)\n",
      "Epoch: [0][70/97], lr: 0.00200\tTime 0.351 (0.384)\tData 0.000 (0.017)\tLoss 7.4554 (7.9516)\tPrec@1 39.844 (38.930)\tPrec@5 90.625 (87.951)\n",
      "Epoch: [0][80/97], lr: 0.00200\tTime 0.348 (0.382)\tData 0.000 (0.017)\tLoss 7.0807 (7.8783)\tPrec@1 42.969 (38.995)\tPrec@5 92.969 (88.628)\n",
      "Epoch: [0][90/97], lr: 0.00200\tTime 0.350 (0.379)\tData 0.000 (0.017)\tLoss 6.7520 (7.8371)\tPrec@1 43.750 (38.874)\tPrec@5 93.750 (88.771)\n",
      "Epoch: [0][96/97], lr: 0.00200\tTime 0.951 (0.384)\tData 0.000 (0.017)\tLoss 6.7257 (7.8005)\tPrec@1 44.915 (39.078)\tPrec@5 94.068 (88.820)\n",
      "Gated Network Weight Gate= Rot:0.00, Flip:0.00, Sc:0.00\n",
      "Test: [0/100]\tTime 0.328 (0.328)\tLoss 17.7608 (17.7608)\tPrec@1 10.000 (10.000)\tPrec@5 42.000 (42.000)\n",
      "Test: [10/100]\tTime 0.073 (0.096)\tLoss 14.1986 (16.4505)\tPrec@1 16.000 (10.818)\tPrec@5 56.000 (48.818)\n",
      "Test: [20/100]\tTime 0.073 (0.085)\tLoss 14.6279 (16.4416)\tPrec@1 10.000 (9.810)\tPrec@5 62.000 (49.333)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 15.1721 (16.3439)\tPrec@1 8.000 (9.903)\tPrec@5 52.000 (49.677)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 16.6171 (16.3795)\tPrec@1 8.000 (9.683)\tPrec@5 43.000 (49.415)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 15.4388 (16.2834)\tPrec@1 12.000 (9.804)\tPrec@5 49.000 (49.765)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 13.7771 (16.1391)\tPrec@1 16.000 (9.885)\tPrec@5 60.000 (49.738)\n",
      "Test: [70/100]\tTime 0.074 (0.076)\tLoss 15.6088 (16.1296)\tPrec@1 8.000 (10.028)\tPrec@5 49.000 (49.930)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 15.7375 (16.0799)\tPrec@1 13.000 (10.111)\tPrec@5 53.000 (50.346)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 16.5838 (16.1639)\tPrec@1 13.000 (9.956)\tPrec@5 53.000 (50.088)\n",
      "val Results: Prec@1 10.000 Prec@5 49.970 Loss 16.21026\n",
      "val Class Accuracy: [1.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 10.000\n",
      "\n",
      "Epoch: [1][0/97], lr: 0.00400\tTime 0.826 (0.826)\tData 0.416 (0.416)\tLoss 7.7464 (7.7464)\tPrec@1 38.281 (38.281)\tPrec@5 83.594 (83.594)\n",
      "Epoch: [1][10/97], lr: 0.00400\tTime 0.472 (0.534)\tData 0.001 (0.050)\tLoss 7.2942 (7.4366)\tPrec@1 32.812 (38.494)\tPrec@5 92.188 (92.472)\n",
      "Epoch: [1][20/97], lr: 0.00400\tTime 0.501 (0.519)\tData 0.001 (0.033)\tLoss 7.3338 (7.4745)\tPrec@1 42.188 (40.067)\tPrec@5 92.969 (91.146)\n",
      "Epoch: [1][30/97], lr: 0.00400\tTime 0.512 (0.515)\tData 0.000 (0.027)\tLoss 7.3938 (7.3981)\tPrec@1 39.062 (39.466)\tPrec@5 91.406 (91.809)\n",
      "Epoch: [1][40/97], lr: 0.00400\tTime 0.477 (0.502)\tData 0.001 (0.024)\tLoss 7.2344 (7.3766)\tPrec@1 37.500 (39.444)\tPrec@5 93.750 (91.825)\n",
      "Epoch: [1][50/97], lr: 0.00400\tTime 0.464 (0.501)\tData 0.001 (0.022)\tLoss 7.7988 (7.3199)\tPrec@1 31.250 (39.553)\tPrec@5 88.281 (92.019)\n",
      "Epoch: [1][60/97], lr: 0.00400\tTime 0.350 (0.483)\tData 0.000 (0.021)\tLoss 7.6040 (7.3400)\tPrec@1 43.750 (39.703)\tPrec@5 89.062 (91.752)\n",
      "Epoch: [1][70/97], lr: 0.00400\tTime 0.338 (0.464)\tData 0.000 (0.020)\tLoss 7.3469 (7.3207)\tPrec@1 41.406 (39.525)\tPrec@5 89.844 (91.714)\n",
      "Epoch: [1][80/97], lr: 0.00400\tTime 0.345 (0.449)\tData 0.000 (0.020)\tLoss 7.2271 (7.3093)\tPrec@1 36.719 (39.371)\tPrec@5 92.969 (91.705)\n",
      "Epoch: [1][90/97], lr: 0.00400\tTime 0.331 (0.438)\tData 0.000 (0.019)\tLoss 7.3687 (7.2942)\tPrec@1 36.719 (39.363)\tPrec@5 91.406 (91.629)\n",
      "Epoch: [1][96/97], lr: 0.00400\tTime 0.334 (0.432)\tData 0.000 (0.020)\tLoss 7.1964 (7.2776)\tPrec@1 38.136 (39.465)\tPrec@5 87.288 (91.593)\n",
      "Gated Network Weight Gate= Rot:0.00, Flip:0.00, Sc:0.00\n",
      "Test: [0/100]\tTime 0.267 (0.267)\tLoss 14.3708 (14.3708)\tPrec@1 10.000 (10.000)\tPrec@5 40.000 (40.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 12.5254 (13.5864)\tPrec@1 16.000 (10.818)\tPrec@5 51.000 (47.273)\n",
      "Test: [20/100]\tTime 0.073 (0.082)\tLoss 12.5699 (13.5584)\tPrec@1 10.000 (9.810)\tPrec@5 59.000 (49.429)\n",
      "Test: [30/100]\tTime 0.074 (0.079)\tLoss 12.9583 (13.4959)\tPrec@1 8.000 (9.903)\tPrec@5 51.000 (50.097)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 13.7085 (13.5295)\tPrec@1 8.000 (9.683)\tPrec@5 46.000 (49.976)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 13.1828 (13.4894)\tPrec@1 12.000 (9.804)\tPrec@5 53.000 (50.314)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 11.8785 (13.4568)\tPrec@1 16.000 (9.885)\tPrec@5 60.000 (50.377)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 13.1456 (13.4268)\tPrec@1 8.000 (10.028)\tPrec@5 50.000 (50.521)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 13.0157 (13.3920)\tPrec@1 13.000 (10.111)\tPrec@5 53.000 (50.778)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 13.1341 (13.4244)\tPrec@1 13.000 (9.956)\tPrec@5 54.000 (50.330)\n",
      "val Results: Prec@1 10.000 Prec@5 50.270 Loss 13.44378\n",
      "val Class Accuracy: [1.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 10.000\n",
      "\n",
      "Epoch: [2][0/97], lr: 0.00600\tTime 0.545 (0.545)\tData 0.266 (0.266)\tLoss 7.3806 (7.3806)\tPrec@1 40.625 (40.625)\tPrec@5 84.375 (84.375)\n",
      "Epoch: [2][10/97], lr: 0.00600\tTime 0.349 (0.386)\tData 0.000 (0.039)\tLoss 6.9361 (7.0806)\tPrec@1 40.625 (40.412)\tPrec@5 95.312 (91.548)\n",
      "Epoch: [2][20/97], lr: 0.00600\tTime 0.356 (0.373)\tData 0.000 (0.028)\tLoss 7.4543 (7.1315)\tPrec@1 33.594 (40.030)\tPrec@5 90.625 (91.146)\n",
      "Epoch: [2][30/97], lr: 0.00600\tTime 0.345 (0.368)\tData 0.000 (0.024)\tLoss 7.1245 (7.1211)\tPrec@1 42.188 (40.348)\tPrec@5 91.406 (91.003)\n",
      "Epoch: [2][40/97], lr: 0.00600\tTime 0.345 (0.367)\tData 0.000 (0.023)\tLoss 7.4039 (7.1271)\tPrec@1 33.594 (39.748)\tPrec@5 90.625 (91.044)\n",
      "Epoch: [2][50/97], lr: 0.00600\tTime 0.370 (0.366)\tData 0.000 (0.021)\tLoss 7.5415 (7.1258)\tPrec@1 27.344 (39.614)\tPrec@5 90.625 (91.222)\n",
      "Epoch: [2][60/97], lr: 0.00600\tTime 0.337 (0.364)\tData 0.000 (0.021)\tLoss 6.8561 (7.1004)\tPrec@1 38.281 (39.562)\tPrec@5 92.969 (91.381)\n",
      "Epoch: [2][70/97], lr: 0.00600\tTime 0.348 (0.362)\tData 0.000 (0.020)\tLoss 6.8423 (7.0846)\tPrec@1 39.844 (39.734)\tPrec@5 92.188 (91.329)\n",
      "Epoch: [2][80/97], lr: 0.00600\tTime 0.347 (0.361)\tData 0.000 (0.020)\tLoss 7.0694 (7.0808)\tPrec@1 40.625 (39.709)\tPrec@5 92.188 (91.300)\n",
      "Epoch: [2][90/97], lr: 0.00600\tTime 0.348 (0.360)\tData 0.000 (0.019)\tLoss 7.0134 (7.1000)\tPrec@1 42.188 (39.526)\tPrec@5 92.188 (91.183)\n",
      "Epoch: [2][96/97], lr: 0.00600\tTime 0.359 (0.361)\tData 0.000 (0.020)\tLoss 7.3654 (7.0924)\tPrec@1 33.051 (39.562)\tPrec@5 90.678 (91.311)\n",
      "Gated Network Weight Gate= Rot:0.00, Flip:0.00, Sc:0.00\n",
      "Test: [0/100]\tTime 0.369 (0.369)\tLoss 13.2872 (13.2872)\tPrec@1 10.000 (10.000)\tPrec@5 41.000 (41.000)\n",
      "Test: [10/100]\tTime 0.073 (0.100)\tLoss 11.6474 (12.5435)\tPrec@1 16.000 (13.545)\tPrec@5 53.000 (48.909)\n",
      "Test: [20/100]\tTime 0.073 (0.087)\tLoss 11.9512 (12.5379)\tPrec@1 13.000 (12.905)\tPrec@5 51.000 (49.381)\n",
      "Test: [30/100]\tTime 0.073 (0.083)\tLoss 12.0010 (12.4910)\tPrec@1 17.000 (12.548)\tPrec@5 53.000 (50.032)\n",
      "Test: [40/100]\tTime 0.073 (0.080)\tLoss 12.8687 (12.5156)\tPrec@1 14.000 (12.366)\tPrec@5 44.000 (49.756)\n",
      "Test: [50/100]\tTime 0.073 (0.079)\tLoss 12.2100 (12.4584)\tPrec@1 16.000 (12.784)\tPrec@5 49.000 (50.157)\n",
      "Test: [60/100]\tTime 0.073 (0.078)\tLoss 11.2388 (12.4336)\tPrec@1 13.000 (12.656)\tPrec@5 56.000 (50.115)\n",
      "Test: [70/100]\tTime 0.073 (0.077)\tLoss 12.1312 (12.4170)\tPrec@1 12.000 (12.732)\tPrec@5 57.000 (50.296)\n",
      "Test: [80/100]\tTime 0.073 (0.077)\tLoss 11.8863 (12.3833)\tPrec@1 17.000 (12.901)\tPrec@5 55.000 (50.469)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 12.2053 (12.4138)\tPrec@1 16.000 (12.824)\tPrec@5 51.000 (50.220)\n",
      "val Results: Prec@1 12.760 Prec@5 50.000 Loss 12.43359\n",
      "val Class Accuracy: [0.640,0.636,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 12.760\n",
      "\n",
      "Epoch: [3][0/97], lr: 0.00800\tTime 0.654 (0.654)\tData 0.337 (0.337)\tLoss 6.7875 (6.7875)\tPrec@1 39.062 (39.062)\tPrec@5 92.188 (92.188)\n",
      "Epoch: [3][10/97], lr: 0.00800\tTime 0.356 (0.395)\tData 0.000 (0.044)\tLoss 6.9150 (6.8920)\tPrec@1 45.312 (40.554)\tPrec@5 93.750 (92.543)\n",
      "Epoch: [3][20/97], lr: 0.00800\tTime 0.352 (0.377)\tData 0.000 (0.031)\tLoss 6.9794 (6.9510)\tPrec@1 36.719 (40.588)\tPrec@5 95.312 (90.923)\n",
      "Epoch: [3][30/97], lr: 0.00800\tTime 0.365 (0.372)\tData 0.000 (0.026)\tLoss 7.2903 (7.0093)\tPrec@1 40.625 (40.222)\tPrec@5 89.062 (90.146)\n",
      "Epoch: [3][40/97], lr: 0.00800\tTime 0.351 (0.370)\tData 0.000 (0.024)\tLoss 6.9210 (7.0079)\tPrec@1 39.062 (40.206)\tPrec@5 89.844 (90.644)\n",
      "Epoch: [3][50/97], lr: 0.00800\tTime 0.350 (0.368)\tData 0.000 (0.023)\tLoss 6.9466 (7.0202)\tPrec@1 35.156 (39.874)\tPrec@5 95.312 (90.885)\n",
      "Epoch: [3][60/97], lr: 0.00800\tTime 0.350 (0.366)\tData 0.000 (0.022)\tLoss 6.7001 (7.0139)\tPrec@1 40.625 (40.074)\tPrec@5 94.531 (91.060)\n",
      "Epoch: [3][70/97], lr: 0.00800\tTime 0.348 (0.364)\tData 0.000 (0.021)\tLoss 7.1891 (7.0170)\tPrec@1 41.406 (40.009)\tPrec@5 86.719 (91.098)\n",
      "Epoch: [3][80/97], lr: 0.00800\tTime 0.354 (0.363)\tData 0.000 (0.020)\tLoss 6.5055 (7.0047)\tPrec@1 43.750 (40.027)\tPrec@5 96.094 (91.377)\n",
      "Epoch: [3][90/97], lr: 0.00800\tTime 0.352 (0.362)\tData 0.000 (0.020)\tLoss 7.2960 (7.0099)\tPrec@1 32.812 (39.998)\tPrec@5 91.406 (91.449)\n",
      "Epoch: [3][96/97], lr: 0.00800\tTime 0.343 (0.361)\tData 0.000 (0.020)\tLoss 7.1025 (7.0070)\tPrec@1 43.220 (40.029)\tPrec@5 88.983 (91.432)\n",
      "Gated Network Weight Gate= Rot:0.00, Flip:0.00, Sc:0.00\n",
      "Test: [0/100]\tTime 0.284 (0.284)\tLoss 13.8959 (13.8959)\tPrec@1 10.000 (10.000)\tPrec@5 42.000 (42.000)\n",
      "Test: [10/100]\tTime 0.073 (0.092)\tLoss 12.0579 (13.0993)\tPrec@1 16.000 (10.818)\tPrec@5 54.000 (49.182)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 12.6283 (13.1523)\tPrec@1 10.000 (9.810)\tPrec@5 52.000 (49.571)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 12.6767 (13.1011)\tPrec@1 8.000 (9.903)\tPrec@5 53.000 (50.194)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 13.6190 (13.1290)\tPrec@1 8.000 (9.683)\tPrec@5 44.000 (49.927)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 12.7796 (13.0702)\tPrec@1 12.000 (9.804)\tPrec@5 49.000 (50.294)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 11.7249 (13.0436)\tPrec@1 16.000 (9.885)\tPrec@5 56.000 (50.246)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 12.8042 (13.0232)\tPrec@1 8.000 (10.014)\tPrec@5 57.000 (50.423)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 12.3718 (12.9820)\tPrec@1 13.000 (10.099)\tPrec@5 55.000 (50.593)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 12.6493 (13.0171)\tPrec@1 13.000 (9.945)\tPrec@5 51.000 (50.352)\n",
      "val Results: Prec@1 10.000 Prec@5 50.120 Loss 13.03509\n",
      "val Class Accuracy: [0.999,0.001,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 12.760\n",
      "\n",
      "Epoch: [4][0/97], lr: 0.01000\tTime 0.618 (0.618)\tData 0.352 (0.352)\tLoss 7.1211 (7.1211)\tPrec@1 39.062 (39.062)\tPrec@5 90.625 (90.625)\n",
      "Epoch: [4][10/97], lr: 0.01000\tTime 0.357 (0.394)\tData 0.000 (0.046)\tLoss 6.5801 (6.7468)\tPrec@1 45.312 (42.472)\tPrec@5 95.312 (93.466)\n",
      "Epoch: [4][20/97], lr: 0.01000\tTime 0.382 (0.382)\tData 0.000 (0.032)\tLoss 6.5997 (6.8434)\tPrec@1 41.406 (41.778)\tPrec@5 96.875 (92.820)\n",
      "Epoch: [4][30/97], lr: 0.01000\tTime 0.370 (0.400)\tData 0.000 (0.027)\tLoss 7.1828 (6.8961)\tPrec@1 39.062 (41.079)\tPrec@5 90.625 (92.112)\n",
      "Epoch: [4][40/97], lr: 0.01000\tTime 0.352 (0.390)\tData 0.000 (0.024)\tLoss 6.9596 (6.8934)\tPrec@1 37.500 (41.025)\tPrec@5 91.406 (91.959)\n",
      "Epoch: [4][50/97], lr: 0.01000\tTime 0.370 (0.386)\tData 0.000 (0.023)\tLoss 6.9352 (6.9142)\tPrec@1 42.969 (41.146)\tPrec@5 90.625 (91.866)\n",
      "Epoch: [4][60/97], lr: 0.01000\tTime 0.364 (0.382)\tData 0.000 (0.022)\tLoss 6.4757 (6.8842)\tPrec@1 51.562 (41.611)\tPrec@5 94.531 (91.931)\n",
      "Epoch: [4][70/97], lr: 0.01000\tTime 0.344 (0.378)\tData 0.000 (0.021)\tLoss 6.5550 (6.9078)\tPrec@1 48.438 (41.725)\tPrec@5 93.750 (91.725)\n",
      "Epoch: [4][80/97], lr: 0.01000\tTime 0.347 (0.375)\tData 0.000 (0.020)\tLoss 7.7217 (6.9238)\tPrec@1 36.719 (41.744)\tPrec@5 86.719 (91.339)\n",
      "Epoch: [4][90/97], lr: 0.01000\tTime 0.344 (0.372)\tData 0.000 (0.020)\tLoss 7.1415 (6.9257)\tPrec@1 38.281 (41.750)\tPrec@5 92.969 (91.441)\n",
      "Epoch: [4][96/97], lr: 0.01000\tTime 0.326 (0.371)\tData 0.000 (0.020)\tLoss 7.2485 (6.9231)\tPrec@1 37.288 (41.762)\tPrec@5 92.373 (91.512)\n",
      "Gated Network Weight Gate= Rot:0.00, Flip:0.00, Sc:0.00\n",
      "Test: [0/100]\tTime 0.238 (0.238)\tLoss 13.6279 (13.6279)\tPrec@1 11.000 (11.000)\tPrec@5 42.000 (42.000)\n",
      "Test: [10/100]\tTime 0.073 (0.088)\tLoss 11.7774 (12.8975)\tPrec@1 17.000 (11.182)\tPrec@5 54.000 (49.545)\n",
      "Test: [20/100]\tTime 0.073 (0.081)\tLoss 12.6646 (12.9784)\tPrec@1 9.000 (10.048)\tPrec@5 52.000 (49.905)\n",
      "Test: [30/100]\tTime 0.073 (0.078)\tLoss 12.3844 (12.9404)\tPrec@1 9.000 (10.387)\tPrec@5 54.000 (50.548)\n",
      "Test: [40/100]\tTime 0.073 (0.077)\tLoss 13.3154 (12.9681)\tPrec@1 10.000 (10.171)\tPrec@5 44.000 (50.317)\n",
      "Test: [50/100]\tTime 0.073 (0.076)\tLoss 12.5417 (12.9072)\tPrec@1 12.000 (10.255)\tPrec@5 49.000 (50.627)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 11.6327 (12.8700)\tPrec@1 17.000 (10.426)\tPrec@5 56.000 (50.557)\n",
      "Test: [70/100]\tTime 0.073 (0.075)\tLoss 12.5619 (12.8512)\tPrec@1 9.000 (10.549)\tPrec@5 57.000 (50.746)\n",
      "Test: [80/100]\tTime 0.073 (0.075)\tLoss 12.1668 (12.8135)\tPrec@1 16.000 (10.679)\tPrec@5 55.000 (50.938)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 12.6316 (12.8447)\tPrec@1 13.000 (10.505)\tPrec@5 51.000 (50.692)\n",
      "val Results: Prec@1 10.550 Prec@5 50.440 Loss 12.86358\n",
      "val Class Accuracy: [0.980,0.075,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 12.760\n",
      "\n",
      "Epoch: [5][0/97], lr: 0.01000\tTime 0.501 (0.501)\tData 0.260 (0.260)\tLoss 6.0641 (6.0641)\tPrec@1 50.000 (50.000)\tPrec@5 95.312 (95.312)\n",
      "Epoch: [5][10/97], lr: 0.01000\tTime 0.350 (0.379)\tData 0.000 (0.038)\tLoss 6.7667 (6.7151)\tPrec@1 42.969 (43.750)\tPrec@5 89.844 (92.472)\n",
      "Epoch: [5][20/97], lr: 0.01000\tTime 0.359 (0.368)\tData 0.000 (0.028)\tLoss 6.4104 (6.7652)\tPrec@1 45.312 (42.857)\tPrec@5 93.750 (92.411)\n",
      "Epoch: [5][30/97], lr: 0.01000\tTime 0.350 (0.365)\tData 0.000 (0.024)\tLoss 6.7259 (6.7749)\tPrec@1 39.844 (42.414)\tPrec@5 92.969 (92.742)\n",
      "Epoch: [5][40/97], lr: 0.01000\tTime 0.350 (0.365)\tData 0.000 (0.022)\tLoss 6.8032 (6.7669)\tPrec@1 39.844 (42.283)\tPrec@5 89.062 (92.435)\n",
      "Epoch: [5][50/97], lr: 0.01000\tTime 0.351 (0.365)\tData 0.000 (0.021)\tLoss 6.3942 (6.7869)\tPrec@1 42.188 (42.080)\tPrec@5 95.312 (92.494)\n",
      "Epoch: [5][60/97], lr: 0.01000\tTime 0.358 (0.364)\tData 0.000 (0.020)\tLoss 6.2398 (6.7928)\tPrec@1 43.750 (41.906)\tPrec@5 88.281 (92.533)\n",
      "Epoch: [5][70/97], lr: 0.01000\tTime 0.344 (0.363)\tData 0.000 (0.020)\tLoss 6.7272 (6.8183)\tPrec@1 42.969 (41.527)\tPrec@5 92.188 (92.176)\n",
      "Epoch: [5][80/97], lr: 0.01000\tTime 0.347 (0.361)\tData 0.000 (0.020)\tLoss 6.5830 (6.8131)\tPrec@1 46.094 (41.667)\tPrec@5 95.312 (92.178)\n",
      "Epoch: [5][90/97], lr: 0.01000\tTime 0.354 (0.360)\tData 0.000 (0.019)\tLoss 6.5157 (6.8016)\tPrec@1 48.438 (41.810)\tPrec@5 95.312 (92.248)\n",
      "Epoch: [5][96/97], lr: 0.01000\tTime 0.332 (0.359)\tData 0.000 (0.020)\tLoss 6.3180 (6.7683)\tPrec@1 44.068 (42.036)\tPrec@5 94.068 (92.350)\n",
      "Gated Network Weight Gate= Rot:0.00, Flip:0.00, Sc:0.00\n",
      "Test: [0/100]\tTime 0.274 (0.274)\tLoss 14.2330 (14.2330)\tPrec@1 10.000 (10.000)\tPrec@5 42.000 (42.000)\n",
      "Test: [10/100]\tTime 0.073 (0.091)\tLoss 12.1091 (13.3706)\tPrec@1 18.000 (11.000)\tPrec@5 57.000 (50.273)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 12.7656 (13.4379)\tPrec@1 10.000 (10.190)\tPrec@5 53.000 (50.810)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 12.8247 (13.3872)\tPrec@1 9.000 (10.452)\tPrec@5 53.000 (51.290)\n",
      "Test: [40/100]\tTime 0.073 (0.078)\tLoss 13.8572 (13.4202)\tPrec@1 10.000 (10.293)\tPrec@5 46.000 (50.976)\n",
      "Test: [50/100]\tTime 0.073 (0.077)\tLoss 12.9503 (13.3596)\tPrec@1 12.000 (10.333)\tPrec@5 47.000 (51.333)\n",
      "Test: [60/100]\tTime 0.073 (0.076)\tLoss 11.7251 (13.3105)\tPrec@1 16.000 (10.459)\tPrec@5 56.000 (51.246)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 13.1099 (13.2865)\tPrec@1 9.000 (10.606)\tPrec@5 57.000 (51.408)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 12.5777 (13.2417)\tPrec@1 14.000 (10.679)\tPrec@5 56.000 (51.580)\n",
      "Test: [90/100]\tTime 0.073 (0.075)\tLoss 12.8933 (13.2776)\tPrec@1 13.000 (10.495)\tPrec@5 54.000 (51.341)\n",
      "val Results: Prec@1 10.540 Prec@5 51.030 Loss 13.29758\n",
      "val Class Accuracy: [0.980,0.074,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 12.760\n",
      "\n",
      "Epoch: [6][0/97], lr: 0.01000\tTime 0.586 (0.586)\tData 0.281 (0.281)\tLoss 6.2115 (6.2115)\tPrec@1 49.219 (49.219)\tPrec@5 92.188 (92.188)\n",
      "Epoch: [6][10/97], lr: 0.01000\tTime 0.358 (0.394)\tData 0.000 (0.040)\tLoss 6.5323 (6.5268)\tPrec@1 52.344 (44.815)\tPrec@5 89.062 (92.259)\n",
      "Epoch: [6][20/97], lr: 0.01000\tTime 0.343 (0.381)\tData 0.000 (0.029)\tLoss 6.7319 (6.6297)\tPrec@1 40.625 (43.676)\tPrec@5 94.531 (92.188)\n",
      "Epoch: [6][30/97], lr: 0.01000\tTime 0.347 (0.373)\tData 0.000 (0.025)\tLoss 6.9391 (6.6567)\tPrec@1 45.312 (43.271)\tPrec@5 90.625 (92.591)\n",
      "Epoch: [6][40/97], lr: 0.01000\tTime 0.349 (0.372)\tData 0.000 (0.023)\tLoss 6.4614 (6.6535)\tPrec@1 41.406 (43.236)\tPrec@5 92.969 (92.359)\n",
      "Epoch: [6][50/97], lr: 0.01000\tTime 0.355 (0.371)\tData 0.000 (0.022)\tLoss 5.8624 (6.6125)\tPrec@1 50.781 (43.536)\tPrec@5 91.406 (92.341)\n",
      "Epoch: [6][60/97], lr: 0.01000\tTime 0.345 (0.369)\tData 0.000 (0.021)\tLoss 5.8959 (6.6108)\tPrec@1 52.344 (43.583)\tPrec@5 95.312 (92.482)\n",
      "Epoch: [6][70/97], lr: 0.01000\tTime 0.344 (0.369)\tData 0.000 (0.020)\tLoss 6.2815 (6.6190)\tPrec@1 51.562 (43.563)\tPrec@5 92.969 (92.254)\n",
      "Epoch: [6][80/97], lr: 0.01000\tTime 0.342 (0.367)\tData 0.000 (0.020)\tLoss 6.1157 (6.6034)\tPrec@1 48.438 (43.692)\tPrec@5 93.750 (92.265)\n",
      "Epoch: [6][90/97], lr: 0.01000\tTime 0.355 (0.365)\tData 0.000 (0.019)\tLoss 6.1198 (6.6034)\tPrec@1 44.531 (43.724)\tPrec@5 93.750 (92.205)\n",
      "Epoch: [6][96/97], lr: 0.01000\tTime 0.337 (0.364)\tData 0.000 (0.020)\tLoss 6.2140 (6.6011)\tPrec@1 50.847 (43.793)\tPrec@5 90.678 (92.197)\n",
      "Gated Network Weight Gate= Rot:0.00, Flip:0.00, Sc:0.00\n",
      "Test: [0/100]\tTime 0.312 (0.312)\tLoss 12.9042 (12.9042)\tPrec@1 13.000 (13.000)\tPrec@5 63.000 (63.000)\n",
      "Test: [10/100]\tTime 0.074 (0.095)\tLoss 11.1686 (12.2462)\tPrec@1 21.000 (14.727)\tPrec@5 73.000 (60.909)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 11.6751 (12.2840)\tPrec@1 15.000 (14.476)\tPrec@5 58.000 (60.143)\n",
      "Test: [30/100]\tTime 0.073 (0.081)\tLoss 11.8337 (12.2294)\tPrec@1 15.000 (14.613)\tPrec@5 62.000 (60.000)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 12.7598 (12.2630)\tPrec@1 13.000 (14.659)\tPrec@5 53.000 (59.244)\n",
      "Test: [50/100]\tTime 0.073 (0.078)\tLoss 11.8840 (12.2119)\tPrec@1 21.000 (14.804)\tPrec@5 53.000 (59.255)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 10.8232 (12.1782)\tPrec@1 18.000 (14.852)\tPrec@5 62.000 (58.754)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 12.0249 (12.1509)\tPrec@1 16.000 (15.070)\tPrec@5 59.000 (59.014)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 11.4262 (12.1163)\tPrec@1 17.000 (15.198)\tPrec@5 62.000 (59.235)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 11.7377 (12.1403)\tPrec@1 22.000 (15.110)\tPrec@5 59.000 (59.407)\n",
      "val Results: Prec@1 15.040 Prec@5 59.250 Loss 12.15894\n",
      "val Class Accuracy: [0.870,0.445,0.189,0.000,0.000,0.000,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 15.040\n",
      "\n",
      "Epoch: [7][0/97], lr: 0.01000\tTime 0.647 (0.647)\tData 0.310 (0.310)\tLoss 6.6138 (6.6138)\tPrec@1 41.406 (41.406)\tPrec@5 90.625 (90.625)\n",
      "Epoch: [7][10/97], lr: 0.01000\tTime 0.361 (0.407)\tData 0.000 (0.041)\tLoss 6.6050 (6.5104)\tPrec@1 49.219 (44.957)\tPrec@5 92.188 (91.974)\n",
      "Epoch: [7][20/97], lr: 0.01000\tTime 0.357 (0.386)\tData 0.000 (0.029)\tLoss 6.8967 (6.4706)\tPrec@1 38.281 (45.164)\tPrec@5 89.062 (93.043)\n",
      "Epoch: [7][30/97], lr: 0.01000\tTime 0.373 (0.380)\tData 0.000 (0.025)\tLoss 6.5706 (6.4868)\tPrec@1 46.875 (45.010)\tPrec@5 87.500 (92.616)\n",
      "Epoch: [7][40/97], lr: 0.01000\tTime 0.351 (0.375)\tData 0.000 (0.023)\tLoss 6.1074 (6.4945)\tPrec@1 49.219 (45.160)\tPrec@5 95.312 (92.245)\n",
      "Epoch: [7][50/97], lr: 0.01000\tTime 0.370 (0.374)\tData 0.000 (0.022)\tLoss 6.8135 (6.4791)\tPrec@1 44.531 (45.221)\tPrec@5 89.844 (92.540)\n",
      "Epoch: [7][60/97], lr: 0.01000\tTime 0.344 (0.371)\tData 0.001 (0.021)\tLoss 6.1662 (6.4740)\tPrec@1 48.438 (45.300)\tPrec@5 89.844 (92.316)\n",
      "Epoch: [7][70/97], lr: 0.01000\tTime 0.346 (0.370)\tData 0.000 (0.020)\tLoss 5.7896 (6.4768)\tPrec@1 51.562 (45.180)\tPrec@5 96.875 (92.518)\n",
      "Epoch: [7][80/97], lr: 0.01000\tTime 0.350 (0.368)\tData 0.000 (0.020)\tLoss 6.4880 (6.4921)\tPrec@1 48.438 (45.120)\tPrec@5 89.062 (92.313)\n",
      "Epoch: [7][90/97], lr: 0.01000\tTime 0.361 (0.366)\tData 0.000 (0.020)\tLoss 6.6332 (6.5139)\tPrec@1 43.750 (44.926)\tPrec@5 91.406 (92.084)\n",
      "Epoch: [7][96/97], lr: 0.01000\tTime 0.335 (0.365)\tData 0.000 (0.020)\tLoss 6.5799 (6.5136)\tPrec@1 38.136 (44.946)\tPrec@5 94.068 (92.141)\n",
      "Gated Network Weight Gate= Rot:0.00, Flip:0.00, Sc:0.00\n",
      "Test: [0/100]\tTime 0.292 (0.292)\tLoss 13.5566 (13.5566)\tPrec@1 11.000 (11.000)\tPrec@5 47.000 (47.000)\n",
      "Test: [10/100]\tTime 0.073 (0.093)\tLoss 11.5477 (12.8003)\tPrec@1 18.000 (13.364)\tPrec@5 61.000 (53.545)\n",
      "Test: [20/100]\tTime 0.073 (0.083)\tLoss 12.1016 (12.8729)\tPrec@1 13.000 (12.238)\tPrec@5 59.000 (53.857)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 12.4322 (12.8098)\tPrec@1 11.000 (12.806)\tPrec@5 58.000 (54.581)\n",
      "Test: [40/100]\tTime 0.074 (0.078)\tLoss 13.2427 (12.8441)\tPrec@1 10.000 (12.610)\tPrec@5 51.000 (54.561)\n",
      "Test: [50/100]\tTime 0.074 (0.077)\tLoss 12.3969 (12.7815)\tPrec@1 12.000 (12.529)\tPrec@5 49.000 (54.784)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 10.9765 (12.7152)\tPrec@1 20.000 (12.590)\tPrec@5 58.000 (54.541)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 12.7232 (12.6891)\tPrec@1 9.000 (12.859)\tPrec@5 58.000 (54.718)\n",
      "Test: [80/100]\tTime 0.073 (0.076)\tLoss 12.0600 (12.6537)\tPrec@1 17.000 (12.963)\tPrec@5 57.000 (54.975)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 12.2883 (12.6806)\tPrec@1 19.000 (12.758)\tPrec@5 57.000 (54.725)\n",
      "val Results: Prec@1 12.780 Prec@5 54.500 Loss 12.70189\n",
      "val Class Accuracy: [0.957,0.089,0.232,0.000,0.000,0.000,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 15.040\n",
      "\n",
      "Epoch: [8][0/97], lr: 0.01000\tTime 0.595 (0.595)\tData 0.312 (0.312)\tLoss 6.3644 (6.3644)\tPrec@1 45.312 (45.312)\tPrec@5 92.969 (92.969)\n",
      "Epoch: [8][10/97], lr: 0.01000\tTime 0.365 (0.402)\tData 0.000 (0.043)\tLoss 6.3272 (6.3392)\tPrec@1 43.750 (45.668)\tPrec@5 92.969 (92.827)\n",
      "Epoch: [8][20/97], lr: 0.01000\tTime 0.376 (0.383)\tData 0.000 (0.030)\tLoss 6.1413 (6.4606)\tPrec@1 49.219 (45.312)\tPrec@5 88.281 (91.629)\n",
      "Epoch: [8][30/97], lr: 0.01000\tTime 0.350 (0.375)\tData 0.000 (0.026)\tLoss 6.7733 (6.4536)\tPrec@1 41.406 (45.060)\tPrec@5 93.750 (91.986)\n",
      "Epoch: [8][40/97], lr: 0.01000\tTime 0.369 (0.374)\tData 0.000 (0.023)\tLoss 6.4729 (6.4393)\tPrec@1 43.750 (45.179)\tPrec@5 93.750 (92.340)\n",
      "Epoch: [8][50/97], lr: 0.01000\tTime 0.363 (0.372)\tData 0.001 (0.022)\tLoss 5.7095 (6.4023)\tPrec@1 52.344 (45.604)\tPrec@5 95.312 (92.325)\n",
      "Epoch: [8][60/97], lr: 0.01000\tTime 0.349 (0.371)\tData 0.000 (0.021)\tLoss 5.9170 (6.4199)\tPrec@1 48.438 (45.492)\tPrec@5 94.531 (92.405)\n",
      "Epoch: [8][70/97], lr: 0.01000\tTime 0.344 (0.369)\tData 0.000 (0.021)\tLoss 6.8536 (6.4207)\tPrec@1 39.844 (45.346)\tPrec@5 87.500 (92.309)\n",
      "Epoch: [8][80/97], lr: 0.01000\tTime 0.344 (0.367)\tData 0.000 (0.020)\tLoss 6.7655 (6.4315)\tPrec@1 43.750 (45.052)\tPrec@5 89.844 (92.052)\n",
      "Epoch: [8][90/97], lr: 0.01000\tTime 0.351 (0.365)\tData 0.000 (0.020)\tLoss 7.1444 (6.4421)\tPrec@1 37.500 (45.158)\tPrec@5 89.844 (92.059)\n",
      "Epoch: [8][96/97], lr: 0.01000\tTime 0.345 (0.364)\tData 0.000 (0.020)\tLoss 6.3469 (6.4545)\tPrec@1 50.847 (45.123)\tPrec@5 86.441 (91.931)\n",
      "Gated Network Weight Gate= Rot:0.00, Flip:0.00, Sc:0.00\n",
      "Test: [0/100]\tTime 0.306 (0.306)\tLoss 12.8939 (12.8939)\tPrec@1 12.000 (12.000)\tPrec@5 61.000 (61.000)\n",
      "Test: [10/100]\tTime 0.073 (0.094)\tLoss 11.1124 (12.2246)\tPrec@1 19.000 (17.182)\tPrec@5 67.000 (62.091)\n",
      "Test: [20/100]\tTime 0.073 (0.084)\tLoss 11.5508 (12.2548)\tPrec@1 18.000 (16.333)\tPrec@5 61.000 (61.619)\n",
      "Test: [30/100]\tTime 0.073 (0.080)\tLoss 11.7990 (12.1983)\tPrec@1 21.000 (16.774)\tPrec@5 66.000 (62.194)\n",
      "Test: [40/100]\tTime 0.073 (0.079)\tLoss 12.6880 (12.2335)\tPrec@1 15.000 (16.951)\tPrec@5 58.000 (61.780)\n",
      "Test: [50/100]\tTime 0.075 (0.078)\tLoss 11.8511 (12.1775)\tPrec@1 24.000 (17.078)\tPrec@5 59.000 (61.922)\n",
      "Test: [60/100]\tTime 0.073 (0.077)\tLoss 10.7391 (12.1343)\tPrec@1 23.000 (17.033)\tPrec@5 64.000 (61.639)\n",
      "Test: [70/100]\tTime 0.073 (0.076)\tLoss 11.9777 (12.1085)\tPrec@1 15.000 (17.254)\tPrec@5 69.000 (61.972)\n",
      "Test: [80/100]\tTime 0.074 (0.076)\tLoss 11.4086 (12.0745)\tPrec@1 19.000 (17.420)\tPrec@5 65.000 (61.926)\n",
      "Test: [90/100]\tTime 0.073 (0.076)\tLoss 11.7926 (12.1010)\tPrec@1 27.000 (17.187)\tPrec@5 64.000 (61.736)\n",
      "val Results: Prec@1 17.130 Prec@5 61.570 Loss 12.12106\n",
      "val Class Accuracy: [0.837,0.439,0.437,0.000,0.000,0.000,0.000,0.000,0.000,0.000]\n",
      "Best Prec@1: 17.130\n",
      "\n",
      "Epoch: [9][0/97], lr: 0.01000\tTime 0.830 (0.830)\tData 0.433 (0.433)\tLoss 6.3918 (6.3918)\tPrec@1 50.000 (50.000)\tPrec@5 96.094 (96.094)\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"cifar_train_lorot-E.py\", line 672, in <module>\n",
      "    main()\n",
      "  File \"cifar_train_lorot-E.py\", line 181, in main\n",
      "    main_worker(args.gpu, ngpus_per_node, args)\n",
      "  File \"cifar_train_lorot-E.py\", line 361, in main_worker\n",
      "    train(\n",
      "  File \"cifar_train_lorot-E.py\", line 452, in train\n",
      "    fliplabel = fliplabel.cuda()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python cifar_train_lorot-E.py --gpu 0 --imb_type exp --exp_str moesigmoidldam --imb_factor 0.01 --loss_type LDAM --train_rule DRW -g -m \"rot fliplr sc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import models\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': 'models',\n",
       " '__doc__': None,\n",
       " '__package__': 'models',\n",
       " '__loader__': <_frozen_importlib_external.SourceFileLoader at 0x7fa083b4fbb0>,\n",
       " '__spec__': ModuleSpec(name='models', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7fa083b4fbb0>, origin='/media/aristo/Data A/documents/kuliah/Project/Localizable-Rotation/Imbalanced/models/__init__.py', submodule_search_locations=['/media/aristo/Data A/documents/kuliah/Project/Localizable-Rotation/Imbalanced/models']),\n",
       " '__path__': ['/media/aristo/Data A/documents/kuliah/Project/Localizable-Rotation/Imbalanced/models'],\n",
       " '__file__': '/media/aristo/Data A/documents/kuliah/Project/Localizable-Rotation/Imbalanced/models/__init__.py',\n",
       " '__cached__': '/media/aristo/Data A/documents/kuliah/Project/Localizable-Rotation/Imbalanced/models/__pycache__/__init__.cpython-38.pyc',\n",
       " '__builtins__': {'__name__': 'builtins',\n",
       "  '__doc__': \"Built-in functions, exceptions, and other objects.\\n\\nNoteworthy: None is the `nil' object; Ellipsis represents `...' in slices.\",\n",
       "  '__package__': '',\n",
       "  '__loader__': _frozen_importlib.BuiltinImporter,\n",
       "  '__spec__': ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>),\n",
       "  '__build_class__': <function __build_class__>,\n",
       "  '__import__': <function __import__>,\n",
       "  'abs': <function abs(x, /)>,\n",
       "  'all': <function all(iterable, /)>,\n",
       "  'any': <function any(iterable, /)>,\n",
       "  'ascii': <function ascii(obj, /)>,\n",
       "  'bin': <function bin(number, /)>,\n",
       "  'breakpoint': <function breakpoint>,\n",
       "  'callable': <function callable(obj, /)>,\n",
       "  'chr': <function chr(i, /)>,\n",
       "  'compile': <function compile(source, filename, mode, flags=0, dont_inherit=False, optimize=-1, *, _feature_version=-1)>,\n",
       "  'delattr': <function delattr(obj, name, /)>,\n",
       "  'dir': <function dir>,\n",
       "  'divmod': <function divmod(x, y, /)>,\n",
       "  'eval': <function eval(source, globals=None, locals=None, /)>,\n",
       "  'exec': <function exec(source, globals=None, locals=None, /)>,\n",
       "  'format': <function format(value, format_spec='', /)>,\n",
       "  'getattr': <function getattr>,\n",
       "  'globals': <function globals()>,\n",
       "  'hasattr': <function hasattr(obj, name, /)>,\n",
       "  'hash': <function hash(obj, /)>,\n",
       "  'hex': <function hex(number, /)>,\n",
       "  'id': <function id(obj, /)>,\n",
       "  'input': <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7fa09c1fd400>>,\n",
       "  'isinstance': <function isinstance(obj, class_or_tuple, /)>,\n",
       "  'issubclass': <function issubclass(cls, class_or_tuple, /)>,\n",
       "  'iter': <function iter>,\n",
       "  'len': <function len(obj, /)>,\n",
       "  'locals': <function locals()>,\n",
       "  'max': <function max>,\n",
       "  'min': <function min>,\n",
       "  'next': <function next>,\n",
       "  'oct': <function oct(number, /)>,\n",
       "  'ord': <function ord(c, /)>,\n",
       "  'pow': <function pow(base, exp, mod=None)>,\n",
       "  'print': <function print>,\n",
       "  'repr': <function repr(obj, /)>,\n",
       "  'round': <function round(number, ndigits=None)>,\n",
       "  'setattr': <function setattr(obj, name, value, /)>,\n",
       "  'sorted': <function sorted(iterable, /, *, key=None, reverse=False)>,\n",
       "  'sum': <function sum(iterable, /, start=0)>,\n",
       "  'vars': <function vars>,\n",
       "  'None': None,\n",
       "  'Ellipsis': Ellipsis,\n",
       "  'NotImplemented': NotImplemented,\n",
       "  'False': False,\n",
       "  'True': True,\n",
       "  'bool': bool,\n",
       "  'memoryview': memoryview,\n",
       "  'bytearray': bytearray,\n",
       "  'bytes': bytes,\n",
       "  'classmethod': classmethod,\n",
       "  'complex': complex,\n",
       "  'dict': dict,\n",
       "  'enumerate': enumerate,\n",
       "  'filter': filter,\n",
       "  'float': float,\n",
       "  'frozenset': frozenset,\n",
       "  'property': property,\n",
       "  'int': int,\n",
       "  'list': list,\n",
       "  'map': map,\n",
       "  'object': object,\n",
       "  'range': range,\n",
       "  'reversed': reversed,\n",
       "  'set': set,\n",
       "  'slice': slice,\n",
       "  'staticmethod': staticmethod,\n",
       "  'str': str,\n",
       "  'super': super,\n",
       "  'tuple': tuple,\n",
       "  'type': type,\n",
       "  'zip': zip,\n",
       "  '__debug__': True,\n",
       "  'BaseException': BaseException,\n",
       "  'Exception': Exception,\n",
       "  'TypeError': TypeError,\n",
       "  'StopAsyncIteration': StopAsyncIteration,\n",
       "  'StopIteration': StopIteration,\n",
       "  'GeneratorExit': GeneratorExit,\n",
       "  'SystemExit': SystemExit,\n",
       "  'KeyboardInterrupt': KeyboardInterrupt,\n",
       "  'ImportError': ImportError,\n",
       "  'ModuleNotFoundError': ModuleNotFoundError,\n",
       "  'OSError': OSError,\n",
       "  'EnvironmentError': OSError,\n",
       "  'IOError': OSError,\n",
       "  'EOFError': EOFError,\n",
       "  'RuntimeError': RuntimeError,\n",
       "  'RecursionError': RecursionError,\n",
       "  'NotImplementedError': NotImplementedError,\n",
       "  'NameError': NameError,\n",
       "  'UnboundLocalError': UnboundLocalError,\n",
       "  'AttributeError': AttributeError,\n",
       "  'SyntaxError': SyntaxError,\n",
       "  'IndentationError': IndentationError,\n",
       "  'TabError': TabError,\n",
       "  'LookupError': LookupError,\n",
       "  'IndexError': IndexError,\n",
       "  'KeyError': KeyError,\n",
       "  'ValueError': ValueError,\n",
       "  'UnicodeError': UnicodeError,\n",
       "  'UnicodeEncodeError': UnicodeEncodeError,\n",
       "  'UnicodeDecodeError': UnicodeDecodeError,\n",
       "  'UnicodeTranslateError': UnicodeTranslateError,\n",
       "  'AssertionError': AssertionError,\n",
       "  'ArithmeticError': ArithmeticError,\n",
       "  'FloatingPointError': FloatingPointError,\n",
       "  'OverflowError': OverflowError,\n",
       "  'ZeroDivisionError': ZeroDivisionError,\n",
       "  'SystemError': SystemError,\n",
       "  'ReferenceError': ReferenceError,\n",
       "  'MemoryError': MemoryError,\n",
       "  'BufferError': BufferError,\n",
       "  'Warning': Warning,\n",
       "  'UserWarning': UserWarning,\n",
       "  'DeprecationWarning': DeprecationWarning,\n",
       "  'PendingDeprecationWarning': PendingDeprecationWarning,\n",
       "  'SyntaxWarning': SyntaxWarning,\n",
       "  'RuntimeWarning': RuntimeWarning,\n",
       "  'FutureWarning': FutureWarning,\n",
       "  'ImportWarning': ImportWarning,\n",
       "  'UnicodeWarning': UnicodeWarning,\n",
       "  'BytesWarning': BytesWarning,\n",
       "  'ResourceWarning': ResourceWarning,\n",
       "  'ConnectionError': ConnectionError,\n",
       "  'BlockingIOError': BlockingIOError,\n",
       "  'BrokenPipeError': BrokenPipeError,\n",
       "  'ChildProcessError': ChildProcessError,\n",
       "  'ConnectionAbortedError': ConnectionAbortedError,\n",
       "  'ConnectionRefusedError': ConnectionRefusedError,\n",
       "  'ConnectionResetError': ConnectionResetError,\n",
       "  'FileExistsError': FileExistsError,\n",
       "  'FileNotFoundError': FileNotFoundError,\n",
       "  'IsADirectoryError': IsADirectoryError,\n",
       "  'NotADirectoryError': NotADirectoryError,\n",
       "  'InterruptedError': InterruptedError,\n",
       "  'PermissionError': PermissionError,\n",
       "  'ProcessLookupError': ProcessLookupError,\n",
       "  'TimeoutError': TimeoutError,\n",
       "  'open': <function io.open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)>,\n",
       "  'copyright': Copyright (c) 2001-2022 Python Software Foundation.\n",
       "  All Rights Reserved.\n",
       "  \n",
       "  Copyright (c) 2000 BeOpen.com.\n",
       "  All Rights Reserved.\n",
       "  \n",
       "  Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n",
       "  All Rights Reserved.\n",
       "  \n",
       "  Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n",
       "  All Rights Reserved.,\n",
       "  'credits':     Thanks to CWI, CNRI, BeOpen.com, Zope Corporation and a cast of thousands\n",
       "      for supporting Python development.  See www.python.org for more information.,\n",
       "  'license': Type license() to see the full license text,\n",
       "  'help': Type help() for interactive help, or help(object) for help about object.,\n",
       "  '__IPYTHON__': True,\n",
       "  'display': <function IPython.core.display.display(*objs, include=None, exclude=None, metadata=None, transient=None, display_id=None, **kwargs)>,\n",
       "  '__pybind11_internals_v4_gcc_libstdcpp_cxxabi1011__': <capsule object NULL at 0x7fa083451750>,\n",
       "  'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7fa09c251a60>>},\n",
       " 'resnet_cifar': <module 'models.resnet_cifar' from '/media/aristo/Data A/documents/kuliah/Project/Localizable-Rotation/Imbalanced/models/resnet_cifar.py'>,\n",
       " 'ResNet_s': models.resnet_cifar.ResNet_s,\n",
       " 'resnet20': <function models.resnet_cifar.resnet20()>,\n",
       " 'resnet32': <function models.resnet_cifar.resnet32(num_classes=10, use_norm=False, num_trans=16)>,\n",
       " 'resnet44': <function models.resnet_cifar.resnet44()>,\n",
       " 'resnet56': <function models.resnet_cifar.resnet56()>,\n",
       " 'resnet110': <function models.resnet_cifar.resnet110()>,\n",
       " 'resnet1202': <function models.resnet_cifar.resnet1202()>,\n",
       " 'resnet_cifar_bt': <module 'models.resnet_cifar_bt' from '/media/aristo/Data A/documents/kuliah/Project/Localizable-Rotation/Imbalanced/models/resnet_cifar_bt.py'>,\n",
       " 'BarlowTwins': models.resnet_cifar_bt.BarlowTwins}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.BarlowTwins(lorot= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BarlowTwins(\n",
      "  (backbone): ResNet(\n",
      "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): LambdaLayer()\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): LambdaLayer()\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (fc): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (fc_lorot): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (projector): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=100, bias=False)\n",
      "    (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=100, out_features=100, bias=False)\n",
      "  )\n",
      "  (bn): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch-test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de4dfb183386e61860a9b4c6b4eb26dd8bb24671ff46a37707bad9f54278c866"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
