{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import models\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from imbalance_cifar import IMBALANCECIFAR10, IMBALANCECIFAR100\n",
    "from losses import FocalLoss, LDAMLoss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorboardX import SummaryWriter\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'cifar10'\n",
    "arch = 'resnet32'\n",
    "loss_type = 'CE'\n",
    "train_rule = 'None'\n",
    "imb_type = 'exp'\n",
    "imb_factor = 0.01\n",
    "rand_number = 0\n",
    "exp_str = 'exp01'\n",
    "workers = 8\n",
    "epoch = 1\n",
    "batch_size = 128\n",
    "lr = 0.1\n",
    "momentum = 1\n",
    "weight_decay = 2e-4\n",
    "print_freq = 10\n",
    "resume = ''\n",
    "root_log = 'log'\n",
    "root_model = 'checkpoint'\n",
    "rot_ratio = 0.1\n",
    "\n",
    "best_acc1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_name = \"_\".join(\n",
    "        [\n",
    "            dataset,\n",
    "            arch,\n",
    "            loss_type,\n",
    "            train_rule,\n",
    "            imb_type,\n",
    "            str(imb_factor),\n",
    "            exp_str,\n",
    "        ]\n",
    "    )\n",
    "root_log = root_log + \"/\" + str(int(rot_ratio * 100))\n",
    "folders = {\n",
    "    'root_log': root_log,\n",
    "    'root_model': root_model,\n",
    "    'store_name': store_name\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating folder checkpoint\n",
      "creating folder log/10/cifar10_resnet32_CE_None_exp_0.01_exp01\n",
      "creating folder checkpoint/cifar10_resnet32_CE_None_exp_0.01_exp01\n"
     ]
    }
   ],
   "source": [
    "prepare_folders(folders, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_trans : 16\n"
     ]
    }
   ],
   "source": [
    "num_classes = 100 if dataset == \"cifar100\" else 10\n",
    "use_norm = True if loss_type == \"LDAM\" else False\n",
    "model = models.__dict__[arch](num_classes=num_classes, use_norm=use_norm)\n",
    "model = torch.nn.DataParallel(model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr,\n",
    "        momentum=momentum,\n",
    "        weight_decay=weight_decay,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_val = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if dataset == \"cifar10\":\n",
    "    train_dataset = IMBALANCECIFAR10(\n",
    "        root=\"./data\",\n",
    "        imb_type=imb_type,\n",
    "        imb_factor=imb_factor,\n",
    "        rand_number=rand_number,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform_train,\n",
    "    )\n",
    "    val_dataset = datasets.CIFAR10(\n",
    "        root=\"./data\", train=False, download=True, transform=transform_val\n",
    "    )\n",
    "elif dataset == \"cifar100\":\n",
    "    train_dataset = IMBALANCECIFAR100(\n",
    "        root=\"./data\",\n",
    "        imb_type=imb_type,\n",
    "        imb_factor=imb_factor,\n",
    "        rand_number=rand_number,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform_train,\n",
    "    )\n",
    "    val_dataset = datasets.CIFAR100(\n",
    "        root=\"./data\", train=False, download=True, transform=transform_val\n",
    "    )\n",
    "else:\n",
    "        warnings.warn(\"Dataset is not listed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls num list:\n",
      "[5000, 2997, 1796, 1077, 645, 387, 232, 139, 83, 50]\n"
     ]
    }
   ],
   "source": [
    "cls_num_list = train_dataset.get_cls_num_list()\n",
    "print(\"cls num list:\")\n",
    "print(cls_num_list)\n",
    "cls_num_list = cls_num_list\n",
    "\n",
    "train_sampler = None\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=(train_sampler is None),\n",
    "    num_workers=workers,\n",
    "    pin_memory=True,\n",
    "    sampler=train_sampler,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=False,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_training = open(\n",
    "    os.path.join(root_log, store_name, \"log_train.csv\"), \"w\"\n",
    ")\n",
    "log_testing = open(\n",
    "    os.path.join(root_log, args.store_name, \"log_test.csv\"), \"w\"\n",
    ")\n",
    "with open(os.path.join(args.root_log, args.store_name, \"args.txt\"), \"w\") as f:\n",
    "    f.write(str(args))\n",
    "tf_writer = SummaryWriter(log_dir=os.path.join(args.root_log, args.store_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch-test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de4dfb183386e61860a9b4c6b4eb26dd8bb24671ff46a37707bad9f54278c866"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
